[
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645542424610,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_time":1440414980200,
        "Owner_last_access_time":1645630582560,
        "Owner_location":"Germany",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1645548706457,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645549016653,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71779667,
        "Question_title":"locked out of wandb local server - change user password",
        "Question_body":"<p>I am using a local weight and biases (wandb) instance running on a server with no internet connection.\nI have a user there and having no problems logging results from the server.<\/p>\n<p>However, when trying to see them in the UI it asked me to login again but unfortunately I forgot my password and reset password doesn't work with the message of <code>Error while trying to reset password<\/code>.<\/p>\n<p>I tried searching all over the documentation and found nothing to help with that.<\/p>\n<p>Any help for locally recovering my account will be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649323903133,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":304,
        "Owner_creation_time":1608753761683,
        "Owner_last_access_time":1663850083317,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1657484089700,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71779667",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67291062,
        "Question_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Question_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619559781187,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|wandb",
        "Question_view_count":385,
        "Owner_creation_time":1490778676137,
        "Owner_last_access_time":1664077488930,
        "Owner_location":null,
        "Owner_reputation":322,
        "Owner_up_votes":239,
        "Owner_down_votes":1,
        "Owner_views":109,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620162973717,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69142852,
        "Question_title":"Weights and Biases watch log causing CUDA out of memory",
        "Question_body":"<p>I am trying to use WandB gradient visualization to debug the gradient flow in my neural net on Google Colab. Without WandB logging, the training runs without error, taking up 11Gb\/16GB on the p100 gpu. However, adding this line <code>wandb.watch(model, log='all', log_freq=3)<\/code> causes a cuda out of memory error.<\/p>\n<p><strong>How does WandB logging create extra GPU memory overhead?<\/strong><\/p>\n<p><strong>Is there some way to reduce the overhead?<\/strong><\/p>\n<p>--adding training loop code--<\/p>\n<pre><code>learning_rate = 0.001\nnum_epochs = 50\n\ndevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\nmodel = MyModel()\n\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nwandb.watch(model, log='all', log_freq=3)\n\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_accuracy = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (name, output_array, input) in enumerate(trainloader):\n        \n        output_array = output_array.to(device)\n        input = input.to(device)\n        comb = torch.zeros(1,1,100,1632).to(device)\n\n        ## forward + backprop + loss\n        output = model(input, comb)\n\n        loss = my_loss(output, output_array)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n\n        temp = get_accuracy(output, output_array)\n\n        print('check 13')\n        !nvidia-smi | grep MiB | awk '{print $9 $10 $11}'\n\n        train_accuracy += temp     \n<\/code><\/pre>\n<p>-----edit-----<\/p>\n<p>I think WandB is creating an extra copy of the gradient during logging preprocessing. Here is the traceback:<\/p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-13de83557b55&gt; in &lt;module&gt;()\n     60         get_ipython().system(&quot;nvidia-smi | grep MiB | awk '{print $9 $10 $11}'&quot;)\n     61 \n---&gt; 62         loss.backward()\n     63 \n     64         print('check 10')\n\n4 frames\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    253                 create_graph=create_graph,\n    254                 inputs=inputs)\n--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    256 \n    257     def register_hook(self, hook):\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    147     Variable._execution_engine.run_backward(\n    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    150 \n    151 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in &lt;lambda&gt;(grad)\n    283             self.log_tensor_stats(grad.data, name)\n    284 \n--&gt; 285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n    286         self._hook_handles[name] = handle\n    287         return handle\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in _callback(grad, log_track)\n    281             if not log_track_update(log_track):\n    282                 return\n--&gt; 283             self.log_tensor_stats(grad.data, name)\n    284 \n    285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    219         # Remove nans from tensor. There's no good way to represent that in histograms.\n    220         flat = flat[~torch.isnan(flat)]\n--&gt; 221         flat = flat[~torch.isinf(flat)]\n    222         if flat.shape == torch.Size([0]):\n    223             # Often the whole tensor is nan or inf. Just don't log it in that case.\n\nRuntimeError: CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 15.90 GiB total capacity; 10.10 GiB already allocated; 717.75 MiB free; 14.27 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>---update----<\/p>\n<p>Indeed, commenting out the offending line <code>flat = flat[~torch.isinf(flat)]<\/code><\/p>\n<p>gets the logging step to just barely fit into the GPU memory.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1631361970417,
        "Question_score":2,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":405,
        "Owner_creation_time":1545588089587,
        "Owner_last_access_time":1657580031330,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1631485669880,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69142852",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73062370,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1658389162900,
        "Question_score":0,
        "Question_tags":"machine-learning|nlp|named-entity-recognition|wandb",
        "Question_view_count":58,
        "Owner_creation_time":1643710211767,
        "Owner_last_access_time":1663833597150,
        "Owner_location":null,
        "Owner_reputation":78,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1658393400157,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72590067,
        "Question_title":"jupyterLab Wandb does not iterative",
        "Question_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655016960480,
        "Question_score":0,
        "Question_tags":"jupyter-lab|wandb",
        "Question_view_count":40,
        "Owner_creation_time":1609152494583,
        "Owner_last_access_time":1663846781627,
        "Owner_location":"South Korea",
        "Owner_reputation":72,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655111602567,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71075704,
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_score":0,
        "Question_tags":"authentication|google-colaboratory|wandb",
        "Question_view_count":316,
        "Owner_creation_time":1644556763937,
        "Owner_last_access_time":1649221420000,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1644559750243,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64413753,
        "Question_title":"wandb - how to get it really silent (weights and biases)",
        "Question_body":"<p>Working with Anaconda-Spyder (python 3.7), I installed the latest release of wandb (0.10.7) and try to use it with tensorflow (2.1.0) and keras (2.3.1). Since then, my console is polluted with lengthy comments due to wandb. So far I am using config and logs (not yet sweeps). It worked well for several runs BUT I cannot handle the outcome of my code that disappear in a flow of messages.<\/p>\n<p>I'd like to get rid of these messages (or find an alternative to wandb...)\nThanks in advance for your help ;-)\nHere is code to import the necessary libraries :<\/p>\n<pre><code>import os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Dropout, Dense, LSTM, Flatten, Activation from tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras import models from tensorflow.keras.models\nimport Sequential from tensorflow.keras.optimizers import Adam import\nwandb os.environ[&quot;WANDB_SILENT&quot;] = &quot;true&quot;\nwandb.init(project=&quot;project_tsa&quot;)\n<\/code><\/pre>\n<p>Later on, I define the wandb config as follows:<\/p>\n<pre><code>wandb.init(config={&quot;project_name&quot;:&quot;project_tsa&quot;,\n                   &quot;architecture&quot;: &quot;ResNet&quot;,\n                   &quot;load_weights&quot;: load_weights,\n                   &quot;epochs&quot;: epochs,\n                   &quot;batch_size&quot;: batch_size,\n                   &quot;iterations&quot;: iterat,\n                   &quot;dropout&quot;: dropout,\n                   &quot;learning_rate&quot;: learning,\n                   &quot;features&quot;: n_feature_maps,\n                   &quot;sequence&quot;: seq_length})\n<\/code><\/pre>\n<p>Eventually, I define several logs :<\/p>\n<pre><code>wandb.log({&quot;precis_pos&quot;: precis})\nwandb.log({&quot;recall_pos&quot;: recall})\nwandb.log({&quot;sortino_pos&quot;: sharpe_all[4]})\nwandb.log({&quot;sortinogain_pos&quot;: (sharpe_all[4]-sharpe_all[3])})\n<\/code><\/pre>\n<p>As soon as wandb.init is present, I automatically get ten lines of warnings :<\/p>\n<pre><code>wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n<\/code><\/pre>\n<p>As soon as I define config (and even worse with the logs), the code ends with more than a hundred lines of warning.... Here are just a few as example :<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;D:\\Anaconda3\\lib\\threading.py&quot;, line 926, in _bootstrap_inner\n    self.run()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 40, in run\n    success = self.push()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 88, in push\n    _, upload_headers, result = self._api.upload_urls(project, [self.save_name])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 62, in wrapper\n    six.reraise(CommError, CommError(message, err), sys.exc_info()[2])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\six.py&quot;, line 702, in reraise\n    raise value.with_traceback(tb)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 24, in wrapper\n    return func(*args, **kwargs)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\internal\\internal_api.py&quot;, line 1039, in upload_urls\n    run = query_result[&quot;model&quot;][&quot;bucket&quot;]\nwandb.errors.error.CommError: 'NoneType' object is not subscriptable\n\n[SpyderKernelApp] WARNING | No such comm: 4562c8582bde4c50aedbd77151a94274\n[SpyderKernelApp] WARNING | No such comm: 197a5ff2d95849a0bd7d021f29e5f90e\n[SpyderKernelApp] WARNING | No such comm: c4ef2bdbcfb340e48c52099c8ac96dc1\n[SpyderKernelApp] WARNING | No such comm: 8898f986226043bfed836c08517299cb\n[SpyderKernelApp] WARNING | No such comm: 7fbb1deccf8c40629d95c57c6cbd2e6b\n[SpyderKernelApp] WARNING | No such comm: 1531c7eb303c4957e97426051d48441b\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603026773210,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":2929,
        "Owner_creation_time":1592919921400,
        "Owner_last_access_time":1663421488220,
        "Owner_location":"Brussels, Belgium",
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1661850129077,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64413753",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71502443,
        "Question_title":"Wandb login permission denied python virtual environment",
        "Question_body":"<p>I am using a remote Slurm cluster from my university and want to get access to my wandb profile. I run my py project in a virtual env with <code>python 3.7.4<\/code>, where I could install successfully <code>wandb<\/code>.<\/p>\n<p>However, when I try to login from command line <code>python -m wandb login<\/code> I get this error<\/p>\n<pre><code>(374_env1) [userX@peregrine ~]$ python -m wandb login\nTraceback (most recent call last):\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 193, in _run_module_as_main\n&quot;__main__&quot;, mod_spec)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 85, in _run_code\nexec(code, run_globals)\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/__main__.py&quot;, line 1, in &lt;module&gt;\nfrom wandb.cli import cli\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 53, in &lt;module&gt;\ndatefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1895, in basicConfig\nh = FileHandler(filename, mode)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1087, in __init__\nStreamHandler.__init__(self, self._open())\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1116, in _open\nreturn open(self.baseFilename, self.mode, encoding=self.encoding)\nPermissionError: [Errno 13] Permission denied: '\/local\/tmp\/debug-cli.log'\n<\/code><\/pre>\n<p>I tried also to login outside of the venv but still permission denied. Is it because as a user from the cluster I don't have enough <code>-m<\/code> permissions? I also tried <code>python -u wandb login<\/code> but still permission denied. ps. I don't have <code>sudo<\/code> permission.\nAny insights?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1647454802023,
        "Question_score":0,
        "Question_tags":"python-3.x|virtualenv|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1366555247053,
        "Owner_last_access_time":1661334085140,
        "Owner_location":null,
        "Owner_reputation":642,
        "Owner_up_votes":126,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Question_last_edit_time":1647455744270,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71502443",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70799787,
        "Question_title":"Can I use Wandb in Tracking my YOLOv4 Training?",
        "Question_body":"<p>I want to track my training in YOLOv4 using wandb but I can't see any tutorial on how to do it. I saw a youtube video but it was a training in YOLOv5.<\/p>\n<p>My wandb account is now logged in in the YOLOv4  training but I cant see no chart in the wandb page. It only displays this page <a href=\"https:\/\/i.stack.imgur.com\/AaAjB.png\" rel=\"nofollow noreferrer\">Wandb YOLOv4<\/a><\/p>\n<p>I want to know how exactly I can use wandb in my YOLOv4 training.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1642760709873,
        "Question_score":0,
        "Question_tags":"yolov4|wandb",
        "Question_view_count":122,
        "Owner_creation_time":1641512289450,
        "Owner_last_access_time":1655351990127,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1642761049737,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70799787",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916901,
        "Question_title":"WANDB Getting a run id based on tag",
        "Question_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650314726207,
        "Question_score":0,
        "Question_tags":"python|metadata|wandb",
        "Question_view_count":645,
        "Owner_creation_time":1444675970627,
        "Owner_last_access_time":1664044672477,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650458488357,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641753400317,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_time":1445719444550,
        "Owner_last_access_time":1664061059603,
        "Owner_location":"Iran, Canada",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641809697277,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70615413,
        "Question_title":"how to handle wandb Invalid filename characters exception on uploading image",
        "Question_body":"<p>I am using windows 10 &amp; venv &amp; python 3.9.7\nthis is my code to upload image to wandb<\/p>\n<pre><code>wandb_log[&quot;Image\/train_image&quot;] = wandb.Image('tmp.jpg')\nwandb.log(wandb_log, step)\n<\/code><\/pre>\n<p>the full directory of image is \u201cC:\\Users\\\uc774\uc900\ud601\\Documents\\Github\\terenz\\tmp.jpg\u201d\nHowever it creates this error<\/p>\n<pre><code>Media Image\/train_image is invalid. Please remove invalid filename characters\n<\/code><\/pre>\n<p>reinstalling wandb did not help to solve this problem.<br \/>\nAny suggestions? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1641517271160,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":94,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70615413",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916491,
        "Question_title":"WANDB run initialization",
        "Question_body":"<p>I wanted to try using wandb to log runs of my ML experiments for a project; but I am not able to initialize the run itself.\nI tried:<\/p>\n<p><code>run = wandb.init(project=&quot;name&quot;,entity=&quot;username&quot;,name=&quot;classification&quot;)<\/code><\/p>\n<p>This results in:\nwandb: W&amp;B API key is configured (use <code>wandb login --relogin<\/code> to force relogin)<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>What can I do to fix this? (I did login through the terminal before launching this cell idk what else to try)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1650311929900,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1650311491997,
        "Owner_last_access_time":1654197360160,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1650312801033,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916491",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66541985,
        "Question_title":"'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file",
        "Question_body":"<p>I am trying to run the custom <a href=\"https:\/\/github.com\/ultralytics\/yolov5\" rel=\"nofollow noreferrer\">yolo model<\/a> on my data set in my local machine. I am following some reference code from the kaggle platform. Here first time I encounter the <code>wandb<\/code> frame work. while doing so I use the following to run the <code>train.py <\/code> file in my jupyter lab.<\/p>\n<pre><code>!WANDB_MODE=&quot;dryrun&quot; python train.py --img 640 --batch 16 --epochs 30 --data D:\/Anil\/Shawn_Research\/Iamge_DataSet\/VinBigData\/New_Direct\/vinbigdata.yaml --weights yolov5x.pt --cache\n<\/code><\/pre>\n<p>This work fine on the kaggle platform but in my local machine it shows following:<\/p>\n<pre><code>'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file.\n<\/code><\/pre>\n<p>While reading the similar thread I realized I might making mistake related to path variable or Environment variable.\nEven I tried to get solution from the <a href=\"https:\/\/docs.wandb.ai\/library\/environment-variables\" rel=\"nofollow noreferrer\">official document<\/a> but couldn't figure out.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615272052933,
        "Question_score":0,
        "Question_tags":"python-3.x|windows|pytorch|wandb",
        "Question_view_count":263,
        "Owner_creation_time":1490268584217,
        "Owner_last_access_time":1663920632157,
        "Owner_location":null,
        "Owner_reputation":37,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1661850709697,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66541985",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73335735,
        "Question_title":"Wandb automatically logged into the wrong user -- why?",
        "Question_body":"<p>I followed the usual instructions:<\/p>\n<pre><code>pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code>(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>fyi useufl command:<\/p>\n<pre><code>cat ~\/.netrc\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<hr \/>\n<p>seems my issue only happens in pycharm, when I run it in the terminal it works... :\/<\/p>\n<hr \/>\n<p>cross: <a href=\"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1660314751363,
        "Question_score":1,
        "Question_tags":"pycharm|wandb",
        "Question_view_count":157,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":1660666336797,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73335735",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67495547,
        "Question_title":"wandb logging PermissionError and OSError",
        "Question_body":"<p>Description:<\/p>\n<ul>\n<li><p>When running experiments using <code>Weights and Biases<\/code> (wandb), I\noccasionally get a <code>PermissionError<\/code> for Python's <code>logging<\/code> library\nand <code>OSError<\/code> for accessing the TLS CA cert.<\/p>\n<\/li>\n<li><p>I had the following stacktrace, repeated many times with different\ntypes of &quot;message&quot;. I can't discern the order of operations, but I'm\nguessing the cert can't be accessed and that causes the script to\ncrash, but I don't know why it only happens sometimes.<\/p>\n<\/li>\n<li><p>If it is relevant, I ran the experiments on an Ubuntu server, authenticated via Kerberos.<\/p>\n<\/li>\n<\/ul>\n<p>What I've tried:<\/p>\n<ul>\n<li>I have manually checked the CA cert, and more than half the time I can successfully run experiments. As such I don't think it's the same as <a href=\"https:\/\/stackoverflow.com\/questions\/49100986\/certbot-could-not-find-a-suitable-tls-ca-certificate-bundle-archlinux\">this<\/a> or <a href=\"https:\/\/stackoverflow.com\/questions\/46119901\/python-requests-cant-find-a-folder-with-a-certificate-when-converted-to-exe\">this<\/a>.<\/li>\n<\/ul>\n<p>Stacktrace<\/p>\n<pre><code>Message: 'handle_request: stop_status'                                                                                                                                      [854\/1967]Arguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1085, in emit\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1065, in flush\nPermissionError: [Errno 13] Permission denied\nCall stack:\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 890, in _bootstrap\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 932, in _bootstrap_inner\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 54, in run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 95, in _run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py&quot;, line 280, in _process\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 175, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 183, in send_request\nMessage: 'send_request: stop_status'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 24, in wrapper\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 681, in check_stop_requested\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 127, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 119, in post\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 61, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 416, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 227, in cert_verify\nOSError: Could not find a suitable TLS CA certificate bundle, invalid path: \/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/certifi\/cacert.pem\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1620775947520,
        "Question_score":0,
        "Question_tags":"python-requests|ssl-certificate|kerberos|wandb",
        "Question_view_count":468,
        "Owner_creation_time":1519031032163,
        "Owner_last_access_time":1663673544740,
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":47,
        "Owner_down_votes":5,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67495547",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71469200,
        "Question_title":"Weights and Biases - cumulative max (highwater mark) for distributed training and sweep",
        "Question_body":"<p>I have an algorithm that I run 10 times, and return the best run by a cumulative maximum - So for each run, I return the highest validation score of the entire run. For example, this graph:\n<a href=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" alt=\"actual validation\" \/><\/a><\/p>\n<p>turns to this graph:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/csUOd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/csUOd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran 7 of these, and grouped them together aggregating with maximum. However, since each experiment validates at different timestep, the resulting graph is not a cumulative maximum of the entire 7 runs. That happens because at each validation point, not all runs are present:\n<a href=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What I would like to have is something like this:\n<a href=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol>\n<li>Is this achievable?<\/li>\n<li>How can I set a sweep that uses the cumulative validation of the entire experiment (not not a single trial)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647267422323,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":101,
        "Owner_creation_time":1553508349353,
        "Owner_last_access_time":1648824868747,
        "Owner_location":"Israel",
        "Owner_reputation":86,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71469200",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73617230,
        "Question_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Question_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662443319723,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":35,
        "Owner_creation_time":1352429442633,
        "Owner_last_access_time":1664036259503,
        "Owner_location":null,
        "Owner_reputation":740,
        "Owner_up_votes":46,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662569956687,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69870886,
        "Question_title":"How to exit the login of wandb?Two people have two accounts on one machine...Anyway other than relogin?",
        "Question_body":"<p>There are two people using wandb api on one machine. If one forgets to relogin before running, the other one's run will be shown in the prevois one's account. I wonder if there is a way to exit the login state and then the other one will know that he\/ she hasn't login before he\/her starts running programs. IN A HURRY for help about this.\nThe senoir students in my lab ask me to try to do so but I simply don't know how and I couldn't find a answer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636274174763,
        "Question_score":0,
        "Question_tags":"authentication|exit|conflict|wandb",
        "Question_view_count":700,
        "Owner_creation_time":1627886181520,
        "Owner_last_access_time":1642858256430,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69870886",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64093979,
        "Question_title":"Weights and Biases: Login and network errors",
        "Question_body":"<p>I recently installed Weights and Biases (wandb) for recording the metrics of my machine learning projects. Everything worked fine when connected to wandb cloud instance or when I used a local docker image. Now, when I tried to access my local wandb instance from over the network, I started to get API error messages. However, I also noticed that wandb was trying to access my server using port 80, instead of 8080. I installed wandb client on a new cloud server and tried to access my server from there. Still, same error message shown below.<\/p>\n<p>This error happens when I use the command: <code>wandb login host=https:\/\/api.wandb.ai<\/code>\nI have tried to delete the .netrc file where the api settings are stored and re-installed wandb. Still same error. Using wandb version 0.10.2 on Ubuntu 18.04; Also, tried downgrading to version 0.8.36, no change.\nIf I try the command: <code>wandb login --relogin<\/code>, I get the same error.<\/p>\n<p>Is there some way to reset wandb so it forgets all these settings, or to resolve this issue directly?<\/p>\n<p>Many thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>\n<pre><code>Retry attempt failed:\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 160, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 84, in create_connection\n    raise err\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 677, in urlopen\n    chunked=chunked,\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 392, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1277, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1323, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1272, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1032, in _send_output\n    self.send(msg)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 972, in send\n    self.connect()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 187, in connect\n    conn = self._new_conn()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 172, in _new_conn\n    self, &quot;Failed to establish a new connection: %s&quot; % e\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 449, in send\n    timeout=timeout\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 727, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/retry.py&quot;, line 439, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/old\/retry.py&quot;, line 96, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/internal\/internal_api.py&quot;, line 128, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 119, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\nwandb: Network error (ConnectionError), entering retry loop. See wandb\/debug-internal.log for full traceback.\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/bin\/wandb&quot;, line 8, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 72, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 212, in login\n    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 29, in login\n    anonymous=anonymous, key=key, relogin=relogin, host=host, force=force\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 128, in _login\n    apikey.write_key(settings, key)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/lib\/apikey.py&quot;, line 223, in write_key\n    raise ValueError(&quot;API key must be 40 characters long, yours was %s&quot; % len(key))\nValueError: API key must be 40 characters long, yours was 26\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601246511380,
        "Question_score":4,
        "Question_tags":"python|wandb",
        "Question_view_count":6277,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1661849922333,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64093979",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_time":1528765704783,
        "Owner_last_access_time":1635485916997,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1634729204573,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72951797,
        "Question_title":"Can you pass agent-specific parameters to a wandb sweep?",
        "Question_body":"<p>I would like to be able to pass a particular parameter with a different value on each agent in my wandb sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657626404050,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":42,
        "Owner_creation_time":1522603483940,
        "Owner_last_access_time":1663947359087,
        "Owner_location":"Paris, France",
        "Owner_reputation":66,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72951797",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67696204,
        "Question_title":"AttributeError: 'NoneType' object has no attribute '_global_run_stack'",
        "Question_body":"<p><strong>Description<\/strong><\/p>\n<p>I am using PTAN library with an A3C model and I am trying to work with <strong>wandb sweep<\/strong> but I've encountered some weird problems, and I am not sure if it's a bug regarding sweep (because if I am going to use just a simple model without any threads involving is going to work properly) or I am doing something wrong.<\/p>\n<p><strong>How to reproduce<\/strong><\/p>\n<p><em><strong>training function:<\/strong><\/em><\/p>\n<pre><code>def train(conf):\n    batch = []\n    step_idx = 0\n    epoch = conf['epochs']\n    try:\n        with commune.RewardTracker(writer, stop_reward=conf['reward_bound']) as tracker:\n            with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n                while True:\n                    if step_idx == epoch:\n                        break\n                    train_entry = train_queue.get()\n                    if isinstance(train_entry, TotalReward):\n                        if tracker.reward(train_entry.reward, step_idx):\n                            break\n                        continue\n                    if isinstance(train_entry, TotalProfit):\n                        tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n                        continue\n                    step_idx += 1\n                    if step_idx % 100 == 0:\n                        torch.save(net.state_dict(), os.path.join(SAVING_FOLDER, PROJECT_NAME))\n\n                    batch.append(train_entry)\n                    if len(batch) &lt; conf['batch_size']:\n                        continue\n\n                    states_v, actions_t, vals_ref_v = commune.unpack_batch(batch, net,\n                                                                           last_val_gamma=conf['gamma'] ** conf['reward_steps'],\n                                                                           device=device)\n                    batch.clear()\n\n                    optimizer.zero_grad()\n                    logits_v, value_v = net(states_v)\n\n                    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n\n                    log_prob_v = F.log_softmax(logits_v, dim=1)\n                    adv_v = vals_ref_v - value_v.detach()\n                    log_prob_actions_v = adv_v * log_prob_v[range(conf['batch_size']), actions_t]\n                    loss_policy_v = -log_prob_actions_v.mean()\n\n                    prob_v = F.softmax(logits_v, dim=1)\n                    entropy_loss_v = conf['entropy_beta'] * (prob_v * log_prob_v).sum(dim=1).mean()\n\n                    loss_v = entropy_loss_v + loss_value_v + loss_policy_v\n                    loss_v.backward()\n                    nn_utils.clip_grad_norm_(net.parameters(), conf['clip_grad'])\n                    optimizer.step()\n\n                    tb_tracker.track(&quot;advantage&quot;, adv_v, step_idx)\n                    tb_tracker.track(&quot;values&quot;, value_v, step_idx)\n                    tb_tracker.track(&quot;batch_rewards&quot;, vals_ref_v, step_idx)\n                    tb_tracker.track(&quot;loss_entropy&quot;, entropy_loss_v, step_idx)\n                    tb_tracker.track(&quot;loss_policy&quot;, loss_policy_v, step_idx)\n                    tb_tracker.track(&quot;loss_value&quot;, loss_value_v, step_idx)\n                    tb_tracker.track(&quot;loss_total&quot;, loss_v, step_idx)\n    finally:\n        for p in data_proc_list:\n            p.terminate()\n            p.join()\n<\/code><\/pre>\n<p><strong>main function:<\/strong><\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    mp.set_start_method('fork')\n    device = torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)\n\n    with open(r'sweep_config.yaml') as file:\n        sweep_config = yaml.load(file, Loader=yaml.FullLoader)\n\n    logs_dir_name = &quot;a3c_stock&quot;\n    wandb.tensorboard.patch(root_logdir=logs_dir_name)\n\n    sweep_id = wandb.sweep(sweep_config, project=&quot;sweep_project&quot;, entity=&quot;vildnex&quot;)\n    wandb.init(config=config_default)\n\n    config = wandb.config\n\n    writer = SummaryWriter(comment=logs_dir_name)\n\n    env = make_env(config)\n    net = commune.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n    net.share_memory()\n\n    if not os.path.isdir(SAVING_FOLDER):\n        os.mkdir(SAVING_FOLDER)\n\n    if os.path.isfile(os.path.join(SAVING_FOLDER, PROJECT_NAME)):\n        net.load_state_dict(torch.load(os.path.join(SAVING_FOLDER, PROJECT_NAME), map_location=device))\n\n    optimizer = optim.RMSprop(net.parameters(), lr=config.learning_rate, eps=1e-3)\n\n    train_queue = mp.Queue(maxsize=config.processes_count)\n    data_proc_list = []\n    dict_conf = dict(config)\n    for _ in range(config.processes_count):\n        data_proc = mp.Process(target=data_func, args=(net, device, train_queue, dict_conf))\n        data_proc.start()\n        data_proc_list.append(data_proc)\n\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>Exception in thread Thread-6:\nTraceback (most recent call last):\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 303, in _run_job\n    self._function()\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 191, in &lt;lambda&gt;\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 105, in train\n    tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/commune.py&quot;, line 118, in profits\n    self.writer.add_scalar(&quot;total_profit&quot;, total_profit, frame)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 344, in add_scalar\n    self._get_file_writer().add_summary(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 250, in _get_file_writer\n    self.file_writer = FileWriter(self.log_dir, self.max_queue,\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 60, in __init__\n    self.event_writer = EventFileWriter(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 157, in __init__\n    _notify_tensorboard_logdir(logdir, save=save, root_logdir=root_logdir_arg)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 167, in _notify_tensorboard_logdir\n    wandb.run._tensorboard_callback(logdir, save=save, root_logdir=root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 804, in _tensorboard_callback\n    self._backend.interface.publish_tbdata(logdir, save, root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 202, in publish_tbdata\n    self._publish(rec)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 518, in _publish\n    raise Exception(&quot;The wandb backend process has shutdown&quot;)\nException: The wandb backend process has shutdown\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 954, in _bootstrap_inner\n    self.run()\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 892, in run\n    self._target(*self._args, **self._kwargs)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 308, in _run_job\n    wandb.finish(exit_code=1)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 2374, in finish\n    wandb.run.finish(exit_code=exit_code)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 1144, in finish\n    if self._wl and len(self._wl._global_run_stack) &gt; 0:\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py&quot;, line 234, in __getattr__\n    return getattr(self._instance, name)\nAttributeError: 'NoneType' object has no attribute '_global_run_stack'\n<\/code><\/pre>\n<p><strong>Environment<\/strong><\/p>\n<ul>\n<li>OS: Manjaro 5.21.5<\/li>\n<li>Environment: PyCharm Local<\/li>\n<li>Python Version: 3.9<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1621980760977,
        "Question_score":0,
        "Question_tags":"python|pytorch|sweeper|wandb",
        "Question_view_count":1192,
        "Owner_creation_time":1479249043857,
        "Owner_last_access_time":1664055565850,
        "Owner_location":null,
        "Owner_reputation":1383,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":177,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67696204",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71617315,
        "Question_title":"Wandb line plots only show bar charts after refresh",
        "Question_body":"<p>My weights and biases (wandb) panels (e.g. for loss) shortly show line plots (x: steps, y: loss), then refresh (showing a spinner for some time) and then only show bar charts.<\/p>\n<p>Editing such panels either shows (a) &quot;Select runs that logged eval\/loss\nto visualize data in this line chart.&quot; on the left or (b) &quot;Showing a bar chart instead of a line chart because all logged values are length one.&quot; on the right.<\/p>\n<p>Does that mean that (a) a value <code>eval\/loss<\/code> is not found for the runs, or (b) only one value is given per run? How can I change this? But why was there a real line plot shown for about a second, before the panel refreshes? Where there values dropped? Why?<\/p>\n<p>Panels shortly show line plots: <a href=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Then panels refresh and only show bar charts: <a href=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Code<\/h2>\n<p>I am using huggingface transformers <code>TrainingArguments<\/code> with argument <code>report_to=&quot;wandb&quot;<\/code> (but with the default <code>logging_steps<\/code> of 500). I am doing 10-fold cross validation without any explicit <code>wandb.log<\/code> call within the cross validation loop. I do all of this in the <code>train()<\/code> function, which has as last command <code>wandb.finish()<\/code>. <code>train()<\/code> is called via <code>wandb.agent(sweep_id, train)<\/code> as I am using all of this within a large sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648212833057,
        "Question_score":0,
        "Question_tags":"machine-learning|pytorch|huggingface-transformers|wandb",
        "Question_view_count":231,
        "Owner_creation_time":1305196069933,
        "Owner_last_access_time":1663753090530,
        "Owner_location":"Karlsruhe, Germany",
        "Owner_reputation":6763,
        "Owner_up_votes":937,
        "Owner_down_votes":28,
        "Owner_views":709,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71617315",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71312243,
        "Question_title":"Log metrics with configuation in Pytorch Lightning using w&b",
        "Question_body":"<p>I am using PyTorch Lightning together with w&amp;b and trying associate metrics with a finite set of configurations. In the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/common\/lightning_module.html\" rel=\"nofollow noreferrer\"><code>LightningModule<\/code><\/a> class I have defined the <code>test_step<\/code> as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Assuming (for simplicity) that the batch size is 1, this will log the accuracy for 1 sample and it will be displayed as a chart in the w&amp;b dashboard.<\/p>\n<p>I would like to associate this accuracy with some configuration of the experimental environment. This configuration might include BDP factor, bandwith delay, queue_size, location, etc. I don't want to plot the configurations I just want to be able to filter or group the accuracy by some configuration value.<\/p>\n<p>The only solution I can come up with is to add these configurations as a querystring:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  # read values in config file\n  # ...\n\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/BDP=2&amp;delay=10ms&amp;queue_size=10&amp;topology=single\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Is there a better solution for this that integrates my desired functionality of being able to group and filter by values like BDP?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646154863157,
        "Question_score":2,
        "Question_tags":"pytorch|metrics|pytorch-lightning|wandb",
        "Question_view_count":315,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":1646156240833,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71312243",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72555212,
        "Question_title":"wandb pytorch: top1 accuracy per class",
        "Question_body":"<p>I have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using wandb . I have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. I am unable to achieve, are there any way to achieve it?<\/p>\n<p><strong>Validation Loader<\/strong><\/p>\n<pre><code> val_loaders = []\n    for nuisance in val_nuisances:\n        val_loaders.append((nuisance, torch.utils.data.DataLoader(\n            datasets.ImageFolder(os.path.join(valdir, nuisance), transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n            batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True,\n        )))\n\n\nval_nuisances = ['shape', 'pose', 'texture', 'context', 'weather']\n<\/code><\/pre>\n<p><strong>Validation Loop<\/strong><\/p>\n<pre><code>def validate(val_loaders, model, criterion, args):\n    overall_top1 = 0\n    for nuisance, val_loader in val_loaders:\n        batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n        losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n        top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n        top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n        progress = ProgressMeter(\n            len(val_loader),\n            [batch_time, losses, top1, top5],\n            prefix=f'Test {nuisance}: ')\n\n        # switch to evaluate mode\n        model.eval()\n\n        with torch.no_grad():\n            end = time.time()\n            for i, (images, target) in enumerate(val_loader):\n                if args.gpu is not None:\n                    images = images.cuda(args.gpu, non_blocking=True)\n                if torch.cuda.is_available():\n                    target = target.cuda(args.gpu, non_blocking=True)\n\n                # compute output\n                output = model(images)\n                loss = criterion(output, target)\n\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n                losses.update(loss.item(), images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % args.print_freq == 0:\n                    progress.display(i)\n\n            progress.display_summary()\n        overall_top1 += top1.avg\n    overall_top1 \/= len(val_loaders)\n    return top1.avg\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654753323610,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":71,
        "Owner_creation_time":1420474938783,
        "Owner_last_access_time":1663917040323,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":2201,
        "Owner_up_votes":186,
        "Owner_down_votes":3,
        "Owner_views":556,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72555212",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73109071,
        "Question_title":"How do I log a confusion matrix into Wanddb?",
        "Question_body":"<p>I'm using pytorch lightning, and at the end of each epoch, I create a confusion matrix from torchmetrics.ConfusionMatrix (see code below). I would like to log this into Wandb, but the Wandb confusion matrix logger only accepts y_targets and y_predictions. Does anyone know how to extract the updated confusion matrix y_targets and y_predictions from a confusion matrix, or alternatively give Wandb my updated confusion matrix in a way that it can be processed into eg a heatmap within wandb?<\/p>\n<pre><code>class ClassificationTask(pl.LightningModule):\n    def __init__(self, model, lr=1e-4, augmentor=augmentor):\n        super().__init__()\n        self.model = model\n        self.lr = lr\n        self.save_hyperparameters() #not being used at the moment, good to have ther in the future\n        self.augmentor=augmentor\n        \n        self.matrix = torchmetrics.ConfusionMatrix(num_classes=9)\n        \n        self.y_trues=[]\n        self.y_preds=[]\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x=self.augmentor(x)#.to('cuda')\n        y_pred = self.model(x)\n        loss = F.cross_entropy(y_pred, y,)  #weights=class_weights_tensor\n        \n\n        acc = accuracy(y_pred, y)\n        metrics = {&quot;train_acc&quot;: acc, &quot;train_loss&quot;: loss}\n        self.log_dict(metrics)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {&quot;val_acc&quot;: acc, &quot;val_loss&quot;: loss, }\n        self.log_dict(metrics)\n        return metrics\n    \n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        self.matrix.update(y_hat,y)\n        return loss, acc\n    \n    def validation_epoch_end(self, outputs):\n        confusion_matrix = self.matrix.compute()\n        wandb.log({&quot;my_conf_mat_id&quot; : confusion_matrix})\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam((self.model.parameters()), lr=self.lr)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658751297303,
        "Question_score":2,
        "Question_tags":"pytorch|confusion-matrix|pytorch-lightning|wandb",
        "Question_view_count":109,
        "Owner_creation_time":1626958179237,
        "Owner_last_access_time":1663922047490,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73109071",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68903269,
        "Question_title":"logging learning rate schedule in keras via weights and biases",
        "Question_body":"<p>I am training a keras model and using a custom learning rate scheduler for the optimizer (of type tf.keras.optimizers.schedules.LearningRateSchedule), and i want to log the learning rate change via the weights&amp;biases framework.\ni couldn't find how to pass it to the WandbCallback object or log it in any way<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1629789446037,
        "Question_score":5,
        "Question_tags":"python|tensorflow|keras|deep-learning|wandb",
        "Question_view_count":968,
        "Owner_creation_time":1541621385633,
        "Owner_last_access_time":1644833613247,
        "Owner_location":"Israel",
        "Owner_reputation":59,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1629798930663,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68903269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659671305703,
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_time":1587186468850,
        "Owner_last_access_time":1663918152847,
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1660033763877,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1659781694627,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66606294,
        "Question_title":"PyTorch Lightning wants create a folder on import due to usage of wandb, which raises error on AWS Lambda",
        "Question_body":"<p>So I want to build a Docker image with PyTorch Lightning that can be used with AWS lambda. However, when the function is invoked it raises an OS Error, that claims it uses a Read-only file system and wandb.py wants to write something.<\/p>\n<p>I tried these things:<\/p>\n<ol>\n<li>Overwrite the wandb.py file of pytroch lightning, that it does not init wandb --&gt; Raises error<\/li>\n<li>Execute a python script in Dockerfile, that the files are created on docker build and exist, when invoking the lambda function --&gt; Same OS error<\/li>\n<\/ol>\n<p>Does someone know a way to skip the wandb.py?<\/p>\n<p>This is the error message:<\/p>\n<pre><code>START RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772 Version: $LATEST\nOpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n[ERROR] OSError: [Errno 30] Read-only file system: '\/home\/sbx_user1051'\nTraceback (most recent call last):\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 234, in load_module\n    return load_source(name, filename, file)\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 171, in load_source\n    module = _load(spec)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 702, in _load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/task\/inference.py&quot;, line 5, in &lt;module&gt;\n    import pytorch_lightning as pl\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/__init__.py&quot;, line 63, in &lt;module&gt;\n    from pytorch_lightning.callbacks import Callback\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/__init__.py&quot;, line 25, in &lt;module&gt;\n    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/swa.py&quot;, line 26, in &lt;module&gt;\n    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/__init__.py&quot;, line 18, in &lt;module&gt;\n    from pytorch_lightning.trainer.trainer import Trainer\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py&quot;, line 30, in &lt;module&gt;\n    from pytorch_lightning.loggers import LightningLoggerBase\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/__init__.py&quot;, line 31, in &lt;module&gt;\n    from pytorch_lightning.loggers.wandb import _WANDB_AVAILABLE, WandbLogger  # noqa: F401\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py&quot;, line 34, in &lt;module&gt;\n    import wandb\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/__init__.py&quot;, line 131, in &lt;module&gt;\n    api = InternalApi()\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/apis\/internal.py&quot;, line 17, in __init__\n    self.api = InternalApi(*args, **kwargs)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 73, in __init__\n    self._settings = Settings(\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 25, in __init__\n    self._global_settings.read([Settings._global_path()])\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 105, in _global_path\n    util.mkdir_exists_ok(config_dir)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 687, in mkdir_exists_ok\n    os.makedirs(path)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 223, in makedirs\n    mkdir(name, mode)\nEND RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772\nREPORT RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772  Duration: 27000.33 ms   Billed Duration: 27001 ms   Memory Size: 10240 MB   Max Memory Used: 241 MB \nUnknown application error occurred\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615578448627,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|pytorch|pytorch-lightning|wandb",
        "Question_view_count":202,
        "Owner_creation_time":1510177189697,
        "Owner_last_access_time":1664037019800,
        "Owner_location":"Dortmund, Deutschland",
        "Owner_reputation":1194,
        "Owner_up_votes":149,
        "Owner_down_votes":11,
        "Owner_views":119,
        "Question_last_edit_time":1661850419560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66606294",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69425994,
        "Question_title":"is there any way to scale axis of plots in wandb?",
        "Question_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633272788153,
        "Question_score":1,
        "Question_tags":"wandb",
        "Question_view_count":604,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633345375677,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69425994",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69147788,
        "Question_title":"Weights & Biases with Transformers and PyTorch?",
        "Question_body":"<p>I'm training an NLP model at work (e-commerce SEO) applying a <code>BERT<\/code> variation for portuguese language (<code>BERTimbau<\/code>) through <code>Transformers<\/code> by Hugging Face.<\/p>\n<p>I didn't used the <code>Trainer<\/code> from Transformers API. I used <code>PyTorch<\/code> to set all parameters through <code>DataLoader.utils<\/code> and <code>adamW<\/code>. I trained my model using <code>run_glue.py<\/code>.<\/p>\n<p><strong>I'm training with a VM on GCP using Jupyterlab<\/strong>. I know that I can use Weights &amp; Biases both for PyTorch and Transformers. But I don't know exactly how to set it using <code>run_glue.py<\/code>. It's my first time using Weights &amp; Biases.<\/p>\n<p><em>After preprocessing and splitting train and test through Sklearn, my code is as it follows<\/em>:<\/p>\n<pre><code>from transformers import BertTokenizer\nimport torch\n#import torchvision\nfrom torch.utils.data import Dataset, TensorDataset\nimport collections.abc as container_abcs\n\n# To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n# Constructs a BERT tokenizer. Based on WordPiece. \n# The tokenization must be performed by the tokenizer included with BERT\ntokenizer = BertTokenizer.from_pretrained('neuralmind\/bert-large-portuguese-cased', \n                                          do_lower_case=True)\n\n# Tokenize all of the sentences and map the tokens to thier word IDs. To convert all the titles from text into encoded form.\n# We will use padding and truncation because the training routine expects all tensors within a batch to have the same dimensions.\nencoded_data_train = tokenizer.batch_encode_plus(\n    df[df.data_type=='train'].text.values, \n    add_special_tokens=True,                            # Add '[CLS]' and '[SEP]'. Sequences encoded with special tokens relative to their model\n    return_attention_mask=True,                         # Return mask according to specific tokenizer defined by max_length\n    pad_to_max_length=True,                             # Pad &amp; truncate all sentences. Pad all titles to certain maximum length                  \n    max_length=128,                                     # Do not need to set max_length=256\n    return_tensors='pt'                                 # Set to use PyTorch tensors\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df.data_type=='val'].text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=128, \n    return_tensors='pt'\n)\n\n# Split the data into input_ids, attention_masks and labels. \n# Converting the input data to the tensor , which can be feeded to the model\ninput_ids_train = encoded_data_train['input_ids']                    # Add the encoded sentence to the list.\nattention_masks_train = encoded_data_train['attention_mask']         # And its attention mask (simply differentiates padding from non-padding).\nlabels_train = torch.tensor(df[df.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type=='val'].label.values)\n\n# Create training data and validation data\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n<\/code><\/pre>\n<pre><code>from transformers import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained(&quot;neuralmind\/bert-large-portuguese-cased&quot;,  # Select your pretrained Model\n                                                      num_labels=len(label_dict),                # Labels tp predict\n                                                      output_attentions=False,                   # Whether the model returns attentions weights. We don\u2019t really care about output_attentions. \n                                                      output_hidden_states=False)                # Whether the model returns all hidden-states. We also don\u2019t need output_hidden_states\n<\/code><\/pre>\n<pre><code>from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32                                                              # Set your batch size according to your GPU memory      \n\ndataloader_train = DataLoader(dataset_train                                  # Use DataLoader to Optimize your model\n                              ,sampler=RandomSampler(dataset_train)          # Random Sampler from your dataset\n                              ,batch_size=batch_size)                        # If your batch_size is too high you will get a warning when you run the model \n                              #,num_workers=4                                # Number of cores\n                              #,pin_memory=True)                             # Use GPU to send your batch \n\ndataloader_validation = DataLoader(dataset_val \n                                   ,sampler=SequentialSampler(dataset_val)   # For validation the order doesn't matter. Sequential Sampler consumes less GPU.\n                                   ,batch_size=batch_size) \n                                   #,num_workers=4\n                                   #,pin_memory=True)\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n# hyperparameters\n# To construct an optimizer, we have to give it an iterable containing the parameters to optimize. \n# Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.\noptimizer = AdamW(model.parameters(),  # AdamW is a class from the huggingface library (as opposed to pytorch) \n                  lr=2e-5,             # args.learning_rate - default is 5e-5\n                  eps=1e-8)            # args.adam_epsilon  - default is 1e-8\n\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \nepochs = 2\n\n# Create the learning rate scheduler that decreases linearly from the initial learning rate set in the optimizer to 0, \n# after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,                               # Default value in run_glue.py                           \n                                            num_training_steps=len(dataloader_train)*epochs)  # Total number of training steps is [number of batches] x [number of epochs]. \n                                                                                              # Note that this is not the same as the number of training samples).\n<\/code><\/pre>\n<pre><code>from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')\n\n<\/code><\/pre>\n<p>And here follows <code>run_glue.py<\/code>:<\/p>\n<pre><code>import random\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\n# from tqdm.notebook import trange, tqdm\n\n'''\nThis training code is based on the 'run_glue.py' script here:\nhttps:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n'''\n\n# Just right before the actual usage select your hardware\ndevice = torch.device('cuda') # or cpu\nmodel = model.to(device)      # send your model to your hardware\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, validation and timings.\ndef evaluate(dataloader_val):\n    '''\n    Put the model in evaluation mode--the dropout layers behave differently\n    during evaluation.\n    '''\n    \n    model.eval()\n    \n    loss_val_total = 0 # Tracking variables \n\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        '''\n         Unpack this training batch from our dataloader.         \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels \n        '''\n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n        \n        '''\n        Tell pytorch not to bother with constructing the compute graph during\n        the forward pass, since this is only needed for backprop (training).\n        '''\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        '''\n        Perform a forward pass (evaluate the model on this training batch).\n        The documentation for this `model` function is here: \n        https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        This will return the loss (rather than the model output) \n        because we have provided the `labels`.\n        It returns different numbers of parameters depending on what arguments\n        arge given and what flags are set. For our useage here, it returns\n        the loss (because we provided labels) and the &quot;logits&quot;--the model\n        outputs prior to activation.\n        '''\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item() # Accumulate the validation loss.\n\n                \n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) # Calculate the average loss over all of the batches.\n\n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n\n    # ========================================\n    #               Training\n    # ========================================\n\n# For each epoch...\nfor epoch in tqdm(range(1, epochs+1)):    \n    '''\n    Put the model into training mode. Don't be mislead--the call to \n    `train` just changes the *mode*, it doesn't *perform* the training.\n    `dropout` and `batchnorm` layers behave differently during training\n    vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    '''\n    \n    model.train()        # Put the model into training mode.\n\n    \n    loss_train_total = 0 # Reset the total loss for this epoch.\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()        \n        '''\n        Always clear any previously calculated gradients before performing a\n        backward pass. PyTorch doesn't do this automatically because \n        accumulating the gradients is &quot;convenient while training RNNs&quot;. \n        (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        '''\n        \n        batch = tuple(b.to(device) for b in batch)\n        '''\n        Unpack this training batch from our dataloader. \n        \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels\n         '''\n        \n        inputs = {'input_ids':      batch[0], #.to(device)\n                  'attention_mask': batch[1], #.to(device)\n                  'labels':         batch[2], #.to(device)\n                 }       \n\n        outputs = model(**inputs)\n        \n\n        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n        loss_train_total += loss.item()         # Accumulate the training loss over all of the batches so that we can\n                                                # calculate the average loss at the end. `loss` is a Tensor containing a\n                                                # single value; the `.item()` function just returns the Python value \n                                                # from the tensor.\n        loss.backward() # Perform a backward pass to calculate the gradients.\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Clip the norm of the gradients to 1.0.\n                                                                   # This is to help prevent the &quot;exploding gradients&quot; problem.\n                                                                   # modified based on their gradients, the learning rate, etc.\n                \n        optimizer.step()        # Update parameters and take a step using the computed gradient.\n                                # The optimizer dictates the &quot;update rule&quot;--how the parameters are\n                                # modified based on their gradients, the learning rate, etc.\n                \n        scheduler.step()        # Update the learning rate.\n\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model') # Save Model\n        \n    tqdm.write(f'\\nEpoch {epoch}') # Show running epoch\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train) # Calculate the average loss over all of the batches.\n           \n    tqdm.write(f'Training loss: {loss_train_avg}') # Show loss average\n    \n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n    \n    # Record all statistics from this epoch.\n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631409889517,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|pytorch|huggingface-transformers|wandb",
        "Question_view_count":427,
        "Owner_creation_time":1618668550253,
        "Owner_last_access_time":1663440294737,
        "Owner_location":"S\u00e3o Paulo - Brazil",
        "Owner_reputation":113,
        "Owner_up_votes":99,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1661850382607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69147788",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67202711,
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619035214083,
        "Question_score":1,
        "Question_tags":"python|pytorch|wandb",
        "Question_view_count":840,
        "Owner_creation_time":1577734207070,
        "Owner_last_access_time":1663602351910,
        "Owner_location":null,
        "Owner_reputation":422,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620426900467,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71990430,
        "Question_title":"Wandb throws Permission denied error although I am logged in",
        "Question_body":"<p>I am using the cleanrl library, in particular the script <a href=\"https:\/\/github.com\/vwxyzjn\/cleanrl\/blob\/master\/cleanrl\/dqn_atari.py\" rel=\"nofollow noreferrer\">dqn_atari.py<\/a>  dqn_atari.py where  I followed the  <a href=\"https:\/\/docs.cleanrl.dev\/advanced\/resume-training\/\" rel=\"nofollow noreferrer\">instructions<\/a> in order to save and load the target and Q-network.<\/p>\n<p>I am running it locally within a conda environment.<\/p>\n<p>I haven't loaded something before, so the error may be due to my wandb configuration. The error is &quot;wandb: ERROR Permission denied to access wandb_entity\/wandb_project_name\/project_id&quot; and appears on line:<\/p>\n<pre><code>model = run.file(&quot;agent.pt&quot;)\n<\/code><\/pre>\n<p>The full  output is:<\/p>\n<pre><code>wandb: Currently logged in as: elena (use `wandb login --relogin` to force relogin)\n    wandb: Tracking run with wandb version 0.12.15\nwandb: Run data is saved locally in \/home\/elena\/workspace\/playground\/cleanrl\/wandb\/run-20220424_180429-2moec0qp\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Resuming run BreakoutNoFrameskip-v4__dqn-save__1__1650816268\nwandb: \u2b50 View project at https:\/\/wandb.ai\/elena\/test\nwandb:  View run at https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nA.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n[Powered by Stella]\n\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/stable_baselines3\/common\/buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 28.24GB &gt; 8.35GB\n  warnings.warn(\nwandb: ERROR Permission denied to access elena\/test\/2moec0qp\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/public.py&quot;, line 2428, in download\n    util.download_file_from_url(path, self.url, Api().api_key)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 1197, in download_file_from_url\n    response.raise_for_status()\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/requests\/models.py&quot;, line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/td_network.pt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;cleanrl\/dqn_atari_save.py&quot;, line 184, in &lt;module&gt;\n    model.download(f&quot;models\/{args.exp_name}\/&quot;)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 58, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\nwandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb:                                                                                \nwandb: \nwandb: Run history:\nwandb: global_step \u2581\nwandb: \nwandb: Run summary:\nwandb: charts\/episodic_return 0\nwandb:         charts\/epsilon 0.01\nwandb:          charts\/update 1969\nwandb:            global_step 0\nwandb: \nwandb: Synced BreakoutNoFrameskip-v4__dqn-save__1__1650816268: https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nwandb: Synced 3 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\nwandb: Find logs at: .\/wandb\/run-20220424_180429-2moec0qp\/logs\n<\/code><\/pre>\n<p>So as you can see I am logged in and I can see the files under &quot;https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp?workspace=user-elena&quot;. Something that drew my attention was &quot;requests.exceptions.HTTPError: 404 Client Error: Not Found for url: <a href=\"https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt%22\" rel=\"nofollow noreferrer\">https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt&quot;<\/a>. This path indeed looks different from the https path, but maybe this is not the issue?\nAny ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650818668313,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":596,
        "Owner_creation_time":1418505278743,
        "Owner_last_access_time":1663932736703,
        "Owner_location":null,
        "Owner_reputation":319,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990430",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71476873,
        "Question_title":"How to log additional single variable with wandb and huggingface transformers",
        "Question_body":"<p>I am using Huggingface's Transformers Trainer object and I really like the support it has for wandb.<\/p>\n<p>For my use case, I have subclassed the Trainer and, in addition to the values that are logged by default, I would like to keep track of one additional variable that gets updated at each training step.<\/p>\n<p>What is the easiest way to add tracking for a single variable with wandb?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647316549707,
        "Question_score":1,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":169,
        "Owner_creation_time":1484868229953,
        "Owner_last_access_time":1654795122843,
        "Owner_location":null,
        "Owner_reputation":507,
        "Owner_up_votes":61,
        "Owner_down_votes":0,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71476873",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68087139,
        "Question_title":"trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests",
        "Question_body":"<p>When saving a version in Kaggle, I get <strong>StdinNotImplementedError: getpass was called, but this frontend does not support input requests<\/strong> whenever I use the Transformers.Trainer class. The general code I use:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments\ntraining_args = TrainingArguments(params)\ntrainer = Trainer(params)\ntrainer.train()\n<\/code><\/pre>\n<p>And the specific cell I am running now:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments,EarlyStoppingCallback\n\nearly_stopping = EarlyStoppingCallback()\n\ntraining_args = TrainingArguments(\n    output_dir=OUT_FINETUNED_MODEL_PATH,          \n    num_train_epochs=20,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    warmup_steps=0,                \n    weight_decay=0.01,               \n    logging_dir='.\/logs',            \n    logging_steps=100,\n    evaluation_strategy=&quot;steps&quot;,\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=&quot;eval_loss&quot;,\n    greater_is_better=False\n    \n)\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,             \n    callbacks=[early_stopping]\n\n)\n\ntrainer.train()\n<\/code><\/pre>\n<p>When trainer.train() is called, I get the error below, which I do not get if I train with native PyTorch. I understood that the error arises since I am asked to input a password, but no password is asked when using native PyTorch code, nor when using the same code with trainer.train() on Google Colab.\nAny solution would be ok, like:<\/p>\n<ol>\n<li>Avoid being asked the password.<\/li>\n<li>Enable input requests when saving a notebook on Kaggle. After that, if I understood correctly, I would need to go to <a href=\"https:\/\/wandb.ai\/authorize\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/authorize<\/a> (after having created an account) and copy the generated key to console. However, I do not understand why wandb should be necessary since I never explicitly used it so far.<\/li>\n<\/ol>\n<pre><code>wandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 741, in init\n    wi.setup(kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 155, in setup\n    wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 210, in _login\n    wlogin.prompt_api_key()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 144, in prompt_api_key\n    no_create=self._settings.force,\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py&quot;, line 135, in prompt_api_key\n    key = input_callback(api_ask).strip()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py&quot;, line 825, in getpass\n    &quot;getpass was called, but this frontend does not support input requests.&quot;\nIPython.core.error.StdinNotImplementedError: getpass was called, but this frontend does not support input requests.\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    740         wi = _WandbInit()\n--&gt; 741         wi.setup(kwargs)\n    742         except_exit = wi.settings._except_exit\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in setup(self, kwargs)\n    154         if not settings._offline and not settings._noop:\n--&gt; 155             wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in _login(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)\n    209     if not key:\n--&gt; 210         wlogin.prompt_api_key()\n    211 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in prompt_api_key(self)\n    143             no_offline=self._settings.force,\n--&gt; 144             no_create=self._settings.force,\n    145         )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local)\n    134             )\n--&gt; 135             key = input_callback(api_ask).strip()\n    136         write_key(settings, key, api=api)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py in getpass(self, prompt, stream)\n    824             raise StdinNotImplementedError(\n--&gt; 825                 &quot;getpass was called, but this frontend does not support input requests.&quot;\n    826             )\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-82-4d1046ab80b8&gt; in &lt;module&gt;\n     42     )\n     43 \n---&gt; 44     trainer.train()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)\n   1067         model.zero_grad()\n   1068 \n-&gt; 1069         self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)\n   1070 \n   1071         # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in on_train_begin(self, args, state, control)\n    338     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n    339         control.should_training_stop = False\n--&gt; 340         return self.call_event(&quot;on_train_begin&quot;, args, state, control)\n    341 \n    342     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)\n    386                 train_dataloader=self.train_dataloader,\n    387                 eval_dataloader=self.eval_dataloader,\n--&gt; 388                 **kwargs,\n    389             )\n    390             # A Callback can skip the return of `control` if it doesn't change it.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in on_train_begin(self, args, state, control, model, **kwargs)\n    627             self._wandb.finish()\n    628         if not self._initialized:\n--&gt; 629             self.setup(args, state, model, **kwargs)\n    630 \n    631     def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in setup(self, args, state, model, **kwargs)\n    604                     project=os.getenv(&quot;WANDB_PROJECT&quot;, &quot;huggingface&quot;),\n    605                     name=run_name,\n--&gt; 606                     **init_args,\n    607                 )\n    608             # add config parameters (run may have been created manually)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    779             if except_exit:\n    780                 os._exit(-1)\n--&gt; 781             six.raise_from(Exception(&quot;problem&quot;), error_seen)\n    782     return run\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)\n\nException: problem\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624378186880,
        "Question_score":2,
        "Question_tags":"huggingface-transformers|kaggle|getpass|wandb",
        "Question_view_count":440,
        "Owner_creation_time":1624376028980,
        "Owner_last_access_time":1642694800790,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68087139",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69008133,
        "Question_title":"How to plot confidence intervals for different training samples",
        "Question_body":"<p>I am working on running training with different divisions of a training set. The plots that I get (using wandb) are fine, but not quite informative in my opinion and high in variance.\n<a href=\"https:\/\/i.stack.imgur.com\/kux5o.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kux5o.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is there a way to plot the mean of the plots, and then confidence intervals around it? Something similar to the picture below. Alternatively, is there a way to plot variance during training?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/veoWc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/veoWc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630473410597,
        "Question_score":1,
        "Question_tags":"python|visualization|confidence-interval|variance|wandb",
        "Question_view_count":232,
        "Owner_creation_time":1570797361193,
        "Owner_last_access_time":1652428368017,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69008133",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70637798,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641691804600,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|boto3|wandb",
        "Question_view_count":121,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1643119769043,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643119052817,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643119813377,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72079835,
        "Question_title":"Pytorch lightning callback for switching dataloader_idx",
        "Question_body":"<p>Is there a callback or something similar used when incrementing the dataloader index? The reason is that I have defined multiple dataloaders and I would like to start a new run with weight and biases for each dataloader.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651433528283,
        "Question_score":0,
        "Question_tags":"pytorch-lightning|wandb",
        "Question_view_count":146,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72079835",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71944710,
        "Question_title":"Multiple lines on same plot with incremental logging - wandb",
        "Question_body":"<p>I am using weights &amp; biases (wandb). I want to group multiple plots into one while using incremental logging, any way to do that?<\/p>\n<p>Say we have 10 metrics, I can add them to the project incrementally, gradually building 10 graphs:<\/p>\n<pre><code>import wandb\nimport math\n\nN_STEPS = 100\n\nwandb.init(project=&quot;someProject&quot;, name=&quot;testMultipleLines&quot;)\nfor epoch in range(N_STEPS):\n    log = {}\n    log['main_metric'] = epoch \/ N_STEPS  # some main metric\n\n    # some other metrics I want to have all on 1 plot\n    other_metrics = {}\n    for j in range(10):\n        other_metrics[f'metric_{j}'] = math.sin(j * math.pi * (epoch \/ N_STEPS))\n    log['other_metrics'] = other_metrics\n\n    wandb.log(log)\n<\/code><\/pre>\n<p>This by default gets presented on the wandb interface as 11 different plots. How can they be grouped through the API (without using the web interface) such that <code>main_metric<\/code> is on one figure and all <code>other_metrics<\/code> are bunched together on a second figure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1650480171990,
        "Question_score":6,
        "Question_tags":"python|machine-learning|plot|wandb",
        "Question_view_count":275,
        "Owner_creation_time":1457253216340,
        "Owner_last_access_time":1663905208120,
        "Owner_location":null,
        "Owner_reputation":667,
        "Owner_up_votes":100,
        "Owner_down_votes":1,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71944710",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72972876,
        "Question_title":"YOLOv5 Evolution Results Not Reproducible wandb",
        "Question_body":"<p>I am running YOLOv5 in a sagemaker notebook.\nThe 10 epoch runs are using the following notebook script making use of the --evolve flag for hyperparameters.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;evolution&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=10\n--evolve=30\n<\/code><\/pre>\n<p>Evolution runs only output one point on the graph at the end of 10 epochs and the outputted hyperparameters do not show reproducible results when running in a 50 epoch run. The blue 50 epoch line showcases using the optimal hyperparameters which should intersect with the highest 10 epoch run, but it doesn't reach anywhere close.\n<a href=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>After finding the optimal hyperparameters I ran a 50 epoch run using those parameters using the following command.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;hyperparam&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--hyp=.\/deepsea-yolov5\/opt\/ml\/input\/data\/hyp.scratch-low.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=50\n<\/code><\/pre>\n<p>However as shown in the picture above, the runs do not intersect with the best-performing hyperparameter run.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657748030047,
        "Question_score":0,
        "Question_tags":"python|yolov5|wandb",
        "Question_view_count":63,
        "Owner_creation_time":1498005510443,
        "Owner_last_access_time":1663651022003,
        "Owner_location":"California, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72972876",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72001154,
        "Question_title":"How to prevent Weights & Biases from saving best model parameters",
        "Question_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650896835733,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|wandb",
        "Question_view_count":204,
        "Owner_creation_time":1514243890900,
        "Owner_last_access_time":1663708014960,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650898428827,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630064062593,
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1630357543657,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14.0,
        "Answer_creation_time":1630122133813,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68952727",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73517766,
        "Question_title":"How to recieve metrics for Object Detection while using pytorch and Weights & Biases?",
        "Question_body":"<p>I have been training and fine tuning few models for detection task on a custom dataset,\nI would like to plot relevant metrics such as mean Average Precision (taking into account the predicted bounding box location and the enclosed object's classification).<\/p>\n<p>I'm using Pytorch and have started using <code>Weights &amp; Biases<\/code> (<a href=\"https:\/\/youtu.be\/G7GH0SeNBMA?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk\" rel=\"nofollow noreferrer\">Weights &amp; Biases integrated with pytorch<\/a>)<\/p>\n<p>For avoiding inventing the wheel, I have used some files from here:<\/p>\n<p><a href=\"https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection<\/a><\/p>\n<p><code>engine.py<\/code> - holds the <strong>train_one_epoch()<\/strong> function<\/p>\n<p><code>cocoeval.py<\/code> - holds the <strong>summarize()<\/strong> function<\/p>\n<p>Now I would like to log those metrics to a <code>Weights &amp; Biases<\/code>,\nso I'll we able to get more clear view and intuition about the fine-tuning phase,but I'm not sure where is the proper place to put the logger invocation.\ncan somebody please assist me?<\/p>\n<pre><code>wandb.watch()\n<\/code><\/pre>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661681279513,
        "Question_score":0,
        "Question_tags":"image-processing|pytorch|torch|wandb",
        "Question_view_count":33,
        "Owner_creation_time":1287433738400,
        "Owner_last_access_time":1664033141413,
        "Owner_location":null,
        "Owner_reputation":610,
        "Owner_up_votes":289,
        "Owner_down_votes":1,
        "Owner_views":130,
        "Question_last_edit_time":1661681944403,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73517766",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71227518,
        "Question_title":"Connecting Julia to Weights & Biases over Python",
        "Question_body":"<p>I am trying to use weights&amp;biases for my models written in Julia. I am using <code>WeightsAndBiasLogger.jl<\/code> and try to test their demo code:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger\n\nargs = (n_epochs=1_000, lr=1e-3)\nlogger = WBLogger(project=&quot;sample-project&quot;)\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>I receive an error: <strong>&quot;ArgumentError: ref of NULL PyObject&quot;<\/strong> (considering the line: logger = WBLogger(project=&quot;sample-project&quot;)\n)<\/p>\n<p>Then I tried to fix this with the following command:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger, PyCall\n\nargs = (n_epochs=1_000, lr=1e-3)\n\nconst logger = PyNULL()\nfunction __init__()\n    copy!(logger, WBLogger(project=&quot;sample-project&quot;))\nend\n\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>It creates the <code>logger<\/code> object, but now the error is:<\/p>\n<p><strong>MethodError: no method matching config!(::PyObject, ::NamedTuple{(:n_epochs, :lr), Tuple{Int64, Float64}})\nClosest candidates are: config!(!Matched::WBLogger, ::Any; kwargs...)<\/strong> (this consider the line: config!()...<\/p>\n<p>So, does anyone know how to solve the issue? Obviously, I am new to Julia, thus I apologize if asking something very stupid. In addition, if you know a better solution to integrate Julia into W&amp;B or any good alternatives, I would be glad to hear it.<\/p>\n<p>PS: Julia ver 1.7.2<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645559620073,
        "Question_score":2,
        "Question_tags":"python|julia|pycall|wandb",
        "Question_view_count":102,
        "Owner_creation_time":1535938696457,
        "Owner_last_access_time":1664053164283,
        "Owner_location":"Roskilde, Denmark",
        "Owner_reputation":1443,
        "Owner_up_votes":339,
        "Owner_down_votes":26,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71227518",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72731861,
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655994282873,
        "Question_score":1,
        "Question_tags":"python|hyperparameters|mlops|wandb",
        "Question_view_count":182,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Question_last_edit_time":1656234648443,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656329528480,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73320449,
        "Question_title":"Plotting the confuison matrix into wandb (pytorch)",
        "Question_body":"<p>I'm training a model and I'm trying to add a confusion matrix, which would be displayed in my <code>wandb<\/code>, but I got lost a bit. Basically, the matrix works; I can print it, but it's not loaded into <code>wandb<\/code>. Everything should be OK, except it's not. Can you please help me? I'm new to all this. Thanks a lot!<\/p>\n<p><strong>the code<\/strong><\/p>\n<pre><code>    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                from sklearn.metrics import f1_score\n                f1_score = f1_score(labels.cpu().data, preds.cpu(), average=None)\n                wandb.log({'F1 score' : f1_score})\n\n                nb_classes = 7\n\n                confusion_matrix = torch.zeros(nb_classes, nb_classes)\n                with torch.no_grad():\n                    for i, (inputs, classes) in enumerate(dataloaders['val']):\n                        inputs = inputs.to(device)\n                        classes = classes.to(device)\n                        outputs = model_ft(inputs)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    for t, p in zip(classes.view(-1), preds.view(-1)):\n                        confusion_matrix[t.long(), p.long()] += 1\n              wandb.log({'matrix' : confusion_matrix})\n                           \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            wandb.log({'epoch loss': epoch_loss,\n                    'epoch acc': epoch_acc})\n            \n            data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n            table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n            wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n\n        \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc, f1_score))\n\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    print('f1_score: {}'.format(f1_score))\n   \n    model.load_state_dict(best_model_wts)\n    return model\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660219911593,
        "Question_score":0,
        "Question_tags":"python|pytorch|conv-neural-network|transfer-learning|wandb",
        "Question_view_count":69,
        "Owner_creation_time":1659890757880,
        "Owner_last_access_time":1660739315393,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660228199173,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70573652,
        "Question_title":"Model stopped training once I introduced << report_to = 'wandb' >> in TrainingArguments",
        "Question_body":"<p>I am downloading the model <a href=\"https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main<\/a> microsoft\/Multilingual-MiniLM-L12-H384 and then using it.<\/p>\n<p>Transformer Version: '4.11.3'<\/p>\n<p>I have written the below code:<\/p>\n<pre><code>import wandb\nwandb.login()\n%env WANDB_LOG_MODEL=true\n\nmodel = tr.BertForSequenceClassification.from_pretrained(&quot;\/home\/pc\/minilm_model&quot;,num_labels=2)\nmodel.to(device)\n\nprint(&quot;hello&quot;)\n\ntraining_args = tr.TrainingArguments(\nreport_to = 'wandb',\noutput_dir='\/home\/pc\/proj\/results2', # output directory\nnum_train_epochs=10, # total number of training epochs\nper_device_train_batch_size=16, # batch size per device during training\nper_device_eval_batch_size=32, # batch size for evaluation\nlearning_rate=2e-5,\nwarmup_steps=1000, # number of warmup steps for learning rate scheduler\nweight_decay=0.01, # strength of weight decay\nlogging_dir='.\/logs', # directory for storing logs\nlogging_steps=1000,\nevaluation_strategy=&quot;epoch&quot;,\nsave_strategy=&quot;no&quot;\n)\n\nprint(&quot;hello&quot;)\n\ntrainer = tr.Trainer(\nmodel=model, # the instantiated  Transformers model to be trained\nargs=training_args, # training arguments, defined above\ntrain_dataset=train_data, # training dataset\neval_dataset=val_data, # evaluation dataset\ncompute_metrics=compute_metrics\n)\n\n<\/code><\/pre>\n<p>After Executing this:<\/p>\n<p>The model stuck at this point:<\/p>\n<p>***** Running training *****<\/p>\n<pre><code>Num examples = 12981\n Num Epochs = 20\n Instantaneous batch size per device = 16\n Total train batch size (w. parallel, distributed &amp; accumulation) = 32\n Gradient Accumulation steps = 1\n Total optimization steps = 8120\nAutomatic Weights &amp; Biases logging enabled, to disable set os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;\n\n<\/code><\/pre>\n<p><strong>What could be the possible solution?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641265169473,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|wandb",
        "Question_view_count":171,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70573652",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67160576,
        "Question_title":"YoloV5 killed at first epoch",
        "Question_body":"<p>I'm using a virtual machine on Windows 10 with this config:<\/p>\n<pre><code>Memory 7.8 GiB\nProcessor Intel\u00ae Core\u2122 i5-6600K CPU @ 3.50GHz \u00d7 3\nGraphics llvmpipe (LLVM 11.0.0, 256 bits)\nDisk Capcity 80.5 GB\nOS Ubuntu 20.10 64 Bit\nVirtualization Oracle\n<\/code><\/pre>\n<p>I installed docker for Ubuntu as described in <a href=\"https:\/\/docs.docker.com\/engine\/install\/ubuntu\/\" rel=\"nofollow noreferrer\">the official documentation<\/a>.<br>\nI pulled the docker image as described on the <a href=\"https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Docker-Quickstart\" rel=\"nofollow noreferrer\">yolo github section for docker<\/a>.<br>\nSince I have no NVIDIA GPU I could not install a driver or CUDA.\nI pulled the aquarium from <a href=\"https:\/\/public.roboflow.com\/object-detection\/aquarium\" rel=\"nofollow noreferrer\">roboflow<\/a> and installed it on a folde aquarium.\nI ran this command to start the image and have my aquarium folder mounted<\/p>\n<pre><code>sudo docker run --ipc=host -it -v &quot;$(pwd)&quot;\/Desktop\/yolo\/aquarium:\/usr\/src\/app\/aquarium ultralytics\/yolov5:latest\n<\/code><\/pre>\n<p>And was greeted with this banner<\/p>\n<blockquote>\n<h1>=============\n== PyTorch ==<\/h1>\n<p>NVIDIA Release 21.03 (build 21060478) PyTorch Version 1.9.0a0+df837d0<\/p>\n<p>Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights\nreserved.<\/p>\n<p>Copyright (c) 2014-2021 Facebook Inc. Copyright (c) 2011-2014 Idiap\nResearch Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind\nTechnologies    (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC\nLaboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU<br \/>\n(Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America\n(Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright\n(c) 2006      Idiap Research Institute (Samy Bengio) Copyright (c)\n2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio,\nJohnny Mariethoz) Copyright (c) 2015      Google Inc. Copyright (c)\n2015      Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors\nAll rights reserved.<\/p>\n<p>NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA\nCORPORATION.  All rights reserved.<\/p>\n<p>Various files include modifications (c) NVIDIA CORPORATION.  All\nrights reserved.<\/p>\n<p>This container image and its contents are governed by the NVIDIA Deep\nLearning Container License. By pulling and using the container, you\naccept the terms and conditions of this license:\n<a href=\"https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license\" rel=\"nofollow noreferrer\">https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license<\/a><\/p>\n<p>WARNING: The NVIDIA Driver was not detected.  GPU functionality will\nnot be available.    Use 'nvidia-docker run' to start this container;\nsee    <a href=\"https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker<\/a> .<\/p>\n<p>NOTE: MOFED driver for multi-node communication was not detected.\nMulti-node communication performance may be reduced.<\/p>\n<\/blockquote>\n<p>So no error there.<br>\nI installed pip and with pip wandb I added wandb. I used <code>wandb login<\/code> and set my API key.<br><br>\nI ran following command:<\/p>\n<pre><code># python train.py --img 640 --batch 16 --epochs 10 --data .\/aquarium\/data.yaml --weights yolov5s.pt --project ip5 --name aquarium5 --nosave --cache\n<\/code><\/pre>\n<p>And received this output:<\/p>\n<pre><code>github: skipping check (Docker image)\nYOLOv5  v5.0-14-g238583b torch 1.9.0a0+df837d0 CPU\n\nNamespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='.\/aquarium\/data.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data\/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='aquarium5', noautoanchor=False, nosave=True, notest=False, project='ip5', quad=False, rect=False, resume=False, save_dir='ip5\/aquarium5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)\ntensorboard: Start with 'tensorboard --logdir ip5', view at http:\/\/localhost:6006\/\nhyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\nwandb: Currently logged in as: pebs (use `wandb login --relogin` to force relogin)\nwandb: Tracking run with wandb version 0.10.26\nwandb: Syncing run aquarium5\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/pebs\/ip5\nwandb:  View run at https:\/\/wandb.ai\/pebs\/ip5\/runs\/1c2j80ii\nwandb: Run data is saved locally in \/usr\/src\/app\/wandb\/run-20210419_102642-1c2j80ii\nwandb: Run `wandb offline` to turn off syncing.\n\nOverriding model.yaml nc=80 with nc=7\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\nModel Summary: 283 layers, 7079724 parameters, 7079724 gradients, 16.4 GFLOPS\n\nTransferred 356\/362 items from yolov5s.pt\nScaled weight_decay = 0.0005\nOptimizer groups: 62 .bias, 62 conv.weight, 59 other\ntrain: Scanning '\/usr\/src\/app\/aquarium\/train\/labels.cache' images and labels... 448 found, 0 missing, 1 empty, 0 corrupted: 100%|\u2588| 448\/448 [00:00&lt;?, ?\ntrain: Caching images (0.4GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 448\/448 [00:01&lt;00:00, 313.77it\/s]\nval: Scanning '\/usr\/src\/app\/aquarium\/valid\/labels.cache' images and labels... 127 found, 0 missing, 0 empty, 0 corrupted: 100%|\u2588| 127\/127 [00:00&lt;?, ?it\nval: Caching images (0.1GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127\/127 [00:00&lt;00:00, 141.31it\/s]\nPlotting labels... \n\nautoanchor: Analyzing anchors... anchors\/target = 5.17, Best Possible Recall (BPR) = 0.9997\nImage sizes 640 train, 640 test\nUsing 3 dataloader workers\nLogging results to ip5\/aquarium5\nStarting training for 10 epochs...\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n  0%|                                                                                                                           | 0\/28 [00:00&lt;?, ?it\/s]Killed\nroot@cf40a6498016:~# \/opt\/conda\/lib\/python3.8\/multiprocessing\/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n<\/code><\/pre>\n<p>From this output I would think that there were 0 epochs completed.<br>\nMy data.yaml contains this code:<\/p>\n<pre><code>train: \/usr\/src\/app\/aquarium\/train\/images\nval: \/usr\/src\/app\/aquarium\/valid\/images\n\nnc: 7\nnames: ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n<\/code><\/pre>\n<p><a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">wandb.ai<\/a> does not display any metrics, but I have the files config.yaml, requirements.txt, wandb-metadata.json and wandb-summary.json.<\/p>\n<p>Why am I not getting any output?<br>\nHas there in fact be no training at all?<br>\nIf there was a training, how can I use my model?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1618829350100,
        "Question_score":2,
        "Question_tags":"python|docker|pytorch|yolov5|wandb",
        "Question_view_count":1939,
        "Owner_creation_time":1430930915943,
        "Owner_last_access_time":1661487827213,
        "Owner_location":"Switzerland",
        "Owner_reputation":2006,
        "Owner_up_votes":255,
        "Owner_down_votes":4,
        "Owner_views":230,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67160576",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71191185,
        "Question_title":"Pip installations on Colab from local",
        "Question_body":"<p>I'd like to use wandb on Colab, and I've installed it through pip on the command line. However, the import isn't recognized on Colab, so I have to run <code>!pip install wandb<\/code> each time.<\/p>\n<p>How can I install <code>wandb<\/code> locally so that I don't have to install it on the Colab notebook each time?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1645328187297,
        "Question_score":0,
        "Question_tags":"python|pip|google-colaboratory|wandb",
        "Question_view_count":516,
        "Owner_creation_time":1558237713157,
        "Owner_last_access_time":1663947795227,
        "Owner_location":null,
        "Owner_reputation":421,
        "Owner_up_votes":20,
        "Owner_down_votes":5,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71191185",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73068169,
        "Question_title":"wandb : move runs from a blocked entity",
        "Question_body":"<p>I accidentally moved several runs from my own user account to a team entity.\nUnfortunatly, this team entity had a restriction on the quantity of experiments tracked, and it now appears as blocked. I have this error message :<\/p>\n<blockquote>\n<p>Your organization is over the limit of 250 tracked hours. Please upgrade your plan to keep using W&amp;B.<\/p>\n<\/blockquote>\n<p>I can't find a way to move back these runs to my free account, does anyone know how to do this ?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658414318473,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":19,
        "Owner_creation_time":1598425704360,
        "Owner_last_access_time":1658749438907,
        "Owner_location":"Toulouse, France",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73068169",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71432453,
        "Question_title":"getting aligned val_loss and train_loss plots for each epoch using WandB rather than separate plots",
        "Question_body":"<p>I have the following code for logging the train and val loss in each epoch using WandB API. I am not sure though why I am not getting val loss and train loss in the same epoch. Any idea how that could be fixed?<\/p>\n<pre><code>wandb.log({&quot;train loss&quot;: train_epoch_loss,\n           &quot;val loss&quot;: val_epoch_loss,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;train acc&quot;: train_epoch_acc,\n           &quot;val acc&quot;: val_epoch_acc,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;best val acc&quot;: best_acc, &quot;epoch&quot;: epoch})\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you see, val loss vs epochs and train loss vs epochs are two completely separate entities while I would like to have both of them in one plot in WandB.\n<a href=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646959611130,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|wandb",
        "Question_view_count":463,
        "Owner_creation_time":1369335377410,
        "Owner_last_access_time":1656440556490,
        "Owner_location":"Boston, MA, United States",
        "Owner_reputation":31183,
        "Owner_up_votes":4284,
        "Owner_down_votes":32,
        "Owner_views":18708,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432453",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69920078,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636577514790,
        "Question_score":0,
        "Question_tags":"tensorflow2.0|tf.keras|wandb",
        "Question_view_count":60,
        "Owner_creation_time":1600299419450,
        "Owner_last_access_time":1662833034530,
        "Owner_location":"Germany",
        "Owner_reputation":62,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636642289973,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71134673,
        "Question_title":"python ray tune unable to stop trial or experiment",
        "Question_body":"<p>I am trying to make ray tune with wandb stop the experiment under certain conditions.<\/p>\n<ul>\n<li>stop all experiment if any trial raises an Exception (so i can fix the code and resume)<\/li>\n<li>stop if my score gets -999<\/li>\n<li>stop if the variable <code>varcannotbezero<\/code> gets 0<\/li>\n<\/ul>\n<p><strong>The following things i tried all failed in achieving desired behavior:<\/strong><\/p>\n<ul>\n<li>stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0}<\/li>\n<li>max_failures=0<\/li>\n<li>defining a Stoper class did also not work<\/li>\n<\/ul>\n<pre><code>class RayStopper(Stopper):\n    def __init__(self):\n        self._start = time.time()\n        #self._deadline = 300\n    def __call__(self, trial_id, result):\n        self.score=result[&quot;score&quot;]\n        self.varcannotbezero=result[&quot;varcannotbezero&quot;]\n        return False\n    def stop_all(self):\n        if self.score==-999 or self.varcannotbezero==0:\n            return True\n        else:\n            return False\n<\/code><\/pre>\n<p>Ray tune just continues to run<\/p>\n<pre><code>    wandb_project=&quot;ABC&quot;\n    wandb_api_key=&quot;KEY&quot;\n    ray.init(configure_logging=False)\n\n    if current_best_params is None:\n        algo = HyperOptSearch()\n    else:\n        algo = HyperOptSearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points)\n    algo = ConcurrencyLimiter(algo, max_concurrent=1)\n\n    scheduler = AsyncHyperBandScheduler()\n    analysis = tune.run(\n        tune_obj,\n        name=&quot;Name&quot;,\n        resources_per_trial={&quot;cpu&quot;: 1},\n        search_alg=algo,\n        scheduler=scheduler,\n        metric=&quot;score&quot;,\n        mode=&quot;max&quot;,\n        num_samples=10,\n        stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0},\n        max_failures=0,\n        config=config,\n        callbacks=[WandbLoggerCallback(project=wandb_project,entity=&quot;mycompany&quot;,api_key=wandb_api_key,log_config=True)],\n        local_dir=local_dir,\n        resume=&quot;AUTO&quot;,\n        verbose=0\n    )\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1644967685327,
        "Question_score":0,
        "Question_tags":"optimization|hyperparameters|ray|ray-tune|wandb",
        "Question_view_count":284,
        "Owner_creation_time":1300741959900,
        "Owner_last_access_time":1664027459750,
        "Owner_location":null,
        "Owner_reputation":2390,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":308,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71134673",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73165796,
        "Question_title":"Wandb first run start time is delayed",
        "Question_body":"<p>I wanted to compare the execution speeds of three data types. The runs were organized in sequence of <code>Original<\/code>, <code>DictList<\/code>, <code>DataFrame<\/code>. So <code>Original<\/code> was the first run. The x-axis is set as <code>Relative Time (Process)<\/code><\/p>\n<p>Problem is that each runs starting time is all different! How can I make sure that they all start at time 0?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PotUf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PotUf.png\" alt=\"Loss Graph\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" alt=\"Elapsed Steps Graph\" \/><\/a><\/p>\n<p>I can't post the entire code. I'll post every point where <code>wandb<\/code> API is called.<\/p>\n<pre><code>import wandb\nif __name__ == &quot;__main__&quot;:\n    wandb.login()\n    datatypes = {&quot;Original&quot;: Original, &quot;DictList&quot;: DictList, &quot;DataFrame&quot;: DataFrame}\n    for type_name, datatype in datatypes.items():\n        wandb_run = wandb.init(project=&quot;Compare&quot;, name=type_name, reinit=True)\n        with wandb_run:\n            # Initialize RL training session\n            storage = datatype()\n            # Run RL training session\n            # Log (loss, elapsed_steps, etc.)    \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659093160443,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":13,
        "Owner_creation_time":1502879704843,
        "Owner_last_access_time":1664071330100,
        "Owner_location":"South Korea",
        "Owner_reputation":1248,
        "Owner_up_votes":800,
        "Owner_down_votes":23,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73165796",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71654404,
        "Question_title":"How to fix Wandb API giving error 400 when deleting artifacts?",
        "Question_body":"<p>I accidentally logged way too much to wandb and would like to delete some artifacts. I've tried the following script, but I get an error 400 whenever I run it:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\n\nwandb.login(key=KEY)\n\napi = wandb.Api(overrides={&quot;project&quot;: PROJECT, &quot;entity&quot;: ENTITY})\n\nfor run in api.runs():\n    files = sorted([f for f in run.logged_artifacts()], key=lambda f: f.updated_at)\n    print(&quot;Total files:&quot;, len(files))\n    print(&quot;Last file:&quot;, files[-1].name)\n    print(&quot;Last file date:&quot;, files[-1].updated_at)\n    for f in files[:-1]:\n        if &quot;.tar&quot; in f.name:\n            # also tried just f.delete()\n            a = api.artifact(f&quot;{PROJECT}\/{f.name}&quot;)\n            print(&quot;Deleting {}&quot;.format(f.name))\n            a.delete()\n<\/code><\/pre>\n<p>All I get is <code>requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648506655603,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":349,
        "Owner_creation_time":1349927421380,
        "Owner_last_access_time":1664055989120,
        "Owner_location":null,
        "Owner_reputation":2248,
        "Owner_up_votes":69,
        "Owner_down_votes":9,
        "Owner_views":403,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71654404",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73642527,
        "Question_title":"How to install wandb on a docker image for arm?",
        "Question_body":"<p>My docker building failed at the <code>RUN <\/code><\/p>\n<p>with:<\/p>\n<pre><code>(meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/Dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/\n\n[+] Building 184.7s (20\/28)\n =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s\n =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io\/continuumio\/miniconda3:latest                                                                           0.0s\n =&gt; [ 1\/24] FROM docker.io\/continuumio\/miniconda3                                                                                                  0.0s\n =&gt; https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main                                                                                     0.3s\n =&gt; CACHED [ 2\/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s\n =&gt; CACHED [ 3\/24] RUN useradd -m bot                                                                                                              0.0s\n =&gt; CACHED [ 4\/24] WORKDIR \/home\/bot                                                                                                               0.0s\n =&gt; CACHED [ 5\/24] ADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json                                                     0.0s\n =&gt; CACHED [ 6\/24] RUN opam init --disable-sandboxing                                                                                              0.0s\n =&gt; CACHED [ 7\/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s\n =&gt; CACHED [ 8\/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s\n =&gt; CACHED [ 9\/24] RUN eval $(opam env)                                                                                                            0.0s\n =&gt; CACHED [10\/24] RUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released                                                               0.0s\n =&gt; CACHED [11\/24] RUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released                                  0.0s\n =&gt; CACHED [12\/24] RUN opam update --all                                                                                                           0.0s\n =&gt; CACHED [13\/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s\n =&gt; [14\/24] RUN opam install -y coq-serapi                                                                                                       176.3s\n =&gt; [15\/24] RUN eval $(opam env)                                                                                                                   0.2s\n =&gt; ERROR [16\/24] RUN pip install wandb --upgrade                                                                                                  8.0s\n------\n &gt; [16\/24] RUN pip install wandb --upgrade:\n#20 0.351 Defaulting to user installation because normal site-packages is not writeable\n#20 0.637 Collecting wandb\n#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n#20 1.365 Requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (61.2.0)\n#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (1.16.0)\n#20 1.409 Collecting promise&lt;3,&gt;=2.0\n#20 1.472   Downloading promise-2.3.tar.gz (19 kB)\n#20 2.087 Collecting PyYAML\n#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)\n#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0\n#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)\n#20 2.648 Collecting setproctitle\n#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n#20 2.763 Collecting Click!=8.0.0,&gt;=7.0\n#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n#20 2.902 Collecting sentry-sdk&gt;=1.0.0\n#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n#20 3.112 Collecting psutil&gt;=5.0.0\n#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)\n#20 3.871 Collecting pathtools\n#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)\n#20 4.431 Collecting shortuuid&gt;=0.5.0\n#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (2.27.1)\n#20 4.568 Collecting docker-pycreds&gt;=0.4.0\n#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n#20 4.695 Collecting GitPython&gt;=1.0.0\n#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1\n#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1\n#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)\n#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)\n#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)\n#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)\n#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1\n#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n#20 5.172 Building wheels for collected packages: promise, psutil, pathtools\n#20 5.172   Building wheel for promise (setup.py): started\n#20 5.851   Building wheel for promise (setup.py): finished with status 'done'\n#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae\n#20 5.852   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n#20 5.854   Building wheel for psutil (setup.py): started\n#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'\n#20 6.226   ERROR: Command errored out with exit status 1:\n#20 6.226    command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb\n#20 6.226        cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 6.226   Complete output (45 lines):\n#20 6.226   running bdist_wheel\n#20 6.226   running build\n#20 6.226   running build_py\n#20 6.226   creating build\n#20 6.226   creating build\/lib.linux-aarch64-3.9\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   running build_ext\n#20 6.226   building 'psutil._psutil_linux' extension\n#20 6.226   creating build\/temp.linux-aarch64-3.9\n#20 6.226   creating build\/temp.linux-aarch64-3.9\/psutil\n#20 6.226   gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 6.226   ----------------------------------------\n#20 6.226   ERROR: Failed building wheel for psutil\n#20 6.226   Running setup.py clean for psutil\n#20 6.550   Building wheel for pathtools (setup.py): started\n#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'\n#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82\n#20 7.135   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n#20 7.136 Successfully built promise pathtools\n#20 7.136 Failed to build psutil\n#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb\n#20 7.262   WARNING: The script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on PATH.\n#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n#20 7.345     Running setup.py install for psutil: started\n#20 7.727     Running setup.py install for psutil: finished with status 'error'\n#20 7.727     ERROR: Command errored out with exit status 1:\n#20 7.727      command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil\n#20 7.727          cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 7.727     Complete output (47 lines):\n#20 7.727     running install\n#20 7.727     \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n#20 7.727       warnings.warn(\n#20 7.727     running build\n#20 7.727     running build_py\n#20 7.727     creating build\n#20 7.727     creating build\/lib.linux-aarch64-3.9\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     running build_ext\n#20 7.727     building 'psutil._psutil_linux' extension\n#20 7.727     creating build\/temp.linux-aarch64-3.9\n#20 7.727     creating build\/temp.linux-aarch64-3.9\/psutil\n#20 7.727     gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 7.727     ----------------------------------------\n#20 7.728 ERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil Check the logs for full command output.\n------\nexecutor failed running [\/bin\/sh -c pip install wandb --upgrade]: exit code: 1\n<\/code><\/pre>\n<p>why?<\/p>\n<p>Docker file so far:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace\n\nRUN useradd -m bot\nWORKDIR \/home\/bot\nUSER bot\n\n## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en\n#RUN wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh  \\\n#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/bin:${PATH}&quot;\n#RUN conda create -n pycoq python=3.9 -y\n## somehow this &quot;works&quot; but conda isn't fully aware of this. Fix later?\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${PATH}&quot;\n\nADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json\n\n# -- setup opam like VP's PyCoq\nRUN opam init --disable-sandboxing\n# compiler + '_' + coq_serapi + '.' + coq_serapi_pin\nRUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda\nRUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1\nRUN eval $(opam env)\n\nRUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released\n# RUN opam pin add -y coq 8.11.0\n# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released']\nRUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released\nRUN opam update --all\nRUN opam pin add -y coq 8.11.0\n\n#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1\nRUN opam install -y coq-serapi\n\nRUN eval $(opam env)\n\n# makes sure depedencies for pycoq are installed once already in the docker image\nENV WANDB_API_KEY=&quot;SECRET&quot;\nRUN pip install wandb --upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1662595635817,
        "Question_score":0,
        "Question_tags":"python|linux|docker|anaconda|wandb",
        "Question_view_count":49,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73642527",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71744288,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649110110203,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|huggingface-tokenizers|fine-tune|wandb|huggingface",
        "Question_view_count":289,
        "Owner_creation_time":1499867951607,
        "Owner_last_access_time":1661234383987,
        "Owner_location":null,
        "Owner_reputation":307,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649156563120,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72166449,
        "Question_title":"Weights and Biases error: The wandb backend process has shutdown",
        "Question_body":"<p>running the colab linked below, I get the following error:<\/p>\n<p>&quot;The wandb backend process has shutdown&quot;<\/p>\n<p>I see nothing suspicious in the way the colab uses wandb and I couldn't find anyone with the same problem. Any help is greatly appreciated. I am using the latest version of wandb in colab.<\/p>\n<p>This is where I set up wandb:<\/p>\n<pre><code>if WANDB:\n    wandb.login()\n<\/code><\/pre>\n<p>and this is the part where I get the error:<\/p>\n<pre><code>#setup wandb if we're using it\n\nif WANDB:\n    experiment_name = os.environ.get(&quot;EXPERIMENT_NAME&quot;)\n    group = experiment_name if experiment_name != &quot;none&quot; else wandb.util.generate_id()\n\ncv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f&quot;Fold {fold}\/{config.folds}&quot;, end=&quot;\\n&quot;*2)\n    fold_directory = os.path.join(config.output_directory, f&quot;fold_{fold}&quot;)    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, &quot;model.pth&quot;)\n    model_config_path = os.path.join(fold_directory, &quot;model_config.json&quot;)\n    checkpoints_directory = os.path.join(fold_directory, &quot;checkpoints\/&quot;)\n    make_directory(checkpoints_directory)\n    \n    #Data collators are objects that will form a batch by using a list of dataset elements as input.\n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[&quot;fold&quot;].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[&quot;anchor&quot;].values, \n                            pair_texts=train_fold[&quot;target&quot;].values,\n                            contexts=train_fold[&quot;title&quot;].values,\n                            targets=train_fold[&quot;score&quot;].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f&quot;Train samples: {len(train_dataset)}&quot;)\n    \n    validation_fold = train[train[&quot;fold&quot;].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[&quot;anchor&quot;].values, \n                                 pair_texts=validation_fold[&quot;target&quot;].values,\n                                 contexts=validation_fold[&quot;title&quot;].values,\n                                 targets=validation_fold[&quot;score&quot;].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=True, \n                                   drop_last=False)\n    \n    print(f&quot;Validation samples: {len(validation_dataset)}&quot;)\n\n\n    model = Model(**config.model)\n    \n    if not os.path.exists(model_config_path): \n        model.config.to_json_file(model_config_path)\n    \n    model_parameters = model.parameters()\n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    training_steps = len(train_loader) * config.epochs\n    \n    if &quot;scheduler&quot; in config:\n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(&quot;warmup&quot;, 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=&quot;min&quot;, \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=&quot;checkpoint.pth&quot;, \n                                       num_candidates=1)\n\n\n    if WANDB:\n        wandb.init()\n        #wandb.init(group=group, name=f&quot;fold_{fold}&quot;, config=config)\n    \n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=&quot;tqdm&quot;)\n    \n    if WANDB:\n        wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f&quot;Model's path: {model_path}&quot;)\n    \n    validation_fold[&quot;prediction&quot;] = validation_outputs.to(&quot;cpu&quot;).numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n    \n    cv_monitor_value = validation_loss if config.cv_monitor_value == &quot;loss&quot; else validation_metrics[config.cv_monitor_value]\n    cv_scores.append(cv_monitor_value)\n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=&quot;\\n&quot;*6)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652063587210,
        "Question_score":6,
        "Question_tags":"python|wandb",
        "Question_view_count":1610,
        "Owner_creation_time":1622105241727,
        "Owner_last_access_time":1661260334760,
        "Owner_location":"Berlin, Deutschland",
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72166449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73412851,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660891849347,
        "Question_score":0,
        "Question_tags":"python|wandb",
        "Question_view_count":61,
        "Owner_creation_time":1557474244067,
        "Owner_last_access_time":1663941664767,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":120,
        "Owner_down_votes":3,
        "Owner_views":140,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1660907847087,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":65392269,
        "Question_title":"access logged values during runtime",
        "Question_body":"<p>How can I retrieve a logged value from wandb before the run was finished?<\/p>\n<pre><code>import os\nimport wandb\nwandb.init(project='someproject')\n\n\ndef loss_a():\n    # do_stuff and log:\n    wandb.log({&quot;loss_a&quot;: 1.0})\n    \ndef loss_b():\n    # do_stuff and log:\n    wandb.log({&quot;loss_b&quot;: 2.0})\n\nfor epoch in range(2):\n    loss_a()\n    loss_b()\n    \n    # somehow retrieve loss_a and loss_b and print them here:\n    print(f'loss_a={??}, loss_b={??}')\n\n<\/code><\/pre>\n<p>After run was finished I can find it with <code>wandb.Api<\/code> to get <code>run.history<\/code>. But it seems that before run was fininshed, accessing <code>run.history<\/code> doesn't work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608551470203,
        "Question_score":0,
        "Question_tags":"machine-learning|wandb",
        "Question_view_count":302,
        "Owner_creation_time":1304429242790,
        "Owner_last_access_time":1663848455777,
        "Owner_location":null,
        "Owner_reputation":3201,
        "Owner_up_votes":946,
        "Owner_down_votes":3,
        "Owner_views":100,
        "Question_last_edit_time":1661850170530,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65392269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71195699,
        "Question_title":"wandb [SSL: WRONG_VERSION_NUMBER] error only thrown in Pycharm using python 3.8",
        "Question_body":"<p>I log the metrics of the training results on the wandb online server. This was working without any problems, till the beginning of this week. Since then i am suddenly unable to connect to the wandb online server and loggin the metrics isn't working anymore. I on Windows 10 using PyCharm with python version 3.8.<\/p>\n<p>The following exception is thrown:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 696, in urlopen\n    self._prepare_proxy(conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 964, in _prepare_proxy\n    conn.connect()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 364, in connect\n    conn = self._connect_tls_proxy(hostname, conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 501, in _connect_tls_proxy\n    socket = ssl_wrap_socket(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 453, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 495, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1040, in _create\n    self.do_handshake()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 439, in send\n    resp = conn.urlopen(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 755, in urlopen\n    retries = retries.increment(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\retry.py&quot;, line 574, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py&quot;, line 132, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\transport\\requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 117, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 655, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 514, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\nwandb: Network error (SSLError), entering retry loop.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: Network error (SSLError), entering retry loop.\n<\/code><\/pre>\n<p>Strange thing is, if i execute the run configuration used in pycharm as a command from it's built in terminal it works fine. Since the same virtual environment is used on the built in terminal i don't understand why this exceptions is thrown if using the run configuration in Pycharm. What am i missing here?<\/p>\n<p>Any help would be appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645369500377,
        "Question_score":1,
        "Question_tags":"python|python-3.x|ssl|pycharm|wandb",
        "Question_view_count":259,
        "Owner_creation_time":1644055951303,
        "Owner_last_access_time":1651943996327,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71195699",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71257152,
        "Question_title":"How to show wandb training progress from run folder",
        "Question_body":"<p>After training neural networks with wandb as the logger, I received a link to show the training results and a folder named &quot;run-...&quot;, I assume that is the logging of the training process. Now I don't have that link, how to show the wandb training process from run folder?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645730259610,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|pytorch|wandb",
        "Question_view_count":86,
        "Owner_creation_time":1504450759857,
        "Owner_last_access_time":1660667476990,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":83,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1645730901420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71257152",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":63469762,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597757510213,
        "Question_score":2,
        "Question_tags":"python|keras|k-fold|wandb",
        "Question_view_count":1043,
        "Owner_creation_time":1486549300030,
        "Owner_last_access_time":1620240688720,
        "Owner_location":"Germany",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1661849871017,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1598032113043,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1599772735680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Question_exclusive_tag":"Weights & Biases"
    }
]