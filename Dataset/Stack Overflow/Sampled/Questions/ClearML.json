[
    {
        "Question_id":68798737,
        "Question_title":"ClearML How to get configurable hyperparameters?",
        "Question_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-16 07:13:10.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"devops|mlops|clearml|trains",
        "Question_view_count":110,
        "Owner_creation_date":"2021-03-17 19:13:18.583 UTC",
        "Owner_last_access_date":"2022-05-12 08:16:10.277 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-08-16 14:06:01.02 UTC",
        "Answer_score":0.0,
        "Owner_location":"Islamabad, Pakistan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65509754,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-12-30 15:54:39.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-31 15:03:29.85 UTC",
        "Question_score":5,
        "Question_tags":"trains|clearml",
        "Question_view_count":740,
        "Owner_creation_date":"2013-10-29 21:50:14.23 UTC",
        "Owner_last_access_date":"2022-09-23 12:45:41.383 UTC",
        "Owner_reputation":2801,
        "Owner_up_votes":371,
        "Owner_down_votes":0,
        "Owner_views":131,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-30 16:18:59.523 UTC",
        "Answer_score":6.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-08-24 07:33:21.413 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65655382,
        "Question_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-10 16:09:56.32 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-10 16:36:37.73 UTC",
        "Question_score":2,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-10 18:24:55.157 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63586251,
        "Question_title":"Will Trains automagically log Tensorboard HParams?",
        "Question_body":"<p>I know that it's possible to send hyper-params as a dictionary to Trains.<\/p>\n<p>But can it also automagically log hyper-params that are logged using the TF2 HParams module?<\/p>\n<p>Edit: This is done in the <a href=\"https:\/\/www.tensorflow.org\/tensorboard\/hyperparameter_tuning_with_hparams\" rel=\"nofollow noreferrer\">HParams tutorial<\/a> using <code>hp.hparams(hparams)<\/code>.<\/p>\n<p><img src=\"https:\/\/www.tensorflow.org\/tensorboard\/images\/hparams_parallel_coordinates.png?raw=1\" alt=\"Tensorboard HParams\" \/><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-25 19:59:10.273 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-31 15:11:37.023 UTC",
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":142,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63586251",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":69127220,
        "Question_title":"how to capture logger values using clearml",
        "Question_body":"<p>I am using clearml for testing algorithms and it works well with library <a href=\"https:\/\/github.com\/DLR-RM\/stable-baselines3\/tree\/f8a08690737000e4d23eb643a21d70486ece038b\" rel=\"nofollow noreferrer\">Stable Baselines 3<\/a>, in which clearml automatically captures all the output and plot them in the Scalars tab.<\/p>\n<p>However, when I switched to another library <a href=\"https:\/\/github.com\/pfnet\/pfrl\" rel=\"nofollow noreferrer\">PFRL<\/a> clearml no longer output anything to the Scalar tab. After looking into the code I found PFRL outputs statistics using <code>logger.info<\/code>, which seemed to be the reason of empty Scalars tab (but there was output in the Console tab).\nI am wondering is there any method that I can make clearml automatically collect them into the Scalars tab.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-10 04:19:25.663 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pytorch|clearml",
        "Question_view_count":97,
        "Owner_creation_date":"2021-09-10 04:10:13.317 UTC",
        "Owner_last_access_date":"2022-07-19 03:27:47.683 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69127220",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72381916,
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 17:24:40.97 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":25,
        "Owner_creation_date":"2022-05-25 17:13:50.777 UTC",
        "Owner_last_access_date":"2022-09-02 10:09:50.107 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-08 21:03:38.003 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70397010,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-17 17:49:10.487 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":194,
        "Owner_creation_date":"2020-09-14 23:01:38.003 UTC",
        "Owner_last_access_date":"2022-08-25 14:13:24.02 UTC",
        "Owner_reputation":46,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-22 08:33:57.74 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56744397,
        "Question_title":"Is there a way to create a graph comparing hyper-parameters vs model accuracy with TRAINS python package?",
        "Question_body":"<p>I would like to run multiple experiments, then report model accuracy per experiment.<\/p>\n\n<p>I'm training a toy MNIST example with pytorch (v1.1.0), but the goal is, once I can compare performance for the toy problem, to have it integrated with the actual code base.<\/p>\n\n<p>As I understand the TRAINS python package, with the \"two lines of code\" all my hyper-parameters are already logged (Command line argparse in my case). <\/p>\n\n<p>What do I need to do in order to report a final scalar and then be able to sort through all the different training experiments (w\/ hyper-parameters) in order to find the best one.<\/p>\n\n<p>What I'd like to get, is a graph\/s where on the X-axis I have hyper-parameter values and on the Y-axis I have the validation accuracy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-24 21:54:32.967 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-05 15:40:56.56 UTC",
        "Question_score":1,
        "Question_tags":"python|deep-learning|pytorch|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_date":"2012-07-21 15:18:16.543 UTC",
        "Owner_last_access_date":"2019-12-26 20:10:54.037 UTC",
        "Owner_reputation":81,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56744397",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66279581,
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-19 14:37:16.903 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-22 14:29:19.64 UTC",
        "Question_score":1,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":279,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-02-19 22:31:43.383 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66320238,
        "Question_title":"ClearML get max value from logged values",
        "Question_body":"<p>I use ClearML to track my tensorboard logs (from PyTorch Lightning) during training.\nAt a point later I start another script which connects to existing task and do some testing.<\/p>\n<p>But unfortenautly I do not have all information in the second script, so I want to query them from the logged values from ClearML server.<\/p>\n<p>How would I do this?<\/p>\n<p>I thought about something like this, but havn't found anything in documentation:<\/p>\n<pre><code>task = Task.init(project_name=&quot;Project&quot;, task_name=&quot;name&quot;, reuse_last_task_id=&quot;Task_id, continue_last_task=True)\nx_value, y_value = task.get_value(key=&quot;val\/acc&quot;, mode=&quot;max&quot;)\nx_value2, y_value2 = task.get_value(key=&quot;epoch&quot;, mode=&quot;x&quot;, x=x_value)\n<\/code><\/pre>\n<ul>\n<li><code>x_value<\/code> would be my epoch or global step<\/li>\n<li><code>y_value<\/code> the maximum value of plot &quot;val\/acc&quot;<\/li>\n<li><code>x_value2<\/code> would be my epoch or global step<\/li>\n<li><code>y_value2<\/code> the value of plot &quot;epoch&quot; at <code>x_value<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-22 17:10:17.123 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-24 11:12:09.957 UTC",
        "Question_score":3,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":101,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To get an existing <code>Task<\/code> object for a running (or completed\/failed) experiment, assuming we know Task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(task_id='aabbcc')\n<\/code><\/pre>\n<p>If we only know the Task project\/name<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(project_name='the project', task_name='the name')\n<\/code><\/pre>\n<p>Notice that if you have multiple task under the same name it will return the most updated one.\nOnce we have the <code>Task<\/code> object, we can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>latest_scalar_values_dict = another_task.get_last_scalar_metrics()\n<\/code><\/pre>\n<p>Which would return all the scalars min\/maxm\/last values, for example:<\/p>\n<pre><code>latest_scalar_values_dict = {\n            'title': {\n                'series': {\n                    'last': 0.5,\n                    'min': 0.1,\n                    'max': 0.9\n                    }\n                }\n            }\n<\/code><\/pre>\n<p><a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get%20metrics#clearml.task.Task.get_last_scalar_metrics\" rel=\"nofollow noreferrer\">documentation here<\/a><\/p>\n<p>If you need to get the entire graphs you can use <code>task.get_reported_scalars()<\/code> <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get_reported_scalars#clearml.task.Task.get_reported_scalars\" rel=\"nofollow noreferrer\">see docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-25 01:04:21.4 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320238",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73279794,
        "Question_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Question_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-08 14:47:32.813 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|catboost|clearml",
        "Question_view_count":54,
        "Owner_creation_date":"2022-08-08 14:30:03.173 UTC",
        "Owner_last_access_date":"2022-09-23 21:14:51.43 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-08 20:52:18.293 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72593187,
        "Question_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-12 14:40:00.58 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":49,
        "Owner_creation_date":"2011-08-25 22:58:29.233 UTC",
        "Owner_last_access_date":"2022-09-24 23:30:23.147 UTC",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Answer_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-14 11:53:01.543 UTC",
        "Answer_score":2.0,
        "Owner_location":"Technion, Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66640850,
        "Question_title":"How to manage datasets in ClearML Web UI?",
        "Question_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-15 15:24:35.663 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-15 15:30:13.44 UTC",
        "Question_score":3,
        "Question_tags":"clearml|trains",
        "Question_view_count":310,
        "Owner_creation_date":"2019-03-07 12:13:00.55 UTC",
        "Owner_last_access_date":"2022-09-19 19:03:50.163 UTC",
        "Owner_reputation":354,
        "Owner_up_votes":54,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Answer_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-15 17:59:46.403 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70423979,
        "Question_title":"Export metrics of ClearML to Prometheus and show them in Grafana",
        "Question_body":"<p>Are there any metrics I can get from the API server? or any docker image I can point to the backend and get some metrics?\nMost important is the see how many tasks running in real-time (like we can see on the worker's page) and also check how much time each task is running (also can be found on the worker's page)<\/p>\n<p>If it does not exist, do they have an API for getting all this information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-20 15:16:30.177 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"prometheus|grafana|metrics|clearml",
        "Question_view_count":70,
        "Owner_creation_date":"2019-09-21 19:04:11.057 UTC",
        "Owner_last_access_date":"2022-09-06 09:31:53.617 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70423979",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62654203,
        "Question_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Question_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-30 09:21:43.533 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-01 02:21:08.433 UTC",
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":228,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-06-30 11:12:59.54 UTC",
        "Answer_score":1.0,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":"2020-06-30 22:50:56.893 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65671395,
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-11 17:06:18.623 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-11 18:39:05.337 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":71432331,
        "Question_title":"ALB host based routing without domain name",
        "Question_body":"<p>I'm trying to configure host based routing in AWS ALB for ClearML server using <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/deploying_clearml\/clearml_server_config.html#configuration-procedures\" rel=\"nofollow noreferrer\">this tutorial<\/a>.\nHowever, I don't have a domain name. So can I only use alb's dns for this routing?<\/p>\n<p>For example, I will have the address as app.<em><strong>.ap-north-east-1.elb.amazonaws.com, api.<\/strong><\/em>.ap-north-east-1.elb.amazonaws.com.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-11 00:25:08.15 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"aws-application-load-balancer|clearml",
        "Question_view_count":190,
        "Owner_creation_date":"2018-10-01 12:52:13.233 UTC",
        "Owner_last_access_date":"2022-08-13 09:22:52.563 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"\u014csaka-shi, \u5927\u962a\u5e9c \u65e5\u672c",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432331",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63606182,
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-26 21:47:08.03 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-31 15:11:39.19 UTC",
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":107,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-27 13:50:33.437 UTC",
        "Answer_score":2.0,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73632015,
        "Question_title":"ClearML, how to query the best performing model for a specific project and metric",
        "Question_body":"<p>I want to download the best performing model for a certain ClearlML project. I have the following content in my ClearML experiment platform:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>According to: <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models<\/a> I can get a list of models for a specific project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list = Model.query_models(\n    # Only models from `examples` project\n    project_name='YOLOv5', \n    # Only models with input name\n    model_name=None,\n    # Only models with `demo` tag but without `TF` tag\n    tags=['demo', '-TF'],\n    # If `True`, only published models\n    only_published=False,\n    # If `True`, include archived models\n    include_archived=True,\n    # Maximum number of models returned\n    max_results=5\n)\n\nprint(model_list)\n<\/code><\/pre>\n<p>Which prints:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[&lt;clearml.model.Model object at 0x7fefbaf22130&gt;, &lt;clearml.model.Model object at 0x7fefbaf22340&gt;]\n<\/code><\/pre>\n<p>So I can run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list[0].get_local_copy()\n<\/code><\/pre>\n<p>and get this specific model. But how do I download the best performing one for this project on a certain metric (in this case mAP_0.5:0.95 MAX)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-07 08:02:57 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-08 14:05:14.387 UTC",
        "Question_score":0,
        "Question_tags":"python|yolov5|clearml",
        "Question_view_count":48,
        "Owner_creation_date":"2018-03-17 08:53:35.913 UTC",
        "Owner_last_access_date":"2022-09-25 05:02:29.74 UTC",
        "Owner_reputation":731,
        "Owner_up_votes":56,
        "Owner_down_votes":8,
        "Owner_views":41,
        "Answer_body":"<p>I ended up doing the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    import clearml\n    from clearml import Dataset, Task, Model, OutputModel\n    assert hasattr(clearml, '__version__')  # verify package import not local dir\nexcept (ImportError, AssertionError):\n    clearml = None\n\ntasks = Task.get_tasks(project_name='YOLOv5', task_name='exp', task_filter={'status': ['completed']})\n\nresults = {}\nbest_task = None\nfor task in tasks:\n    results[task.id] = task.get_last_scalar_metrics()['metrics']['mAP_0.5:0.95']['max']\n\nbest_model_task_id = max(results, key=results.get)\nmodel_list = Task.get_task(best_model_task_id).get_models()\ndest = model_list['output'][0].get_local_copy()\nprint('Saved model at:', dest)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-07 08:17:13.323 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-09-07 12:23:52.007 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73632015",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73442798,
        "Question_title":"Does ClearML have accounting of information security events",
        "Question_body":"<p>ClearML is one of the most famous MLOps tools existing. It has logging of machine learning processes, however I couldn't find any information regarding its system of accounting of <strong>information security events<\/strong>.<\/p>\n<p>My question is: does ClearML have such system? Does it register\/log events of client-server interaction? If ClearML does, then what format is used?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-22 09:19:05.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"logging|clearml",
        "Question_view_count":37,
        "Owner_creation_date":"2022-08-22 09:04:07.617 UTC",
        "Owner_last_access_date":"2022-09-22 13:02:34.37 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73442798",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56768436,
        "Question_title":"How to Backup\/Restore TRAINS-server when moving from AMI to local machine",
        "Question_body":"<p>I recently started using TRAINS, with the server in AWS AMI. We are currently using v0.9.0.<\/p>\n\n<p>I would like to move the TRAINS-server to run on our on-premises kubernetes cluster. However, I don't want to lose the data on the current server in AWS (experiments, models, logins, etc...).\nIs there a way to backup the current server and restore it to the local server?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-26 08:27:33.917 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-05 15:40:59.227 UTC",
        "Question_score":1,
        "Question_tags":"kubernetes|deep-learning|trains|clearml",
        "Question_view_count":2703,
        "Owner_creation_date":"2016-11-08 19:13:17.167 UTC",
        "Owner_last_access_date":"2019-06-26 11:21:41.617 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56768436",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":58656998,
        "Question_title":"Parallel Coordinates Plot in TRAINS",
        "Question_body":"<p>Is there a way to create a parallel coordinates plot in TRAINS (<a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">https:\/\/github.com\/allegroai\/trains<\/a>) package to compare several hyper-parameters in respect to a specific metric?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-01 09:39:12.533 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-04 17:26:50.993 UTC",
        "Question_score":2,
        "Question_tags":"python|deep-learning|trains|clearml",
        "Question_view_count":57,
        "Owner_creation_date":"2013-07-17 06:10:02.27 UTC",
        "Owner_last_access_date":"2020-01-23 21:26:10.29 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58656998",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73570817,
        "Question_title":"ClearML - dynamically updating Plotly plots?",
        "Question_body":"<p>I have a question related to ClearML plot logging. We are currently using:<\/p>\n<pre><code>self.task_logger.report_table(&quot;TableSpaceName&quot;, &quot;Some Info&quot;, iteration=0, table_plot=df)\n<\/code><\/pre>\n<p>To report tables. They appear under &quot;PLOTS&quot; section. Similarly, we are reporting plotly graphs:<\/p>\n<pre><code>self.task_logger.report_plotly(\n        title=&quot;PlotTitle&quot;, iteration=0, series='SeriesName', figure=fig\n    )\n<\/code><\/pre>\n<p>Both work fine. The issue is, each new <code>report_plotly<\/code> call, instead of replacing the image in the section, creates a new one, and leaves the previous one present too. This cloggs the PLOTS section (tables and figures). The question is, how does one report a plot, so that it's reported in-place (Such as e.g., scalars, where sample plot gets updated in time)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-01 14:11:36.99 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-01 14:18:53.803 UTC",
        "Question_score":0,
        "Question_tags":"python|plot|clearml",
        "Question_view_count":23,
        "Owner_creation_date":"2014-04-11 11:22:10.877 UTC",
        "Owner_last_access_date":"2022-09-24 18:35:51.867 UTC",
        "Owner_reputation":1968,
        "Owner_up_votes":43,
        "Owner_down_votes":6,
        "Owner_views":190,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73570817",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73108016,
        "Question_title":"Where do augmentations in ClearML run?",
        "Question_body":"<p>In ClearML Dataviews, it is possible to add <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/hyperdatasets\/dataviews\/#data-augmentation\" rel=\"nofollow noreferrer\">augmentations<\/a>.<\/p>\n<p>Where do these augmentations run?<\/p>\n<p>Options<\/p>\n<ol>\n<li>Original data gets downloaded to local, then runs (on which device? How is multiprocessing handled?)<\/li>\n<li>Only augmented data gets downloaded to local cache, augmentations run remotely (who pays for compute? How fast? Should pipelines be changed accordingly?)<\/li>\n<\/ol>\n<p>I couldn't find this in the docs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-25 10:52:01.69 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"deep-learning|data-augmentation|mlops|clearml",
        "Question_view_count":22,
        "Owner_creation_date":"2011-08-25 22:58:29.233 UTC",
        "Owner_last_access_date":"2022-09-24 23:30:23.147 UTC",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Technion, Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73108016",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64305945,
        "Question_title":"pip install trains fails",
        "Question_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2020-10-11 15:44:31.093 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-30 18:45:56.11 UTC",
        "Question_score":1,
        "Question_tags":"python|pip|trains|clearml",
        "Question_view_count":1031,
        "Owner_creation_date":"2011-08-25 22:58:29.233 UTC",
        "Owner_last_access_date":"2022-09-24 23:30:23.147 UTC",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Answer_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-15 13:18:50.97 UTC",
        "Answer_score":1.0,
        "Owner_location":"Technion, Israel",
        "Answer_last_edit_date":"2020-11-02 09:09:43.097 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70592241,
        "Question_title":"How to Deploy a ClearML Agent in a k8s setting?",
        "Question_body":"<p>I want to deploy a ClearML agent in the kubernetes environment while using the ClearML's Free Tier Demo server.\nI was able to deploy the Agent pod in the k8s cluster with the <code>allegroai\/clearml-agent<\/code> docker image. But was not able to link this agent to the ClearML Demo server.\nCan anyone help me with solving this issue of configuring the API access and secret keys for the k8s pod of CLearML Agent.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-05 11:40:24.047 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"kubernetes|clearml",
        "Question_view_count":112,
        "Owner_creation_date":"2022-01-05 11:34:02.503 UTC",
        "Owner_last_access_date":"2022-03-11 13:38:29.747 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70592241",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":57557070,
        "Question_title":"trains with grid search",
        "Question_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-19 12:46:46.593 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-04 16:44:00.297 UTC",
        "Question_score":1,
        "Question_tags":"python|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_date":"2014-11-25 19:03:49.38 UTC",
        "Owner_last_access_date":"2022-09-22 18:38:52.353 UTC",
        "Owner_reputation":161,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-19 18:51:44.54 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2019-08-19 19:15:22.213 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":67496760,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-12 02:58:51.577 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-14 19:57:30.56 UTC",
        "Question_score":1,
        "Question_tags":"docker|amazon-s3|wsl-2|rclone|clearml",
        "Question_view_count":770,
        "Owner_creation_date":"2013-03-06 14:43:00.91 UTC",
        "Owner_last_access_date":"2022-09-16 18:05:09.15 UTC",
        "Owner_reputation":4013,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-14 16:30:41.94 UTC",
        "Answer_score":0.0,
        "Owner_location":"Akron, OH, USA",
        "Answer_last_edit_date":"2021-05-14 17:07:32.113 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64666735,
        "Question_title":"How to fix trainserver empty server?",
        "Question_body":"<p>Im trying to install an allegroai trains-server on a k8s cluster.<\/p>\n<p>I tried the following 3 methods<\/p>\n<ul>\n<li><a href=\"https:\/\/allegro.ai\/docs\/deploying_trains\/trains_server_linux_mac\/\" rel=\"nofollow noreferrer\">bare linux installtion<\/a><\/li>\n<li><a href=\"https:\/\/allegro.ai\/docs\/deploying_trains\/trains_server_kubernetes\/\" rel=\"nofollow noreferrer\">k8s manifest installation<\/a><\/li>\n<li><a href=\"https:\/\/allegro.ai\/docs\/deploying_trains\/trains_server_kubernetes_helm\/\" rel=\"nofollow noreferrer\">helm installation<\/a><\/li>\n<\/ul>\n<p>I followed the linux installation to the letter,\nand in the k8s installations used the following command to access the exposed port of the trains webserver\nkubectl port-forward -n trains svc\/webserver-service 9999:80<\/p>\n<p>In all three cases i manage to get to the server, but it looks empty and most operations fail.<a href=\"https:\/\/i.stack.imgur.com\/OMhYi.png\" rel=\"nofollow noreferrer\">Here is a screen shot of what the webserver looks like<\/a>.<\/p>\n<p>I tried doing all 3 multiple times from scratch and even rebuilt my k8s cluster but nothing works.<\/p>\n<p>Does anyone know how to solve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-11-03 16:27:07.883 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-02 14:59:55.2 UTC",
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":47,
        "Owner_creation_date":"2020-11-03 16:20:18.717 UTC",
        "Owner_last_access_date":"2021-09-14 21:30:14.477 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64666735",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56762990,
        "Question_title":"How to manually register a sci-kit model with TRAINS python auto-magical experiment manager?",
        "Question_body":"<p>I'm working mostly with scikit-learn, as far as I understand, the TRAINS auto-magic doesn't catch scikit-learn model store\/load automatically.<\/p>\n\n<p>How do I manually register the model after I have 'pickled' it.<\/p>\n\n<p>For Example:<\/p>\n\n<pre><code>import pickle\nwith open(\"model.pkl\", \"wb\") as file:  \n    pickle.dump(my_model, file)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-06-25 22:52:28.14 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-04 16:43:46.447 UTC",
        "Question_score":1,
        "Question_tags":"python|machine-learning|scikit-learn|trains|clearml",
        "Question_view_count":155,
        "Owner_creation_date":"2018-06-11 14:35:01.343 UTC",
        "Owner_last_access_date":"2022-03-23 09:09:41.957 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56762990",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66317706,
        "Question_title":"ClearML Web UI custom column not persistent",
        "Question_body":"<p>I'm using the experiments page of a project in ClearML Web UI to visualize some custom metrics. Therefore I've customized my table vie (<a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/webapp\/webapp_exp_table.html?highlight=customize#adding-metrics\" rel=\"nofollow noreferrer\">https:\/\/allegro.ai\/clearml\/docs\/docs\/webapp\/webapp_exp_table.html?highlight=customize#adding-metrics<\/a>)\nBut whenever I leave the page for another project and go back, the table will be resetted.\nIs there a way to store configuration for a specific project?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-22 14:35:23.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-24 11:11:41.757 UTC",
        "Question_score":1,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":52,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66317706",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73456627,
        "Question_title":"ClearML Remote execution does not use requirements.txt",
        "Question_body":"<p>I want to execute a function on a worker with clearml. I use the code below to do this. But worker will always create a new env, although I set it to use my python binary via environmet settings. I also tried to pass a requirements.txt but this will be ignored or misinterpreted.\nThe main issue is that I need opencv_contrib, but clearml always installs opencv afterwards and the contrib package will get overwritten, so the contrib methods are not available and the remote execution fails.<\/p>\n<p>How can I force to use my existing python (conda) environment?\nHow can I force to install the packages I define in my requirements.txt?<\/p>\n<pre><code>    def my_func(path:str):\n        # do calculation\n        return \n    paths = ['\/media\/hdd\/my_folder_1\/']\n    for i, path in enumerate(paths):\n        task.create_function_task(my_func, func_name=my_func-{i}',\n                              task_name=f'my_func - {i}', path=path)\n<\/code><\/pre>\n<p>Starting script for my clearml worker<\/p>\n<pre><code>#!\/bin\/bash\n\nexport CLEARML_HOST_IP=127.0.0.1\nexport CLEARML_AGENT_SKIP_PIP_VENV_INSTALL=\/home\/user\/miniconda3\/envs\/my_env\/bin\/python\nclearml-agent daemon --detached --queue default\n<\/code><\/pre>\n<p>I'm using latest clearml on ubuntu 20.04.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-23 09:54:08.26 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":14,
        "Owner_creation_date":"2020-11-03 08:23:14.42 UTC",
        "Owner_last_access_date":"2022-09-20 10:40:35.373 UTC",
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73456627",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73716806,
        "Question_title":"How to make ClearML not upload annotations twice when they have the same ID?",
        "Question_body":"<p>The following uploads two annotations, though I expected there to be only one<\/p>\n<pre><code>from typing import List\nfrom allegroai import Dataset, DatasetVersion, SingleFrame, DataView\nfrom allegroai.dataframe.annotation import BoundingBox2D\n\nallegro_frame = SingleFrame(\n    source=&quot;\/irrelevant\/source.png&quot;\n)\nann_id = &quot;the_id&quot;\nlabel = &quot;the_label&quot;\nannotation = BoundingBox2D(id=ann_id)\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\n\nallegro_frames: List[SingleFrame] = [\n    allegro_frame\n]\n\ndataset_name = r&quot;clml_test_dataset&quot;\nversion_name = r&quot;clml_test_version&quot;\ndataset = Dataset.create(dataset_name=dataset_name)\nversion = DatasetVersion.create_version(dataset_name=dataset_name, version_name=version_name)\nversion.add_frames(allegro_frames)\n<\/code><\/pre>\n<p>What's the correct way to make only one annotation be uploaded for the frame?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-14 12:24:40.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":5,
        "Owner_creation_date":"2011-08-25 22:58:29.233 UTC",
        "Owner_last_access_date":"2022-09-24 23:30:23.147 UTC",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Technion, Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73716806",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62591953,
        "Question_title":"Does clone experiment work on sklearn functions?",
        "Question_body":"<p>I'm trying to run a script and I'm constantly getting this while cloning experiment in allegro.ai\nAttributeError: 'Namespace' object has no attribute 'get'\nCan anybody help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-06-26 09:35:25.703 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-01 11:36:41.127 UTC",
        "Question_score":0,
        "Question_tags":"trains|clearml",
        "Question_view_count":27,
        "Owner_creation_date":"2017-09-11 18:02:08.44 UTC",
        "Owner_last_access_date":"2020-06-26 09:33:06.227 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62591953",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62332672,
        "Question_title":"Tracking separate train\/test processes with Trains",
        "Question_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 20:05:31.703 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-04 14:36:45.757 UTC",
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":99,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-06-11 20:16:15.913 UTC",
        "Answer_score":1.0,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":"2020-06-22 13:43:37.2 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62552414,
        "Question_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Question_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-24 09:58:43.417 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-01 20:03:37.76 UTC",
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":129,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2020-06-24 11:14:51.033 UTC",
        "Answer_score":1.0,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64636294,
        "Question_title":"Trains: reusing previous task id",
        "Question_body":"<p>I am using <code>reuse_last_task_id=True<\/code> to overwrite an existing task (with same project and task name). But the experiment contains the torch model and therefore does not overwrite the existing task but creates a new one. How can I detach the model from the task?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-01 19:49:39.11 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-31 12:37:52.433 UTC",
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":133,
        "Owner_creation_date":"2014-09-11 05:09:09.42 UTC",
        "Owner_last_access_date":"2022-09-25 00:10:01.54 UTC",
        "Owner_reputation":187,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64636294",
        "Question_exclusive_tag":"ClearML"
    }
]