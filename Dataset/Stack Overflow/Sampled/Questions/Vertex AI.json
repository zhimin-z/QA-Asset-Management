[
    {
        "Question_id":69880325,
        "Question_title":"Got \"400 Request contains an invalid argument.\" error from Vertex AI",
        "Question_body":"<p>I uploaded my custom model on Vertex AI and used it for custom inference until last week.<\/p>\n<p>But today when I tried to inference with the same code(actually, it was the same code from the official <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">example code<\/a>), I received this error message.<\/p>\n<pre><code>InvalidArgument: 400 Request contains an invalid argument.\n<\/code><\/pre>\n<p>The detailed error message from grpc is below.<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.INVALID_ARGUMENT\ndetails = &quot;Request contains an invalid argument.&quot;\ndebug_error_string = &quot;{&quot;created&quot;:&quot;@1636358161.014643000&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:&lt;some_ip_address&gt;:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1070,&quot;grpc_message&quot;:&quot;Request contains an invalid argument.&quot;,&quot;grpc_status&quot;:3}&quot;&gt;\n<\/code><\/pre>\n<p>I've searched a couple of hours with this error message but I couldn't find any hints for solving this problem.<\/p>\n<p>I ran my code on the GCP VM instance, set the API endpoint and region with the model endpoint.\nAre there any recent changes from Vertex AI API?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-08 08:10:49.39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-08 23:23:20.607 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":338,
        "Owner_creation_date":"2021-08-27 00:34:21.567 UTC",
        "Owner_last_access_date":"2022-08-11 02:53:07.69 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69880325",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71755724,
        "Question_title":"Cannot use tensorboard with Vertex AI Custom job",
        "Question_body":"<p>I'm trying to launch a custom training job using Vertex AI through <a href=\"https:\/\/github.com\/deepmind\/xmanager\" rel=\"nofollow noreferrer\">XManager<\/a>. When running Custom jobs with tensorboard enabled I get a tensorboard instance in <code>experiments -&gt; tensorboard instances<\/code> and a button on the custom job page that says <code>OPEN TENSORBOARD<\/code>. However, this leads to an empty page that says <code>Not found: TensorboardExperiment<\/code>.<\/p>\n<ul>\n<li>I observed this behaviour when running my own custom job and when running XManager's example <a href=\"https:\/\/github.com\/deepmind\/xmanager\/tree\/main\/examples\/cifar10_tensorflow\" rel=\"nofollow noreferrer\">cifar10_tensorflow<\/a>. Note that in both cases the job runs to completion without problems.<\/li>\n<li>I can visualise the logs locally via the standard tensorboard package and passing as <code>log_dir<\/code> the cloud storage directory containing the experiments logs.<\/li>\n<li>I can upload experiment logs to Vertex AI tensorboard manually using<\/li>\n<\/ul>\n<pre><code>tb-gcp-uploader --tensorboard_resource_name \\\n  TENSORBOARD_INSTANCE_NAME \\\n  --logdir=LOG_DIR \\\n  --experiment_name=TB_EXPERIMENT_NAME --one_shot=True\n<\/code><\/pre>\n<ul>\n<li>For more details check out the discussion: <a href=\"https:\/\/github.com\/deepmind\/xmanager\/issues\/15\" rel=\"nofollow noreferrer\">https:\/\/github.com\/deepmind\/xmanager\/issues\/15<\/a><\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-05 17:04:41.173 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-05 17:39:30.237 UTC",
        "Question_score":1,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_date":"2016-01-04 14:55:18.69 UTC",
        "Owner_last_access_date":"2022-08-25 11:51:00.77 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71755724",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70855730,
        "Question_title":"Is it possible from a gcp vertex job to hit an http endpoint in another gcp pod in the same project?",
        "Question_body":"<p>I have a custom container vertex endpoint that is passed a url as input so that the job can call it to get a particular frame of data needed for the job. (gcs:\/\/ buckets do work) but I want to specifically use an http request to a server in the same gcp project.<\/p>\n<p>I have tried setting the endpoint up as private using the --networks param on the endpoint but then get the message:<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Making request from public OnePlatform API is not allowed on a private Endpoint peered with network (projects\/11111111111\/global\/networks\/some-dev-project-vpc).&quot;,\n    &quot;status&quot;: &quot;FAILED_PRECONDITION&quot;\n  }\n}\n<\/code><\/pre>\n<p>when I try to hit that private vertex endpoint.  I've tried curling it from within a running pod in the same project, but that didn't work either.<\/p>\n<p>Is there a way to do this?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-25 21:11:36.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-26 05:34:22.993 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":274,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>The error states that your request is to a public API, which may because you are using the public url schema to make your prediction. The structure of vertex endpoints differ between private and public, so double check that you are using the private endpoint url for your requests.<\/p>\n<p><strong>Public<\/strong><\/p>\n<pre><code>https:\/\/{REGION}-aiplatform.googleapis.com\/v1\/projects\/{PROJECT}\/locations\/{REGION}\/endpoints\/{ENDPOINT_ID}:predict\n<\/code><\/pre>\n<p><strong>Private<\/strong><\/p>\n<pre><code>http:\/\/{ENDPOINT_ID}.aiplatform.googleapis.com\/v1\/models\/{DEPLOYED_MODEL_ID}:predict\n<\/code><\/pre>\n<p>You can generate a private endpoint url using the following gcloud command:<\/p>\n<pre><code>gcloud beta ai endpoints describe {ENDPOINT_ID} \\\n  --region=us-central1 \\\n  --format=&quot;value(deployedModels.privateEndpoints.predictHttpUri)&quot;\n<\/code><\/pre>\n<p>More documentation on private endpoints can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#private-predict-uri-format\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-28 00:50:21.11 UTC",
        "Answer_score":3.0,
        "Owner_location":"Berkeley, CA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70855730",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72314675,
        "Question_title":"Managing data drift when using w2vec embeddings on VertexAI",
        "Question_body":"<p>So I am looking into moving my models from GCP's AI Platform to Vertex AI, my main motivation for it being the fact that Vertex AI has automatic email notifications when your data skews or drifts (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring<\/a>).<\/p>\n<p>So if you start receiving dodgy data that doesn't resemble the training set, they send you an email telling you which features (columns) of the data you are trying to predict are drifting away from your training data.<\/p>\n<p>However, I am unsure how this would work in my case since my data is text data that has been encoded using word2vec embeddings. Therefore, my dataset has 300 columns but I don't know what feature each of the columns refers to.<\/p>\n<p>Is this sort of data drift analysis still useful in my particular case?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-20 06:47:31.863 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"machine-learning|google-cloud-platform|word2vec|google-cloud-vertex-ai",
        "Question_view_count":133,
        "Owner_creation_date":"2021-02-03 09:38:25.2 UTC",
        "Owner_last_access_date":"2022-09-22 13:24:14.067 UTC",
        "Owner_reputation":57,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, Reino Unido",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72314675",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71885282,
        "Question_title":"Constructing a Vertex AI Pipeline with a custom training container and a model serving container",
        "Question_body":"<p>I'd like to be able to train a model with a training app container that I've made and saved to my artifact registry. I want to be able to deploy a model with a flask app and with a \/predict route that can handle some logic -- not necessarily just predicting an input json. It'll also need a \/healthz route I understand. So basically I want a pipeline that performs a training job on a model training container that I make, and deploys the model with a flask app with a model serving container that I make. Looking around on Overflow, I wonder if <a href=\"https:\/\/stackoverflow.com\/questions\/68075940\/vertex-pipeline-custompythonpackagetrainingjobrunop-not-supplying-workerpoolspe\">this<\/a> question's pipeline has the correct layout I'll eventually want to have. So, something like this:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline():\n        training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n            project=project_id,\n            display_name=training_job_name,\n            model_display_name=model_display_name,\n            python_package_gcs_uri=python_package_gcs_uri,\n            python_module=python_module,\n            container_uri=container_uri,\n            staging_bucket=staging_bucket,\n            model_serving_container_image_uri=model_serving_container_image_uri)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project_id,\n            display_name=model_display_name,\n            artifact_uri=output_dir,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n        model_deploy_op = gcc_aip.ModelDeployOp(\n            project=project_id,\n            model=model_upload_op.outputs[&quot;model&quot;],\n            endpoint=aiplatform.Endpoint(\n                endpoint_name='0000000000').resource_name,\n            deployed_model_display_name=model_display_name,\n            machine_type=&quot;n1-standard-2&quot;,\n            traffic_percentage=100)\n\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_spec_path)\n<\/code><\/pre>\n<p>I'm hoping that <code>model_serving_container_image_uri<\/code> and <code>serving_container_image_uri<\/code> both refer to the URI for the model serving container I'm going to make. I've already made a training container that trains a model and saves <code>saved_model.pb<\/code> to Google Cloud Storage. Other than having a flask app that handles the prediction and health check routes and a Dockerfile that exposes a port for the flask app, what else will I need to do to ensure the model serving container works in this pipeline? Where in the code do I install the model from GCS? In the Dockerfile? How is the model serving container meant to work so that everything will go swimmingly in the construction of the pipeline? I'm having trouble finding any tutorials or examples of precisely what I'm trying to do anywhere even though this seems like a pretty common scenario.<\/p>\n<p>To that end, I attempted this with the following pipeline:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline(\n        project: str = [redacted project ID],\n        display_name: str = &quot;custom-pipe&quot;,\n        model_display_name: str = &quot;test_model&quot;,\n        training_container_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-training-test&quot;,\n        model_serving_container_image_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-model-serving-test&quot;,\n        model_serving_container_predict_route: str = &quot;\/predict&quot;,\n        model_serving_container_health_route: str = &quot;\/healthz&quot;,\n        model_serving_container_ports: str = &quot;8080&quot;\n):\n        training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n            display_name = display_name,\n            container_uri=training_container_uri,\n            model_serving_container_image_uri=model_serving_container_image_uri,\n            model_serving_container_predict_route = model_serving_container_predict_route,\n            model_serving_container_health_route = model_serving_container_health_route,\n            model_serving_container_ports = model_serving_container_ports)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project,\n            display_name=model_display_name,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n#        model_deploy_op = gcc_aip.ModelDeployOp(\n#            project=project,\n#            model=model_upload_op.outputs[&quot;model&quot;],\n#            endpoint=aiplatform.Endpoint(\n#                endpoint_name='0000000000').resource_name,\n#            deployed_model_display_name=model_display_name,\n#            machine_type=&quot;n1-standard-2&quot;,\n#            traffic_percentage=100)\n<\/code><\/pre>\n<p>Which is failing with<\/p>\n<pre><code>google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.trainingPipelines.create' denied on resource '\/\/aiplatform.googleapis.com\/projects\/u15c36a5b7a72fabfp-tp\/locations\/us-central1' (or it may not exist).\n<\/code><\/pre>\n<p>Despite the fact that my service account has the Viewer and Kubernetes Engine Admin roles needed to work AI Platform pipelines. My training container uploads my model to Google Cloud Storage and my model serving container I've made downloads it and uses it for serving at <code>\/predict<\/code>.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-15 14:28:20.06 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2022-04-19 17:06:27.397 UTC",
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":421,
        "Owner_creation_date":"2022-04-13 14:58:44.49 UTC",
        "Owner_last_access_date":"2022-09-21 14:23:56.59 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71885282",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70577610,
        "Question_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Question_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-04 10:50:52.093 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":92,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-04 14:07:40.58 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-03-20 07:08:11.087 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69779123,
        "Question_title":"Vertex AI Tensorboard trough user interface",
        "Question_body":"<p>I have been using the Vertex AI training service with a custom container for my own machine learning pipeline. I would like to get tensorboard logs into the experiments tab to see in real-time the metrics while the model is training.<\/p>\n<p>I was wondering if it is possible to set a custom training job in the user interface setting a <code>TENSORBOARD_INSTANCE_NAME<\/code>. It seems that this is only possible through a json-post-request.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-30 11:18:07.87 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_date":"2021-08-10 14:58:20.49 UTC",
        "Owner_last_access_date":"2022-09-14 19:08:59.14 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Colombia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69779123",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72871214,
        "Question_title":"Vertex AI - Deployment failed",
        "Question_body":"<p>I'm trying to deploy my custom-trained model using a custom-container, i.e. create an endpoint from a model that I created.\nI'm doing the same thing with AI Platform (same model &amp; container) and it works fine there.<\/p>\n<p>At the first try I deployed the model successfully, but ever since whenever I try to create an endpoint it says &quot;deploying&quot; for 1+ hours and then it fails with the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at (link)\n<\/code><\/pre>\n<p>The log shows the following:<\/p>\n<pre><code>* Running on all addresses (0.0.0.0)\n WARNING: This is a development server. Do not use it in a production deployment.\n* Running on http:\/\/127.0.0.1:8080\n[05\/Jul\/2022 12:00:37] &quot;[33mGET \/v1\/endpoints\/1\/deployedModels\/2025850174177280000 HTTP\/1.1[0m&quot; 404 -\n[05\/Jul\/2022 12:00:38] &quot;[33mGET \/v1\/endpoints\/1\/deployedModels\/2025850174177280000 HTTP\/1.1[0m&quot; 404 -\n<\/code><\/pre>\n<p>Where the last line is being spammed until it ultimately fails.<\/p>\n<p>My flask app is as follows:<\/p>\n<pre><code>import base64\nimport os.path\nimport pickle\nfrom typing import Dict, Any\nfrom flask import Flask, request, jsonify\nfrom streamliner.models.general_model import GeneralModel\n\nclass Predictor:\n    def __init__(self, model: GeneralModel):\n        self._model = model\n\n    def predict(self, instance: str) -&gt; Dict[str, Any]:\n        decoded_pickle = base64.b64decode(instance)\n        features_df = pickle.loads(decoded_pickle)\n        prediction = self._model.predict(features_df).tolist()\n        return {&quot;prediction&quot;: prediction}\n\napp = Flask(__name__)\nwith open('.\/model.pkl', 'rb') as model_file:\n    model = pickle.load(model_file)\n    predictor = Predictor(model=model)\n\n\n@app.route(&quot;\/predict&quot;, methods=['POST'])\ndef predict() -&gt; Any:\n    if request.method == &quot;POST&quot;:\n        instance = request.get_json()\n        instance = instance['instances'][0]\n        predictions = predictor.predict(instance)\n        return jsonify(predictions)\n\n\n@app.route(&quot;\/health&quot;)\ndef health() -&gt; str:\n    return &quot;ok&quot;\n\n\nif __name__ == '__main__':\n    port = int(os.environ.get(&quot;PORT&quot;, 8080))\n    app.run(host='0.0.0.0', port=port)\n<\/code><\/pre>\n<p>The deployment code which I do through Python is irrelevant because the problem persists when I deploy through GCP's UI.<\/p>\n<p>The model creation code is as follows:<\/p>\n<pre><code>def upload_model(self):\n    model = {\n        &quot;name&quot;: self.model_name_on_platform,\n        &quot;display_name&quot;: self.model_name_on_platform,\n        &quot;version_aliases&quot;: [&quot;default&quot;, self.run_id],\n        &quot;container_spec&quot;: {\n            &quot;image_uri&quot;: f'{REGION}-docker.pkg.dev\/{GCP_PROJECT_ID}\/{self.repository_name}\/{self.run_id}',\n            &quot;predict_route&quot;: &quot;\/predict&quot;,\n            &quot;health_route&quot;: &quot;\/health&quot;,\n        },\n    }\n    parent = self.model_service_client.common_location_path(project=GCP_PROJECT_ID, location=REGION)\n    model_path = self.model_service_client.model_path(project=GCP_PROJECT_ID,\n                                                      location=REGION,\n                                                      model=self.model_name_on_platform)\n    upload_model_request_specifications = {'parent': parent, 'model': model,\n                                           'model_id': self.model_name_on_platform}\n    try:\n        print(&quot;trying to get model&quot;)\n        self.get_model(model_path=model_path)\n    except NotFound:\n        print(&quot;didn't find model, creating a new one&quot;)\n    else:\n        print(&quot;found an existing model, creating a new version under it&quot;)\n        upload_model_request_specifications['parent_model'] = model_path\n    upload_model_request = model_service.UploadModelRequest(upload_model_request_specifications)\n    response = self.model_service_client.upload_model(request=upload_model_request, timeout=1800)\n    print(&quot;Long running operation:&quot;, response.operation.name)\n    upload_model_response = response.result(timeout=1800)\n    print(&quot;upload_model_response:&quot;, upload_model_response)\n<\/code><\/pre>\n<p>My problem is very close to <a href=\"https:\/\/stackoverflow.com\/questions\/69316032\/custom-container-deployment-in-vertex-ai\">this one<\/a> with the difference that I do have a health check.<\/p>\n<p>Why would it work on the first deployment and fail ever since? Why would it work on AI Platform but fail on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-05 14:34:45.97 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-14 19:56:43.173 UTC",
        "Question_score":2,
        "Question_tags":"docker|flask|google-cloud-vertex-ai",
        "Question_view_count":299,
        "Owner_creation_date":"2021-10-19 12:36:59.98 UTC",
        "Owner_last_access_date":"2022-09-19 13:44:39.633 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72871214",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68806278,
        "Question_title":"Example notebook, sample for google cloud pipeline component",
        "Question_body":"<p>I am looking for a sample or a tutorial notebook with specifically the &quot;CustomPythonPackageTrainingJobRunOp&quot; google cloud pipeline component. I have been trying to use this but keep getting into errors.<\/p>\n<p>PS: I have already posted a question about the errors <a href=\"https:\/\/stackoverflow.com\/questions\/68795098\/error-when-trying-to-use-custompythonpackagetrainingjobrunop-in-vertexai-pipelin\">here<\/a> and also requested a sample, but posting again regarding the sample since I feel that that is a pretty long post and the request is buried with other code details.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-16 16:28:35.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":117,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68806278",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71351821,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-03-04 13:00:26.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":1298,
        "Owner_creation_date":"2014-08-05 13:37:06.703 UTC",
        "Owner_last_access_date":"2022-09-22 12:30:19.65 UTC",
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-06 16:02:17.36 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69373666,
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-29 09:23:26.933 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-29 11:28:26.017 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_date":"2011-03-18 10:16:38.977 UTC",
        "Owner_last_access_date":"2022-02-21 16:31:56.407 UTC",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-03 16:29:02.093 UTC",
        "Answer_score":1.0,
        "Owner_location":"Switzerland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69373666",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68915712,
        "Question_title":"Google Auto ML taking huge time for forcasting",
        "Question_body":"<p>I have around 500 time series dataset for a period of 2.5 yrs with a granularity of 1 day for each series. This amounts to roughly 1 million data points.\nI want to forecast for 2 weeks in 1 day granularity for each of the time series. There might be correlation among these 500 time series.\nAfter ensuring that I have data for each timestamp, we are feeding these (500) time series to autoML where each time series is identified by \u201cseries identifier\u201d.\nSo, our input to the autoML (Forecasting) is timestamp, series identifier, features, and target value. I have 30 feature which are combination of categorical and numerical.\nWith this setup, if I feed to autoML, it is taking more than 20 hrs for training which is not cost effective for me.<\/p>\n<p>Please help me to optimized this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-25 01:22:19.447 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"time-series|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_date":"2017-06-19 14:28:31.323 UTC",
        "Owner_last_access_date":"2022-04-04 01:07:49.423 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68915712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73727583,
        "Question_title":"IDtoken retrieval in Vertex AI pipeline fails randomly",
        "Question_body":"<p>In vertex AI pipelines we have created a custom component that trigger a cloud run.\nIn order to do so we need to fetch the Id Token Credentials of the running identity.\nThis work most of the time but fails randomly every 5 to 10 hours.<\/p>\n<p>here is our code :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>auth_req = google.auth.transport.requests.Request()\nid_token = google.oauth2.id_token.fetch_id_token(auth_req, url_dbt_server)\nreturn id_token\n<\/code><\/pre>\n<p>the error message is:<\/p>\n<pre><code>google.auth.exceptions.RefreshError: (&quot;Failed to retrieve http:\/\/metadata.google.internal\/computeMetadata\/v1\/instance\/service-accounts\/default\/identity?audience=&lt;Our audience&gt;&amp;format=full from the Google Compute Enginemetadata service. Status: 404 Response:\\nb'Not Found\\\\n'&quot;, &lt;google.auth.transport.requests._Response object at 0x7f8f0d78aca0&gt;)\n<\/code><\/pre>\n<p>How can I solve (or at least go around) this problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 08:04:09.14 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-15 08:13:01.173 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-run|google-cloud-vertex-ai",
        "Question_view_count":16,
        "Owner_creation_date":"2022-09-15 07:46:08.907 UTC",
        "Owner_last_access_date":"2022-09-23 14:30:02.967 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727583",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73463378,
        "Question_title":"Vertex AI model showing failure but 0 bytes in prediction.errors",
        "Question_body":"<p>I'm running a vertex batch job on a custom model with 676 instances in my jsonl.<\/p>\n<p>I get results for all 676 instances but the job shows &quot;Due to one or more errors, this batch prediction job was canceled on Aug 22, 2022 at 09:04PM&quot;<\/p>\n<p>The error file prediction.errors_stats-00000-of-00001 has 0 bytes and there is a directory:<\/p>\n<pre><code>beam-temp-prediction.errors_stats-e289aa8c228c11eda06642010a800fdc \n<\/code><\/pre>\n<p>with 4 files with 0 bytes\u00a0in them.<\/p>\n<pre><code>34e19e3d-e717-4c6f-860c-5c2a177f1e93.prediction.errors_stats\n7c9374c8-65f7-4a97-8a95-6068ba9cd936.prediction.errors_stats\n981a634b-d27c-4285-b871-648b25ca87b8.prediction.errors_stats\nf14dbc29-42d7-480e-8b79-95e799e132bf.prediction.errors_stats\n<\/code><\/pre>\n<p>Any ideas on why the job is showing failure but the results seem ok?<\/p>\n<p>Job params:<\/p>\n<pre><code>{&quot;displayName&quot;: &lt;MY_DISPLAY_NAME&gt;,\n &quot;model&quot;: &lt;MY_MODEL&gt;,\n &quot;inputConfig&quot;: {&quot;instancesFormat&quot;: &quot;jsonl&quot;,\n                 &quot;gcsSource&quot;: {&quot;uris&quot;: [&quot;gs:\/\/&lt;MY_BUCKET&gt;\/MY_INSTANCES.jsonl&quot;]}},\n &quot;outputConfig&quot;: {&quot;predictionsFormat&quot;: &quot;jsonl&quot;,\n                  &quot;gcsDestination&quot;: {&quot;outputUriPrefix&quot;: &quot;gs:\/\/&lt;MY_OUTPUTS&gt;\/outputs\/2022-08-23&quot;}},\n&quot;dedicatedResources&quot;: {&quot;machineSpec&quot;: {&quot;machineType&quot;: &quot;n1-standard-8&quot;, \n&quot;acceleratorType&quot;: null, &quot;acceleratorCount&quot;: null},\n&quot;startingReplicaCount&quot;: 2},\n&quot;manualBatchTuningParameters&quot;: {&quot;batch_size&quot;: 1}}\n<\/code><\/pre>\n<p>Note: I've also tried leaving out the 'startingReplicaCount and manualBatchTuningParameters.<\/p>\n<p>I'm using a curl command to initiate the batch job:<\/p>\n<pre><code>    curl -X POST \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n-H &quot;Content-Type: application\/json; charset=utf-8&quot; \\\n-d @$INPUT_JSON \\\n&quot;https:\/\/$LOCATION-aiplatform.googleapis.com\/v1\/projects\/$PROJECT\/locations\/$LOCATION\/batchPredictionJobs&quot;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-23 18:14:26.123 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-23 18:23:57.73 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":42,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Berkeley, CA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73463378",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70070421,
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-22 18:04:35.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-23 16:51:56.647 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-vertex-ai",
        "Question_view_count":319,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-23 06:05:01.513 UTC",
        "Answer_score":1.0,
        "Owner_location":"Boston, MA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72692781,
        "Question_title":"Error call Google Vertex AI endpoint from a python backend",
        "Question_body":"<p>I am trying to send an http post request to my google vertex ai endpoint for prediction. Though I do set the Bearer Token in the request header, the request still fails with the below error:<\/p>\n<pre><code>{\n&quot;error&quot;: {\n    &quot;code&quot;: 401,\n    &quot;message&quot;: &quot;Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.&quot;,\n    &quot;status&quot;: &quot;UNAUTHENTICATED&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.ErrorInfo&quot;,\n            &quot;reason&quot;: &quot;ACCESS_TOKEN_TYPE_UNSUPPORTED&quot;,\n            &quot;metadata&quot;: {\n                &quot;service&quot;: &quot;aiplatform.googleapis.com&quot;,\n                &quot;method&quot;: &quot;google.cloud.aiplatform.v1.PredictionService.Predict&quot;\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n<p>}<\/p>\n<p>Since I am making this call from a python backend, I'm not sure if OAuth 2 as suggested in the message would be wise and applicable choice.<\/p>\n<p>The model is already deployed and endpointed test on vertex ai and it worked fine. What I am trying to do is send same prediction task via an http post request using postman and this is what failed.<\/p>\n<p>The request url looks like this:<\/p>\n<pre><code>https:\/\/[LOCATION]-aiplatform.googleapis.com\/v1\/projects\/[PROJECT ID]\/locations\/[LOCATION]\/endpoints\/[ENDPOINT ID]:predict\n<\/code><\/pre>\n<p>Where token bearer is set in the potman authorization tab and instance set in request body.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2022-06-20 20:43:46.93 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-22 23:22:27.09 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-endpoints|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":353,
        "Owner_creation_date":"2015-03-21 06:55:29.353 UTC",
        "Owner_last_access_date":"2022-09-02 13:58:42.187 UTC",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangkok",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72692781",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70296482,
        "Question_title":"AutoML Tables Batch Prediction- Produces an empty table in google big query",
        "Question_body":"<p>I am new to google cloud platform's Vertex AI. In Vertex AI I have created a new batch prediction and would like a Bigquery output table. However when I create the new batch prediction the output table is empty. I am not sure what the issue is. Any advise please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2021-12-09 20:43:34.653 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-21 02:09:25.217 UTC",
        "Question_score":1,
        "Question_tags":"google-bigquery|prediction|forecasting|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_date":"2018-04-17 13:34:40.007 UTC",
        "Owner_last_access_date":"2022-08-25 15:46:35.577 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70296482",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71471108,
        "Question_title":"Why does Explainable AI not find implementations of the model?",
        "Question_body":"<p>In this <a href=\"https:\/\/www.kaggle.com\/ryanholbrook\/vertex-ai-with-mlb-player-digital-engagement\" rel=\"nofollow noreferrer\">Notebook<\/a>, we use Explainable AI SDK from Google to load a model, right after saving it. This fails with a message that the model is missing.<\/p>\n<p>But note<\/p>\n<ul>\n<li>the info message saying that the model <em>was<\/em> saved<\/li>\n<li>checking  <code>working\/model<\/code> shows that the model is there.<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jNCkD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jNCkD.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>However, <code>working\/model\/assets<\/code> is empty.<\/li>\n<\/ul>\n<p>Why do we get this error message?  How can we avoid it?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    model_path = &quot;working\/model&quot;\n\n    model.save(model_path)\n    \n    builder = SavedModelMetadataBuilder(model_path)\n    builder.set_numeric_metadata(\n        &quot;numpy_inputs&quot;,\n        input_baselines=[X_train.median().tolist()],  # attributions relative to the median of the target\n        index_feature_mapping=X_train.columns.tolist(),  # the names of each feature\n    )\n    builder.save_metadata(model_path)\n    \n    explainer = explainable_ai_sdk.load_model_from_local_path(\n        model_path=model_path,\n        config=configs.SampledShapleyConfig(path_count=20),\n    )\n    \n<\/code><\/pre>\n<pre><code>INFO:tensorflow:Assets written to: working\/model\/assets\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n\/tmp\/ipykernel_26061\/1928503840.py in &lt;module&gt;\n     18 explainer = explainable_ai_sdk.load_model_from_local_path(\n     19     model_path=model_path,\n---&gt; 20     config=configs.SampledShapleyConfig(path_count=20),\n     21 )\n     22 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/explainable_ai_sdk\/model\/model_factory.py in load_model_from_local_path(model_path, config)\n    128   &quot;&quot;&quot;\n    129   if _LOCAL_MODEL_KEY not in _MODEL_REGISTRY:\n--&gt; 130     raise NotImplementedError('There are no implementations of local model.')\n    131   return _MODEL_REGISTRY[_LOCAL_MODEL_KEY](model_path, config)\n    132 \n\nNotImplementedError: There are no implementations of local model.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-14 16:28:13.843 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":42,
        "Owner_creation_date":"2008-11-20 08:57:51.293 UTC",
        "Owner_last_access_date":"2022-09-24 19:18:28.08 UTC",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71471108",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73046482,
        "Question_title":"`systemd` error in Workbench instances created using custom container",
        "Question_body":"<p>I am creating a Vertex AI Workbench instance from a custom container. Below is my <code>Dockerfile<\/code>:<\/p>\n<pre><code>FROM gcr.io\/deeplearning-platform-release\/base-cu113\nRUN pip install ml_collections\n<\/code><\/pre>\n<p>Note that I have intentionally kept the Dockerfile short here because it will be enough to reproduce the issue.<\/p>\n<p>The Workbench instance gets created but when I run <code>sudo shutdown now<\/code> from a terminal within the instance, it leads to:<\/p>\n<pre><code>System has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nFailed to talk to init daemon.\n<\/code><\/pre>\n<p>Anything to mitigate this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-20 05:44:05.91 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":80,
        "Owner_creation_date":"2017-02-28 15:43:38.107 UTC",
        "Owner_last_access_date":"2022-09-20 03:33:53.823 UTC",
        "Owner_reputation":326,
        "Owner_up_votes":153,
        "Owner_down_votes":1,
        "Owner_views":231,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73046482",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72106030,
        "Question_title":"I am not able to create a feature store in vertexAI using labels",
        "Question_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-03 22:20:13.553 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-04 00:16:51.107 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":83,
        "Owner_creation_date":"2018-07-01 14:59:34.833 UTC",
        "Owner_last_access_date":"2022-09-24 15:38:47.317 UTC",
        "Owner_reputation":1043,
        "Owner_up_votes":17,
        "Owner_down_votes":6,
        "Owner_views":212,
        "Answer_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-05-05 00:16:53.3 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-05-05 00:25:05.26 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73623659,
        "Question_title":"Is it possible to import custom source files into Kubeflow components?",
        "Question_body":"<p>I know that Kubeflow only modifies the container with the specified libraries to be installed. But I want to use my custom module in the training Component section of the pipeline.<\/p>\n<p>So let me clarify my case; I'm deploying a GCP Vertex AI pipeline which exists of preprocessing and training  steps. And there is also custom library that I created using some libraries like scikit. My main issue is that I want to re-use that library objects within my training step which looks like;<\/p>\n<pre><code>    packages_to_install = [\n        &quot;pandas&quot;,\n        &quot;sklearn&quot;,\n        &quot;mycustomlibrary?&quot;\n    ],\n)\ndef train_xgb_model(\n    dataset: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \n    from MyCustomLibrary import XGBClassifier\n    import pandas as pd\n    \n    data = pd.read_csv(dataset.path)\n\n    model = XGBClassifier(\n        objective=&quot;binary:logistic&quot;\n    )\n    model.fit(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    score = model.score(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    model_artifact.metadata[&quot;train_score&quot;] = float(score)\n    model_artifact.metadata[&quot;framework&quot;] = &quot;XGBoost&quot;\n    \n    model.save_model(model_artifact.path)``` \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 14:24:14.327 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":31,
        "Owner_creation_date":"2018-06-25 06:49:27.513 UTC",
        "Owner_last_access_date":"2022-09-12 08:12:55.84 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73623659",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73606697,
        "Question_title":"Model path injection and gcsfuse",
        "Question_body":"<p>I observed that VertexAI uses gs:\/\/ rather than \/gcs as the path to store trained models (started a few days ago).<\/p>\n<p>In my understanding, when gcsfuse is mounted correctly (which is the case for me - according to the logs), then I would expect nothing else than \/gcs\/path_to_where_to_store_a_model as per the documentation. Am I doing something wrong, or did something change?<\/p>\n<p>Anyone have an idea?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-09-05 08:40:29.287 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|gcsfuse",
        "Question_view_count":76,
        "Owner_creation_date":"2021-11-03 10:25:15.277 UTC",
        "Owner_last_access_date":"2022-09-22 11:34:07.727 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73606697",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68764644,
        "Question_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Question_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-12 22:02:54.297 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":223,
        "Owner_creation_date":"2018-10-30 18:54:03.62 UTC",
        "Owner_last_access_date":"2022-09-23 17:00:36.977 UTC",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Answer_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-13 04:16:45.167 UTC",
        "Answer_score":1.0,
        "Owner_location":"Utah, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68764644",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73047089,
        "Question_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Question_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-20 06:48:34.453 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-21 12:34:49.243 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|johnsnowlabs-spark-nlp",
        "Question_view_count":238,
        "Owner_creation_date":"2022-05-03 20:17:35.81 UTC",
        "Owner_last_access_date":"2022-09-18 18:37:06.877 UTC",
        "Owner_reputation":105,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-21 06:20:07.35 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-07-21 13:49:12.227 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70729582,
        "Question_title":"Batch prediction Input",
        "Question_body":"<p>I have a tensorflow model deployed on Vertex AI of Google Cloud. The model definition is:<\/p>\n<pre><code>item_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=item_vocab, mask_token=None),\n  tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dim)\n])\n\nuser_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=user_vocab, mask_token=None),\n  # We add an additional embedding to account for unknown tokens.\n  tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dim)\n])\n\n\nclass NCF_model(tf.keras.Model):\n    def __init__(self,user_model, item_model):\n        super(NCF_model, self).__init__()\n        # define all layers in init\n        \n        self.user_model = user_model\n        self.item_model  = item_model\n        self.concat_layer   = tf.keras.layers.Concatenate()\n        self.feed_forward_1 = tf.keras.layers.Dense(32,activation= 'relu')\n        self.feed_forward_2 = tf.keras.layers.Dense(64,activation= 'relu')\n        self.final = tf.keras.layers.Dense(1,activation= 'sigmoid')\n\n\n    def call(self, inputs ,training=False):\n        user_id , item_id = inputs[:,0], inputs[:,1]\n        x = self.user_model(user_id)\n        y = self.item_model(item_id)\n\n        x = self.concat_layer([x,y])\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = self.final(x)\n\n\n        return x\n<\/code><\/pre>\n<p>The model has two string inputs and it outputs a probability value.\nWhen I use the following input in the batch prediction file, I get an empty prediction file.\nSample of csv input file:<\/p>\n<pre><code>userid,itemid\nyuu,190767\nyuu,364\nyuu,154828\nyuu,72998\nyuu,130618\nyuu,183979\nyuu,588\n<\/code><\/pre>\n<p>When I use a jsonl file with the following input.<\/p>\n<pre><code>{&quot;input&quot;:[&quot;yuu&quot;, &quot;190767&quot;]}\n<\/code><\/pre>\n<p>I get the following error.<\/p>\n<pre><code>('Post request fails. Cannot get predictions. Error: Exceeded retries: Non-OK result 400 ({\\n    &quot;error&quot;: &quot;Failed to process element: 0 key: input of \\'instances\\' list. Error: INVALID_ARGUMENT: JSON object: does not have named input: input&quot;\\n}) from server, retry=3.', 1)\n<\/code><\/pre>\n<p>What seems to be going wrong with these inputs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-16 11:25:44.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":323,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Gurugram, Haryana, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70729582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73690729,
        "Question_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Question_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-12 14:15:36.247 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-13 05:46:49.75 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73690729",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72640182,
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-16 03:36:01.587 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-31 08:43:09.79 UTC",
        "Question_score":0,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines|mlops",
        "Question_view_count":107,
        "Owner_creation_date":"2021-05-21 18:13:40.567 UTC",
        "Owner_last_access_date":"2022-08-09 15:09:16.75 UTC",
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-31 08:50:07.333 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73279871,
        "Question_title":"Proper Format of Vertex AI AutoML Action Recognition Data Labels",
        "Question_body":"<p>I'm trying to build an action recognition model in Vertex AI AutoML. I've studied the documentation thoroughly, but so far my model is not able to make any decent predictions in the wild. I've made three attempts so far, and my most recent attempt had a precision-recall curve that could be described as 'respectable', but the predictions are really awful. I'll try to explain my process below as best as I can.<\/p>\n<h2>The Raw Data<\/h2>\n<p>I recorded the same action in 34 ~3 min videos, with the number of actions in each video varying between 30 and 100. The actions themselves take &lt; 1 second. I recorded the data from 4 cameras at multiple angles, and because I was moving around a lot, there was plenty of variance in each action performed. While each raw video contains only one action, there are a total of six classes of action we hope to identify.<\/p>\n<h2>First Model Attempt<\/h2>\n<p>According to the Vertex AI documentation, it's expecting time segments for the actions The annotation JSONL\/CSV documentation says as much, but somewhere else in the documentation it says it's expecting the maximal point at which the action is performed if you wish to label the videos inside the console. Anyways, I created a labeling job and my team and I labeled all the time segments for the actions in the videos. The precision-recall curve alluded to some kind of data leakage, and when we inspected the batch prediction results we discovered that it appeared that the model was training on the 0th frame of the time segment. We were careful not to include any frames that weren't part of the action, but due to the nature of the actions, they all essentially start and end at a 'neutral' spot. At seemingly random intervals in the video, multiple or all actions would be labeled, but ONLY in those spots.<\/p>\n<h2>Second Attempt<\/h2>\n<p>We took the annotation data that we had built with the labeling job and chopped up the original videos into a series of subclips. We had all the labels and the time segments, so we did this with a simple script. We did not remove any of the frames of the video, so the neutral position in the beginning and end frames were still present. The precision-recall curve again looked suspicious, but slightly better. Inference in the wild yielded the same results.<\/p>\n<h2>Third Attempt<\/h2>\n<p>After further reviewing the documentation, Vertex AI appeared to contradict itself in what it expects in the data labels:<\/p>\n<blockquote>\n<p>When the action starts appearing that you want to identify, slowly\nprogress through till you find the center or the most representative\nmoment of the action using &quot;Next frame&quot; option.<\/p>\n<\/blockquote>\n<p>To avoid spending a ton of time on another labeling task (takes us about three days), we labeled a subset of the original subclip dataset according to this information and trained a model to analyze the precision-recall curve. FINALLY something much more respectable. However, the inferences in the wild were still terrible, suffering from the same.<\/p>\n<p>My question is: <strong>do I need to annotate negative action sequences?<\/strong> In the object tracking or object detection documentation it says that adding a <code>None_of_the_above<\/code> label would help the model to identify that which it doesn't need to focus on. And again in the action recognition documentation it points out a limitation in the labeling console:<\/p>\n<blockquote>\n<p>Limitation: There's a limitation when using the VAR labeling console,\nwhich means if you want to use the labeling tool to label actions, you\nmust label all the actions in that video.<\/p>\n<\/blockquote>\n<p>I can write a script to fill in the dead space in the video as a negative action sequence, but I'd like to know what the best practice is before going down that route and spending the money to train yet another terrible model.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-08 14:53:09.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|automl|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_date":"2013-02-10 22:40:48.187 UTC",
        "Owner_last_access_date":"2022-09-23 16:38:41.943 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Miami, FL, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279871",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73052584,
        "Question_title":"Set the name for each ParallelFor iteration in KFP v2 on Vertex AI",
        "Question_body":"<p>I am currently using <code>kfp.dsl.ParallelFor<\/code> to train 300 models. It looks something like this:<\/p>\n<pre><code>...\n\nmodels_to_train_op = get_models()\n\nwith dsl.ParallelFor(models_to_train_op.outputs[&quot;data&quot;], parallelism=100) as item:\n  prepare_data_op = prepare_data(item)\n  train_model_op = train_model(prepare_data_op[&quot;train_data&quot;]\n\n...\n<\/code><\/pre>\n<p>Currently, the iterations in Vertex AI are labeled in a dropdown as something like <code>for-loop-worker-0<\/code>, <code>for-loop-worker-1<\/code>, and so on. For tasks (like <code>prepare_data_op<\/code>, there's a function called <code>set_display_name<\/code>. Is there a similar method that allows you to set the iteration name? It would be helpful to relate them to the training data so that it's easier to look through the dropdown UI that Vertex AI provides.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-20 13:31:49.257 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines|kfp",
        "Question_view_count":185,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Boston, MA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73052584",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70137519,
        "Question_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Question_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-27 18:19:33.043 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":285,
        "Owner_creation_date":"2010-09-29 16:12:19.11 UTC",
        "Owner_last_access_date":"2022-09-21 15:37:30.203 UTC",
        "Owner_reputation":8508,
        "Owner_up_votes":347,
        "Owner_down_votes":4,
        "Owner_views":144,
        "Answer_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-12-08 02:37:26.247 UTC",
        "Answer_score":1.0,
        "Owner_location":"Chicago, IL",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70614514,
        "Question_title":"Google Cloud Vertex AI Notebook Scheduled Runs Aren't Running Code?",
        "Question_body":"<p>I've followed their <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-schedule-execution-console\" rel=\"nofollow noreferrer\">instructions<\/a> to how to set up a managed Jupyter notebook and schedule a run, and I tossed in some pretty standard parameters and my bucket.<\/p>\n<p>After setting up the schedule, however, the run just comes out as &quot;Failed&quot;, and when I get &quot;view results&quot;, I just get my code back (with no output indication). For some reason it's just not running. Ideas?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/r0vra.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r0vra.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>[2]<a href=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-06 22:44:38.66 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-06 22:48:04.197 UTC",
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":457,
        "Owner_creation_date":"2018-04-09 01:06:01.717 UTC",
        "Owner_last_access_date":"2022-01-21 07:39:34.45 UTC",
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70614514",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71132848,
        "Question_title":"Google.Cloud.AIPlatform.V1 Received http2 header with status: 404",
        "Question_body":"<p>We are trying to call the Google.Cloud.AIPlatform.V1 predict API using the .Net client and keep getting the following error:   Received http2 header with status: 404<\/p>\n<p>We setup credentials using the API key and environment variable:  GOOGLE_APPLICATION_CREDENTIALS<\/p>\n<p>Here is the code to call the vertex AI predict API:<\/p>\n<pre><code>const string projectId = &quot;xxxxxx&quot;;\nconst string location = &quot;us-central1&quot;; \nconst string endpointId = &quot;xxxxxx&quot;;  \n\nPredictionServiceClient client = PredictionServiceClient.Create();\n\nvar structVal = Google.Protobuf.WellKnownTypes.Value.ForStruct(new Struct\n{\n    Fields =\n    {\n    [&quot;mimeType&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(&quot;text\/plain&quot;),\n    \/\/ Sample contents is a string constant defined in a separate file\n    [&quot;content&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(Consts.SampleContents)\n    }\n});\n\nPredictRequest req = new PredictRequest()\n{\n    EndpointAsEndpointName = EndpointName.FromProjectLocationEndpoint(projectId, location, endpointId),\n    Instances = { structVal }\n};\n\nPredictResponse response = client.Predict(req);\n<\/code><\/pre>\n<p>The full error returned:<\/p>\n<p>Status(StatusCode=&quot;Unimplemented&quot;, Detail=&quot;Received http2 header with status: 404&quot;, DebugException=&quot;Grpc.Core.Internal.CoreErrorDetailException: {&quot;created&quot;:&quot;@1644947338.412000000&quot;,&quot;description&quot;:&quot;Received http2 :status header with non-200 OK status&quot;,&quot;file&quot;:&quot;......\\src\\core\\ext\\filters\\http\\client\\http_client_filter.cc&quot;,&quot;file_line&quot;:134,&quot;grpc_message&quot;:&quot;Received http2 header with status: 404&quot;,&quot;grpc_status&quot;:12,&quot;value&quot;:&quot;404&quot;}&quot;)<\/p>\n<p>I validate the same call using CURL and was able to successfully make the call.<\/p>\n<pre><code>curl -X POST -H &quot;Authorization: Bearer XXXXX&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/us-central1-aiplatform.googleapis.com\/ui\/projects\/XXXXXX\/locations\/us-central1\/endpoints\/XXXXXX:predict -d @payload.json\n<\/code><\/pre>\n<p>Any help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 20:18:14.297 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"c#|google-cloud-vertex-ai",
        "Question_view_count":204,
        "Owner_creation_date":"2018-12-26 14:50:25.527 UTC",
        "Owner_last_access_date":"2022-09-24 17:43:12.653 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71132848",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72073763,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-05-01 03:22:15.713 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":358,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-17 15:43:32.843 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69843015,
        "Question_title":"OSError: [WinError 123] when I create kfp component",
        "Question_body":"<p>I'm trying to create a pipeline in Vertex AI with kfp using my own components from local notebook in Spyder.<\/p>\n<p>When I run the following piece of code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;pandas&quot;])\ndef create_dataset(\n    gcs_csv_path_train: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(gcs_csv_path_train)\n    dataset = df.pop('Class')\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '&lt;ipython-input-11-b28c15ec667f&gt;'\n<\/code><\/pre>\n<p>The error is not raised if I use a Jupyter notebook online.<\/p>\n<p>What am I doing wrong? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-04 16:54:14.3 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":105,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Alatri, Frosinone, FR",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69843015",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68643520,
        "Question_title":"Creating a parametrized Vertex AI pipeline in GCP",
        "Question_body":"<p>In our application, we currently expose a UI where a user can select some basic settings (model type, input features, hyperparameters) to specify a forecast model. Every time a user specifies such a model, the backend python application looks at these settings, pulls training data from relevant db, trains the relevant model and stores the model file which is then used at prediction time. The model is then re-trained according to a fixed frequency. We are looking to replace this entire flow with GCP, but I am not sure of the right approach on this. My initial thought is to write the entire backend application as a single VertexAI pipeline, where any time the user specifies a model, the pipeline is run and creates and deploys a model (custom or AutoML) which is then used at prediction time. What I am not sure  is whether I can do the following:<\/p>\n<ol>\n<li>Since there is one pipeline which runs every time a user specifies a model, the pipeline will need to be parametrized. Say, user specifies a model for metric A, the pipeline creates and deploys model1, then for metric B, the pipeline deploys model2 and so on. So the pipeline will need to be parametrized.<\/li>\n<li>Can we actually pull data from different sources (other than BigQuery, Cloud Storage) in the pipeline?<\/li>\n<li>How could I schedule the pipeline runs for each model separately, i.e., say model A needs to be trained biweekly, model B needs to be trained weekly etc. Since there is just 1 pipeline and many deployed models, I am not even sure how to set the scheduling for the pipeline.<\/li>\n<\/ol>\n<p>I am relatively new to GCP Vertex AI and exploring things so I am not sure I am on the right path. Does a single pipeline for this use case even make sense, or should I be considering writing a custom python application which then creates a new pipeline every single time a model is requested?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-03 22:41:48.117 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":213,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68643520",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72514594,
        "Question_title":"GCP Vertex AI Workbench custom image - persistence with gcs don't work",
        "Question_body":"<p>While creating a workbench with a custom jupyter image I choose backup\/ persistence with GCS (google cloud storage) and it doesn't work.\nFor now, I tried to test with:<\/p>\n<ul>\n<li><code>jupyter\/base-notebook:python-3.8.8<\/code> <a href=\"https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/base-v2\/Dockerfile\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/base-v2\/Dockerfile<\/a><\/li>\n<li><code>gcr.io\/deeplearning-platform-release\/base-cpu:latest<\/code> <a href=\"https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/vertex\/Dockerfile\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/vertex\/Dockerfile<\/a><\/li>\n<\/ul>\n<p>I thought it was a problem with permissions on the folder named <code>jupyter<\/code> which is created always while creating a workbench on every jupyter image (custom \/ pre-built by gcp).\nIs there any method to accomplish persistence with gcs with a custom docker image?\n<a href=\"https:\/\/i.stack.imgur.com\/srXbE.png\" rel=\"nofollow noreferrer\">Screen showing option with backup on gcs<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-06 07:49:45.307 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|docker-image|google-cloud-vertex-ai|gcs",
        "Question_view_count":249,
        "Owner_creation_date":"2018-09-17 15:05:44.723 UTC",
        "Owner_last_access_date":"2022-09-15 15:25:15.08 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72514594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68775021,
        "Question_title":"Working link for Google Cloud pipeline components docs?",
        "Question_body":"<p>Does anyone have a working link for the docs for Google Cloud pipeline components. The link in the <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/google-cloud\" rel=\"nofollow noreferrer\">github page<\/a> under &quot;ReadTheDocs page&quot; is broken. Tried some other tutorial notebooks, such as <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/ai-platform-samples\/blob\/master\/ai-platform-unified\/notebooks\/official\/pipelines\/google-cloud-pipeline-components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this one<\/a>, the link under &quot;The components are documented here.&quot; seems to be broken too.<\/p>\n<p>Edit:<\/p>\n<p>The <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/\" rel=\"nofollow noreferrer\">link<\/a> is up now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-13 15:36:37.68 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-15 13:17:59.457 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68775021",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72907038,
        "Question_title":"Cost of deploying a TensorFlow model in GCP?",
        "Question_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 05:32:59.287 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-08 05:39:36.587 UTC",
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2019-09-26 00:32:01.527 UTC",
        "Owner_last_access_date":"2022-09-24 06:51:48.563 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-08 12:27:10.41 UTC",
        "Answer_score":1.0,
        "Owner_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71498754,
        "Question_title":"Secret Manager from Vertex AI Pipeline",
        "Question_body":"<p>I am working in GCP creating a Vertex AI pipeline with kubeflow and it is time for me to store my API keys more securely. I am very new to GCP and unfamiliar with the environment so I've been trying to follow a few tutorials but have hit a roadblock. I want to store my secrets in Secret Manager and then later access them from the pipeline I've written. I have no problem creating secrets and viewing them in the GUI but when it comes to compiling my pipeline i get the error: <code>google.api_core.exceptions.PermissionDenied: 403 Permission denied on resource project...<\/code><\/p>\n<p>So it seems that the account running my pipelines does not have access to the secrets I have created. My question is then, how do I check which account is running the pipeline so I can grant it access? Or is there really another underlying problem here?<\/p>\n<p>Code trying to access the secret:<\/p>\n<pre><code> client = secretmanager.SecretManagerServiceClient()\n secret_name = &quot;secret_name&quot;\n request = {'name': f&quot;path\/{secret_name}\/versions\/latest&quot;}\n response = client.access_secret_version(request)\n secret_string = response.payload.data.decode(&quot;UTF-8&quot;)\n<\/code><\/pre>\n<p>EDIT: I can add that I have been playing around a lot with account permissions but my best guess is that the account that is found under Vertex AI&gt;Workbench&gt;the notebook I am using's notebook details&gt;Service account is the one that needs permission. Is this not it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_date":"2022-03-16 14:21:53.49 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-17 11:00:39.537 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-secret-manager|google-cloud-vertex-ai",
        "Question_view_count":311,
        "Owner_creation_date":"2014-08-05 13:37:06.703 UTC",
        "Owner_last_access_date":"2022-09-22 12:30:19.65 UTC",
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71498754",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72237600,
        "Question_title":"GCP Vertex AI Managed Notebook cannot use custom container",
        "Question_body":"<p>In GCP Vertex AI, I created a Managed Notebook by specifying one of our custom containers which work perfectly with User-Managed Notebook kernels.\nThe Managed Notebook starts, and Jupyter Lab seems to work without any signs of error.<\/p>\n<p>Unfortunately, if I look at the available kernels in Jupyter Lab, only the default kernels are listed but not my custom kernel.<\/p>\n<p>An activity log entry on the right shows a spinning wheel &quot;Loading kernel from [custom container]&quot; which never disappears.<br \/>\nTaking a look at the terminal,<\/p>\n<p><em>docker image ls<\/em><\/p>\n<p>does not show the custom container either; obviously, it was not even pulled to the Managed Notebook.<\/p>\n<p>If I perform<\/p>\n<p><em>docker pull [custom container]<\/em><\/p>\n<p>in the terminal, to test connectivity to the Artifact Registry then it pulls the container correctly as expected.\nHowever, the custom kernel is still not visible in Jupyter Lab (even after a notebook restart).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-14 05:31:17.78 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_date":"2022-05-14 05:03:06.737 UTC",
        "Owner_last_access_date":"2022-05-14 18:13:06.367 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72237600",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73830357,
        "Question_title":"issues with passing command line args to custom training job vertex AI",
        "Question_body":"<p>I am trying to run basic custom training job<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n display_name='testjob-name',\n container_uri='gcr.io\/prj-id\/image-name:latest',\n project=project_id,\n credentials= credentials,\n staging_bucket= 'stage-bucket'\n)\n<\/code><\/pre>\n<p>using below code to run the job<\/p>\n<pre><code>job.run(\n        args=['--data_dir', '\/gcs\/bucket\/folder',\n        '--model_dir', '\/gcs\/bucket\/model',\n        '--configs', 'internal-config.yml'],\n        replica_count=1,\n        sync=True\n    )\n\n<\/code><\/pre>\n<p>Training job is getting exited with code 127 when I am passing args with job.run(). kindly help, what is the correct way to send command line arguments to custom training python script.\nThere isn\u2019t much coming up in logs either.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 16:02:25.487 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|custom-training",
        "Question_view_count":22,
        "Owner_creation_date":"2015-06-05 12:51:43.627 UTC",
        "Owner_last_access_date":"2022-09-25 05:29:10.83 UTC",
        "Owner_reputation":130,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73830357",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68456262,
        "Question_title":"GCP - Vertex AI Model - Access GCS failed",
        "Question_body":"<p>We have a Vertex AI model that was created using a custom image.\nWe are trying to access a bucket on startup but we are getting the following error:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/...?projection=noAcl&amp;prettyPrint=false: {service account name} does not have storage.buckets.get access to the Google Cloud Storage bucket.\n<\/code><\/pre>\n<p>The problem is that I can't find the service account that is mentioned in the error to give it the right access permissions..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-20 14:04:17.557 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":291,
        "Owner_creation_date":"2016-06-26 21:49:44.157 UTC",
        "Owner_last_access_date":"2022-09-24 07:38:31.393 UTC",
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68456262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70803996,
        "Question_title":"How to set environment variables for a User-managed notebook in Vertex AI",
        "Question_body":"<p>I am trying to set some environment variables for a user managed notebook in google cloud Vertex AI. I don't want to set this from a jupyter notebook itself because I want these environment variables to be available to anyone who opens a jupyter notebook from this notebook instance. This is what I have tried so far but nothing has worked:<\/p>\n<ol>\n<li>I have an existing user managed notebook. I ssh'd into the notebook vm and then set a environment variable, <code>export TEST_VAR=TEST_VARIABLE_WAS_SET<\/code> there. However, when I open a jupyter notebook from the console and do <code>os.environ[&quot;TEST_VAR&quot;]<\/code>, it gives a key error. So, I am assuming that this has something to do with the fact that the jupyter lab session that Vertex AI starts is in a different shell session or something similar. I also tried to add the following two metadata keys to the vm, and then restarted the vm, but it did not work:<\/li>\n<\/ol>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata startup-script-url=$GCS_BUCKET_NAME\/script.sh<\/code><\/p>\n<p>where script.sh is:<\/p>\n<pre><code>#!\/bin\/bash\n\nexport TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<p>AND<\/p>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata container-env-file=$GCS_BUCKET_NAME\/notebook-env.txt<\/code><\/p>\n<p>where notebook-env.txt is<\/p>\n<pre><code>TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<ol start=\"2\">\n<li>I also tried to create a new instance of a user managed notebook from the cloud console. In that I tried to provide a script in the &quot;Select a script to run after creation&quot; and also through the &quot;Metadata&quot; option by providing, the key as <code>startup-script-url<\/code> and the value as the script location on google cloud storage. The script was the same startup script earlier.<\/li>\n<\/ol>\n<p>So, how do I achieve this, for existing user managed notebooks and when I create new ones?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-21 15:48:01.007 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2022-01-23 22:07:38.733 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-ai-platform|google-cloud-vertex-ai|google-notebook",
        "Question_view_count":851,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70803996",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72184371,
        "Question_title":"Why is my Vertex AI dataset not displayed?",
        "Question_body":"<p>I created a Vetex AI dataset in <code>us-central1<\/code> and confirm it exists using:<\/p>\n<pre><code>vertex_ai.TabularDataset.list()\n<\/code><\/pre>\n<p>When I look at the UI I don't see any datsets, but I see a region drop-down, but no <code>us-central1<\/code>. Why is that? (The project is the correct one).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-10 10:07:24.937 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":89,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>It <strong>is<\/strong> there but at the beginning of the list, not with the other US ones.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-12 12:14:37.267 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72184371",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73154492,
        "Question_title":"Vertex AI custom job: cannot launch it via python",
        "Question_body":"<p>Trying to follow the example <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#create_custom_job-python\" rel=\"nofollow noreferrer\">here<\/a> to launch a custom job in vertex ai.<\/p>\n<p>This is the code I am using<\/p>\n<pre><code>from google.cloud import aiplatform\n\n\ndef create_custom_job_sample(\n    project: str,\n    display_name: str,\n    container_image_uri: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n    custom_job = {\n        &quot;display_name&quot;: display_name,\n        &quot;job_spec&quot;: {\n            &quot;worker_pool_specs&quot;: [\n                {\n                    &quot;machine_spec&quot;: {\n                        &quot;machine_type&quot;: &quot;n1-standard-4&quot;\n                    },\n                    &quot;replica_count&quot;: 1,\n                    &quot;container_spec&quot;: {\n                        &quot;image_uri&quot;: container_image_uri,\n                        &quot;command&quot;: [],\n                        &quot;args&quot;: [],\n                    },\n                }\n            ]\n        },\n    }\n    parent = f&quot;projects\/{project}\/locations\/{location}&quot;\n    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n    print(&quot;response:&quot;, response)\n\ncreate_custom_job_sample(\n    &quot;MY_PROJECT&quot;,\n    &quot;job-123&quot;,\n    &quot;europe-west1-docker.pkg.dev\/&lt;MYPROJECT&gt;\/&lt;MY_IMAGE_URI&gt;&quot;,\n    &quot;europe-west1&quot;,\n    &quot;eu-west1-aiplatform.googleapis.com&quot;\n)\n<\/code><\/pre>\n<p>However I get an error that start with<\/p>\n<pre><code>E0728 15:00:53.742356000 4417760704 hpack_parser.cc:1234]              Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8\n<\/code><\/pre>\n<p>and ends with\n<code>google.api_core.exceptions.Unknown: None Stream removed <\/code><\/p>\n<p>Don't understand what the problem is. The job starts correctly from the terminal with<\/p>\n<pre><code>gcloud ai custom-jobs create \\\n  --region=europe-west1 \\\n  --display-name=test-job-1 \\\n  --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,executor-image-uri=&lt;MY_IMAGE_URI&gt;,local-package-path=.,script=myfolder\/myscript.py\n<\/code><\/pre>\n<p>Could someone help me?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-28 14:26:29.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":160,
        "Owner_creation_date":"2017-01-27 15:07:04.88 UTC",
        "Owner_last_access_date":"2022-09-22 15:19:15.02 UTC",
        "Owner_reputation":1991,
        "Owner_up_votes":468,
        "Owner_down_votes":27,
        "Owner_views":107,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73154492",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71016472,
        "Question_title":"Vertex AI forecasting AutoML datatype mismatch",
        "Question_body":"<p>I could train the vertex AI AutoML forecating model but when I do batch prediction I get following error<\/p>\n<blockquote>\n<p>Batch prediction job batch_prediction encountered the following\nerrors:<\/p>\n<pre><code>Column &quot;sales&quot; expects type: NUMBER, the actual type is: STRING.\n<\/code><\/pre>\n<\/blockquote>\n<p>Below is a sample of test set I am passing for batch prediction in big query.<\/p>\n<p>According to the documentation for batch prediction we have to send some training\/historical data and forecasting dates. I did just that.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2022-02-07 09:49:06.773 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-07 12:36:35.457 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_date":"2015-12-16 17:49:09.287 UTC",
        "Owner_last_access_date":"2022-08-22 11:39:25.35 UTC",
        "Owner_reputation":500,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":"<p>Google recommend you to use the same input format for ingraining and prediction. Seams you have trained your model using a input format here the column sales were a <code>numeric<\/code> type, and now in the prediction you a using a BigQuery table with the <code>sales<\/code> column as <code>string<\/code>.<\/p>\n<p>Delete this table and import the data again defining the schema <strong>manually<\/strong>, and set sales as a numeric field, as following:<\/p>\n<pre><code>date:DATE,\nstore_product_id:STRING,\nsales:NUMERIC\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-09 15:31:12.183 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71016472",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69168991,
        "Question_title":"Authenticating model upload to VertexAI job from Cloud Scheduler",
        "Question_body":"<p>I am trying to run a custom training job on VertexAI. The goal is to train a model, save the model to cloud storage and then upload it to VertexAI as a VertexAI Model object. When I run the job from local workstation, it runs, but when I run the job from Cloud Scheduler  it fails. Details below.<\/p>\n<p><strong>Python Code for the job:<\/strong><\/p>\n<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport pickle\nfrom google.cloud import storage\nfrom google.cloud import aiplatform\n\nprint(&quot;FITTING THE MODEL&quot;)\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n# fit the model\nmodel.fit(X, y)\n\n\nprint(&quot;SAVING THE MODEL TO CLOUD STORAGE&quot;)\nif 'AIP_MODEL_DIR' not in os.environ:\n    raise KeyError(\n        'The `AIP_MODEL_DIR` environment variable has not been' +\n        'set. See https:\/\/cloud.google.com\/ai-platform-unified\/docs\/tutorials\/image-recognition-custom\/training'\n    )\n\nartifact_filename = 'model' + '.pkl'\n# Save model artifact to local filesystem (doesn't persist)\nlocal_path = artifact_filename\nwith open(local_path, 'wb') as model_file:\n    pickle.dump(model, model_file)\n\n# Upload model artifact to Cloud Storage\nmodel_directory = os.environ['AIP_MODEL_DIR']\nstorage_path = os.path.join(model_directory, artifact_filename)\nblob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\nblob.upload_from_filename(local_path)\n\n\nprint (&quot;UPLOADING MODEL TO VertexAI&quot;)\n\n# Upload the model to vertex ai\nproject=&quot;...&quot;\nlocation=&quot;...&quot;\ndisplay_name=&quot;custom_mdoel&quot;\nartifact_uri=model_directory\nserving_container_image_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/tf-cpu.2-4:latest&quot;\ndescription=&quot;test model&quot;\nsync=True\n\naiplatform.init(project=project, location=location)\nmodel = aiplatform.Model.upload(\n    display_name=display_name,\n    artifact_uri=artifact_uri,\n    serving_container_image_uri=serving_container_image_uri,\n    description=description,\n    sync=sync,\n)\nmodel.wait()\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p><strong>Running from Local Workstation:<\/strong>\nI set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the location of the Compute Engine default service account keys I have downloaded on my local workstation. I also set the AIP_MODEL_DIR environment variable to point to a cloud storage bucket. After I run the script, I can see the model.pkl file being created in the cloud storage bucket and the Model object  being created in VertexAI.<\/p>\n<p><strong>Triggering the training job from Cloud Scheduler:<\/strong>\nThis is what I ultimately want to achieve - to run the custom training job periodically from Cloud Scheduler. I have converted the python script above into a docker image and uploaded to google artifact registry. The job specification for the Cloud Scheduler is below, I can provide additional details if required. The service account email in the <code>oauth_token<\/code> is the same whose keys I  use to set the GOOGLE_APPLICATION_CREDENTIALS environment variable. When I run this, (either from local workstation or directly in a VertexAI notebook), I can see that the Cloud Scheduler job gets created which keeps triggering the custom job. The custom job is able to train the model and save it to the cloud storage. However, it is not able to upload it to VertexAI and I get the error meessages, <code>status = StatusCode.PERMISSION_DENIED<\/code> and <code>{...&quot;grpc_message&quot;:&quot;Request had insufficient authentication scopes.&quot;,&quot;grpc_status&quot;:7<\/code>}. Cannot figure out  what the authentication issue is because in both cases I am using the same service account.<\/p>\n<pre><code>job = {\n  &quot;name&quot;: f'projects\/{project_id}\/locations\/{location}\/jobs\/test_job',\n  &quot;description&quot;: &quot;Test scheduler job&quot;,\n  &quot;http_target&quot;: {\n    &quot;uri&quot;: f'https:\/\/{location}-aiplatform.googleapis.com\/v1\/projects\/{project_id}\/locations\/{location}\/customJobs',\n    &quot;http_method&quot;: &quot;POST&quot;,\n    &quot;headers&quot;: {\n      &quot;User-Agent&quot;: &quot;Google-Cloud-Scheduler&quot;,\n      &quot;Content-Type&quot;: &quot;application\/json; charset=utf-8&quot;\n    },\n    &quot;body&quot;: &quot;...&quot; \/\/ the custom training job body,\n    &quot;oauth_token&quot;: {\n      &quot;service_account_email&quot;: &quot;...&quot;,\n      &quot;scope&quot;: &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;\n    }\n  },\n  &quot;schedule&quot;: &quot;* * * * *&quot;,\n  &quot;time_zone&quot;: &quot;Africa\/Abidjan&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-13 20:56:27.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-iam|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":260,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69168991",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70740897,
        "Question_title":"Specifying Machine Type in Vertex AI Pipeline",
        "Question_body":"<p>I have a pipeline component defined like this:<\/p>\n<pre><code>data_task = run_ssd_data_op(\n        labels_path=input_labels,\n        data_config=config_task.outputs[&quot;output_data_config&quot;],\n        training_config=config_task.outputs[&quot;output_training_config&quot;],\n        assets_json=dump_conversion_task.outputs[&quot;output_ssd_query&quot;]\n    )\ndata_task.execution_options.caching_strategy.max_cache_staleness = &quot;P0D&quot;\ndata_task.container.add_resource_request('cpu', cpu_request)\ndata_task.container.add_resource_request('memory', memory_request)\n<\/code><\/pre>\n<p>When I run the pipeline on VertexAI the above component runs on an E2 machine type which matches the CPU and RAM requirements.<\/p>\n<p>However, the component runs much more slowly on VertexAI than on the Kubeflow pipeline I setup using AIPlatform. I configured that cluster to use N1-highmem-32 machines for this job.<\/p>\n<p>I would like to request that this component is run on an <code>n1-highmem-32<\/code> machine, how can I do that?<\/p>\n<p>For the GPU component of the pipeline I could use the line:<\/p>\n<pre><code>training_task.add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'NVIDIA_TESLA_T4').set_gpu_limit(\n        gpu_request)\n<\/code><\/pre>\n<p>What is the equivalent <code>node_selector_constraint<\/code> that I need to apply to my <code>data_task<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-01-17 11:50:52.48 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":603,
        "Owner_creation_date":"2012-10-15 12:32:21.05 UTC",
        "Owner_last_access_date":"2022-09-20 11:48:04.983 UTC",
        "Owner_reputation":3827,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":573,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Morecambe, United Kingdom",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70740897",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69566674,
        "Question_title":"Vertex AI endpoints timing out",
        "Question_body":"<p>I am using vertex-ai endpoints to serve a deep learning service.<\/p>\n<p>My service takes approximately 30s - 2 minutes to respond on CPU depending on the size of the input. I noticed that when the input size takes more than one minute to respond, the API fails, giving me this error:<\/p>\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n  &lt;meta charset=utf-8&gt;\n  &lt;meta name=viewport content=&quot;initial-scale=1, minimum-scale=1, width=device-width&quot;&gt;\n  &lt;title&gt;Error 502 (Server Error)!!1&lt;\/title&gt;\n  &lt;style&gt;\n    *{margin:0;padding:0}html,code{font:15px\/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* &gt; body{background:url(\/\/www.google.com\/images\/errors\/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/1x\/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat 0% 0%\/100% 100%;-moz-border-image:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  &lt;\/style&gt;\n  &lt;a href=\/\/www.google.com\/&gt;&lt;span id=logo aria-label=Google&gt;&lt;\/span&gt;&lt;\/a&gt;\n  &lt;p&gt;&lt;b&gt;502.&lt;\/b&gt; &lt;ins&gt;That\u2019s an error.&lt;\/ins&gt;\n  &lt;p&gt;The server encountered a temporary error and could not complete your request.&lt;p&gt;Please try again in 30 seconds.  &lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>\n<p>When I retry, I keep getting the same error. Once I decrease the input size, the API starts working again. For these reasons, I believe this is a timeout issue.<\/p>\n<p>So my question is: how can I change the timeout value in vertex-ai endpoints? I read through all the documentation, and it doesn't seem to be mentioned anywhere.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-10-14 07:29:25.023 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_date":"2014-08-04 11:08:23.73 UTC",
        "Owner_last_access_date":"2022-09-22 10:39:16.893 UTC",
        "Owner_reputation":387,
        "Owner_up_votes":37,
        "Owner_down_votes":2,
        "Owner_views":41,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69566674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70648776,
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-10 06:58:21.177 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-11 10:27:33.597 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_date":"2020-01-23 17:50:31.103 UTC",
        "Owner_last_access_date":"2022-09-15 02:08:52.26 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-10 10:36:28.04 UTC",
        "Answer_score":1.0,
        "Owner_location":"Tempe, AZ, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72232134,
        "Question_title":"What is the \"BigQuery project for exporting data\" in Vertex AI Model training",
        "Question_body":"<p>I am following <a href=\"https:\/\/codelabs.developers.google.com\/codelabs\/vertex-ai-custom-code-training#4\" rel=\"nofollow noreferrer\">this guide<\/a> in setting up a vertex AI pipeline. I am told to supply the &quot;BigQuery project for exporting data&quot;, except that the guide is not clear on what this BigQuery project is. I have tried supplying the name of my GCP Project to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0X5Gt.png\" rel=\"nofollow noreferrer\">Here is the photo of the field needing this BigQuery project<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UZvX3.png\" rel=\"nofollow noreferrer\">Here is a photo of the guide that requests this information<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-13 15:37:29.073 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":37,
        "Owner_creation_date":"2022-05-13 15:33:40.877 UTC",
        "Owner_last_access_date":"2022-06-11 07:01:53.7 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72232134",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72389357,
        "Question_title":"Why does gcloud not see my managed notebook?",
        "Question_body":"<p>I have a Vertex AI managed notebook in <code>europe-west1-d<\/code>. I try both (locally and in the notebook):<\/p>\n<pre><code>gcloud notebooks instances list --location=europe-west1-d\ngcloud compute instances list --filter=&quot;my-notebook-name&quot; --format &quot;[box]&quot;\n<\/code><\/pre>\n<p>and both return nothing. What am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-26 09:04:53.293 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"gcloud|google-cloud-vertex-ai",
        "Question_view_count":29,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>When creating a Vertex AI managed notebook, it is expected that the instance won't appear on your project. By design, the notebook instance is created in a Google managed project and is not visible to the end user.<\/p>\n<p>But if you want to see instance details of your managed notebooks, you can use the Notebooks API to send a request to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/reference\/rest\/v1\/projects.locations.runtimes\/list\" rel=\"nofollow noreferrer\">runtimes.list<\/a>. See example request:<\/p>\n<pre><code>project=your-project-here\nlocation=us-central1 #adjust based on your location\n\ncurl -X GET -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/notebooks.googleapis.com\/v1\/projects\/${project}\/locations\/${location}\/runtimes&quot;\n<\/code><\/pre>\n<p>Response output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-27 00:16:32.52 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72389357",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70494454,
        "Question_title":"How to run a vertex AI pipeline for Automl Time series forecasting?",
        "Question_body":"<p>I've been trying this for very long time to run a Automl time series forecasting as pipeline in GCP.<\/p>\n<p>I am able to run the time series model with GUI manually but not able to make a pipeline because of the documentation.<\/p>\n<p>In GCP documentation they provide vertex AI pipeline docs for classification regression video and image processing<\/p>\n<p><strong>but the time series alone missing<\/strong><\/p>\n<p>Any one who had done auto ml forecasting as pipeline in GCP please help me with that..<\/p>\n<pre><code>                            !! Thankyou !!\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-27 10:45:38.79 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|time-series|pipeline|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":73,
        "Owner_creation_date":"2021-09-13 13:57:06.92 UTC",
        "Owner_last_access_date":"2022-01-08 05:03:49.143 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Madurai, Tamil Nadu, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70494454",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71038823,
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-08 18:13:44.883 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|protocol-buffers|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2018-03-01 19:41:46.78 UTC",
        "Owner_last_access_date":"2022-09-25 03:26:23.16 UTC",
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-09 05:52:12.89 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-02-09 06:15:44.88 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69257475,
        "Question_title":"Custom Model for Batch Prediction on Vertex.ai",
        "Question_body":"<p>I want to run batch predictions inside Google Cloud's vertex.ai using a custom trained model.  I was able to find documentation to get online prediction working with a custom built docker image by setting up an endpoint, but I can't seem to find any documentation on what the Dockerfile should be for batch prediction.  Specifically how does my custom code get fed the input and where does it put the output?<\/p>\n<p>The documentation I've found is <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions?_ga=2.262616524.-1738078585.1553812508&amp;_gac=1.83826788.1630620723.Cj0KCQjw7MGJBhD-ARIsAMZ0eesi3e2_fghoLZcWRKqw_ZbncT3LeZvgYPu929bJELdeiX3RSNHPApcaAo8dEALw_wcB#custom-trained_3\" rel=\"nofollow noreferrer\">here<\/a>, it certainly looks possible to use a custom model and when I tried it didn't complain, but eventually it did throw an error.  According to the documentation no endpoint is required for running batch jobs.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":10,
        "Question_creation_date":"2021-09-20 15:55:16.607 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":480,
        "Owner_creation_date":"2018-10-30 18:54:03.62 UTC",
        "Owner_last_access_date":"2022-09-23 17:00:36.977 UTC",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Utah, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69257475",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71986344,
        "Question_title":"VertexAI Batch Inference Failing for Custom Container Model",
        "Question_body":"<p>I'm having trouble executing VertexAI's batch inference, despite endpoint deployment and inference working perfectly. My TensorFlow model has been trained in a custom Docker container with the following arguments:<\/p>\n<pre><code>aiplatform.CustomContainerTrainingJob(\n        display_name=display_name,\n        command=[&quot;python3&quot;, &quot;train.py&quot;],\n        container_uri=container_uri,\n        model_serving_container_image_uri=container_uri,\n        model_serving_container_environment_variables=env_vars,\n        model_serving_container_predict_route='\/predict',\n        model_serving_container_health_route='\/health',\n        model_serving_container_command=[\n            &quot;gunicorn&quot;,\n            &quot;src.inference:app&quot;,\n            &quot;--bind&quot;,\n            &quot;0.0.0.0:5000&quot;,\n            &quot;-k&quot;,\n            &quot;uvicorn.workers.UvicornWorker&quot;,\n            &quot;-t&quot;,\n            &quot;6000&quot;,\n        ],\n        model_serving_container_ports=[5000],\n)\n<\/code><\/pre>\n<p>I have a Flask endpoint defined for predict and health essentially defined below:<\/p>\n<pre><code>@app.get(f&quot;\/health&quot;)\ndef health_check_batch():\n    return 200\n\n@app.post(f&quot;\/predict&quot;)\ndef predict_batch(request_body: dict):\n    pred_df = pd.DataFrame(request_body['instances'],\n                           columns = request_body['parameters']['columns'])\n    # do some model inference things\n    return {&quot;predictions&quot;: predictions.tolist()}\n<\/code><\/pre>\n<p>As described, when training a model and deploying to an endpoint, I can successfully hit the API with JSON schema like:<\/p>\n<pre><code>{&quot;instances&quot;:[[1,2], [1,3]], &quot;parameters&quot;:{&quot;columns&quot;:[&quot;first&quot;, &quot;second&quot;]}}\n<\/code><\/pre>\n<p>This also works when using the endpoint Python SDK and feeding in instances\/parameters as functional arguments.<\/p>\n<p>However, I've tried performing batch inference with a CSV file and a JSONL file, and every time it fails with an Error Code 3. I can't find logs on why it failed in Logs Explorer either. I've read through all the documentation I could find and have seen other's successfully invoke batch inference, but haven't been able to find a guide. Does anyone have recommendations on batch file structure or the structure of my APIs? Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-24 07:29:05.693 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":111,
        "Owner_creation_date":"2016-06-28 17:55:10.36 UTC",
        "Owner_last_access_date":"2022-05-23 21:53:23.47 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71986344",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73542706,
        "Question_title":"How to solve VertexAI prediction endpoint error?",
        "Question_body":"<p>I am trying to get predictions from an endpoint that is already created in VertexAI through UI.<\/p>\n<p>I am getting an error when I run the following code<\/p>\n<pre><code>from typing import Dict\nimport pandas as pd\nfrom google.cloud import aiplatform\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nfrom google.oauth2 import service_account\n\nkey_path = '..\/golden-tempest-xxxxx.json'\ncredentials = service_account.Credentials.from_service_account_file(key_path, scopes=[\n    &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;], )\n\n\naiplatform.init(\n    project='golden-tempest-xxxxx',\n    location='us-central1',\n    credentials=credentials,\n)\n\ndef predict_tabular_classification_sample(\n    project: str,\n    endpoint_id: str,\n    instance_dict: Dict,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n    instance = json_format.ParseDict(instance_dict, Value())\n    instances = [instance]\n    parameters_dict = {}\n    parameters = json_format.ParseDict(parameters_dict, Value())\n    endpoint = client.endpoint_path(\n        project=project, location=location, endpoint=endpoint_id\n    )\n    response = client.predict(\n        endpoint=endpoint, instances=instances, parameters=parameters\n    )\n    print(&quot;response&quot;)\n    print(&quot; deployed_model_id:&quot;, response.deployed_model_id)\n    # See gs:\/\/google-cloud-aiplatform\/schema\/predict\/prediction\/tabular_classification_1.0.0.yaml for the format of the predictions.\n    predictions = response.predictions\n    for prediction in predictions:\n        print(&quot; prediction:&quot;, dict(prediction))\n\n\ndf = pd.read_csv('..\/btc_test_classification_2.csv')\n\ndf_dict = df.to_dict('index')\n\npredict_tabular_classification_sample(\n    project=&quot;xxxxx&quot;,\n    endpoint_id=&quot;886215168080478208&quot;,\n    location=&quot;us-central1&quot;,\n    instance_dict={'instances': [df_dict[4]]}\n)\n<\/code><\/pre>\n<p><strong>Error<\/strong>:<\/p>\n<pre><code>InvalidArgument: 400 {&quot;error&quot;: &quot;Column prefix: . Error: Missing struct property: tick_count.&quot;}\n<\/code><\/pre>\n<p>My training data contains the same columns I have in test. <a href=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" alt=\"columns\" \/><\/a><\/p>\n<p>I am not sure why I am getting this error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-08-30 12:46:34.927 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-api|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_date":"2015-03-14 11:20:42.323 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:39.243 UTC",
        "Owner_reputation":1026,
        "Owner_up_votes":236,
        "Owner_down_votes":1,
        "Owner_views":256,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73542706",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68131742,
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-25 13:25:10.52 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-29 13:04:34.477 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":606,
        "Owner_creation_date":"2017-04-04 08:33:44.967 UTC",
        "Owner_last_access_date":"2022-09-23 09:42:54.4 UTC",
        "Owner_reputation":3107,
        "Owner_up_votes":442,
        "Owner_down_votes":19,
        "Owner_views":434,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-06-28 14:08:58.667 UTC",
        "Answer_score":1.0,
        "Owner_location":"Herefordshire, UK",
        "Answer_last_edit_date":"2021-06-29 09:56:38.497 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72007351,
        "Question_title":"Vertex AI Pipelines: APIs from v1 namespace not supported in v2 compiler",
        "Question_body":"<p>I started taking <a href=\"https:\/\/stackoverflow.com\/a\/71973994\/16706763\">my first steps<\/a> in Vertex AI on past week. As you can see, that basic code works, however it displays the following warning:<\/p>\n<pre><code>\/home\/jupyter\/.local\/lib\/python3.7\/site-packages\/kfp\/v2\/compiler\/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n  category=FutureWarning,\n<\/code><\/pre>\n<p>Plus, if I run the same pipeline again, &quot;the warning disappears&quot;. What am I missing in my code, which could make that warning disappear at all times (i.e., solve it)?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-04-26 01:24:50.497 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-26 17:06:58.687 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":199,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72007351",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70833206,
        "Question_title":"Update a custom container model in VertexAI in GCP",
        "Question_body":"<p>I have already deployed a custom container in VertexAI and it is working fine, but now I would like to update my model and endpoint. I have pushed the new version of my Docker image to the Artifact Registry and I was expecting this latest version was gonna be automatically connected to VertexAI, but apparently, VertexAI is still using my old version.<\/p>\n<p>How can I update the model in VertexAI automatically without having to create a new model and endpoint every time I make a new release and change the Docker image?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-24 11:33:48.623 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":142,
        "Owner_creation_date":"2020-12-09 17:29:57 UTC",
        "Owner_last_access_date":"2022-09-21 16:18:53.273 UTC",
        "Owner_reputation":306,
        "Owner_up_votes":53,
        "Owner_down_votes":2,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Madrid, Espa\u00f1a",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833206",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73515143,
        "Question_title":"Google Vertex Auto-ML Forecast every 30 mins predict 2 hours",
        "Question_body":"<p><strong>GOAL<\/strong><\/p>\n<p>Every 30 mins I get a new bunch of price related data:<\/p>\n<ul>\n<li>CurrentDatetime, CurrentPrice, Feature1, Feature2<\/li>\n<\/ul>\n<p>I want to predict the price in 2 hours from now, so 4x30mins (4 steps into the future)<\/p>\n<p><strong>PROBLEM DESCRIPTION<\/strong><\/p>\n<p>I am puzzled what google vertex auto-ml forecasting is doing and if I can trust the results I am getting. Also unsure how to use a trained model for batch predicting.<\/p>\n<p><strong>WHAT I DID<\/strong><\/p>\n<p>I think the way to set up the training dataset is to add:<\/p>\n<ol>\n<li>TargetDateTime column (2 hours ahead of CurrentDatetime)<\/li>\n<li>TargetActualPrice column (the actual price 2 hours into the future)<\/li>\n<li>TimeSeriesId column (always equal to 1 as all the data is one time<br \/>\nseries).<\/li>\n<\/ol>\n<p>This means, every 30 mins I now have:<\/p>\n<ul>\n<li>CurrentDatetime, CurrentPrice, Feature1, Feature2, TargetDateTime, TargetActualPrice, TimeSeriesId<\/li>\n<\/ul>\n<p>I use this dataset to train an auto-ml forecast model, setting:<\/p>\n<ul>\n<li>&quot;series Identifier Column&quot; to TimeSeriesId,<\/li>\n<li>&quot;Target Column&quot; to TargetActualPrice,<\/li>\n<li>&quot;Timestamp Column&quot; to TargetDateTime<\/li>\n<li>&quot;Data granularity&quot; to 30mins<\/li>\n<li>&quot;Forecast Horizon&quot; to 4<\/li>\n<li>&quot;Context Window&quot; to 4  (use last 2 hours of historic data to predict next 2 hours)<\/li>\n<li>Split train\/val\/test chronologically (on TargetDateTime as is Timestamp column)<\/li>\n<\/ul>\n<p>This model trains and gives some results.<\/p>\n<p>Looking at the saved test data set, I can see 4 rows for each TargetDateTime, with a <em>predictedvalue<\/em> column containing a price prediction and a <em>predicted_on_TargetDateTime<\/em> column which goes from CurrentDateTime to TargetDateTime in 30 mins intervals.<\/p>\n<p>This makes sense, for every 30 mins of input data, the model makes 4 predictions, each 30 mins into the future, ending up with a prediction 2 hours into the future. Great.<\/p>\n<p><strong>PROBLEM 1 : Batch predictions<\/strong><\/p>\n<p>I get confused when I try to use this trained model to make batch predictions. The crux of the problem is that Vertex will look at the batch input dataset, find the first row (30 min input data) for which there is no actual price data yet (TargetActualPrice is null) and then predict the next 4 steps (2 hours). This seems to mean, to make a next prediction, I would need to wait for the actuals of the previous prediction. But that means, when I get the next set of input data (30 mins later, and 1.30 hrs out from previous prediction target), I cannot use the model to make a new prediction cause the previous prediction has not TargetActualPrice yet.<\/p>\n<p>To make it more explicit, suppose I have the following batch data:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>CurrentDatetime<\/th>\n<th>CurrentPrice<\/th>\n<th>Feature1<\/th>\n<th>Feature2<\/th>\n<th>TargetDateTime<\/th>\n<th>TargetActualPrice<\/th>\n<th>TimeSeriesId<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>11:00<\/td>\n<td>$2.1<\/td>\n<td>3.4<\/td>\n<td>abc<\/td>\n<td>13:00<\/td>\n<td>$2.4<\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>11:30<\/td>\n<td>$2.2<\/td>\n<td>3.3<\/td>\n<td>abd<\/td>\n<td>13:30<\/td>\n<td>$2.5<\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>12:00<\/td>\n<td>$2.3<\/td>\n<td>3.1<\/td>\n<td>abe<\/td>\n<td>14:00<\/td>\n<td>$2.6<\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>12:30<\/td>\n<td>$2.3<\/td>\n<td>3.0<\/td>\n<td>abe<\/td>\n<td>14:30<\/td>\n<td>$2.7<\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>13:00<\/td>\n<td>$2.4<\/td>\n<td>2.9<\/td>\n<td>abf<\/td>\n<td>15:00<\/td>\n<td><em>null<\/em><\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>13:30<\/td>\n<td>$2.5<\/td>\n<td>2.8<\/td>\n<td>abg<\/td>\n<td>15:30<\/td>\n<td><em>null<\/em><\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>14:00<\/td>\n<td>$2.6<\/td>\n<td>2.7<\/td>\n<td>abh<\/td>\n<td>16:00<\/td>\n<td><em>null<\/em><\/td>\n<td>1<\/td>\n<\/tr>\n<tr>\n<td>14:30<\/td>\n<td>$2.7<\/td>\n<td>2.6<\/td>\n<td>abi<\/td>\n<td>16:30<\/td>\n<td><em>null<\/em><\/td>\n<td>1<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>In the batch data above, I have 2 hours (4 rows) of historic data with actuals (11:00-12:30). Current time is 14:30 so I don't have actuals for 15:00 yet. The last prediction made was with the 13:00 input data (as it is the first row with actual data = null). The 13:30 - 14:30 rows I cannot use for a new prediction until I have the 15:00 actuals.<\/p>\n<p>This doesn't make sense to me. I should be able to make a new 4 hour prediction every 30 mins? I must be doing something wrong?<\/p>\n<p>Is the solution that, when I get the next 30 mins of input data, should I put the last predicted value into the actuals column (and update with real actuals once I have it) to proceed with next prediction? Seems cumbersome.<\/p>\n<p><strong>PROBLEM 2 : Leakage<\/strong><\/p>\n<p>My other concern with this is how Vertex is training and calculating the results. I am worried that when (during training) Vertex picks up the next row of 30 mins data, it will create a prediction based on the previous 4x30 mins of data (2 hour &quot;Context window&quot;) <em>INCLUDING<\/em> the TargetActualPrice data for those rows. But this would be incorrect, as the TargetActualPrice value is 2 hours into the future and not yet available when the next 30 mins of data comes in. This would mean leakage of actual data, predicting using actuals before they are known (ie cheating).<\/p>\n<p><strong>SUMMARY<\/strong><\/p>\n<p>In summary, I  am hoping someone can tell if I am setting the dataset up incorrectly, and\/or how to batch predict every 30 mins.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-28 00:00:29.567 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-28 23:41:25.927 UTC",
        "Question_score":0,
        "Question_tags":"time-series|forecasting|google-cloud-vertex-ai",
        "Question_view_count":39,
        "Owner_creation_date":"2022-08-27 22:16:32.177 UTC",
        "Owner_last_access_date":"2022-09-20 04:18:42.897 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73515143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72174602,
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-09 15:35:21.003 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-10 14:43:07.553 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71101070,
        "Question_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Question_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-13 13:26:50.23 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":225,
        "Owner_creation_date":"2016-06-06 14:08:12.253 UTC",
        "Owner_last_access_date":"2022-09-22 14:59:43.617 UTC",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Answer_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-14 21:24:49.257 UTC",
        "Answer_score":1.0,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71101070",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69478536,
        "Question_title":"How to log metrics from component using YAML specification?",
        "Question_body":"<p>We are using kpf v2 in Google Clouds Vertex AI  and want to log metrics from a component built by a yaml specification.<\/p>\n<p>We can only make this work from a Python function based component were we can call the <code>mlpipeline-metrics<\/code> (of type <code>Output[Metrics]<\/code>) with <code>log_metrics(..)<\/code> directly.<\/p>\n<p>The docs at <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/pipelines-metrics\/\" rel=\"nofollow noreferrer\">https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/pipelines-metrics\/<\/a> says that you can write your metrics in json from inside the container to an OutputPath named MLPipeline Metrics. But this doesn't work in Vertex AI since the metr8cs does not show up in the console.<\/p>\n<p>Our component is set up like this:<\/p>\n<pre><code>name: a component\n\noutputs:\n- {name: MLPipeline Metrics, type: Metrics}\n\nimplementation:\n  container:\n    image: eu.gcr.io\/project\/knark\/base:latest\n    command: [python3, -m, module.foo,\n              --metrics, {outputPath: MLPipeline Metrics}\n<\/code><\/pre>\n<p>With the metrics file being written like this:<\/p>\n<pre><code>parser = ArgumentParser()\nparser.add_argument('-m', '--metrics', default='\/mlpipeline-metrics.json')\nopts = parser.parse_args()\n\nmetrics = {'cluster-entropy': .5, 'inertia': 100.}\n\nwith open(metrics_path, 'w') as f:\n    json.dump({'metrics': [{'name': m, 'value': v, 'format': 'RAW'} for m, v in metrics.items()]}, f)\n<\/code><\/pre>\n<p>We also tried setting<\/p>\n<pre><code>    fileOutputs:\n      mlpipeline-metrics: \/mlpipeline-metrics.json\n<\/code><\/pre>\n<p>in the component which seems to be the way to do it earlier but this doesn't appear to work either.<\/p>\n<p>Are there any details I'm missing in writing out the metrics or does this not work at all or is it unsupported in Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-10-07 09:17:37.667 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_date":"2019-01-08 10:22:10.44 UTC",
        "Owner_last_access_date":"2021-11-16 22:08:35.06 UTC",
        "Owner_reputation":111,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69478536",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71214828,
        "Question_title":"Vertex AI 504 Errors in batch job - How to fix\/troubleshoot",
        "Question_body":"<p>We have a Vertex AI model that takes a relatively long time to return a prediction.<\/p>\n<p>When hitting the model endpoint with one instance, things work fine.  But batch jobs of size say 1000 instances end up with around 150 504 errors (upstream request timeout). (We actually need to send batches of 65K but I'm troubleshooting with 1000).<\/p>\n<p>I tried increasing the number of replicas assuming that the # of instances handed to the model would be (1000\/# of replicas) but that doesn't seem to be the case.<\/p>\n<p>I then read that the default batch size is 64 and so tried decreasing the batch size to 4 like this from the python code that creates the batch job:<\/p>\n<p>model_parameters = dict(batch_size=4)<\/p>\n<pre><code>def run_batch_prediction_job(vertex_config):\n\n    aiplatform.init(\n        project=vertex_config.vertex_project, location=vertex_config.location\n    )\n\n    model = aiplatform.Model(vertex_config.model_resource_name)\n\n    model_params = dict(batch_size=4)\n    batch_params = dict(\n        job_display_name=vertex_config.job_display_name,\n        gcs_source=vertex_config.gcs_source,\n        gcs_destination_prefix=vertex_config.gcs_destination,\n        machine_type=vertex_config.machine_type,\n        accelerator_count=vertex_config.accelerator_count,\n        accelerator_type=vertex_config.accelerator_type,\n        starting_replica_count=replica_count,\n        max_replica_count=replica_count,\n        sync=vertex_config.sync,\n        model_parameters=model_params\n    )\n\n    batch_prediction_job = model.batch_predict(**batch_params)\n\n    batch_prediction_job.wait()\n\n    return batch_prediction_job\n<\/code><\/pre>\n<p>I've also tried increasing the machine type to n1-high-cpu-16 and that helped somewhat but I'm not sure I understand how batches are sent to replicas?<\/p>\n<p>Is there another way to decrease the number of instances sent to the model?\nOr is there a way to increase the timeout?\nIs there log output I can use to help figure this out?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-22 01:15:43.853 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":194,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Berkeley, CA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71214828",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73838593,
        "Question_title":"CGP Vertex AI notebook - turn on Secure Boot",
        "Question_body":"<p>When creating a notebook instance via GCP console, I can check a box &quot;Turn on Secure Boot&quot;. Is it possible to turn on Secure Boot when creating the notebook using gcloud command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-24 15:50:57.44 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-24 15:54:33.08 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcloud|google-cloud-vertex-ai",
        "Question_view_count":13,
        "Owner_creation_date":"2020-06-22 20:10:29.63 UTC",
        "Owner_last_access_date":"2022-09-24 20:56:33.457 UTC",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>It's a little involved, but <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/shielded-vm\" rel=\"nofollow noreferrer\">yes you can<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-24 16:39:49.95 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73838593",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69157376,
        "Question_title":"How to view and Interprete Vertex AI Logs",
        "Question_body":"<p>We have deployed Models in the Vertex AI endpoint.\nNow we want to know and interpret logs regarding events\nof Node creation, POD creation, user API call matric etc.<\/p>\n<p>Is there any way or key by which we can filter the logs for Analysis?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-09-13 04:45:36.94 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-22 13:19:31.87 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":309,
        "Owner_creation_date":"2017-01-03 19:47:17.57 UTC",
        "Owner_last_access_date":"2022-02-22 06:30:56.963 UTC",
        "Owner_reputation":72,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69157376",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70001381,
        "Question_title":"creating custom model on Google vertex ai",
        "Question_body":"<p>I should use Google\u2019s managed ML platform Vertex AI to build an end-to-end machine learning workflow for an internship. Although I completely follow the tutorial, when I run a training job, I see this error message:<\/p>\n<pre><code>Training pipeline failed with error message: There are no files under &quot;gs:\/\/dps-fuel-bucket\/mpg\/model&quot; to copy.\n<\/code><\/pre>\n<p>based on the tutorial, we should not have a \/model directory in the bucket. And the model should create this directory and save the final result there.<\/p>\n<pre><code># Export model and save to GCS\nmodel.save(BUCKET + '\/mpg\/model')\n<\/code><\/pre>\n<p>I added this directory but still face this error.\nDoes anybody have any idea, thanks in advance :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 08:56:13.937 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-bucket",
        "Question_view_count":132,
        "Owner_creation_date":"2021-11-17 08:37:31.163 UTC",
        "Owner_last_access_date":"2022-03-17 11:22:27.09 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70001381",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68331232,
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-10 20:15:41.62 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_date":"2016-02-18 09:07:52.727 UTC",
        "Owner_last_access_date":"2022-09-23 20:24:50.547 UTC",
        "Owner_reputation":405,
        "Owner_up_votes":36,
        "Owner_down_votes":1,
        "Owner_views":100,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-20 11:50:02.423 UTC",
        "Answer_score":1.0,
        "Owner_location":"Varna, Bulgaria",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69703127,
        "Question_title":"How to invoke custom prediction logic in Vertex AI?",
        "Question_body":"<p>Goal: serve prediction requests from a Vertex AI Endpoint by executing custom prediction logic.<\/p>\n<p>Detailed steps:\nFor example, we may already uploaded have an image_quality.pb model (developed in a non-vertex-ai pythonic environment) in a GCS bucket<\/p>\n<p>Next, we want to create a custom image inference logic by deserializing the deployed  model and serving the inference functionality in a vertex AI endpoint<\/p>\n<p>Finally, we want to pass a list of images (stored in another GCS bucket) to that endpoint.<\/p>\n<p>We also want to see the logs and metrics in tensorboard.<\/p>\n<p>Existing Vertex AI code samples provide examples for invoking model.batch_predict \/ endpoint. predict, but don't mention how to execute custom prediction code.<\/p>\n<p>It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-25 05:13:57.68 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-10-26 05:56:41.977 UTC",
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":133,
        "Owner_creation_date":"2015-02-08 00:42:09.317 UTC",
        "Owner_last_access_date":"2021-11-29 04:25:35.947 UTC",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703127",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71557442,
        "Question_title":"How combine results from multiple models in Google Vertex AI?",
        "Question_body":"<p>I have multiple models in Google Vertex AI and I want to create an endpoint to serve my predictions.\nI need to run aggregation algorithms, like the Voting algorithm on the output of my models.\nI have not found any ways of using the models together so that I can run the voting algorithms on the results.\nDo I have to create a new model, curl my existing models and then run my algorithms on the results?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-03-21 12:11:39.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":253,
        "Owner_creation_date":"2013-06-28 08:22:58.7 UTC",
        "Owner_last_access_date":"2022-09-19 12:57:37.43 UTC",
        "Owner_reputation":134,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":74,
        "Answer_body":"<p>There is no in-built provision to implement aggregation algorithms in Vertex AI. To <code>curl<\/code> results from the models then aggregate them, we would need to deploy all of them to individual endpoints. Instead, I would suggest the below method to deploy the models and the meta-model(aggregate model) to a single endpoint using <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom containers for prediction<\/a>. The custom container requirements can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>You can load the model artifacts from GCS into a custom container. If the same set of models are used (i.e) the input models to the meta-model do not change, you can package them inside the container to reduce load time. Then, a custom HTTP logic can be used to return the aggregation output like so. This is a sample custom flask server logic.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_models_from_gcs():\n    ## Pull the required model artifacts from GCS and load them here.\n    models = [model_1, model_2, model_3]\n    return models\n\ndef aggregate_predictions(predictions):\n    ## Your aggregation algorithm here\n    return aggregated_result\n\n\n@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    models = get_models_from_gcs()\n    predictions = []\n    \n    for model in models:\n        predictions.append(model.predict(preprocessed_inputs))\n\n    aggregated_result = aggregate_predictions(predictions)\n\n    return {&quot;aggregated_predictions&quot;: aggregated_result}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-22 11:53:45.93 UTC",
        "Answer_score":1.0,
        "Owner_location":"Oslo, Norway",
        "Answer_last_edit_date":"2022-03-22 12:01:24.41 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71557442",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73210389,
        "Question_title":"Vertex AI Executor gives NoSuchKernel",
        "Question_body":"<p>I have a simple hello-world ipynb file in a Vertex AI notebook instance that looks like this:<\/p>\n<pre><code>print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>When setting up an executor for this as shown below I receive the following error in the executor logs: <em>jupyter_client.kernelspec.NoSuchKernel: No such kernel named local-python3\nerror<\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The notebook has the following metadata<\/p>\n<pre><code>{\n    &quot;kernelspec&quot;: {\n        &quot;display_name&quot;: &quot;Python 3 (Local)&quot;,\n        &quot;language&quot;: &quot;python&quot;,\n        &quot;name&quot;: &quot;local-python3&quot;\n    },\n    &quot;language_info&quot;: {\n        &quot;codemirror_mode&quot;: {\n            &quot;name&quot;: &quot;ipython&quot;,\n            &quot;version&quot;: 3\n        },\n        &quot;file_extension&quot;: &quot;.py&quot;,\n        &quot;mimetype&quot;: &quot;text\/x-python&quot;,\n        &quot;name&quot;: &quot;python&quot;,\n        &quot;nbconvert_exporter&quot;: &quot;python&quot;,\n        &quot;pygments_lexer&quot;: &quot;ipython3&quot;,\n        &quot;version&quot;: &quot;3.7.12&quot;\n    }\n}\n<\/code><\/pre>\n<p>What would require to run this notebook successfully? I looked into the possibility of customer containers but that should be to much of a complex solution for such.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-02 15:51:43.42 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_date":"2020-04-21 08:20:19.48 UTC",
        "Owner_last_access_date":"2022-09-21 12:47:49.243 UTC",
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73210389",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73025242,
        "Question_title":"Batch Prediction with scikit-learn using jsonl in Vertex AI",
        "Question_body":"<p>I have a scikit-learn model successfully trained and loaded onto Vertex AI, but I can't seem to do batch prediction with jsonl. I've tried using these formats with jsonl:<\/p>\n<pre><code>{&quot;dense_input&quot;: [1, 2, 3, ...]}\n{&quot;dense_input&quot;: [4, 5, 6, ...]}\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>{&quot;val_1&quot;: 1, &quot;val_2&quot;: 2, ...}\n{&quot;val_1&quot;: 4, &quot;val_2&quot;: 5, ...}\n<\/code><\/pre>\n<p>but I get this error for both:<\/p>\n<blockquote>\n<p>('Post request fails. Cannot get predictions. Error: Predictions are not in the response. Got: {&quot;error&quot;: &quot;Prediction failed: Exception during sklearn prediction: float() argument must be a string or a number, not 'dict'&quot;}.', 2)<\/p>\n<\/blockquote>\n<p>I've tried batch prediction using a CSV file and it works fine, but I'm having difficulty with the jsonl file. Does anyone know what's the problem? Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-18 15:56:45.463 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"json|google-cloud-platform|scikit-learn|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":73,
        "Owner_creation_date":"2022-07-18 15:32:34.55 UTC",
        "Owner_last_access_date":"2022-09-18 18:25:28.147 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73025242",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73584495,
        "Question_title":"Vertex AI endpoints - dataset creation after every batch prediction",
        "Question_body":"<p>Vertex AI endpoints provide Batch Prediction functionality that is very useful to perform predictions on a large amount of data.<\/p>\n<p>However, every time I make a new batch prediction the endpoint creates a new Dataset inside BigQuery. This is very uncomfortable because if I make 100 batch predictions per day, I will obtain 100 new Datasets inside BigQuery.<\/p>\n<p>Is there a way to make all predictions converge inside the same Dataset? I mean, each prediction is a new Table inside a specific Dataset of BigQuery.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-02 14:56:44.653 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":56,
        "Owner_creation_date":"2021-01-23 09:17:32.76 UTC",
        "Owner_last_access_date":"2022-09-05 08:22:27.55 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73584495",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70670669,
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-11 17:04:40.143 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":168,
        "Owner_creation_date":"2020-01-23 17:50:31.103 UTC",
        "Owner_last_access_date":"2022-09-15 02:08:52.26 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-12 07:56:38.663 UTC",
        "Answer_score":1.0,
        "Owner_location":"Tempe, AZ, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70428593,
        "Question_title":"Vertex AI seems to think a deployed model input shape is different then when predicting locally",
        "Question_body":"<p>I'm getting what I see as strange behavior out of a deployed model in vertex ai.  I have a CNN model built with tensorflow\/keras version 2.7.  My input data is a 3 dimensional array with the follow shape (1, 570, 33).  When I pass the input data to the model locally I have a correct response.<\/p>\n<pre><code>model = keras.models.load_model('model')\nx = model.predict(input_data) # input_data is a numpy array of shape (1, 570, 33)   \nprint(x)\n[[0.1259355  0.9124526  0.65782744 0.2628207 ]]\n<\/code><\/pre>\n<p>This is a correct prediction and the model does what it is trained to do.  No problems<\/p>\n<p>When I upload the model to Vertex AI using the prebuilt Tensorflow 2.7 docker container with no extra settings (no acceleration for example) and deploy that model to an endpoint this is what I get when I call predict with the same input_data formatted for Vertex AI.<\/p>\n<pre><code>resp = client.predict(\n    endpoint=endpoint_path,\n    instances=input_data.toList(),\n    parameters=parameters,\n)\n\ninput must be 4-dimensional[1,570,33]\\n\\t [[{{function_node __inference__wrapped_model_28143}}{{node sequential\/conv2d\/BiasAdd}}]]\n<\/code><\/pre>\n<p>Here is the summary of of the model<\/p>\n<pre><code>Model: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)             (None, 570, 33, 32)       320       \n                                                                \nbatch_normalization (BatchN  (None, 570, 33, 32)      128       \normalization)                                                   \n                                                                \nactivation (Activation)     (None, 570, 33, 32)       0         \n                                                                \nconv2d_1 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_1 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_1 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_2 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_2 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_2 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_3 (Conv2D)           (None, 285, 17, 64)       18496     \n                                                                \nbatch_normalization_3 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_3 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_4 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_4 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_4 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_5 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_5 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_5 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_6 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_6 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_6 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_7 (Conv2D)           (None, 143, 9, 96)        55392     \n                                                                \nbatch_normalization_7 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_7 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_8 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_8 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_8 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_9 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_9 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_9 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_10 (Conv2D)          (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_10 (Bat  (None, 143, 9, 96)       384       \nchNormalization)                                                \n                                                                \nactivation_10 (Activation)  (None, 143, 9, 96)        0         \n                                                                \nconv2d_11 (Conv2D)          (None, 72, 5, 128)        110720    \n                                                                \nbatch_normalization_11 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_11 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_12 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_12 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_12 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_13 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_13 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_13 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_14 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_14 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_14 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_15 (Conv2D)          (None, 36, 3, 160)        184480    \n                                                                \nbatch_normalization_15 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_15 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_16 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_16 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_16 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_17 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_17 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_17 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_18 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_18 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_18 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_19 (Conv2D)          (None, 18, 2, 192)        276672    \n                                                                \nbatch_normalization_19 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_19 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_20 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_20 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_20 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_21 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_21 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_21 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_22 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_22 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_22 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_23 (Conv2D)          (None, 9, 1, 224)         387296    \n                                                                \nbatch_normalization_23 (Bat  (None, 9, 1, 224)        896       \nchNormalization)                                                \n                                                                \nactivation_23 (Activation)  (None, 9, 1, 224)         0         \n                                                                \nreshape (Reshape)           (None, 9, 224)            0         \n                                                                \nmasking (Masking)           (None, 9, 224)            0         \n                                                                \nlambda (Lambda)             (None, 224)               0         \n                                                                \ndense (Dense)               (None, 4)                 900       \n                                                                \n=================================================================\nTotal params: 3,554,532\nTrainable params: 3,548,772\nNon-trainable params: 5,760\n<\/code><\/pre>\n<p>I've got a classic case of 'It works on my machine' here and could use any input or help :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-20 22:12:38.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-21 00:24:23.673 UTC",
        "Question_score":1,
        "Question_tags":"python|numpy|tensorflow|keras|google-cloud-vertex-ai",
        "Question_view_count":178,
        "Owner_creation_date":"2011-11-15 04:07:02.043 UTC",
        "Owner_last_access_date":"2022-09-23 19:05:45.94 UTC",
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70428593",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70159582,
        "Question_title":"Google Cloud Platform - Vertex AI Image classification training fails with no specific error message",
        "Question_body":"<p>I'm doing an image classification task using Vertex AI and after about 3 hours of training it fails. The error message is nondescript. It says &quot;Training pipeline failed with error message: Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.&quot;<\/p>\n<p>It's done that for three of my models using the same image dataset (about 45k large). What could be the error here? How can I find out?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-11-29 18:27:51.447 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-29 21:56:51.49 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|google-cloud-platform|computer-vision|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_date":"2014-02-18 22:34:13.817 UTC",
        "Owner_last_access_date":"2022-09-22 15:59:48.447 UTC",
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70159582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69543552,
        "Question_title":"Vertex Pipeline Metric values not being added to metrics artifact?",
        "Question_body":"<p>We are trying to return some metrics from our Vertex Pipeline, such that they are visible in the Run Comparison and Metadata tools in the Vertex UI.<\/p>\n<p>I saw <a href=\"https:\/\/codelabs.developers.google.com\/vertex-mlmd-pipelines#4\" rel=\"nofollow noreferrer\">here<\/a> that we can use this output type <code>Output[Metrics]<\/code>, and the subsequent <code>metrics.log_metric(&quot;metric_name&quot;, metric_val)<\/code> method to add the metrics, and it seemed from the available documentation that this would be enough.<\/p>\n<p>We want to use the reusable component method as opposed to python function based components, around which the example is based. So we implemented it within our component code like so:<\/p>\n<p>We added the output in the component.yaml:<\/p>\n<pre><code>outputs:\n    - name: metrics\n      type: Metrics\n      description: evaluation metrics path\n<\/code><\/pre>\n<p>then added the output to the command in the implemenation:<\/p>\n<pre><code>        command: [\n            python3, main.py,\n            --gcs-test-data-path,       {inputValue: gcs_test_data_path},\n            --gcs-model-path,  {inputValue: gcs_model_path},\n            --gcs-output-bucket-id,  {inputValue: gcs_output_bucket_id},\n            --project-id, {inputValue: project_id},\n            --timestamp, {inputValue: timestamp},\n            --batch-size, {inputValue: batch_size},\n            --img-height, {inputValue: img_height},\n            --img-width,  {inputValue: img_width},\n            --img-depth,  {inputValue: img_depth},\n            --metrics,  {outputPath: metrics},\n        ]\n<\/code><\/pre>\n<p>Next in the components main python script, we parse this argument with argparse:<\/p>\n<pre><code>PARSER.add_argument('--metrics',\n                    type=Metrics,\n                    required=False,\n                    help='evaluation metrics output')\n<\/code><\/pre>\n<p>and pass it to the components main function:<\/p>\n<pre><code>if __name__ == '__main__':\n    ARGS = PARSER.parse_args()\n    evaluation(gcs_test_data_path=ARGS.gcs_test_data_path,\n               gcs_model_path=ARGS.gcs_model_path,\n               gcs_output_bucket_id=ARGS.gcs_output_bucket_id,\n               project_id=ARGS.project_id,\n               timestamp=ARGS.timestamp,\n               batch_size=ARGS.batch_size,\n               img_height=ARGS.img_height,\n               img_width=ARGS.img_width,\n               img_depth=ARGS.img_depth,\n               metrics=ARGS.metrics,\n               )\n<\/code><\/pre>\n<p>in the declaration of the component function, we then typed this metrics parameter as <code>Output[Metrics]<\/code><\/p>\n<pre><code>from kfp.v2.dsl import Output, Metrics\n\ndef evaluation(gcs_test_data_path: str,\n               gcs_model_path: str,\n               gcs_output_bucket_id: str,\n               metrics: Output[Metrics],\n               project_id: str,\n               timestamp: str,\n               batch_size: int,\n               img_height: int,\n               img_width: int,\n               img_depth: int):\n<\/code><\/pre>\n<p>finally, we implement the log_metric method within this evaluation function:<\/p>\n<pre><code>    metrics.log_metric('accuracy', acc)\n    metrics.log_metric('precision', prec)\n    metrics.log_metric('recall', recall)\n    metrics.log_metric('f1-score', f_1)\n<\/code><\/pre>\n<p>When we run this pipeline, we can see this metric artifact materialised in the DAG:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TG5cr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TG5cr.png\" alt=\"Metrics artifact visible in the DAG\" \/><\/a><\/p>\n<p>And Metrics Artifacts are listed in the Metadata UI in Vertex:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jehCH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jehCH.png\" alt=\"Metrics Artifacts are listed in the metadata UI\" \/><\/a><\/p>\n<p>However, clicking through to view the artifacts JSON, there is no Metadata listed:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/E3IFE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E3IFE.png\" alt=\"No Metadata attached to artifact\" \/><\/a><\/p>\n<p>In addition, No Metadata is visible when comparing runs in the pipeline UI:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qZPD2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qZPD2.png\" alt=\"No Metadata in the pipeline UI\" \/><\/a><\/p>\n<p>Finally, navigating to the Objects URI in GCS, we are met with 'Requested entity was not found.', which I assume indicates that nothing was written to GCS:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6z7pJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6z7pJ.png\" alt=\"No Object in GCS\" \/><\/a><\/p>\n<p>Are we doing something wrong with this implementation of metrics in the reusable components? From what I can tell, this all seems right to me, but it's hard to tell given the docs at this point seem to focus primarily on examples with Python Function based components.<\/p>\n<p>Do we perhaps need to proactively write this Metrics object to an OutputPath?<\/p>\n<p>Any helps is appreciated.<\/p>\n<p>----- UPDATE ----<\/p>\n<p>I have since been able to get artifact metadata and URI To update. In the end we used kfp sdk to generate a yaml file based on a @component decorated python function, we then adapted this format for our reusable components.\nOur component.yaml now looks like this:<\/p>\n<pre><code>name: predict\ndescription: Prepare and create predictions request\nimplementation:\n    container:\n      args:\n      - --executor_input\n      - executorInput: null\n      - --function_to_execute\n      - predict\n      command:\n      - python3\n      - -m\n      - kfp.v2.components.executor_main\n      - --component_module_path\n      - predict.py\n      image: gcr.io\/PROJECT_ID\/kfp\/components\/predict:latest\ninputs: \n    - name: input_1\n      type: String\n    - name: intput_2\n      type: String\noutputs:\n    - name: output_1\n      type: Dataset\n    - name: output_2\n      type: Dataset\n<\/code><\/pre>\n<p>with this change to the yaml, we can now successfully update the artifacts metadata dictionary, and uri through <code>artifact.path = '\/path\/to\/file'<\/code>. These updates are displayed in the Vertex UI.<\/p>\n<p>I am still unsure why the <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/v2\/component-development\/#creating-a-component-specification\" rel=\"nofollow noreferrer\">component.yaml format specified in the Kubeflow documentation<\/a> does not work - I think this may be a bug with Vertex Pipelines.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-12 15:52:31.997 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-01 14:48:10.49 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":355,
        "Owner_creation_date":"2021-07-12 12:40:50.203 UTC",
        "Owner_last_access_date":"2022-08-18 15:04:10.047 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69543552",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70754016,
        "Question_title":"Creating a Vertex AI Workbench with a Non Organization Account and problems with constraints\/compute.vmExternalIpAccess",
        "Question_body":"<p>I'm trying to create a Vertex AI Workbench on GCP, but every time I try I get the following error:<\/p>\n<p><em>&lt;Workbench Name&gt; Constraint constraints\/compute.vmExternalIpAccess violated for project &lt;Project ID&gt;. Add instance &lt;Workbench ID&gt; to the constraint to use external IP with it.<\/em><\/p>\n<p>I went to the Organization Policies page to edit the constraint: <em>constraints\/compute.vmExternalIpAccess<\/em> and saw that is denied for all (which is odd because in the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints\" rel=\"nofollow noreferrer\">constraints documentation<\/a> it says that is should be enabled for all by default). Now, the problem is that when I go to edit the constraint, it says that it requires this set of permissions:<\/p>\n<ul>\n<li><em>orgpolicy.policies.create<\/em><\/li>\n<li><em>orgpolicy.policies.delete<\/em><\/li>\n<li><em>orgpolicy.policies.update<\/em><\/li>\n<li><em>orgpolicy.policy.get<\/em><\/li>\n<\/ul>\n<p>which are all part of the role: <em>roles\/orgpolicy.policyAdmin<\/em> that can only be granted at an organization level, and well, I have a Non Organization Account.<\/p>\n<p>Am I missing something?<\/p>\n<p>Thank for your time!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2022-01-18 10:28:24.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|workbench|google-cloud-vertex-ai",
        "Question_view_count":333,
        "Owner_creation_date":"2012-11-20 02:51:44.267 UTC",
        "Owner_last_access_date":"2022-05-25 19:01:53.253 UTC",
        "Owner_reputation":215,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70754016",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70623713,
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-07 15:42:30.64 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-07 20:47:51.21 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70916238,
        "Question_title":"AlphaFold on VertexAI - Stuck in setting up notebook for 2 hours",
        "Question_body":"<p>I am trying to run AlphaFold on VertexAI as explained <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/running-alphafold-on-vertexai\" rel=\"nofollow noreferrer\">here<\/a>. However, my instance creation is stuck in this state for roughly two hours now. There is no error message either. I am wondering if something has gone wrong or this is just the expected time it will take to setup a new instance?<\/p>\n<p>I actually tried with two different notebooks. One is the <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/raw\/main\/community-content\/alphafold_on_workbench\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">default one<\/a> linked in the above article and the other is <a href=\"https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb<\/a><\/p>\n<p>Both are in the same state for roughly the same time.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-30 15:43:01.82 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_date":"2012-11-30 09:46:45.153 UTC",
        "Owner_last_access_date":"2022-09-23 18:26:35.997 UTC",
        "Owner_reputation":388,
        "Owner_up_votes":411,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Islamabad Capital Territory, Pakistan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70916238",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69878915,
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2021-11-08 05:15:55.97 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":1271,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2021-11-08 20:54:50.01 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-11-08 21:00:49.843 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70664460,
        "Question_title":"Is it possible to create and train natural-language google-cloud model using python sdk?",
        "Question_body":"<p>I want to create a Vertex pipeline using KFP for training natural language model, and I can't find a python API to use for creating and training model, I know that there is the option of creating the model from the console, but I am looking for a way to do it on my git repository.<\/p>\n<p>any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-11 09:36:09.17 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|automl|google-cloud-vertex-ai|google-natural-language",
        "Question_view_count":13,
        "Owner_creation_date":"2019-07-16 08:24:31.333 UTC",
        "Owner_last_access_date":"2022-09-22 08:10:40.627 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70664460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68465990,
        "Question_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Question_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-21 08:03:17.743 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-21 09:06:10.043 UTC",
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":742,
        "Owner_creation_date":"2016-06-26 21:49:44.157 UTC",
        "Owner_last_access_date":"2022-09-24 07:38:31.393 UTC",
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-30 17:29:41.5 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-08-25 23:59:47.217 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68465990",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70435505,
        "Question_title":"Vertex AI AutoML getting data about Model, Dataset, Training Job",
        "Question_body":"<p>I am using Vertex AI for AutoML Video classification and I would like to get some data that I'm seeing in Web UI (Cloud Console) (Model\/Dataset detail).\nI'm using AI platform Python SDK or REST API.<\/p>\n<p>For example Model API returns 'training videos' but not test videos (web Model detail, tab EVALUATE)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" alt=\"Vertex AI Model evaluation\" \/><\/a><\/p>\n<p>then for example in tab Model Properties on the web I can't obtain Training time, Total items, Algorithm, Objective, Total Items<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" alt=\"Vertex AI Model properties\" \/><\/a><\/p>\n<p>For Dataset detail, I would like to get number of labeled\/unlabeled videos, labels and correspoding number<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" alt=\"Dataset detail, labels\" \/><\/a><\/p>\n<p>This is code I'm using to get the data (as component in Vertex AI Pipeline):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_metadata(project_id, region, model_id):\n    import requests\n\n    import google.auth\n    import google.cloud.aiplatform as aip\n    from google.cloud import aiplatform_v1\n    from google.protobuf import json_format\n    from google.auth.transport import requests as grequests\n\n    aip.init(project=project_id, location=region)\n    API_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(region)\n    model = aip.Model(model_id)\n    model_dict = model.to_dict()\n    model_metadata = model_dict['metadata']\n\n    model_name = model_dict['displayName']\n    model_creation_date = model_dict['createTime']\n    model_type = model_metadata['modelType']\n    number_training = model_metadata['trainingDataItemsCount']\n\n    client_options = {\n        &quot;api_endpoint&quot;: API_ENDPOINT\n    }\n    model_path = model.resource_name\n    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)\n    list_eval_request = aiplatform_v1.types.ListModelEvaluationsRequest(parent=model_path)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n\n    eval_name = ''\n    for val in list_eval:\n        eval_name = val.name\n    get_eval_request = aiplatform_v1.types.GetModelEvaluationRequest(name=eval_name)\n    model_eval = client_model.get_model_evaluation(request=get_eval_request)\n    model_eval_data = json_format.MessageToDict(model_eval._pb)\n\n    model_metrics = model_eval_data['metrics']\n    average_precision = model_metrics.get('auPrc')\n    confidence_metrics = model_metrics['confidenceMetrics']\n    confidence_threshold = -1\n    f1_score = -1\n    precision = -1\n    recall = -1\n\n    for item in confidence_metrics:\n        confidence_threshold_temp = item['confidenceThreshold']\n        if confidence_threshold_temp &gt;= 0.5:\n            confidence_threshold = confidence_threshold_temp\n            f1_score = item['f1Score']\n            precision = item['precision']\n            recall = item['recall']\n            break\n    # auc_precision = precision\n    # auc_recall = recall\n\n    credentials, _ = google.auth.default()\n    r = grequests.Request()\n    credentials.refresh(r)\n    training_pipeline_resource_name = model_dict['trainingPipeline']\n\n    training_pipeline_url = f'https:\/\/{API_ENDPOINT}\/v1beta1\/{training_pipeline_resource_name}'\n    headers = {\n        'Authorization': f'Bearer {credentials.token}'\n    }\n    r = requests.get(training_pipeline_url, headers=headers)\n    training_pipeline_detail = r.json()\n    input_data_config = training_pipeline_detail.get('inputDataConfig', {})\n    dataset_id = input_data_config.get('datasetId', '')\n    fraction_split = input_data_config.get('fractionSplit', {})\n    test_fraction = fraction_split.get('testFraction')\n    training_fraction = fraction_split.get('trainingFraction')\n    data_split = f'{training_fraction}\/{test_fraction}'\n\n    dataset = aip.VideoDataset(dataset_id)\n    dataset_resource = json_format.MessageToDict(dataset.gca_resource._pb)\n    dataset_name = dataset_resource.get('displayName')\n    dataset_creation_date = dataset_resource.get('createTime')\n    labels = dataset_resource['labels']\n    dataset_type = labels.get('aiplatform.googleapis.com\/dataset_metadata_schema')\n\n    data = {\n        'model_id': model_id,\n        'model_name': model_name,\n        'model_creation_date': model_creation_date,\n        'model_type': model_type,\n        'number_training': number_training,\n        'average_precision': average_precision,\n        'precision': precision,\n        'recall': recall,\n        'data_split': data_split,\n        'dataset_name': dataset_name,\n        'dataset_type': dataset_type,\n        'dataset_id': dataset_id,\n        'dataset_creation_date': dataset_creation_date,        \n    }\n\n<\/code><\/pre>\n<p>Also for example what I found is that on training job when I created dataset, training model via WebUI I can obtain data split (training\/testing ratio), but when I'm doing this in Vertex AI Pipelines, I'm not explicitly setting data split for AutoMLVideoTrainingJobRunOp, I can't get data split from Training job detail, so it seems that it saved only when it's explicitly set.<\/p>\n<p>Other thing I noticed is when API requests are made for Cloud Console (inspecting Chrome Dev Tools) it returns more (richer) data (items) then for public Vertex AI APIs.<\/p>\n<p>I'm not sure if this is temporary or intentional\/permanent behaviour.<\/p>\n<p>I would appreciate thoughts\/comments\/help with this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-12-21 12:29:50.567 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai|google-cloud-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":395,
        "Owner_creation_date":"2013-06-25 21:45:24.86 UTC",
        "Owner_last_access_date":"2022-09-24 17:35:26.44 UTC",
        "Owner_reputation":276,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Prague, Czech Republic",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70435505",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72783902,
        "Question_title":"Is it possible to run Vertex AI Workbench on Spot machines?",
        "Question_body":"<p>I'm trying to save budget on jupyter notebooks on Google Cloud but couldn't find a way to run Vertex AI Workbench (Notebooks) on spot machines.\nWhat are my alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-28 09:25:32.77 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-notebook|jupyter|google-cloud-vertex-ai",
        "Question_view_count":176,
        "Owner_creation_date":"2011-11-25 20:39:39.12 UTC",
        "Owner_last_access_date":"2022-08-22 07:33:27.01 UTC",
        "Owner_reputation":73,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72783902",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72864285,
        "Question_title":"Retrieving the cpu, gpu specs on a deployed Deep Learning VM",
        "Question_body":"<p>I already deployed a Deep Learning VM by configuring cpu and gpu. I would like to retrieve that information.<\/p>\n<p>Is there a way to find out what gpu and cpu specification my deployed DeepLearning VM has?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-05 05:13:13.507 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-06 00:47:02.413 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|deep-learning|google-cloud-vertex-ai|google-ai-platform|google-dl-platform",
        "Question_view_count":61,
        "Owner_creation_date":"2013-05-26 03:45:19.747 UTC",
        "Owner_last_access_date":"2022-09-22 01:29:51.95 UTC",
        "Owner_reputation":311,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72864285",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68675615,
        "Question_title":"Tracking resources used by VertexAI pipeline",
        "Question_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-06 03:07:12.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-vertex-ai",
        "Question_view_count":272,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-06 14:42:50.907 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-08-11 14:27:16.933 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675615",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73584269,
        "Question_title":"Vertex AI automatic retraining",
        "Question_body":"<p>I\u2019m trying to create a Vertex AI endpoint with Monitoring enabled that can trigger a Vertex AI pipeline execution when one of the deployed models drops its performance.\nHowever, Vertex AI does not provide any built-in feature to do it.\nIs there a method to capture the alert thrown by Vertex AI Monitoring and trigger the Pipeline?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-02 14:41:11.993 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":48,
        "Owner_creation_date":"2018-11-30 14:55:35.083 UTC",
        "Owner_last_access_date":"2022-09-23 15:08:04.723 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73584269",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73085293,
        "Question_title":"Library is not installed on PATH - How can I install on path?",
        "Question_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-22 19:34:07.04 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|installation|package|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_date":"2021-05-21 18:13:40.567 UTC",
        "Owner_last_access_date":"2022-08-09 15:09:16.75 UTC",
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-23 17:21:01.407 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72593351,
        "Question_title":"Jupyter Lab instance crashes with 502 error",
        "Question_body":"<p>I am using a JupyterLab virtual notebook instance from GCP Vertex AI Workbench.<\/p>\n<p>I am reading 2 billion rows of data where each row is comprised of 3 columns of\n8 bytes each.<\/p>\n<p>I am reading 100 million rows of data at a time and concatenating it to Pandas dataframe.<\/p>\n<p>All of sudden, the notebook becomes unresponsive with 502 error.<\/p>\n<p>I realize that the virtual machine crashed.<\/p>\n<p>Here is the spec to the virtual machine:\nn1-standard 64  240GB RAM\n100 GB drive<\/p>\n<p>One time, I was successful to reach 2 billion rows.\nBut all of sudden, to my dismay, it crashed with that error.<\/p>\n<p>Google doc just mentions to restart the kernel.\nThat is not so easy when it took more than 1 hour to read 2 billion rows of data.\nThis means more than 1 hour of work just got wasted.<\/p>\n<p>What is causing this error?\nWhy the error occurs so inconsistently?\nWhere is the error message for this to crash?\nOr is this an error related to pandas dataframe?\nI am creating a dataframe that have 2 billion rows.\nIf pandas cannot handle rows of this magnitude, it should simply\ncause a run time error, not crashing a virtual machine.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-12 15:00:19.523 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-15 01:13:39.327 UTC",
        "Question_score":1,
        "Question_tags":"pandas|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_date":"2016-07-11 00:52:26.11 UTC",
        "Owner_last_access_date":"2022-09-25 03:59:16.887 UTC",
        "Owner_reputation":932,
        "Owner_up_votes":192,
        "Owner_down_votes":10,
        "Owner_views":367,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"overland park, kansas",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593351",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73746352,
        "Question_title":"I am wondering about limitations of BQML. What are some of the biggest limitations for data science projects when using BQML?",
        "Question_body":"<p>I am  dealing with an unbalanced panel dataset. So, I have a timestamp column as well as an orders column, a categories columns, product name,  office location column and selling price column and a few other columns. If I am trying to forecast how much I would be selling for both every category and product name for the next year given three years worth of data, Would BQML be a good place to start? Any recommended tutorials to get me started?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-16 14:29:05.493 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-bigquery|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training|multivariate-time-series",
        "Question_view_count":22,
        "Owner_creation_date":"2022-09-07 21:07:02.113 UTC",
        "Owner_last_access_date":"2022-09-24 19:41:56.25 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73746352",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72423143,
        "Question_title":"How to load images from Vertex AI managed dataset inside Python training code?",
        "Question_body":"<p>I am trying to create a <strong>custom training job<\/strong> in Vertex AI. I created a managed dataset stored in the same bucket I am exporting the training code to.\nI have a Python code that looks like this:<\/p>\n<pre><code>#Defining paths \nTRAIN_PATH = os.environ['AIP_TRAINING_DATA_URI']\nVAL_PATH = os.environ['AIP_VALIDATION_DATA_URI']\n\n#skipped model definition#\n\ntrain_datagen = image.ImageDataGenerator(rescale = 1.\/255, shear_range = 0.2,zoom_range = 0.2, horizontal_flip = True)\n\ntest_dataset = image.ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = 'binary')\nvalidation_generator = test_dataset.flow_from_directory(\n    VAL_PATH,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = 'binary')\n\nhist_new = model.fit(\n     train_generator, ...)\n<\/code><\/pre>\n<p>The question is, how do I load the images so the ImageDataGenerator can use them?\nThe error I get when starting the training job is:<\/p>\n<pre><code> No such file or directory: 'gs:\/\/(bucket name)\/dataset-5820440723492700160-image_classification_multi_label-2022-05-29T10:53:33.245485Z\/training-*'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-29 11:14:59.06 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|google-cloud-vertex-ai",
        "Question_view_count":98,
        "Owner_creation_date":"2019-01-04 19:50:49.567 UTC",
        "Owner_last_access_date":"2022-07-31 14:05:54.17 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72423143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68569313,
        "Question_title":"How can I run Google Cloud's \"AI Notebooks\" on a schedule automatically?",
        "Question_body":"<p>Notebooks in the Google Cloud Platform has been great for Python development in the cloud, but the last missing piece is just running existing notebooks on a schedule. There's a million different tools (Airflow, Papermill, Google Cloud Jobs, Google Cloud Scheduler, Google Cloud Cron Jobs), and as someone not as familiar with Cloud, it's really easy to get lost. Any suggestion? Thanks guys!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-29 02:25:24.81 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-03 08:39:21.7 UTC",
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1317,
        "Owner_creation_date":"2018-04-09 01:06:01.717 UTC",
        "Owner_last_access_date":"2022-01-21 07:39:34.45 UTC",
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68569313",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72580712,
        "Question_title":"kubeflow component - why so many ways to define a component and what are the differences?",
        "Question_body":"<p>Please help understand what are the meaningful\/significant differences among different ways to create kubeflow pipeline components and the reason for having so many ways?<\/p>\n<pre><code>from kfp.components import func_to_container_op\n\n@func_to_container_op\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.v2.dsl import component\n\n@component\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.components import create_component_from_func\n\n@create_component_from_func\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-10 23:44:45.283 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":77,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72580712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70003299,
        "Question_title":"Vertex AI returns a different result from the local tflite model",
        "Question_body":"<p>I uploaded my tflite model on Vertex AI and made an endpoint, and I requested inference with some input value, but it returns a different result from my local tflite model's inference result.<\/p>\n<p>The input value is float32 array(actually sampled audio data) and I used <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this code<\/a> for the request. Although there was the same input array, the local tflite model and the model which is uploaded on Vertex AI returns quite big different result.<\/p>\n<p>Is there any possibility of distortion on the value while it transfers to the Vertex AI instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 11:09:27.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-17 22:29:31.083 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":175,
        "Owner_creation_date":"2021-08-27 00:34:21.567 UTC",
        "Owner_last_access_date":"2022-08-11 02:53:07.69 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70003299",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72989531,
        "Question_title":"WebScrapping error in Google Cloud - Vertex AI Workbench (using Python3 & Selenium)",
        "Question_body":"<p>I am trying to use Workbench Managed Notebooks to schedule some Jupyter notebooks to run Selenium for webscrapping some pages.<\/p>\n<p>My code is below:<\/p>\n<pre><code>from get_gecko_driver import GetGeckoDriver\nget_driver = GetGeckoDriver()\nget_driver = GetGeckoDriver()\nget_driver.install()\n\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\n\nopts = FirefoxOptions()\nopts.add_argument(&quot;--headless&quot;)\nbrowser = webdriver.Firefox(options=opts)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nSessionNotCreatedException                Traceback (most recent call last)\n\/tmp\/ipykernel_1\/3371773802.py in &lt;module&gt;\n      4 opts = FirefoxOptions()\n      5 opts.add_argument(&quot;--headless&quot;)\n----&gt; 6 browser = webdriver.Firefox(options=opts)\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/firefox\/webdriver.py in __init__(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\n    178             command_executor=executor,\n    179             options=options,\n--&gt; 180             keep_alive=True)\n    181 \n    182         self._is_remote = False\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in __init__(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\n    275         self._authenticator_id = None\n    276         self.start_client()\n--&gt; 277         self.start_session(capabilities, browser_profile)\n    278 \n    279     def __repr__(self):\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in start_session(self, capabilities, browser_profile)\n    368         w3c_caps = _make_w3c_caps(capabilities)\n    369         parameters = {&quot;capabilities&quot;: w3c_caps}\n--&gt; 370         response = self.execute(Command.NEW_SESSION, parameters)\n    371         if 'sessionId' not in response:\n    372             response = response['value']\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in execute(self, driver_command, params)\n    433         response = self.command_executor.execute(driver_command, params)\n    434         if response:\n--&gt; 435             self.error_handler.check_response(response)\n    436             response['value'] = self._unwrap_value(\n    437                 response.get('value', None))\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/errorhandler.py in check_response(self, response)\n    245                 alert_text = value['alert'].get('text')\n    246             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 247         raise exception_class(message, screen, stacktrace)\n    248 \n    249     def _value_or_default(self, obj: Mapping[_KT, _VT], key: _KT, default: _VT) -&gt; _VT:\n\nSessionNotCreatedException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line\n<\/code><\/pre>\n<p>It seems that I need to setup somehow the location of Firefox, but I don't know if this is possible and how to do it (or if there are some alternatives that are easier to setup e.g. chromium).<\/p>\n<p><em>Please Note: this code is running perfectly fine when I run Jupyter notebooks in VM instances but I cannot schedule those notebooks to run automatically so I guess my only option is to go with the Vertex AI &gt; Workbench &gt; Managed Notebooks solution.<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 05:47:23.243 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|selenium|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":137,
        "Owner_creation_date":"2021-06-17 07:19:52.097 UTC",
        "Owner_last_access_date":"2022-07-22 04:46:15.717 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72989531",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70379395,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-16 12:56:03.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-17 07:35:42.407 UTC",
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":346,
        "Owner_creation_date":"2014-08-05 13:37:06.703 UTC",
        "Owner_last_access_date":"2022-09-22 12:30:19.65 UTC",
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-12-16 15:09:19.983 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-12-17 14:14:16.897 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70831704,
        "Question_title":"Batch Prediction Job non-blocking",
        "Question_body":"<p>I am running a Vertex AI batch prediction using the python API.\nThe function I am using is from the google cloud docs:<\/p>\n<pre><code>def create_batch_prediction_job_dedicated_resources_sample(\n    key_path,\n    project: str,\n    location: str,\n    model_display_name: str,\n    job_display_name: str,\n    gcs_source: Union[str, Sequence[str]],\n    gcs_destination: str,\n    machine_type: str = &quot;n1-standard-2&quot;,\n    sync: bool = True,\n):\n    credentials = service_account.Credentials.from_service_account_file(\n    key_path)\n\n# Initilaize an aiplatfrom object\n aiplatform.init(project=project, location=location, credentials=credentials)\n\n# Get a list of Models by Model name\n models = aiplatform.Model.list(filter=f'display_name=&quot;{model_display_name}&quot;')\n model_resource_name = models[0].resource_name\n\n# Get the model\n my_model = aiplatform.Model(model_resource_name)\n\n batch_prediction_job = my_model.batch_predict(\n    job_display_name=job_display_name,\n    gcs_source=gcs_source,\n    gcs_destination_prefix=gcs_destination,\n    machine_type=machine_type,\n    sync=sync,\n)\n\n #batch_prediction_job.wait_for_resource_creation()\n batch_prediction_job.wait()\n\n print(batch_prediction_job.display_name)\n print(batch_prediction_job.resource_name)\n print(batch_prediction_job.state)\n return batch_prediction_job\n\ndatetime_today = datetime.datetime.now()\nmodel_display_name = 'test_model'\nkey_path = 'vertex_key.json'\nproject = 'my_project'\nlocation = 'asia-south1'\njob_display_name = 'batch_prediction_' + str(datetime_today)\nmodel_name = '1234'\ngcs_source = 'gs:\/\/my_bucket\/Cleaned_Data\/user_item_pairs.jsonl'\ngcs_destination = 'gs:\/\/my_bucket\/prediction'\n\ncreate_batch_prediction_job_dedicated_resources_sample(key_path,project,location,model_display_name,job_display_name,\n                                                      gcs_source,gcs_destination)\n<\/code><\/pre>\n<p>OUTPUT:<\/p>\n<pre><code>92 current state:\nJobState.JOB_STATE_RUNNING\nINFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects\/my_project\/locations\/asia-south1\/batchPredictionJobs\/37737350127597649\n<\/code><\/pre>\n<p>The above output is being printed on the terminal over and over after every few seconds.<\/p>\n<p>The issue that I have is that the python program calling this function keeps on running until it is force stopped. I have tried both <code>batch_prediction_job.wait()<\/code> &amp; <code>batch_prediction_job.wait_for_resource_creation()<\/code> with the same results.<\/p>\n<p>How do I start a batch_prediction_job without waiting for it to complete and terminating the program just after the job has be created?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-01-24 09:36:50.77 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-27 05:35:34.943 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":270,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Gurugram, Haryana, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70831704",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73345417,
        "Question_title":"Vertex AI Instance Out of Disk Space",
        "Question_body":"<p>I accidentally ran out of disk space on my Vertex AI instance. There's no way to connect to it by any means now. Using the jupyter failed as there's not enough disk space:\n<code>OSError: [Errno 28] No space left on device<\/code><\/p>\n<p>I tried to increase disk space for both boot and data disks using <code>gcloud compute disks resize<\/code>, but it still doesn't work despite disk space being shown as increased in the machine info panel.<\/p>\n<p>Also tried connecting through ssh but got timeouts. My guess is that it's still caused by disk space.<\/p>\n<p>So is there any ways to recover the instance without a hard reset?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-13 15:12:11.073 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-13 15:13:15.093 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":71,
        "Owner_creation_date":"2022-08-13 15:02:38.963 UTC",
        "Owner_last_access_date":"2022-09-20 21:40:25.03 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73345417",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68795098,
        "Question_title":"Error when trying to use CustomPythonPackageTrainingJobRunOp in VertexAI pipeline",
        "Question_body":"<p>I am using the google cloud pipeline component CustomPythonPackageTrainingJobRunOp in a VertexAI pipeline . I have been able to run this package successfully as a CustomTrainingJob before. I can see multiple (11) error messages in the logs but the only one that seems to make sense to me is, &quot;ValueError: too many values to unpack (expected 2) &quot; but I am unable to figure out the solution. I can add all the other error messages too if required. I am logging some messages at the start of the training code so I know the errors happen before the training code is executed. I am completely stuck on this. Links to samples where someone has used CustomPythonPackageTrainingJobRunOp in a pipeline would very helpful as well. Below is the pipeline code that I am trying to execute:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name)\ndef pipeline(\n    project: str = &quot;adsfafs-321118&quot;,\n    location: str = &quot;us-central1&quot;,\n    display_name: str = &quot;vertex_pipeline&quot;,\n    python_package_gcs_uri: str = &quot;gs:\/\/vertex\/training\/training-package-3.0.tar.gz&quot;,\n    python_module_name: str = &quot;trainer.task&quot;,\n    container_uri: str = &quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n    staging_bucket: str = &quot;vertex_bucket&quot;,\n    base_output_dir: str = &quot;gs:\/\/vertex_artifacts\/custom_training\/&quot;\n):\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=python_module_name,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=package_path\n)\n\napi_client = AIPlatformClient(project_id=project_id, region=region)\n\nresponse = api_client.create_run_from_job_spec(\n    package_path,\n    pipeline_root=pipeline_root_path\n)\n\n<\/code><\/pre>\n<p>In the documentation for CustomPythonPackageTrainingJobRunOp, the type of the argument &quot;python_module&quot; seems to be &quot;google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob&quot; instead of string, which seems odd. However, I tried to re-define the pipeline, where I have replaced argument python_module in CustomPythonPackageTrainingJobRunOp with a CustomPythonPackageTrainingJob object instead of a string, as below but still getting the same error:<\/p>\n<pre><code>def pipeline(\n    project: str = &quot;...&quot;,\n    location: str = &quot;...&quot;,\n    display_name: str = &quot;...&quot;,\n    python_package_gcs_uri: str = &quot;...&quot;,\n    python_module_name: str = &quot;...&quot;,\n    container_uri: str = &quot;...&quot;,\n    staging_bucket: str = &quot;...&quot;,\n    base_output_dir: str = &quot;...&quot;,\n):\n\n    job = aiplatform.CustomPythonPackageTrainingJob(\n        display_name= display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module_name=python_module_name,\n        container_uri=container_uri,\n        staging_bucket=staging_bucket\n    )\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=job,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>Added the args that I was passing and had forgotten to add here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-15 20:22:26.03 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-16 22:09:40.473 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68795098",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69565796,
        "Question_title":"When can one find logs for Vertex AI Batch Prediction jobs?",
        "Question_body":"<p>I couldn't find relevant information in the Documentation. I have tried all options and links in the batch transform pages.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-14 06:08:05.233 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":492,
        "Owner_creation_date":"2021-10-14 05:56:24.71 UTC",
        "Owner_last_access_date":"2022-09-23 07:04:34.437 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69565796",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72013855,
        "Question_title":"Alter JSON file in CLOUD SHELL Terminal",
        "Question_body":"<p>this is my file :<\/p>\n<pre><code>$ cat INPUT-JSON\n{&quot;endpointId&quot;: &quot;1411183591831896064&quot;, &quot;instance&quot;: &quot;[{age: 40.77430558, ClientID: '997', income: 44964.0106, loan: 3944.219318}]&quot;}\n<\/code><\/pre>\n<p>I want to alter it to :<\/p>\n<pre><code>$ cat INPUT-JSON\n{&quot;endpointId&quot;: &quot;1411183591831896064&quot;, &quot;instance&quot;: &quot;[{age: 30.00, ClientID: '998', income: 50000.00, loan: 20000.00}]&quot;}\n<\/code><\/pre>\n<p>How do I do that using CLOUD SHELL Terminal ? (on google cloud platform)<\/p>\n<p>(this is part of a Qwiklab : Vertex AI: Predicting Loan Risk with AutoML from <a href=\"https:\/\/www.cloudskillsboost.google\/course_templates\/3?hl=es_419&amp;locale=fr_CA&amp;skip_cache=true&amp;utm_campaign=cgc&amp;utm_medium=website&amp;utm_source=gcp_training\" rel=\"nofollow noreferrer\">https:\/\/www.cloudskillsboost.google\/course_templates\/3?hl=es_419&amp;locale=fr_CA&amp;skip_cache=true&amp;utm_campaign=cgc&amp;utm_medium=website&amp;utm_source=gcp_training<\/a> )<\/p>\n<p>Thanks a lot for your help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-26 12:20:21.613 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"json|command-line-interface|google-cloud-vertex-ai",
        "Question_view_count":276,
        "Owner_creation_date":"2022-01-14 12:42:50.64 UTC",
        "Owner_last_access_date":"2022-09-15 21:28:40.643 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72013855",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72650977,
        "Question_title":"Vertex AI Workbench notebooks unresponsive",
        "Question_body":"<p>Having various problems accessing with GCP Vertex AI Workbench managed notebooks.  Could really use some suggestions about recovering, and avoiding further failure.<\/p>\n<p>The original behavior (two days ago) was<\/p>\n<ul>\n<li>After working in the JupyterLab instance for a bit over an hour (creating a handful of notebooks within the instance), some kind of connectivity is lost.\n<ul>\n<li>Inside the JupyterLab interface:  cells won't run, the notebook is unable to save to disk or export, and restarting the kernel doesn't work.<\/li>\n<li>On-screen error pop-up:  502, with message mentioning &quot;bad gateway&quot; or something like that<\/li>\n<li>Back out in the console screen for managing my Workbench instances, I was able to use the Reset command to get the instance back to a working state.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>**Note: **  This instance was provisioned with a setting to suspend after one hour of idle time.  It's not obviously relevant; the failure was a little more than an hour after creation, but there certainly wasn't an hour of idle time before things went to heck.<\/p>\n<p>Today, I came back and was again able to work in the same instance for a bit over an hour, but then the same symptoms locked in.  Couldn't execute code, couldn't save the notebook.<\/p>\n<p>However, things are worse now, because hitting Reset has led to an endless period of spinning cursor.  The instance won't complete its reset and can't start.  When I hover over the spinning cursor where the OPEN JUPYTERLAB button ought to be, a hover box says &quot;Setting up proxy to JupyterLab&quot;.<\/p>\n<p>The hover text for the Instance status says:  &quot;Provisioning&quot;.<\/p>\n<p>More:  I also tried creating a new notebook instance from the Workbench console screen, and it's stuck in the same condition -- just spinning, never reaching running state.  If I try to Reset it, a minor little pop-up appears at the bottom of the screen like so:\n<a href=\"https:\/\/i.stack.imgur.com\/GA5fl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GA5fl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Subsequently, the hover text raised by the Reset button is:\n<a href=\"https:\/\/i.stack.imgur.com\/7cbZI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7cbZI.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>At the least, I'm hoping to regain access to the initial instance at least once to recover some code in the notebooks (and go run it in a less flaky cloud service).  At best, you could help me manage this so that this GCP service is actually viable over time for me.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-06-16 19:27:31.767 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-02 08:08:31.85 UTC",
        "Question_score":3,
        "Question_tags":"jupyter-notebook|google-cloud-ml|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":622,
        "Owner_creation_date":"2015-11-23 03:15:36.167 UTC",
        "Owner_last_access_date":"2022-09-23 21:05:58.28 UTC",
        "Owner_reputation":919,
        "Owner_up_votes":400,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Austin, TX, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72650977",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72165809,
        "Question_title":"Vertex AI endpoint prediction error : ValueError: Unable to coerce value",
        "Question_body":"<p>I built and deployed an XGBoost regressor model on vertex AI and I  am trying to make some predictions using Vertex AI python SDK.\nHere's my code:<\/p>\n<pre><code>client = aiplatform.gapic.PredictionServiceClient.from_service_account_json(filename=&quot;filename.json&quot;,client_options=client_options)\n\nendpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)\n \nresponse = client.predict(endpoint=endpoint, instances=instances)\n   \n\npredictions = response.predictions\n<\/code><\/pre>\n<p>Here is the value of the variable instances :<\/p>\n<pre><code>array([[4.8700000e+00, 3.6505380e+06, 2.0000000e+01, 2.0210000e+03,\n    4.0000000e+00, 5.3000000e+01, 0.0000000e+00, 0.0000000e+00,\n    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n    0.0000000e+00],\n   [3.7100000e+00, 1.2678515e+07, 2.0000000e+01, 2.0210000e+03,\n    4.0000000e+00, 5.3000000e+01, 0.0000000e+00, 0.0000000e+00,\n    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n    0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n    0.0000000e+00]])\n<\/code><\/pre>\n<p>(Here I am trying to make two predictions).\nWhen doing this I am getting an error:<\/p>\n<blockquote>\n<p>Unable to coerce value<\/p>\n<\/blockquote>\n<p>I tried to look at the number of features allowed by the model and it is the same as the input here (29). I tried looking into the xgboost code and according to the code the input is correct.\nSo I am guessing it is an issue on vertex AI side. But I don't know how to fix it.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-08 23:43:43.96 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|google-cloud-platform|xgboost|google-cloud-vertex-ai",
        "Question_view_count":272,
        "Owner_creation_date":"2022-05-08 23:24:49.28 UTC",
        "Owner_last_access_date":"2022-08-03 20:52:55.127 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72165809",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72860901,
        "Question_title":"How can I change the security setting and enable terminal for a Vertex AI managed notebook?",
        "Question_body":"<p>I created a notebook using Vertex AI without enabling terminal first, but I want to enable terminal now so that I can run a Python file from a terminal. Is there any way I can change the setting retrospectively?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-04 18:44:02.433 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-06 04:37:07.957 UTC",
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":110,
        "Owner_creation_date":"2022-06-30 04:56:56.23 UTC",
        "Owner_last_access_date":"2022-09-18 19:07:37.86 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>As of now, when you create a Notebook instance with unchecked <em>&quot;Enable terminal&quot;<\/em> like the below screenshot, you <strong>cannot re-enable this option once the Notebook instance is already created<\/strong>.\n<a href=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The only workaround is to <strong>recreate the Notebook instance<\/strong> and then enable it.<\/p>\n<p>Right now, there is already a <a href=\"https:\/\/issuetracker.google.com\/222694899\" rel=\"nofollow noreferrer\">Feature Request<\/a> for this. You can <strong>star<\/strong> the public issue tracker feature request and add <strong>\u2018Me too\u2019<\/strong> in the thread. This will bring more attention to the request as more users request support for it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-08 03:33:52.893 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72860901",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73565771,
        "Question_title":"How does Google Vertex AI Matching engine deny list work?",
        "Question_body":"<p>How does <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/filtering#denylist\" rel=\"nofollow noreferrer\">Vertex matching engine deny list<\/a> work?<\/p>\n<p>Let's say I have a class fruit which will ONLY have deny list tokens (no allow) such as &quot;apple&quot;, &quot;mango&quot;, etc. How do I filter out &quot;mango&quot; in the query (search all fruits except mango)? I have tried the following method but it does not work as expected:<\/p>\n<p>json:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{&quot;id&quot;: &quot;1&quot;, &quot;embedding&quot;:[0.002792,0.000492], &quot;restricts&quot;: [{&quot;namespace&quot;: &quot;fruit&quot;, &quot;deny&quot;: [&quot;mango&quot;]}]}\n<\/code><\/pre>\n<p>code to query:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>deny_namespace = match_service_pb2.Namespace()\ndeny_namespace.name = &quot;fruit&quot;\ndeny_namespace.deny_tokens.append(&quot;mango&quot;)\nrequest.restricts.append(deny_namespace)\n<\/code><\/pre>\n<p>I have coded this similar to allow list which has worked for me but with deny tokens it does not seem to skip deny tokens even after completely overwriting the index.<\/p>\n<p>Barely changing the field &quot;deny&quot; to &quot;allow&quot; works but &quot;deny&quot; fails to work as expected (it does not throw any error though).<\/p>\n<p>Full code<\/p>\n<p>query<\/p>\n<p><a href=\"https:\/\/gist.github.com\/niladridutt\/673d4aa2a6225fa47d8aad7398b4cbd1\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/673d4aa2a6225fa47d8aad7398b4cbd1<\/a><\/p>\n<p>Proto files-<\/p>\n<p><a href=\"https:\/\/gist.github.com\/niladridutt\/746833b8d61ec366c8c61de57c784ac4\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/746833b8d61ec366c8c61de57c784ac4<\/a>\n<a href=\"https:\/\/gist.github.com\/niladridutt\/31e9dc3432e206589729989acddf1225\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/31e9dc3432e206589729989acddf1225<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-01 07:41:20.61 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-01 10:37:31.937 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_date":"2022-09-01 07:36:06.697 UTC",
        "Owner_last_access_date":"2022-09-23 12:07:06.403 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73565771",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73112914,
        "Question_title":"Google Vertex AI fails AutoML training due to large BigQuery dataset being too large",
        "Question_body":"<p>I am currently training some models via Googles AutoML feature contained within their Vertex AI products.<\/p>\n<p>The normal pipeline is creating a dataset, which I do by creating a table in Bigquery, and then starting the training process.<\/p>\n<p>This has normally worked before but for my latest dataset I get the following error message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: The size of source BigQuery table is larger than 107374182400 bytes.<\/p>\n<\/blockquote>\n<p>While it seemed unlikely to me that the table is actually too large for AutoML, I tried re-training on a new dataset that's a 50% sample of the original table but the same error occured.<\/p>\n<p>Is my dataset really to large for AutoML to handle or is there another issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-25 17:05:43.937 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_date":"2016-02-16 11:11:55.903 UTC",
        "Owner_last_access_date":"2022-09-20 09:07:15.447 UTC",
        "Owner_reputation":1179,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Answer_body":"<p>There are some perspectives of limits for AutoML Tables -- not only size in bytes (100GB as maximum supported size), but also number of rows (~200bi lines) and number of columns (up to 1000 columns).<\/p>\n<p>You can find more details on <a href=\"https:\/\/cloud.google.com\/automl-tables\/docs\/quotas#limits\" rel=\"nofollow noreferrer\">AutoML Tables limits<\/a> documentation.<\/p>\n<p>Is your source data within those limits?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-07-25 18:19:54.927 UTC",
        "Answer_score":2.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73112914",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68793294,
        "Question_title":"How to schedule repeated runs of a custom training job in Vertex AI",
        "Question_body":"<p>I have packaged my training code as a python package and then am able to run it as a custom training job on Vertex AI. Now, I wanted to be able to schedule this job to run, say every 2 weeks, and re-train the model. The Scheduling settings in the CustomJoBSpec allow only 2 fields, &quot;timeout&quot; and &quot;restartJobOnWorkerRestart&quot; so it's not possible using the scheduling settings in the CustomJobSpec. One way to achieve this I could think of was to create a Vertex AI pipeline with a single step using the &quot;CustomPythonPackageTrainingJobRunOp&quot; Google Cloud Pipeline Component and then scheduling the pipeline to run as I see fit. Are there better alternatives to achieve this?<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>I was able to schedule the custom training job using Cloud Scheduler, but I found using the create_schedule_from_job_spec method in the AIPlatformClient very easy to use in the Vertex AI pipeline. The steps I took to schedule the custom job using Cloud Scheduler in gcp are as follows, <a href=\"https:\/\/cloud.google.com\/scheduler\/docs\/http-target-auth#setting_up_the_service_account\" rel=\"nofollow noreferrer\">link<\/a> to google docs:<\/p>\n<ol>\n<li>Set target type to HTTP<\/li>\n<li>For the url to specify the custom job, I followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#curl\" rel=\"nofollow noreferrer\">this<\/a> link to get the url<\/li>\n<li>For the authentication, under Auth header, I selected the &quot;Add OAauth token&quot;<\/li>\n<\/ol>\n<p>You also need to have a &quot;Cloud Scheduler service account&quot; with  a &quot;Cloud Scheduler Service Agent role granted to it&quot; in your project. Although the docs ay this should have been set up automatically if you enabled the Cloud Scheduler API after March 19, 2019, this was not the case for me and had to add the service account with the role manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2021-08-15 16:12:43.683 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-16 21:06:01.557 UTC",
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":2669,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68793294",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69219230,
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-17 07:14:28.123 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-18 17:51:18.697 UTC",
        "Question_score":2,
        "Question_tags":"java|google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_date":"2008-11-20 08:57:51.293 UTC",
        "Owner_last_access_date":"2022-09-24 19:18:28.08 UTC",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-09-17 10:50:53.753 UTC",
        "Answer_score":3.0,
        "Owner_location":"Israel",
        "Answer_last_edit_date":"2021-09-17 14:14:20.71 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69977440,
        "Question_title":"How to use kfp Artifact with sklearn?",
        "Question_body":"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:<\/p>\n<ol>\n<li>read data from a big query table<\/li>\n<li>create a pandas <code>DataFrame<\/code><\/li>\n<li>use the <code>DataFrame<\/code> to train a K-Means model<\/li>\n<li>deploy the model to an endpoint<\/li>\n<\/ol>\n<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]<\/code> as output because <code>pd.DataFrame<\/code> type that I found <a href=\"https:\/\/stackoverflow.com\/questions\/43890844\/pythonic-type-hints-with-pandas\">here<\/a> did not work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    df: Output[Artifact],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    df = client.list_rows(table).to_dataframe()\n<\/code><\/pre>\n<p>Here the code of the step 3:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])\ndef kmeans_training(\n        dataset: Input[Artifact],\n        model: Output[Model],\n        num_clusters: int,\n):\n    from sklearn.cluster import KMeans\n    model = KMeans(num_clusters, random_state=220417)\n    model.fit(dataset)\n<\/code><\/pre>\n<p>The run of the pipeline is stopped due to the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: float() argument must be a string or a number, not 'Artifact'\n<\/code><\/pre>\n<p>Is it possible to convert Artifact to <code>numpy array<\/code> or <code>Dataframe<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-15 16:10:03.783 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp",
        "Question_view_count":339,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Alatri, Frosinone, FR",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69977440",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69658459,
        "Question_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Question_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-21 08:27:28.29 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-26 10:54:23.99 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":330,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-11-09 07:41:34.643 UTC",
        "Answer_score":1.0,
        "Owner_location":"Alatri, Frosinone, FR",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70928172,
        "Question_title":"BigQuery cannot find GeoPandas",
        "Question_body":"<p>I am trying to load a BigQuery table into a GeoDataFrame via:<\/p>\n<pre><code>gdf = \\\nclient.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      .result()\\\n      .to_geodataframe()\n<\/code><\/pre>\n<p>However, I get <strong><code>ValueError: The geopandas library is not installed<\/code><\/strong>:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n\/tmp\/ipykernel_782\/24642811.py in &lt;module&gt;\n      1 gdf = \\\n----&gt; 2 client.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      3       .result()\\\n      4       .to_geodataframe()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/bigquery\/table.py in to_geodataframe(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_column)\n   2098         &quot;&quot;&quot;\n   2099         if geopandas is None:\n-&gt; 2100             raise ValueError(_NO_GEOPANDAS_ERROR)\n   2101 \n   2102         geography_columns = set(\n\nValueError: The geopandas library is not installed, please install geopandas to use the to_geodataframe() function.\n<\/code><\/pre>\n<p>But <code>geopandas<\/code> is definitely installed. <code>import geopandas<\/code> works, <code>!pip3 install geopandas<\/code> results in <code>Requirement already satisfied<\/code> messages. I am on a Vertex AI workbench Jupyter notebook.<\/p>\n<p>Loading the data to a Pandas dataframe works fine:<\/p>\n<pre><code>df = \\\nclient.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      .result()\\\n      .to_dataframe()\n<\/code><\/pre>\n<p>I read about <code>to_geodataframe()<\/code> <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/bigquery\/latest\/google.cloud.bigquery.table.RowIterator#google_cloud_bigquery_table_RowIterator_to_geodataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p><strong>How could I fix the <code>to_geodataframe()<\/code> attribute of the <code>RowIterator<\/code> object <code>client.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;).result()<\/code>?<\/strong><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-31 15:05:28.033 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-bigquery|geopandas|google-cloud-vertex-ai",
        "Question_view_count":173,
        "Owner_creation_date":"2017-09-05 19:27:41.23 UTC",
        "Owner_last_access_date":"2022-09-24 21:09:42.077 UTC",
        "Owner_reputation":5748,
        "Owner_up_votes":2565,
        "Owner_down_votes":761,
        "Owner_views":969,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Oslo, Norway",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928172",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68614794,
        "Question_title":"How to create MLOps vertex ai pipeline with custom sklearn code?",
        "Question_body":"<p>I'm trying to build MLOps pipeline using vertex ai but failing to deploy it<\/p>\n<pre><code>@dsl.pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=&quot;pipeline-test-1&quot;,\n)\ndef pipeline(\nserving_container_image_uri: str = &quot;us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest&quot;\n):\n    dataset_op = get_data()\n    train_op = train_xgb_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    train_knn = knn_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    \n    eval_op = eval_model(\n        test_set=dataset_op.outputs[&quot;dataset_test&quot;],\n        xgb_model=train_op.outputs[&quot;model_artifact&quot;],\n        knn_model=train_knn.outputs['best_model_artifact']\n    )\n    \n    endpoint_op = gcc_aip.ModelDeployOp(\n    project=PROJECT_ID,\n    model=eval_op.outputs[&quot;model_artifacts&quot;],\n    machine_type=&quot;n1-standard-4&quot;,\n    )\n    \n    #endpoint_op.after(eval_op)\n    \ncompiler.Compiler().compile(pipeline_func=pipeline,\n        package_path='xgb_pipe.json')\n<\/code><\/pre>\n<p>gcc_aip.ModelDeployOp is throwing error that correct model id or name should be pass<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-01 22:23:54.787 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"mlops|google-cloud-vertex-ai",
        "Question_view_count":154,
        "Owner_creation_date":"2015-11-25 08:05:38.667 UTC",
        "Owner_last_access_date":"2022-06-15 12:39:30.893 UTC",
        "Owner_reputation":73,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68614794",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71468270,
        "Question_title":"How to make a prediction to a private Vertex AI endpoint with Node.js client libraries?",
        "Question_body":"<p>Documentation on this is a bit vague at the time of posting <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#sending-prediction-to-private-endpoint\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#sending-prediction-to-private-endpoint<\/a> , they only mention how to do it with curl.<\/p>\n<p>I would like to use the node.js client library if possible, but I've only managed to find examples that don't use a private endpoint ie: <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-custom-trained-model.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-custom-trained-model.js<\/a> .<\/p>\n<p>I've read through the type definitions of <code>PredictionServiceClient<\/code> imported from <code>@google-cloud\/aiplatform<\/code> and didn't find a way to plug in my private endpoint. I've tried making the request anyway by simply specifying the resource name by doing <code>const endpoint = projects\/${project}\/locations\/${location}\/endpoints\/${endpointId}<\/code> but this leads to the following error:<\/p>\n<pre><code>Error: 13 INTERNAL: Received RST_STREAM with code 0\n    at Object.callErrorFromStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/call.ts:81:24)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client.ts:343:36)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client-interceptors.ts:462:34)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client-interceptors.ts:424:48)\n    at \/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/call-stream.ts:323:24\n    at processTicksAndRejections (node:internal\/process\/task_queues:78:11) {\n  code: 13,\n  details: 'Received RST_STREAM with code 0',\n  metadata: Metadata { internalRepr: Map(0) {}, options: {} }\n}\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>(async () =&gt; {\n        const client = new v1beta1.PredictionServiceClient();\n        const location = &quot;****&quot;;\n        const project = &quot;****&quot;;\n        const endpointId = &quot;****&quot;\n        const endpoint = `projects\/${project}\/locations\/${location}\/endpoints\/${endpointId}`;\n\n        const parameters = {\n            structValue: {\n                fields: {},\n            },\n        };\n\n        const toInstance = (obj: any) =&gt; (\n            {\n                structValue: {\n                    fields: {\n                        ****\n                    }\n                }\n            });\n\n        const instance = toInstance(****);\n        const instances = [instance];\n\n        const res = await client.predict({\n            instances,\n            endpoint,\n            parameters\n        });\n        console.log(res);\n    })();\n<\/code><\/pre>\n<p>Is it possible to make this kind of request atm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":10,
        "Question_creation_date":"2022-03-14 13:05:35.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"node.js|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":676,
        "Owner_creation_date":"2020-02-15 20:36:26.27 UTC",
        "Owner_last_access_date":"2022-09-08 19:19:18.53 UTC",
        "Owner_reputation":185,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71468270",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71952492,
        "Question_title":"Vertex AI scheduled notebook doesn't work, though working manually",
        "Question_body":"<p>There is a scheduled notebook, that uses BigQuery client and service account with Owner rights. When I run the cells manually, it makes an update to BQ table. There is one project for both BQ and Vertex AI.<\/p>\n<p>I've found a similar question, but there is no output in bucket folder:\n<a href=\"https:\/\/stackoverflow.com\/questions\/70614514\/google-cloud-vertex-ai-notebook-scheduled-runs-arent-running-code\">Google Cloud Vertex AI Notebook Scheduled Runs Aren&#39;t Running Code?<\/a><\/p>\n<p>In schedules section this notebook is stuck on Initializing:\n<a href=\"https:\/\/i.stack.imgur.com\/PoYIq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PoYIq.png\" alt=\"The endlesly initializing notebook is 'a_test_marketplace_update_1650393238920'\" \/><\/a><\/p>\n<p>Here's the notebook:\n<a href=\"https:\/\/i.stack.imgur.com\/cGwfQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cGwfQ.png\" alt=\"First part of notebook\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h7c4D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h7c4D.png\" alt=\"Second part of notebook\" \/><\/a><\/p>\n<p>Update: I've tried to schedule cells one by one, and all of the stuck attempts cannot get through BigQuery:<\/p>\n<pre><code>os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'dialogflow-293713-f89fd8f4ed2d.json'\n\nbigquery_client = bigquery.Client()\n\nQUERY = f&quot;&quot;&quot;\nINSERT `dialogflow-293713.chats.\u0415\u0436\u0435\u0434\u043d\u0435\u0432\u043d\u0430\u044f \u0441\u0432\u043e\u0434\u043a\u0430 \u043c\u0430\u0440\u043a\u0435\u0442\u043f\u043b\u0435\u0439\u0441\u0430` (date, effectiveness, operatorWorkload)\nVALUES({period}, {effectiveness}, {redirectedToSales}, {operatorWorkload})\n&quot;&quot;&quot;\n\nQuery_Results = bigquery_client.query(QUERY)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-21 09:57:14.513 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-21 11:09:40.52 UTC",
        "Question_score":3,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":212,
        "Owner_creation_date":"2018-10-20 14:58:32.807 UTC",
        "Owner_last_access_date":"2022-09-21 12:34:46.017 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":37,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Saint Petersburg, \u0420\u043e\u0441\u0441\u0438\u044f",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71952492",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72042363,
        "Question_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Question_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 10:49:48.007 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-28 13:15:19.75 UTC",
        "Question_score":2,
        "Question_tags":"docker|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":248,
        "Owner_creation_date":"2022-03-24 16:46:24.823 UTC",
        "Owner_last_access_date":"2022-09-24 21:22:42.907 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-24 14:20:18.333 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-08-24 14:25:20.347 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68753717,
        "Question_title":"Vertex AI notebook kernel lost on PC sleep",
        "Question_body":"<p>While using Vertex AI notebook instance kernel on GCP, the notebook gets detached everytime my system sleeps.<\/p>\n<p>How can I keep my notebook running even if my system shuts down?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-12 08:04:37.41 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-12 10:34:05.987 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":551,
        "Owner_creation_date":"2012-11-17 11:31:07.41 UTC",
        "Owner_last_access_date":"2022-09-22 08:40:30.397 UTC",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68753717",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73811793,
        "Question_title":"How do I undeploy a model from an endpoint without knowing its id in Vertex AI?",
        "Question_body":"<p>I have managed to undeploy a model from an endpoint using <code>UndeployModelRequest<\/code>:<\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)\n    deployed_models_info = model_info.deployed_models\n    deployed_model_id=model_info.deployed_models[0].deployed_model_id       \n    \n    undeploy_request = aiplatform_v1.types.UndeployModelRequest\n                       (endpoint=end_point, deployed_model_id=deployed_model_id)\n\n    client.undeploy_model(request=undeploy_request)\n<\/code><\/pre>\n<p>but all this depends on knowing <code>model_id<\/code>. I want to be able to just undeploy a model from an endpoint without knowing the model's id (there will only be one model per endpoint ever). Is that possible or can I get the model id from the endpoint somehow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 08:49:45.407 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":14,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73811793",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70610069,
        "Question_title":"Is there any parameter in any SDK for enabling access logging in GCP Vertex AI?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/kV1Vb.png\" rel=\"nofollow noreferrer\">enabling access logging on the UI<\/a>seeking for some support in terms of enabling logging via the SDK(Vertex AI or AI platform or any other).Just as we enable it on the UI(pls refer attached file) &amp; other times via gcloud command like this-&gt;\ngcloud ai endpoints deploy-model ENDPOINT_ID--region=LOCATION --model=MODEL_ID --display-name=DEPLOYED_MODEL_NAME --machine-type=MACHINE_TYPE --accelerator=count=2,type=nvidia-tesla-t4 --disable-container-logging --enable-access-logging<\/p>\n<p>Does AI platform or vertex ai or any other SDK comprise of any API\/parameter which would allow us to enable access logging? If yes, could you please point in that direction?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-06 16:11:46.96 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"logging|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":24,
        "Owner_creation_date":"2021-09-27 14:24:53.853 UTC",
        "Owner_last_access_date":"2022-07-19 05:49:44.61 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70610069",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73368320,
        "Question_title":"Vertax AI pipeline quota",
        "Question_body":"<p>I got a custom_model_training_cpus error when runing a submitted pipeline on Vertex AI. I could not find any documents. And I am using the n1-standard-4 for the deployment machine, I do not see any issue. Any commnents would be much appriciated.<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus, cause=null; Failed to create custom job for the task.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgCJD.png\" rel=\"nofollow noreferrer\">DAG flow and error message<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-16 02:58:55.81 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":110,
        "Owner_creation_date":"2022-08-16 02:53:42.463 UTC",
        "Owner_last_access_date":"2022-08-19 02:01:06.55 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368320",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69203143,
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-16 06:09:14.497 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_date":"2015-12-04 05:40:05.093 UTC",
        "Owner_last_access_date":"2022-09-22 02:24:42.53 UTC",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-20 02:13:08.633 UTC",
        "Answer_score":2.0,
        "Owner_location":"Manila, NCR, Philippines",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73033284,
        "Question_title":"How to extract model file(s) from Vertex AI?",
        "Question_body":"<p>I have been trying to import model file(s) from Vertex AI to Workbench environment. My pipeline consists of preprocessing, training and batch predictions. Sometimes the training fails due to unknown reasons yet I want the batch predictions from the latest model in that case. Is there a way to access the trained models using Python?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 08:05:44.86 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-19 09:03:36.54 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":34,
        "Owner_creation_date":"2022-04-14 11:12:57.04 UTC",
        "Owner_last_access_date":"2022-09-24 05:03:36.517 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73033284",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69174883,
        "Question_title":"Batch predictions Vertext AI",
        "Question_body":"<p>How do I create <code>JSONL<\/code> file which contains list of files in Google Cloud Bucket for Batch prediction in Vertex AI?\nWhat I've tried so far.<\/p>\n<ol>\n<li>Get list of file from bucket and write it to a txt file\n<code>gsutil ls gs:\/\/bucket\/dir &gt; list.txt<\/code><\/li>\n<li>Convert <code>list.txt<\/code> to <code>list.jsonl<\/code> following <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions?_ga=2.136818160.-1986884001.1629344951&amp;_gac=1.159283784.1630027944.Cj0KCQiAv8PyBRDMARIsAFo4wK2vd8OO5X9HFkii9knl6nSzFOK_srkD484THsck1a6r5rqonXafFkkaAgBgEALw_wcB#batch_request_input\" rel=\"nofollow noreferrer\">Vertext AI docs<\/a>:<\/li>\n<\/ol>\n<pre><code>{&quot;content&quot;: &quot;gs:\/\/sourcebucket\/datasets\/images\/source_image1.jpg&quot;, &quot;mimeType&quot;: &quot;image\/jpeg&quot;}\n{&quot;content&quot;: &quot;gs:\/\/sourcebucket\/datasets\/images\/source_image2.jpg&quot;, &quot;mimeType&quot;: &quot;image\/jpeg&quot;}\n<\/code><\/pre>\n<p>After create batch prediction, I got this error: <code>cannot be parsed as JSONL.<\/code>\nHow do I correct the format of this <code>JSONL<\/code> file?\nAlso, is there anyway to directly export list files in bucket to <code>JSONL<\/code> file format?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-14 09:01:41.267 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-14 09:12:49.777 UTC",
        "Question_score":1,
        "Question_tags":"python|gsutil|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_date":"2021-06-17 10:11:32.18 UTC",
        "Owner_last_access_date":"2021-10-16 12:25:40.05 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69174883",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68370906,
        "Question_title":"Google Cloud Platform Dataset can't show image in VertexAI",
        "Question_body":"<p>Thanks All:\nWhen I use GCP-VertexAI and upload image from my computer. Next step to Browse page, all images always are loading. I can't any picture in Browse, and labing. But we can find upload image in Cloud Storage. How should I do for check my images in Browse?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4gFz.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-14 01:16:16.42 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_date":"2021-07-14 01:02:29.82 UTC",
        "Owner_last_access_date":"2021-11-22 09:19:29.89 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Taipei, \u53f0\u7063",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68370906",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70054399,
        "Question_title":"Writing log to gcloud Vertex AI Endpoint using gcloud client fails with google.api_core.exceptions.MethodNotImplemented: 501",
        "Question_body":"<p>Trying to use <a href=\"https:\/\/pypi.org\/project\/google-cloud-logging\/\" rel=\"nofollow noreferrer\">google logging client library<\/a> for writing logs into gcloud, specifically, i'm interested in writing logs that will be attached to a managed resource, in this case, a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/introduction-unified-platform\" rel=\"nofollow noreferrer\">Vertex AI<\/a> endpoint:<\/p>\n<p>Code sample:<\/p>\n<pre><code>import logging\nfrom google.api_core.client_options import ClientOptions\nimport google.cloud.logging_v2 as logging_v2\nfrom google.oauth2 import service_account\n\n\ndef init_module_logger(module_name: str) -&gt; logging.Logger:\n\n    module_logger = logging.getLogger(module_name)\n    module_logger.setLevel(settings.LOG_LEVEL)\n    credentials= service_account.Credentials.from_service_account_info(json.loads(SA_KEY_JSON))\n    client = logging_v2.client.Client(\n        credentials=credentials,\n        client_options=ClientOptions(api_endpoint=&quot;us-east1-aiplatform.googleapis.com&quot;),\n    )\n    handler = client.get_default_handler(\n        resource=Resource(\n            type=&quot;aiplatform.googleapis.com\/Endpoint&quot;,\n            labels={&quot;endpoint_id&quot;: &quot;ENDPOINT_NUMBER_ID&quot;, \n            &quot;location&quot;: &quot;us-east1&quot;},\n        )\n    )\n    #Assume we have the formatter\n    handler.setFormatter(ENRICHED_FORMATTER)\n    module_logger.addHandler(handler)\n    return module_logger\n\n\nlogger = init_module_logger(__name__)\nlogger.info(&quot;This Fails with 501&quot;)\n<\/code><\/pre>\n<p>And i am getting:<\/p>\n<blockquote>\n<p>google.api_core.exceptions.MethodNotImplemented: 501 The GRPC target\nis not implemented on the server, host:\nus-east1-aiplatform.googleapis.com, method:\n\/google.logging.v2.LoggingServiceV2\/WriteLogEntries. Sent all pending\nlogs.<\/p>\n<\/blockquote>\n<p>I thought we need to enable api and was told it's enabled, and that we have: <a href=\"https:\/\/www.googleapis.com\/auth\/logging.write\" rel=\"nofollow noreferrer\">https:\/\/www.googleapis.com\/auth\/logging.write<\/a>\nwhat could be causing the error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-21 12:08:58.787 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-21 12:31:17.707 UTC",
        "Question_score":0,
        "Question_tags":"gcloud|google-cloud-vertex-ai",
        "Question_view_count":174,
        "Owner_creation_date":"2020-12-23 08:27:36.58 UTC",
        "Owner_last_access_date":"2022-09-22 12:31:57.143 UTC",
        "Owner_reputation":496,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70054399",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71430286,
        "Question_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Question_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_date":"2022-03-10 20:18:07.337 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-11 08:39:34.513 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|model|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":936,
        "Owner_creation_date":"2019-07-09 21:04:51.28 UTC",
        "Owner_last_access_date":"2022-09-24 15:30:31.527 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-03-11 04:05:59.037 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-03-11 11:27:04.38 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72051655,
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-29 00:45:48.017 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":339,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-03 07:28:28.65 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-05-03 07:38:01.837 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72047442,
        "Question_title":"How can I use a ML model trained with Google Vertex AI with scikit learn?",
        "Question_body":"<p>I have a problem with Vertex AI. I have trained a model using the API for Vertex AI in Python. After the training, I want to retrieve the model and use it as a model obtained with a scikit-learn regressor. In particular, I have to use the library &quot;lime&quot; which has a method to find explanations for a particular prediction.<\/p>\n<p>This is the code for finding the model with Vertex AI API.<\/p>\n<pre><code>model = dag.run(\n    dataset=my_dataset,\n    target_column=&quot;t&quot;,\n    training_fraction_split=0.6,\n    validation_fraction_split=0.2,\n    test_fraction_split=0.2,\n    budget_milli_node_hours=1000,\n    model_display_name=&quot;model_&quot;+TIMESTAMP,\n    disable_early_stopping=False,\n)\n<\/code><\/pre>\n<p>And this is the function I have to use: <a href=\"https:\/\/lime-ml.readthedocs.io\/en\/latest\/lime.html#module-lime.lime_tabular\" rel=\"nofollow noreferrer\">https:\/\/lime-ml.readthedocs.io\/en\/latest\/lime.html#module-lime.lime_tabular<\/a><\/p>\n<p>As you can see it requires the training data and then in the part of the <code>explain_instance<\/code> function, it requires the prediction function of the model.<\/p>\n<p>I know that I can obtain the model in this way:<\/p>\n<p><code>model = aiplatform.Model(&quot;path to my model&quot;)<\/code><\/p>\n<p>But how I can obtain the prediction function from the model trained with AutoML? Thank you!!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 16:46:04.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-12 14:54:37.353 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":96,
        "Owner_creation_date":"2022-04-21 15:07:13.377 UTC",
        "Owner_last_access_date":"2022-08-09 16:00:58.2 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72047442",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72385022,
        "Question_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Question_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-25 23:09:26.993 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_date":"2019-07-13 14:14:21.237 UTC",
        "Owner_last_access_date":"2022-09-24 22:32:14.17 UTC",
        "Owner_reputation":315,
        "Owner_up_votes":44,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-27 03:09:01.597 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72385022",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73230096,
        "Question_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Question_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 03:37:25.23 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|gcloud|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":107,
        "Owner_creation_date":"2013-01-30 04:13:24.743 UTC",
        "Owner_last_access_date":"2022-09-22 00:56:52.8 UTC",
        "Owner_reputation":3943,
        "Owner_up_votes":747,
        "Owner_down_votes":5,
        "Owner_views":339,
        "Answer_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-11 07:51:47.547 UTC",
        "Answer_score":0.0,
        "Owner_location":"Melbourne, Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73230096",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69540618,
        "Question_title":"Kubeflow, passing Python dataframe across components?",
        "Question_body":"<p>I am writing a Kubeflow component which reads an input query and creates a <code>dataframe<\/code>, roughly as:<\/p>\n<pre><code>from kfp.v2.dsl import component \n\n@component(...)\ndef read_and_write():\n    # read the input query \n    # transform to dataframe \n    sql.to_dataframe()\n<\/code><\/pre>\n<p>I was wondering how I can pass this dataframe to the next operation in my Kubeflow pipeline.\nIs this possible? Or do I have to save the dataframe in a csv or other formats and then pass the output path of this?\nThank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-12 12:32:47.303 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|tfx|google-cloud-vertex-ai",
        "Question_view_count":879,
        "Owner_creation_date":"2021-06-22 08:58:12.607 UTC",
        "Owner_last_access_date":"2022-04-05 15:39:38.49 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69540618",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70932012,
        "Question_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Question_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2022-01-31 19:50:05.453 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-19 15:30:43.35 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter|google-cloud-vertex-ai",
        "Question_view_count":514,
        "Owner_creation_date":"2021-05-28 09:49:06.03 UTC",
        "Owner_last_access_date":"2022-03-28 19:00:47.687 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-02 06:08:15.707 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71738221,
        "Question_title":"How to get preprocess\/postprocess steps from model created using Google Vertex AI?",
        "Question_body":"<p>A client of mine wants to run their Google Vertex AI model on NVIDIA Jetson boards using TensorRT as accelerator. The problem with this is that their model uses certain operators (DecodeJpeg) that are not supported by ONNX. I've been able to isolate the feature extrator subgraph from the model, so everything supported by ONNX is being used, while the preprocess and postprocess will be written separate from the model.<\/p>\n<p>I'm asking because I need to be provided the pre\/postprocess of the model so I could implement them separately, so is there a way to get pre\/postprocess from Google Vertex AI console?<\/p>\n<p>I've tried running a loop that rescales the image to a squared tile from 0 to 512, but none of those gave the adequate result.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2022-04-04 13:36:49.933 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|onnx|tensorrt|google-cloud-vertex-ai",
        "Question_view_count":107,
        "Owner_creation_date":"2021-03-22 12:13:48.997 UTC",
        "Owner_last_access_date":"2022-05-25 13:38:20.393 UTC",
        "Owner_reputation":16,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738221",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72254372,
        "Question_title":"GCP's Vertex AI(AI Platform) PipelineServiceClient gives unimplemented error",
        "Question_body":"<p>When trying to list pipelines with <code>PipelineServiceClient<\/code> <code>list_pipeline_jobs<\/code> method as given <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.services.pipeline_service.PipelineServiceClient#google_cloud_aiplatform_v1_services_pipeline_service_PipelineServiceClient_list_pipeline_jobs\" rel=\"nofollow noreferrer\">here<\/a>, I get the following error:<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNIMPLEMENTED\ndetails = &quot;Received http2 header with status: 404&quot;\n...\n<\/code><\/pre>\n<p>How is the API unimplemented, how do I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-16 05:06:48.807 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_date":"2019-02-17 05:09:55.09 UTC",
        "Owner_last_access_date":"2022-09-21 08:01:13.577 UTC",
        "Owner_reputation":434,
        "Owner_up_votes":106,
        "Owner_down_votes":34,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72254372",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72741757,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-24 09:18:22.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_date":"2022-06-24 09:02:40.9 UTC",
        "Owner_last_access_date":"2022-08-08 09:41:44.647 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-24 11:59:04.493 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-06-27 15:22:17.227 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70976273,
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-03 18:13:40.147 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_date":"2018-03-01 19:41:46.78 UTC",
        "Owner_last_access_date":"2022-09-25 03:26:23.16 UTC",
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-02-04 02:35:20.973 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72014493,
        "Question_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Question_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-26 13:05:04.193 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-27 20:18:25.967 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|spacy-3|google-cloud-vertex-ai|spacy-transformers",
        "Question_view_count":234,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-27 21:12:04.243 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70984360,
        "Question_title":"Vertex AI Pipeline is not using the GPU",
        "Question_body":"<p>I am building a customized pipeline with the following step:<\/p>\n<pre><code>     trainer_task = (trainer(download_task.output).set_cpu_request(&quot;16&quot;).set_memory_request(&quot;60G&quot;).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', &quot;NVIDIA_TESLA_K80&quot;).set_gpu_limit(2))\n<\/code><\/pre>\n<p>However, when I check the number of GPU's available it says zero, and the only visible device is a CPU device. I am migrating a project from Kubeflow, and this is the first time using Vertex AI, so I am not pretty sure why this is happening.<\/p>\n<p>The involved step is component that loads a docker image from the Artifact Registry and installs Tensorflow-gpu==2.4.1.<\/p>\n<p>Am I missing something? Why is not enabling the specified GPUs?<\/p>\n<p>Any help will be highly appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-04 09:52:13.55 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|google-cloud-platform|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":250,
        "Owner_creation_date":"2022-02-04 09:41:08.103 UTC",
        "Owner_last_access_date":"2022-06-05 13:09:20.203 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70984360",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71896741,
        "Question_title":"Adding label in AutoML for text classification",
        "Question_body":"<p>I am trying to create a text dataset in a <code>Pipeline<\/code> for a text classification but I believe I am doing it the wrong way or at least I don't get it. The csv passing only contains two columns <code>message<\/code> and <code>label<\/code> which is true or false.<\/p>\n<p>Inside my pipeline I am creating dataset like this which I am not very sure how dataset is recognizing that column <code>label<\/code> is the independent variable.<\/p>\n<pre><code>dataset = gcp_aip.TextDatasetCreateOp(\n    project = project # my project id,\n    display_name = display_name # reference name,\n    gcs_source  = src_uris # path to my data in gcs,\n    import_schema_uri = aiplatform.schema.dataset.ioformat.text.single_label_classification, \n)\n<\/code><\/pre>\n<p>once created the dataset, i do training like this within the <code>Pipeline<\/code><\/p>\n<pre><code># training\nmodel = gcp_aip.AutoMLTextTrainingJobRunOp(\n    project = project,\n    display_name = display_name,\n    prediction_type = &quot;classification&quot;,\n    multi_label = False,   \n    dataset = dataset.outputs[&quot;dataset&quot;],\n)\n<\/code><\/pre>\n<p>Not sure if creation and training is doing correctly since I never specified that <code>label<\/code> is my label column and needs to use <code>message<\/code> as a feature.<\/p>\n<p>In vertex ai the dataset created look like this<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Puts.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Puts.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But in my training section the results from the AutML, looks like this, dont know why, label with 0% is there, which makes me doubt about the insertion of the data<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-16 19:08:12.427 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-19 08:14:53.117 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|text|vertex|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_date":"2017-09-18 15:22:24.073 UTC",
        "Owner_last_access_date":"2022-09-23 08:17:36.407 UTC",
        "Owner_reputation":1102,
        "Owner_up_votes":108,
        "Owner_down_votes":1,
        "Owner_views":140,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71896741",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73308825,
        "Question_title":"Can Vertex AI model monitoring job send a pubsub message instead of email?",
        "Question_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email.<\/p>\n<pre><code>alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\nemail_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n    user_emails=NOTIFY_EMAILS\n)\n<\/code><\/pre>\n<p>Is there any way to instead send a Pubsub message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-10 15:18:54.353 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-monitoring|google-cloud-vertex-ai",
        "Question_view_count":86,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>You can configure the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring#set-up-alerts\" rel=\"nofollow noreferrer\">alert to be sent to Cloud Logging<\/a>. To enable Cloud Logging alerts you have to set the <code>enableLogging<\/code> field on your <code>ModelMonitoringAlertConfig<\/code> configuration to <code>TRUE<\/code>. Then you can forward the logs to any service that Cloud Logging supports, Pub\/Sub is one of these.<\/p>\n<p>For this you\u2019ll need one of the following permissions:<\/p>\n<ul>\n<li>Owner (roles\/owner)<\/li>\n<li>Logging Admin (roles\/logging.admin)<\/li>\n<li>Logs Configuration Writer (roles\/logging.configWriter)<\/li>\n<\/ul>\n<p>Then you need to <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#creating_sink\" rel=\"nofollow noreferrer\">create a sink<\/a>.<\/p>\n<p>After that you have created the sink you\u2019ll need to set the <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#dest-auth\" rel=\"nofollow noreferrer\">destination permissions<\/a>.<\/p>\n<p>While Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping logs that help with supportability. Using these logs can help you quickly troubleshoot and identify issues with your applications.<\/p>\n<p>Logs routed to Pub\/Sub are generally available within seconds, with 99% of logs available in less than 60 seconds.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-08-11 19:37:08.34 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73308825",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73642322,
        "Question_title":"Running custom Docker container with GPU using Vertex AI Pipelines - how to install NVIDIA driver?",
        "Question_body":"<p>I need to run a custom Docker container with GPU support using Vertex AI Pipelines, and I'm not seeing a clear way to do that. This requires several components:<\/p>\n<ol>\n<li>Applications (pytorch in my case)<\/li>\n<li>CUDA toolkit<\/li>\n<li>CUDA driver<\/li>\n<li>NVIDIA GPUs<\/li>\n<\/ol>\n<p>I can use a <a href=\"https:\/\/github.com\/NVIDIA\/nvidia-docker\" rel=\"nofollow noreferrer\">NVIDIA Docker<\/a> base image for #1 and #2, and a GCP accelerator for #4, but how do I install the CUDA driver in a Vertex AI pipeline? There's documentation on how to install NVIDIA drivers <a href=\"https:\/\/cloud.google.com\/compute\/docs\/gpus\/install-drivers-gpu#no-secure-boot\" rel=\"nofollow noreferrer\">on GCE instances<\/a> and <a href=\"https:\/\/cloud.google.com\/kubernetes-engine\/docs\/how-to\/gpus#installing_drivers\" rel=\"nofollow noreferrer\">GKE nodes<\/a>, but nothing for Vertex AI.<\/p>\n<p>One option could be to <a href=\"https:\/\/cloud.google.com\/deep-learning-containers\/docs\/derivative-container\" rel=\"nofollow noreferrer\">create a derivative container based on a GCP Deep Learning Container<\/a>, but then I have to use a GCP container and don't have as much control over the environment.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-07 23:22:40.08 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"nvidia|google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_date":"2013-12-10 22:23:13.85 UTC",
        "Owner_last_access_date":"2022-09-21 23:19:21.323 UTC",
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73642322",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72821008,
        "Question_title":"Vertex AI updating dataset and train model",
        "Question_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-30 19:19:40.96 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 22:30:59.89 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72821008",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70673890,
        "Question_title":"Automl Vision Error - No valid preprocessed examples",
        "Question_body":"<p>I have set up an Image classification (Single-label).<\/p>\n<p>The model trained for 18 min 25 sec before I recieved the following error:<\/p>\n<p>Due to one or more errors, this training job was canceled on Jan 11, 2022 at 07:34AM\nBatch prediction job GAF-prediction-test encountered the following errors:\nNo valid preprocessed examples.<\/p>\n<p>There is no documentation that I could find that explains this error type. Anyone with any ideas what this means?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-01-11 21:52:15.917 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-12 02:39:33.04 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":130,
        "Owner_creation_date":"2015-04-12 06:31:14.637 UTC",
        "Owner_last_access_date":"2022-09-23 17:09:47.08 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Lexington, KY, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70673890",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69936296,
        "Question_title":"Vertex AI custom container batch prediction",
        "Question_body":"<p>I have created a custom container for prediction and successfully uploaded the model to Vertex AI. I was also able to deploy the model to an endpoint and successfully request predictions from the endpoint. Within the custom container code, I use the <code>parameters<\/code> field as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">here<\/a>, which I then supply later on when making an online prediction request.\nMy questions are regarding requesting batch predictions from a custom container for prediction.<\/p>\n<ol>\n<li><p>I cannot find any documentation that describes what happens when I request a batch prediction. Say, for example, I use the <code>my_model.batch_predict<\/code> function from the Python SDK and set the <code>instances_format<\/code> to &quot;csv&quot; and provide the <code>gcs_source<\/code>. Now, I have setup my custom container to expect prediction requests at <code>\/predict<\/code> as described in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">documentation<\/a>. Does Vertex AI make a POST request to this path, converting the cvs data into the appropriate POST body?<\/p>\n<\/li>\n<li><p>How do I specify the <code>parameters<\/code> field for batch prediction as I did for online prediction?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-11 23:44:50.16 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":808,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69936296",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71500193,
        "Question_title":"How to train MLM model XLM Roberta large on google machine specs fastly with less memory",
        "Question_body":"<p>I am fine tuning masked language model from <code>XLM Roberta large<\/code> on google machine specs.\nI made couple of experiments and was strange to see few results.<\/p>\n<pre><code>&quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 4 Running ( 4 data*4 GPU=16 data points)\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4 , accelerator_type=&quot;NVIDIA_TESLA_A100&quot;on 4,12,672 data batch size 8 failed\n &quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 16 failed\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4.,accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 32 failed\n<\/code><\/pre>\n<p>I was not able to train model with <code>batch size <\/code> more than 4 on # of GPU's. It stopped in mid-way.<\/p>\n<p>Here is the code I am using.<\/p>\n<pre><code>training_args = tr.TrainingArguments(\n#     disable_tqdm=True,\n    output_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/results_mlm_exp2', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n    ,save_strategy=&quot;no&quot;\n    ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n    ,logging_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/logs_mlm_exp1'        # directory for storing logs\n    ,logging_steps=40000\n    ,logging_strategy='no'\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data\n    \n)\n<\/code><\/pre>\n<p><strong>My Questions<\/strong><\/p>\n<p>How can I train with larger batch size on <code>a2-highgpu-4g<\/code> machine?<\/p>\n<p>Which parameters can I include in <code>TrainingArguments<\/code> so that training is fast and occupies small memory?<\/p>\n<p>Thanks in advance.<\/p>\n<h3>Versions<\/h3>\n<pre><code>torch==1.11.0+cu113 \n\ntorchvision==0.12.0+cu113  \n\ntorchaudio==0.11.0+cu113 \n\ntransformers==4.17.0\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-16 15:51:24.443 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|google-cloud-platform|pytorch|huggingface-transformers|google-cloud-vertex-ai",
        "Question_view_count":210,
        "Owner_creation_date":"2018-06-07 08:44:46.053 UTC",
        "Owner_last_access_date":"2022-09-23 09:15:48.837 UTC",
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71500193",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73816577,
        "Question_title":"How do I ingest float from BigQuery into featurestore?",
        "Question_body":"<p>I have a <code>float<\/code> type field in BQ. I try to ingest this into a <code>double<\/code> type field in featurestore but get:<\/p>\n<pre><code>Source schema does not match the expected schema for this import. \nMissing fields in source: feature_field. Type mismatches in source: \nExpected type and mode [STRING, NULLABLE] for BQ_field, but got [FLOAT, ].\n<\/code><\/pre>\n<p>How can I import it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 14:40:15.367 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":7,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73816577",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70838510,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-24 18:12:06.007 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_date":"2020-08-31 11:39:36.143 UTC",
        "Owner_last_access_date":"2022-09-18 19:46:19.503 UTC",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-25 03:21:58.69 UTC",
        "Answer_score":1.0,
        "Owner_location":"Versailles, France",
        "Answer_last_edit_date":"2022-01-25 03:35:51.06 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73415068,
        "Question_title":"HuggingFace Trainer() does nothing - only on Vertex AI workbench, works on colab",
        "Question_body":"<p>I am having issues getting the Trainer() function in huggingface to actually do anything on Vertex AI workbench notebooks.<\/p>\n<p>I'm totally stumped and have no idea how to even begin to try debug this.<\/p>\n<p>I made this small notebook: <a href=\"https:\/\/github.com\/andrewm4894\/colabs\/blob\/master\/huggingface_text_classification_quickstart.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/andrewm4894\/colabs\/blob\/master\/huggingface_text_classification_quickstart.ipynb<\/a><\/p>\n<p>If you set <code>framework=pytorch<\/code> and run it in colab it runs fine.<\/p>\n<p>I wanted to move from colab to something more persistent so tried Vertex AI Workbench notebooks on GCP. I created a user managed notebook (PyTorch:1.11, 8 vCPUs, 30 GB RAM, NVIDIA Tesla T4 x 1) and if i try run the same example notebook in jupyterlab on the notebook it just seems to hang on the <code>Trainer()<\/code> call and do nothing.<\/p>\n<p>It looks like the GPU is not doing anything either for some reason (it might not be supposed to since i think Trainer() is some pretraining step):<\/p>\n<pre><code>(base) jupyter@pytorch-1-11-20220819-104457:~$ nvidia-smi\nFri Aug 19 09:56:10 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N\/A   41C    P8     9W \/  70W |      3MiB \/ 15109MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/discuss.huggingface.co\/t\/it-takes-so-long-before-the-model-start-training-wav2vec2-fine-tuning\/5384\" rel=\"nofollow noreferrer\">this thread<\/a> that maybe seems like a similar problem so i played with as many <code>Trainer()<\/code> args as i could but no luck.<\/p>\n<p>So im kind of totally blocked here - i refactored the code to be able to use Tensorflow which does work for me (after i installed tensorflow on the notebook) but its much slower for some reason.<\/p>\n<p>Basically this was all working great (in my actual real code im working on) on colab's but when i tried to move to Vertex AI Notebooks i seem to be now blocked by this strange issue.<\/p>\n<p>Any help or advice much appreciated, i'm new to HuggingFace and Pytorch etc too so not even sure what things i might try or ways to try run in debug etc maybe.<\/p>\n<h2>Workaround<\/h2>\n<p>i noticed that if i make a new workbook <code>NumPy\/SciPy\/scikit-learn 4 vCPUs, 15 GB RAM , NVIDIA Tesla T4 x<\/code> (instead of the official pytorch one from the dropdown) and install pytorch myself with <code>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch<\/code> it all works.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-08-19 10:02:40.993 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-05 15:07:49.103 UTC",
        "Question_score":0,
        "Question_tags":"jupyter-notebook|pytorch|huggingface-transformers|google-cloud-vertex-ai|distilbert",
        "Question_view_count":80,
        "Owner_creation_date":"2012-12-20 16:14:56.517 UTC",
        "Owner_last_access_date":"2022-09-22 09:34:29.823 UTC",
        "Owner_reputation":1352,
        "Owner_up_votes":222,
        "Owner_down_votes":3,
        "Owner_views":197,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73415068",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73113256,
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_date":"2022-07-25 17:37:33.85 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-17 13:12:47.557 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|hyperparameters|google-cloud-vertex-ai|spacy-3",
        "Question_view_count":133,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2022-08-16 17:10:10.153 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-08-18 19:17:08.733 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69577270,
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-14 20:57:29.023 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-14 21:57:40.77 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-15 04:13:37.373 UTC",
        "Answer_score":2.0,
        "Owner_location":"Boston, MA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73241532,
        "Question_title":"AI\/ML Assisted labeling in Vertex AI",
        "Question_body":"<p>Is there a feature in Vertex AI which will allow AI\/ML to assist in labeling data? This usually works by providing a small set of labeled data, followed by a model creation which assists in labeling more data. As more and more data is labeled the model keeps getting better.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 19:48:45.143 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":58,
        "Owner_creation_date":"2012-05-30 05:48:34.657 UTC",
        "Owner_last_access_date":"2022-09-24 23:10:38.117 UTC",
        "Owner_reputation":840,
        "Owner_up_votes":1070,
        "Owner_down_votes":1,
        "Owner_views":196,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73241532",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70782169,
        "Question_title":"Vertex AI batch prediction location",
        "Question_body":"<p>When I initiate a batch prediction job on Vertex AI of google cloud, I have to specify a cloud storage bucket location. Suppose I provided the bucket location, <code>'my_bucket\/prediction\/'<\/code>, then the prediction files are stored in something like: <code>gs:\/\/my_bucket\/prediction\/prediction-test_model-2022_01_17T01_46_39_898Z<\/code>, which is a subdirectory within the bucket location I provided. The prediction files are stored within that subdirectory and are named:<\/p>\n<pre><code>prediction.results-00000-of-00002\nprediction.results-00001-of-00002\n<\/code><\/pre>\n<p>Is there any way to programmatically get the final export location from the batch prediction name, id or any other parameter as shown below in the details of the batch prediction job?\n<a href=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-20 07:23:48.603 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":554,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Gurugram, Haryana, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70782169",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73250860,
        "Question_title":"Vertex API - Failed to build Pipeline Internal Error",
        "Question_body":"<p>I built a dataset using 30 books in .txt format. Loaded them up started training and pipeline build and it kept failing with an &quot;Internal Error&quot;. Does anyone out there have an idea of what is the root cause of this error? I am using the full Vertex AI AutoML preconfigured models as this is a quick demo. Please Advise..<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 13:51:29.087 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":8,
        "Owner_creation_date":"2022-08-05 13:45:00.253 UTC",
        "Owner_last_access_date":"2022-09-12 19:30:38.87 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73250860",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72619696,
        "Question_title":"GCP AI Platform Vertex endpoint model undeploy : 404 The DeployedModel with ID `2367889687867` is missing",
        "Question_body":"<p>I am trying to un-deploy model from an endpoint following <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_undeploy\" rel=\"nofollow noreferrer\">this documentation<\/a>.<\/p>\n<pre><code>Endpoint.undeploy(deployed_model_id=model_id)\n<\/code><\/pre>\n<p>I even tried <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/undeployModel\" rel=\"nofollow noreferrer\">google api<\/a>. Same Issue with this as well.<\/p>\n<p>Getting 404 error<\/p>\n<blockquote>\n<p>The Deployed Model with ID <code>2367889687867<\/code> is missing.<\/p>\n<\/blockquote>\n<p><strong>INFO:<\/strong><\/p>\n<ol>\n<li>Both model and Endpoint are in same region.<\/li>\n<li>There is a single model deployed in the endpoint with <code>traffic_percentage=100<\/code>.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2022-06-14 15:31:32.347 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-23 03:13:34.26 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-api|google-api-python-client|google-cloud-vertex-ai",
        "Question_view_count":271,
        "Owner_creation_date":"2019-02-21 19:57:27.857 UTC",
        "Owner_last_access_date":"2022-09-25 05:50:48.347 UTC",
        "Owner_reputation":363,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>The <code>deployed_model_id<\/code> is different from the <code>model_id<\/code>.That\u2019s why you are getting the Error 404, it is searching for something that is not the same.<\/p>\n<p>You can get the <code>deployed_model_id<\/code> by:<\/p>\n<ul>\n<li>list_models()<\/li>\n<li>list()<\/li>\n<\/ul>\n<p>Using <code>list_models()<\/code> brings you a list of all the deployed models ids, while using <code>list()<\/code> only brings one, you can add filters such as <code>display_name<\/code>, <code>model_id<\/code>, <code>region<\/code>, etc.<\/p>\n<pre><code>list(\n    filter= \u2018display_name= \u201cdisplay_name\u201d\u2019,\n)\n<\/code><\/pre>\n<p>You also can get the <code>deployed_model_id<\/code> using the Cloud SDK.<\/p>\n<pre><code>gcloud ai models list --region=$REGION --filter=&quot;DISPLAY_NAME: $NAME&quot; | grep &quot;MODEL_ID&quot; | cut -f2 -d: | sed 's\/\\s\/\/'\n<\/code><\/pre>\n<p>Additionally, you can specify the <code>deployed_model_id<\/code> when you are deploying your model using Cloud SDK the command should look like:<\/p>\n<pre><code>gcloud ai endpoints deploy-model $endpoint --project=$project --region=$region --model=$model_id --display-name=$model_name --deployed-model-id=$deployed_model_id\n<\/code><\/pre>\n<p>There are some flags that are required when you deploy a model such as endpoint, project, region, model and display name. And there are others that are <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/deploy-model#:%7E:text=the%20uploaded%20model.-,OPTIONAL%20FLAGS,-%2D%2Daccelerator%3D%5Bcount\" rel=\"nofollow noreferrer\">optional flags<\/a> that you can use deployed_model_id is one of them.(I don\u2019t know if this is possible but you could set the deployed_model_id as the same as the model_id).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-15 17:40:21.51 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72619696",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71496966,
        "Question_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Question_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-16 12:19:41.04 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":220,
        "Owner_creation_date":"2019-07-09 21:04:51.28 UTC",
        "Owner_last_access_date":"2022-09-24 15:30:31.527 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-05 10:50:32.383 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73251212,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 14:19:46.783 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-05 15:18:48.223 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70833594,
        "Question_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Question_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-01-24 12:04:52.21 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-25 04:46:29.933 UTC",
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":707,
        "Owner_creation_date":"2017-07-27 05:06:29.64 UTC",
        "Owner_last_access_date":"2022-09-22 07:17:58.047 UTC",
        "Owner_reputation":329,
        "Owner_up_votes":55,
        "Owner_down_votes":0,
        "Owner_views":88,
        "Answer_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-02 07:20:30.193 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70596614,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - InvalidArgument 400 error",
        "Question_body":"<p>I'm attempting to run a Vertex AI custom training job using the python SDK, following the general instructions laid out in <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\" rel=\"nofollow noreferrer\">this readme<\/a>. My code is as follows (sensitive data removed):<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n    display_name='python_api_test',\n    container_uri='{URI FOR CUSTOM CONTAINER IN GOOGLE ARTIFACT REGISTRY}',\n    staging_bucket='{GCS BUCKET PATH IN 'gs:\/\/' FORMAT}',\n    model_serving_container_image_uri='us-docker.pkg.dev\/vertex-ai\/prediction\/tf2-cpu.2-4:latest',\n)\n\njob.run(\n    model_display_name='python_api_model',\n    args='{ARG PASSED TO CONTAINER ENTRYPOINT}',\n    replica_count=1,\n    machine_type='n1-standard-4',\n    accelerator_type='NVIDIA_TESLA_T4',\n    accelerator_count=2,\n    environment_variables={\n        {A COUPLE OF SECRETS PASSED TO CONTAINER IN DICTIONARY FORMAT}\n    }\n)\n<\/code><\/pre>\n<p>When I execute <code>job.run()<\/code>, I get the following error:<\/p>\n<pre><code>InvalidArgument: 400 Unable to parse `training_pipeline.training_task_inputs` into custom task `inputs` defined in the file: gs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml\n<\/code><\/pre>\n<p>The full traceback does not show where it is unhappy with any specific inputs. I've successfully run jobs in the same container using the Vertex CLI.I'm confident that there is nothing wrong with my <code>aiplatform.init()<\/code> (I'm running the job from a Vertex workbench machine in the same project).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 16:53:55.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":79,
        "Owner_creation_date":"2016-05-24 16:22:09.61 UTC",
        "Owner_last_access_date":"2022-09-13 16:37:07.037 UTC",
        "Owner_reputation":457,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70596614",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73047696,
        "Question_title":"Annotation specs - AutoML (VertexAi)",
        "Question_body":"<p>We're trying to build an imaged based product search for our webshop using the vertex ai image classification model (single label).<\/p>\n<p>Currently we have around 20k products with xx images per product.\nSo our dataset containing 20k of labels (one for each product - product number), but on import we receive the following error message:<\/p>\n<p><code>There are too many AnnotationSpecs in the dataset. Up to 5000 AnnotationSpecs are allowed in one Dataset. Check your csv\/jsonl format with our public documentation.<\/code><\/p>\n<p>Looks like not more than 5000 labels are allowed per Dataset... This quota is not really visible in the documentation - or we didn't find it.<\/p>\n<p>Anyway, any ideas how we can make it work? Does we have to build 5 Datasets with 5 different Endpoints and than query every Enpoint for matching?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-20 07:39:36.407 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":38,
        "Owner_creation_date":"2022-07-20 07:24:37.233 UTC",
        "Owner_last_access_date":"2022-09-19 15:58:45.803 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73047696",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73033530,
        "Question_title":"Getting explainable predictions from Vertex AI for custom trained model",
        "Question_body":"<p>I've created a custom docker container to deploy my model on Vertex AI (the model uses LightGBM, so I can't use the pre-built containers provided by Vertex AI for TF\/SKL\/XGBoost).\nI'm getting errors while trying to get <strong>explainable<\/strong> predictions from the model (I deployed the model and normal predictions are working just fine). I have gone through the official Vertex AI guide(s) for getting predictions\/explanations, and also tried different ways of configuring the explanation parameters and metadata, but it's still not working. The errors are not very informative, and this is what they look like:<\/p>\n<pre><code>400 {&quot;error&quot;: &quot;Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}&quot;}\n<\/code><\/pre>\n<p>This notebook from Vertex provided by Google has some examples on how to configure the explanation parameters and metadata for models trained with different frameworks. I'm trying something similar.<\/p>\n<p><a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage4\/get_started_with_vertex_xai.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage4\/get_started_with_vertex_xai.ipynb<\/a><\/p>\n<p>The model is a classifier that takes tabular input with 5 features (2 string, 3 numeric), and output value from model.predict() is 0\/1 for each input instance. My custom container returns predictions in this format:<\/p>\n<pre><code># Input for prediction\nraw_input = request.get_json()\ninput = raw_input['instances']\ndf = pd.DataFrame(input, columns = ['A', 'B', 'C', 'D', 'E'])\n# Prediction from model\npredictions = model.predict(df).tolist()\nresponse = jsonify({&quot;predictions&quot;: predictions})\nreturn response\n<\/code><\/pre>\n<p>This is how I am configuring the explanation parameters and metadata for the model:<\/p>\n<pre><code># Explanation parameters\nPARAMETERS = {&quot;sampled_shapley_attribution&quot;: {&quot;path_count&quot;: 10}}\nexp_parameters = aip.explain.ExplanationParameters(PARAMETERS)\n\n# Explanation metadata (this is probably the part that is causing the errors)\nCOLUMNS = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]\nexp_metadata = aip.explain.ExplanationMetadata(\n    inputs={\n        &quot;features&quot;: {&quot;index_feature_mapping&quot;: COLUMNS, &quot;encoding&quot;: &quot;BAG_OF_FEATURES&quot;}\n    },\n    outputs={&quot;predictions&quot;: {}},\n)\n<\/code><\/pre>\n<p>For getting predictions\/explanations, I tried using the below format, among others:<\/p>\n<pre><code>instance_1 = {&quot;A&quot;: &lt;value&gt;, &quot;B&quot;: &lt;&gt;, &quot;C&quot;: &lt;&gt;, &quot;D&quot;: &lt;&gt;, &quot;E&quot;: &lt;&gt;}\ninstance_2 = {&quot;A&quot;: &lt;value&gt;, &quot;B&quot;: &lt;&gt;, &quot;C&quot;: &lt;&gt;, &quot;D&quot;: &lt;&gt;, &quot;E&quot;: &lt;&gt;}\ninputs = [instance_1, instance_2]\npredictions = endpoint.predict(instances=inputs) # Works fine\nexplanations = endpoint.explain(instances=inputs) # Returns error\n<\/code><\/pre>\n<p>Could you please suggest me how to correctly configure the explanation metadata, or provide input in the right format to the explain API, to get explanations from Vertex AI? I have tried many different formats, but nothing is working so far. :(<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 08:23:06.007 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"docker|lightgbm|google-cloud-vertex-ai|xai",
        "Question_view_count":81,
        "Owner_creation_date":"2022-07-19 07:36:13.307 UTC",
        "Owner_last_access_date":"2022-09-22 23:30:30.837 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73033530",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73573487,
        "Question_title":"Vertex AI model version using Python SDK",
        "Question_body":"<p>Vertex AI offers a very interesting Model Registry that allows you to store all trained models and track all their versions.<\/p>\n<p>However, I don't manage to create new versions of the same model using the Python SDK. In particular, I have a Vertex AI Pipeline that performs: 1) data preprocessing, 2) feature engineering, 3) feature store creation, and in the end, 4) train a model with AutoML Tabular.<\/p>\n<p>The code of the Pipeline component dedicated to the point 4 is:<\/p>\n<pre><code> automl_training_electric_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n    project=project_bq,\n    model_display_name=&quot;pred-model&quot;,\n    display_name=&quot;pred-model&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-prc&quot;,\n    budget_milli_node_hours=1000,\n    dataset=comp5a.outputs[&quot;dataset&quot;],\n    target_column=&quot;fault&quot;,\n    location=location\n)\n<\/code><\/pre>\n<p>In the Google documentation I didn't find anything that could help me in creating new versions of the &quot;pred-model&quot;, in fact, any time I run the pipeline, Vertex AI creates a new model with the same name.<\/p>\n<p>I would like that at each training, AutoML creates a new version of the same model. E.g., v1, v2, v3.<\/p>\n<p>Here, the current situation, in which the same model is replicated and not versioned:\n<a href=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-01 17:40:58.413 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":58,
        "Owner_creation_date":"2014-11-06 09:41:52.943 UTC",
        "Owner_last_access_date":"2022-09-24 19:13:38.297 UTC",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73573487",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73434003,
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-08-21 11:30:10.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_date":"2021-12-12 12:36:30.127 UTC",
        "Owner_last_access_date":"2022-09-19 20:00:07.14 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-23 15:00:06.083 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73434003",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73236524,
        "Question_title":"Feasibility of calling vertex AI endpoints from dataflow streaming pipeline",
        "Question_body":"<p>I have an application where a streaming dataflow pipeline does inference on an incoming stream of images. It does so by loading a tensorflow CNN model saved in a GCS location as h5 file and using that model loaded in a dataflow user defined PTransform to do the inference.<\/p>\n<p>I have been going through GCP documentations but the following is still not clear to me.\nInstead of having to load the tensorflow model from a GCS bucket, is it possible to deploy them model on vertex AI endpoint and call the endpoint to do inference on an image from a dataflow Ptransform? Is it feasible?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 13:06:36.257 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-dataflow|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_date":"2018-06-21 09:40:34.03 UTC",
        "Owner_last_access_date":"2022-09-21 02:44:50.143 UTC",
        "Owner_reputation":361,
        "Owner_up_votes":14,
        "Owner_down_votes":4,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73236524",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69217165,
        "Question_title":"When running custom container in Kubeflow, how to pass arguments to the container?",
        "Question_body":"<p>I created a container image, and want to run it as part of my Kubeflow pipeline.<\/p>\n<p>I tested the image locally and have no problem running it:<\/p>\n<pre><code>docker run $IMAGE_URI --my_argument 28\n<\/code><\/pre>\n<p>I tried running it in Kubeflow using pre-built component, [CustomContainerTrainingJobRunOp][1], but failed. It seems that the problem is with the arguments, since it had no problem when hard-coded the argument value in the Python code.  How do I correctly pass the argument in this component? Thanks.<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root='gs:\/\/a-gcs-bucket')\ndef pipeline():\n    \n    subclass_training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n        project=project_id, \n        display_name=model_display_name,\n        container_uri=subclass_container_uri,\n        args=[&quot;--my_argument&quot;, 28],\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_uri,\n        #dataset=dataset_create_op.outputs[&quot;dataset&quot;]\n    )\n\nThank you!\n\n\n  [1]: https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-17 02:46:37.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-17 03:01:40.167 UTC",
        "Question_score":0,
        "Question_tags":"containers|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":316,
        "Owner_creation_date":"2017-10-27 01:07:11.177 UTC",
        "Owner_last_access_date":"2022-09-05 21:01:42.29 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69217165",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70449117,
        "Question_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Question_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-22 12:32:34.14 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_date":"2014-05-14 02:59:39.907 UTC",
        "Owner_last_access_date":"2022-09-12 20:42:48.213 UTC",
        "Owner_reputation":490,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":65,
        "Answer_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-01-03 09:21:47.43 UTC",
        "Answer_score":1.0,
        "Owner_location":"Rio de Janeiro - RJ, Brasil",
        "Answer_last_edit_date":"2022-01-03 10:11:36.607 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72481852,
        "Question_title":"VertexAI AutoML SavedModel's directory doesn't have the correct structure",
        "Question_body":"<p>I'm trying to export an AutoML model trained on VexterAI to Storage, but the structure of the directory of destiny is wrong. The model was exported with the format id <code>tf-saved-model<\/code>, a TensorFlow model. The command I used to export was the following:<\/p>\n<pre><code>response = model.export_model(export_format_id='tf-saved-model', artifact_destination='gs:\/\/storage_path\/')\n<\/code><\/pre>\n<p>And I believe the destination directory should have the structure of a TensorFlow SavedModel like:<\/p>\n<pre><code> model\/\n    \u251c\u2500\u2500 assets\/\n    \u2502   \u251c\u2500\u2500 ...\n    \u251c\u2500\u2500 saved_model.pb\n    \u251c\u2500\u2500 variables\/\n    \u2502   \u251c\u2500\u2500 ...\n<\/code><\/pre>\n<p>But my directory has the following structure:<\/p>\n<pre><code>model \/\n   \u251c\u2500\u2500 environment.json\n   \u251c\u2500\u2500 feature_attributions.yaml\n   \u251c\u2500\u2500 lower_bound\/\n   |   \u251c\u2500\u2500 001\/\n   |   |   \u251c\u2500\u2500 assets.extra\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 assets\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 saved_model.pb\n   |   |   \u251c\u2500\u2500 variables\/\n   |   |   |   \u251c\u2500\u2500 ...\n   \u251c\u2500\u2500 predict\/\n   |   \u251c\u2500\u2500 001\/\n   |   |   \u251c\u2500\u2500 assets.extra\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 assets\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 saved_model.pb\n   |   |   \u251c\u2500\u2500 variables\/\n   |   |   |   \u251c\u2500\u2500 ...\n   \u251c\u2500\u2500 upper_bound\/\n   |   \u251c\u2500\u2500 001\/\n   |   |   \u251c\u2500\u2500 assets.extra\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 assets\/\n   |   |   |   \u251c\u2500\u2500 ...\n   |   |   \u251c\u2500\u2500 saved_model.pb\n   |   |   \u251c\u2500\u2500 variables\/\n   |   |   |   \u251c\u2500\u2500 ...\n   | ...\n\n\n<\/code><\/pre>\n<p>I had already searched through so many documentation pages, and I didn't find what I'm doing wrong... Can someone help me?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-06-02 20:13:07.963 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-storage|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_date":"2019-07-13 14:14:21.237 UTC",
        "Owner_last_access_date":"2022-09-24 22:32:14.17 UTC",
        "Owner_reputation":315,
        "Owner_up_votes":44,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72481852",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72732651,
        "Question_title":"how to configure proxy in .Net application",
        "Question_body":"<p>I am facing problem while calling <strong>gcp api's (Vertex Ai &amp; Bigquery)<\/strong>. we have been using these api's from last few months. The behavior is unpredictable, sometimes we get api response successfully and sometimes its failing.<\/p>\n<p>what we noticed that the same host <strong>us-central1-aiplatform.googleapis.com<\/strong> communication is disrupted.<\/p>\n<p>Currently we are connecting with <strong>GOOGLE_APPLICATION_CREDENTIALS<\/strong>.<\/p>\n<p>I want to go through proxy. any suggestions ? or any other way I can solve this ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 15:19:49.337 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"c#|.net|google-cloud-platform|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732651",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73419949,
        "Question_title":"Vertex AI quota limit error when current usage percentage is zero",
        "Question_body":"<p>In the context of a personal project, Im trying to use Vertex AI to run a TFX pipeline to train a model using custom training, based on <a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/tfx\/gcp\/vertex_pipelines_vertex_training\" rel=\"nofollow noreferrer\">this guide<\/a>. When I run the pipeline I get the error:<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus<\/p>\n<\/blockquote>\n<p>On the IAM quotas I have limit &quot;1&quot; for the resource &quot;Custom model training CPUs for N1\/E2 machine types per region&quot;, for all regions, and 0% current usage for each one of them. I even tried multiple regions and multiple types of machines (n1, e2, ...) and I alway get that quota limit error.<\/p>\n<p>Can anyone explain why Im getting this quota error?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-19 16:52:14.727 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":28,
        "Owner_creation_date":"2016-08-09 10:03:39.32 UTC",
        "Owner_last_access_date":"2022-09-24 14:10:31.477 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":223,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73419949",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72427594,
        "Question_title":"Questions on json and GCP",
        "Question_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-29 22:06:49.46 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|endpoint|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_date":"2021-09-25 19:17:36.473 UTC",
        "Owner_last_access_date":"2022-08-21 11:12:39.213 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-29 22:33:13.763 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-05-29 22:43:13.883 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70023419,
        "Question_title":"Vertex AI - cannot create managed notebook instance",
        "Question_body":"<p>I'm following all the (easy) steps in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-create-console\" rel=\"nofollow noreferrer\">documentation<\/a>, but I'm stuck at clicking the &quot;CREATE&quot; button. When I click it, the process runs for a few seconds, then the button re-appears, like I never clicked it.<\/p>\n<p>If I go back to the &quot;Managed Notebooks&quot; page, no instance is present.<\/p>\n<p>Am I missing something basic? Has someone the same problem as mine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-18 16:27:44.337 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":281,
        "Owner_creation_date":"2015-12-16 14:51:58.267 UTC",
        "Owner_last_access_date":"2022-09-23 10:21:22.25 UTC",
        "Owner_reputation":947,
        "Owner_up_votes":625,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Milano, MI, Italia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70023419",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72126124,
        "Question_title":"Import of exported Vertex-AI AutoML model in production fails",
        "Question_body":"<p>I want to deploy a Vertex-AI model in a production project which has been trained in a training project.<\/p>\n<pre><code>----TRAINING PRJ-----     --------PRODUCTION PRJ---------\nTrain &gt; test &gt; export  &gt;  import &gt; deploy | batch predict\n<\/code><\/pre>\n<p>I follow <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#automl-tabular-container\" rel=\"nofollow noreferrer\">these instructions<\/a> and get a success email:<\/p>\n<blockquote>\n<p>Vertex AI finished uploading model &quot;xxxxxx&quot;.<\/p>\n<p>Operation State: Succeeded<\/p>\n<\/blockquote>\n<p>but when I try to test the model with a Batch prediction I always get a failed message:<\/p>\n<blockquote>\n<p>Due to an error, Vertex AI was unable to batch predict using model &quot;TEST&quot;.<\/p>\n<p>Operation State: Failed with errors Error<\/p>\n<p>Messages: INTERNAL<\/p>\n<\/blockquote>\n<p>Please note deploying the model to an endpoint and testing with a JSON request it <strong>does provide the expected response<\/strong>.<\/p>\n<p>I tried several container types besides the one suggested <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#automl-tabular-container\" rel=\"nofollow noreferrer\">here<\/a>, included the one stated in the exported model's <code>environment.json<\/code> <code>container_uri<\/code>: <strong>the batch prediction always fails with message INTERNAL<\/strong><\/p>\n<p>Any clue?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EXrc0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EXrc0.png\" alt=\"Vertex-AI screenshot\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/t6wWJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t6wWJ.png\" alt=\"Model details\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-05-05 11:03:10.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-06 09:07:05.243 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":164,
        "Owner_creation_date":"2010-04-06 10:15:10.493 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:35.203 UTC",
        "Owner_reputation":10832,
        "Owner_up_votes":509,
        "Owner_down_votes":3,
        "Owner_views":761,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Milan, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72126124",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70338795,
        "Question_title":"How to fix \"Kernel Unknown\" error in JupyterLab on Google Vertex AI notebooks (Python 3)",
        "Question_body":"<p>When I try to open an existing or new notebook on my Google Vertex Notebooks instance (on Python 3.7.6), no code cells run and it says &quot;kernel unknown&quot; at the top left. When I try to run a code cell, an asterisk appears as if it were running, but nothing else happens. Everything else works fine (e.g. opening the terminal, editing markdown cells in a notebook, viewing files). The issue started happening immediately after I tried to update conda to fix conflicting package errors.  What can I do to fix the issue and be able to run code cells again?<\/p>\n<p>(Please let me know if I'm leaving out important information)<\/p>\n<p>screenshot of the issue:\n<img src=\"https:\/\/i.stack.imgur.com\/7dwuB.png\" alt=\"error screenshot\" \/><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-12-13 17:43:00.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-13 17:52:55.813 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|conda|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":1110,
        "Owner_creation_date":"2021-12-13 17:33:05.647 UTC",
        "Owner_last_access_date":"2022-04-08 22:36:48.013 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70338795",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73731002,
        "Question_title":"Why do I get There are no registered serializers for type \"google.VertexEndpoint\"",
        "Question_body":"<p>I am trying to deploy a model using <code>ModelDeployOp<\/code> in a Vertex AI pipeline component. My code is:<\/p>\n<pre><code>parent = client.common_location_path(project=project, location=location)\nrequest = aiplatform_v1.ListEndpointsRequest(parent=parent)\n\npage_result = client.list_endpoints(request=request)\nfor response in page_result:\n     latest_endpoint=response\n\n\ndeploy_op=gcc_aip.ModelDeployOp(\nmodel=model_name,\nendpoint=latest_endpoint,\ndedicated_resources_min_replica_count=1,\ndedicated_resources_max_replica_count=1,\ndedicated_resources_machine_type=&quot;n1-standard-4&quot;,\n)\n<\/code><\/pre>\n<p>but i get:<\/p>\n<pre><code>TypeError: There are no registered serializers for type &quot;google.VertexEndpoint&quot;.\n<\/code><\/pre>\n<p>How can I fix this or maybe find a better way of getting the endpoint?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 12:17:30.463 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":9,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73731002",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69721067,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-10-26 09:59:23.74 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-09 08:34:24.247 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-api-python-client|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":357,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-11-08 09:36:18.673 UTC",
        "Answer_score":1.0,
        "Owner_location":"Alatri, Frosinone, FR",
        "Answer_last_edit_date":"2021-12-02 19:02:48.683 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73403004,
        "Question_title":"Using ipyleaflet within a Vertex AI Managed Notebook running on a Docker image",
        "Question_body":"<p>TL;DR How does one get ipyleaflet to work in a Vertex AI Managed Notebook booted from a custom Docker image?<\/p>\n<p><strong>Objective<\/strong><\/p>\n<p>Following on <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">this thread<\/a>, I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. I am trying to supply a custom Docker image, such that when a Jupyter Lab notebook is launched from the running instance, it (a) contains some modules for performing and visualizing cartographic analysis, and, importantly, (b) permits visualization with ipyleaflet and associated modules.<\/p>\n<p><strong>What I've tried<\/strong><\/p>\n<p>Thus far, I have succeed in creating a Docker image (derivative of <a href=\"https:\/\/cloud.google.com\/deep-learning-containers\/docs\/choosing-container\" rel=\"nofollow noreferrer\">Google image<\/a>) that supplies, in the running Jupyter Lab, a dedicated environment (explicitly exposed kernel) with the correct modules (in particular, geemap, earthengine-api, ipyleaflet, ipywidgets). The modules are all importable and appear sound. However, so far as I can tell, supplying a custom Docker image during the build process, effectively breaks the ipyleaflet (and presumably widgets, events, etc) connection that Google's Jupyter Lab base image has if one creates a Managed Notebook <em>without<\/em> supplying a Docker image. Attempts to create map visualizations returns, &quot;Error displaying widget: model not found&quot;, discussed at <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">2<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">3<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">4<\/a>. In other words, if one creates a Managed Notebook <em>without<\/em> a Docker image, starts the notebook instance, launches Jupyter Lab, opens a notebook, and then uses <code>%pip install xyz<\/code> for the modules of interest, ipyleaflet-based mapping works fine. I suspect that the nuance of difference here, is that the latter method (<code>%pip install<\/code>ing from within the notebook), is being layered on top of a fully formed base Jupyter Lab container (per @gogasca's comment <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">here<\/a>, that the Google Managed Jupyter Lab runs as a container that is not customizable).<\/p>\n<p><strong>Questions<\/strong><\/p>\n<p>So, what I would love to know is:<\/p>\n<p><strong>(1)<\/strong> How does one retain ipyleaflet (and associated modules) functionality in a Managed Notebook that is based on a user supplied Docker image?<\/p>\n<p><strong>(2)<\/strong> Is there a way to effectively replicate the <code>%pip install<\/code> approach when using a custom Docker image, such that commands specified in the Docker file are layered on top of a fully formed base Google image.<\/p>\n<p>To question (2), I suspect that when the gcloud sdk for Managed Notebooks is available (currently under the impression that this is a work in progress), it will be possible to provide a post-startup-script, as in <a href=\"https:\/\/medium.com\/@gogasca_\/ai-platform-notebooks-with-voila-c3c57d4e8e\" rel=\"nofollow noreferrer\">this example<\/a>. I am aware that there are REST and Terraform build options available that might satisfy my Managed Notebook needs. These require quite a bit more legwork though, so I am interested in simpler solutions, if they exist.<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>Adding content for partial reproducibility. Steps to reproduce:<\/p>\n<ol>\n<li>A custom Docker image was created using the interface at <a href=\"https:\/\/shell.cloud.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/shell.cloud.google.com\/<\/a>.<\/li>\n<li>Within the cloud shell, from the terminal, set gcloud configuration parameters with <code>gcloud config set project yourProjectIDHere<\/code><\/li>\n<li>Create a file named Dockerfile with the following content:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>FROM python:3.7.4-buster \nENV VIRTUAL_ENV=\/env\/testEnvironment \nRUN python3 -m venv $VIRTUAL_ENV --system-site-packages \nENV PATH=&quot;$VIRTUAL_ENV\/bin:$PATH&quot; \n# Install dependencies: \nCOPY requirements.txt . \nRUN $VIRTUAL_ENV\/bin\/pip install -r requirements.txt\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"4\">\n<li>Create a file named requirements.txt with the following content<\/li>\n<\/ol>\n<blockquote>\n<pre><code>ipython\nipykernel\ngeemap\nearthengine-api\nipyleaflet\nfolium\nvoila\nipywidgets\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"5\">\n<li>From the cloud shell terminal, build your Docker image (this takes a long time to build. We will use a Google deep learning library derivative once we get our workflow sorted):\n<code>docker build . -f Dockerfile -t &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Push container to GCP Container Registry\n<code>docker push &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Move to Google cloud console and create a new Managed Notebook within your GCP.<\/li>\n<li>Select the defaults except for (a) use Service account permissions; (b) use &quot;Networks shared with me&quot; associated with the host project, including both a Network and Shared subnetwork (presuming that this isn't going to change installation components, so probably doesn't matter); (c) uncheck &quot;Enable external IPs&quot;; (d) check &quot;Enable terminal&quot;; and (e) check &quot;Provide custom docker images&quot;.<\/li>\n<li>From the &quot;Provide custom docker images&quot; dialog, select the image you just created.<\/li>\n<li>Create the Managed Notebook, and once created, open Jupyter Lab.<\/li>\n<li>The Docker image should expose a kernel in the Jupyter Lab environment. Open a new notebook using the kernel.<\/li>\n<li>Load and test modules within the notebook<\/li>\n<\/ol>\n<blockquote>\n<pre><code># Import and initialize the earthengine-api by whatever means fit your use case\nimport ee\nee.Initialize( ... )\n# Test authentication pathways\nprint(ee.Image(&quot;NASA\/NASADEM_HGT\/001&quot;).get(&quot;title&quot;).getInfo())\n# Provided you have your authentication set-up correctly...\n\n# Test import of various other modules   \nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport geemap\nfrom IPython.display import display, HTML, Image\nimport ipywidgets as widgets\nfrom ipywidgets import Layout\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Button, HBox, VBox\nfrom ipywidgets import HTML\nimport voila\n\n# Test inline mapping using ipyleaflet-based map     \nMap = geemap.Map(center=(-0.2557968807155925, 119.46629773460036), zoom=5)\nMap\n\n# Receive error.\n\n# This is also replicable without earthengine-api or geemap, by just trying to make a basic ipyleaflet map\nfrom ipyleaflet import Map, basemaps, basemap_to_tiles\nm = Map(basemap=basemap_to_tiles(basemaps.OpenStreetMap.Mapnik), center=(48.204793, 350.121558), zoom=3)\nm\n<\/code><\/pre>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-08-18 12:21:40.527 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-01 18:59:54.213 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":148,
        "Owner_creation_date":"2016-03-30 15:15:05.81 UTC",
        "Owner_last_access_date":"2022-09-22 14:32:04.693 UTC",
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73403004",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72279841,
        "Question_title":"Vertex AI Workbench User-managed notebooks crashing with Huggingface Datasets",
        "Question_body":"<p>I am attempting to load a <strong>Huggingface<\/strong> dataset in a <strong>User-managed notebook<\/strong> in the Vertex AI workbench. I have tried memory-optimized machines such as <code>m1-ultramem-160<\/code> and <code>m1-megamem-96<\/code>. I selected an additional <strong>2000 GB<\/strong> of SDD boot\/disk space.<\/p>\n<p>The dataset downloads about 60% of the way before JupyterLab crashes.<\/p>\n<p>For example, this crashes the kernel:<\/p>\n<p><code>! pip install datasets<\/code><\/p>\n<pre><code>pile_dataset = load_dataset('the_pile', 'all', split = 'train')\n<\/code><\/pre>\n<p>I am unsure if I need to do anything specific to increase the available memory allocated to the JupyterLab instance.<\/p>\n<p>Any help would be greatly appreciated.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-17 19:45:09.73 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-02 08:27:55.153 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook|huggingface-datasets",
        "Question_view_count":371,
        "Owner_creation_date":"2019-08-18 19:57:05.893 UTC",
        "Owner_last_access_date":"2022-09-10 00:12:38.677 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72279841",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73458040,
        "Question_title":"How do I enable_logging for ModelMonitoringAlertConfig in GCP?",
        "Question_body":"<p>I am trying to <code>enable_logging<\/code> in <code>ModelMonitoringAlertConfig<\/code> I have tried:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as vertex_ai_beta\n...\n    alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\n    enable_logging=True,\n    email_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n        user_emails=NOTIFY_EMAILS\n    )\n)\n<\/code><\/pre>\n<p>gives:<\/p>\n<pre><code> Unknown field for ModelMonitoringAlertConfig: enable_logging\n<\/code><\/pre>\n<p>but <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.ModelMonitoringAlertConfig\" rel=\"nofollow noreferrer\">this<\/a> suggests it should work. What am I missing?<\/p>\n<p>(I have also tried <code>aiplatform_v1beta1<\/code>.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-23 11:38:48.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":19,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458040",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71937033,
        "Question_title":"Google Cloud Platform - Vertex AI training with custom data format",
        "Question_body":"<p>I need to train a custom OCR in vertex AI. My data with have folder of cropped image, each image is a line, and a csv file with 2 columns: image name and text in image.\nBut when I tried to import it into a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets\" rel=\"nofollow noreferrer\">dataset<\/a> in vertex AI, I see that image dataset only support for classification, segmentation, object detection. All of dataset have fixed number of label, but my data have a infinite number of labels(if we view text in image as label), so all types doesn't match with my requirement. Can I use vertex AI for training, and how to do that ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-20 09:11:51.14 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|ocr|google-cloud-vertex-ai",
        "Question_view_count":303,
        "Owner_creation_date":"2014-10-09 13:12:23.897 UTC",
        "Owner_last_access_date":"2022-09-24 04:54:07.693 UTC",
        "Owner_reputation":803,
        "Owner_up_votes":76,
        "Owner_down_votes":8,
        "Owner_views":114,
        "Answer_body":"<p>Since Vertex AI managed datasets do not support OCR applications, you can train and deploy a custom model using Vertex AI\u2019s training and prediction services.<\/p>\n<p>I found a good <a href=\"https:\/\/medium.com\/geekculture\/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b\" rel=\"nofollow noreferrer\">article<\/a> on building an OCR system from scratch. This OCR system is implemented in 2 steps<\/p>\n<ol>\n<li>Text detection<\/li>\n<li>Text recognition<\/li>\n<\/ol>\n<p>Please note that this article is not officially supported by Google Cloud.<\/p>\n<p>Once you have tested the model locally, you can train the same on Vertex AI using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/custom-training\" rel=\"nofollow noreferrer\">custom model training service<\/a>. Please follow this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for step-by-step instructions on training and deploying a custom model.<\/p>\n<p>Once the training is complete, the model can be deployed for inference using a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers\" rel=\"nofollow noreferrer\">pre-built container<\/a> offered by Vertex AI or a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">custom container<\/a> based on your requirements. You can also choose between batch predictions for synchronous requests and online predictions for asynchronous requests.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-09 10:17:21.187 UTC",
        "Answer_score":2.0,
        "Owner_location":"Hanoi, Vietnam",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71937033",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69348337,
        "Question_title":"Schedule batch predictions Vertex AI",
        "Question_body":"<p>I have created a forecasting model using AutoML on Vertex AI. I want to use this model to make batch predictions every week. Is there a way to schedule this?<\/p>\n<p>The data to make those predictions is stored in a bigquery table, which is updated every week.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-27 14:23:07.397 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":729,
        "Owner_creation_date":"2021-08-25 10:01:17 UTC",
        "Owner_last_access_date":"2021-12-18 13:59:43.64 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69348337",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71245000,
        "Question_title":"Vertex AI Pipeline Failed Precondition",
        "Question_body":"<p>I have been following this video:\n<a href=\"https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s<\/a><\/p>\n<p>Code located at:\n<a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a>\n(I have done the last two steps as per the video which isn't an issue for google_cloud_pipeline_components version: 0.1.1)<\/p>\n<p>I have created a pipeline in vertex ai which ran and used the following code to create the pipeline (from video not code extract in link above):<\/p>\n<pre><code>#run pipeline\nresponse = api_client.create_run_from_job_spec(\n    &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n    parameter_values = {\n    &quot;project&quot; : PROJECT_ID,\n    &quot;display_name&quot; : DISPLAY_NAME\n    }\n)\n    \n<\/code><\/pre>\n<p>and in the GCP logs I get the following error:<\/p>\n<pre><code>&quot;google.api_core.exceptions.FailedPrecondition: 400 BigQuery Dataset location `eu` must be in the same location as the service location `us-central1`.\n<\/code><\/pre>\n<p>I get the error at the dataset_create_op stage:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project = project, display_name = display_name, bq_source = bq_source\n)\n<\/code><\/pre>\n<p>My dataset is configured in EU (the whole region) so I don't understand where us-central1 is coming from (or what the service location is?).<\/p>\n<p>Here is the all the code I have used:<\/p>\n<pre><code> PROJECT_ID = &quot;marketingtown&quot;\n BUCKET_NAME = f&quot;gs:\/\/lookalike_model&quot;\n from typing import NamedTuple\n import kfp\n from kfp import dsl\n from kfp.v2 import compiler\n from kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                            OutputPath, ClassificationMetrics, \n Metrics, component)\n from kfp.v2.components.types.artifact_types import Dataset\n from kfp.v2.google.client import AIPlatformClient\n from google.cloud import aiplatform\n from google_cloud_pipeline_components import aiplatform as gcc_aip\n\n #set environment variables\n PATH = %env PATH\n %env PATH = (PATH):\/\/home\/jupyter\/.local\/bin\n REGION = &quot;europe-west2&quot;\n    \n #cloud storage path where artifact is created by pipeline\n PIPELINE_ROOT = f&quot;{BUCKET_NAME}\/pipeline_root\/&quot;\n PIPELINE_ROOT\n import time\n DISPLAY_NAME = f&quot;lookalike_model_pipeline_{str(int(time.time()))}&quot;\n print(DISPLAY_NAME)\n \n@kfp.dsl.pipeline(name = &quot;lookalike-model-training-v2&quot;, \npipeline_root = PIPELINE_ROOT)\n\ndef pipeline(\n    bq_source : str = f&quot;bq:\/\/{PROJECT_ID}.MLOp_pipeline_temp.lookalike_training_set&quot;,\n    display_name : str = DISPLAY_NAME,\n    project : str = PROJECT_ID,\n    gcp_region : str = &quot;europe-west2&quot;,\n    api_endpoint : str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str : str = '{&quot;auPrc&quot; : 0.3}'\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project = project, display_name = display_name, bq_source = bq_source\n    )\n    \n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;], #dataset from previous step\n        target_column=&quot;sale&quot;,\n    )\n    \n    #outputted evaluation metrics\n    model_eval_task = classification_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n    )\n    \n    #if deployment threshold is mean, deploy\n    with dsl.Condition(\n        model_eval_task.outputs[&quot;dep_decision&quot;] == &quot;true&quot;,\n        name=&quot;deploy_decision&quot;,\n    ):\n        \n    endpoint_op = gcc_aip.EndpointCreateOp(\n        project=project,\n        location=gcp_region,\n        display_name=&quot;train-automl-beans&quot;,\n    )\n        \n    #deploys model to an endpoint\n    gcc_aip.ModelDeployOp(\n        model=training_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_op.outputs[&quot;endpoint&quot;],\n        min_replica_count=1,\n        max_replica_count=1,\n        machine_type=&quot;n1-standard-4&quot;,\n        )\n   \n\n     compiler.Compiler().compile(\n        pipeline_func = pipeline, package_path = &quot;tab_classif_pipeline.json&quot;\n    )\n\n    #run pipeline\n    response = api_client.create_run_from_job_spec(\n        &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n        &quot;project&quot; : PROJECT_ID,\n        &quot;display_name&quot; : DISPLAY_NAME\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2022-02-23 22:57:22.94 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-24 10:08:18.213 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|pipeline|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":420,
        "Owner_creation_date":"2019-07-09 21:04:51.28 UTC",
        "Owner_last_access_date":"2022-09-24 15:30:31.527 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project=project,\n    display_name=display_name, \n    bq_source=bq_source,\n    location = gcp_region\n)\n<\/code><\/pre>\n<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-09 12:14:43.057 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71245000",
        "Question_exclusive_tag":"Vertex AI"
    }
]