[
    {
        "Question_id":71430286,
        "Question_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Question_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1646943487337,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|model|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":936,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1646987974513,
        "Answer_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1646971559037,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646998024380,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73461369,
        "Question_title":"How should Pubsub, acting a log sink, fire a function without sending the log?",
        "Question_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email and have adapted it to send a Pubsub message, with @Jose Gutierrez Paliza's help.<\/p>\n<p>I have got this working, sort of. But what seems to be happening is that Pubsub pushes the log to  a function which errors.<\/p>\n<p>My log sink includes:\n<a href=\"https:\/\/i.stack.imgur.com\/00ksv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/00ksv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I look at logs I see an INFO entry:<\/p>\n<pre><code>   my-fn an_id Event data: {&quot;insertId&quot;:&quot;another_id...\n<\/code><\/pre>\n<p>followed by a separate ERROR entry:<\/p>\n<pre><code>...\nValueError: The pipeline parameter insertId is not found in the pipeline job input definitions.\n<\/code><\/pre>\n<p>So I assume Pubsub is sending the log to the function which gets extraneous crap, including <code>insertId<\/code>.<\/p>\n<p>I can run the pipeline fine via Jupyter:<\/p>\n<pre><code>from google.cloud import pubsub\n\npublish_client = pubsub.PublisherClient()\ntopic = f'projects\/{PROJECT}\/topics\/{PUBSUB_TOPIC}'\ndata = {}\nmessage = json.dumps(data)\n\n_ = publish_client.publish(topic, message.encode())\n<\/code><\/pre>\n<p>So how do I the equivalent via Pubsub?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1661268443880,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-functions|google-cloud-pubsub|google-cloud-monitoring|google-cloud-vertex-ai",
        "Question_view_count":60,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73461369",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69878915,
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1636348555970,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":1271,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1636404890010,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636405249843,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68456262,
        "Question_title":"GCP - Vertex AI Model - Access GCS failed",
        "Question_body":"<p>We have a Vertex AI model that was created using a custom image.\nWe are trying to access a bucket on startup but we are getting the following error:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/...?projection=noAcl&amp;prettyPrint=false: {service account name} does not have storage.buckets.get access to the Google Cloud Storage bucket.\n<\/code><\/pre>\n<p>The problem is that I can't find the service account that is mentioned in the error to give it the right access permissions..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626789857557,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":291,
        "Owner_creation_time":1466977784157,
        "Owner_last_access_time":1664005111393,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68456262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73027674,
        "Question_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Question_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658172774733,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":114,
        "Owner_creation_time":1455270976143,
        "Owner_last_access_time":1663869676733,
        "Owner_location":"Grand Rapids, MI, USA",
        "Owner_reputation":1269,
        "Owner_up_votes":134,
        "Owner_down_votes":0,
        "Owner_views":261,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658194851973,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73027674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68569313,
        "Question_title":"How can I run Google Cloud's \"AI Notebooks\" on a schedule automatically?",
        "Question_body":"<p>Notebooks in the Google Cloud Platform has been great for Python development in the cloud, but the last missing piece is just running existing notebooks on a schedule. There's a million different tools (Airflow, Papermill, Google Cloud Jobs, Google Cloud Scheduler, Google Cloud Cron Jobs), and as someone not as familiar with Cloud, it's really easy to get lost. Any suggestion? Thanks guys!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1627525524810,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1317,
        "Owner_creation_time":1523235961717,
        "Owner_last_access_time":1642750774450,
        "Owner_location":null,
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":1654245561700,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68569313",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69703127,
        "Question_title":"How to invoke custom prediction logic in Vertex AI?",
        "Question_body":"<p>Goal: serve prediction requests from a Vertex AI Endpoint by executing custom prediction logic.<\/p>\n<p>Detailed steps:\nFor example, we may already uploaded have an image_quality.pb model (developed in a non-vertex-ai pythonic environment) in a GCS bucket<\/p>\n<p>Next, we want to create a custom image inference logic by deserializing the deployed  model and serving the inference functionality in a vertex AI endpoint<\/p>\n<p>Finally, we want to pass a list of images (stored in another GCS bucket) to that endpoint.<\/p>\n<p>We also want to see the logs and metrics in tensorboard.<\/p>\n<p>Existing Vertex AI code samples provide examples for invoking model.batch_predict \/ endpoint. predict, but don't mention how to execute custom prediction code.<\/p>\n<p>It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635138837680,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":133,
        "Owner_creation_time":1423356129317,
        "Owner_last_access_time":1638159935947,
        "Owner_location":null,
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Question_last_edit_time":1635227801977,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703127",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68370906,
        "Question_title":"Google Cloud Platform Dataset can't show image in VertexAI",
        "Question_body":"<p>Thanks All:\nWhen I use GCP-VertexAI and upload image from my computer. Next step to Browse page, all images always are loading. I can't any picture in Browse, and labing. But we can find upload image in Cloud Storage. How should I do for check my images in Browse?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4gFz.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626225376420,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_time":1626224549820,
        "Owner_last_access_time":1637572769890,
        "Owner_location":"Taipei, \u53f0\u7063",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68370906",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73251212,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659712728223,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73817659,
        "Question_title":"How to login as human labeler on GCP Vertex AI",
        "Question_body":"<p>I set up a Labeling Task in Vertex-AI, and assigned a team.\nThe manager of that team received an email to manage the <a href=\"https:\/\/datacompute.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/datacompute.google.com\/<\/a> console.\nNone of the human labelers received such an email.\nWhat do they have to do to start labeling? Is there a console for them?<\/p>\n<p>Any advice would be amazing!\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663862524327,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":8,
        "Owner_creation_time":1663862345683,
        "Owner_last_access_time":1663864771053,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73817659",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70137519,
        "Question_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Question_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638037173043,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":285,
        "Owner_creation_time":1285776739110,
        "Owner_last_access_time":1663774650203,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":8508,
        "Owner_up_votes":347,
        "Owner_down_votes":4,
        "Owner_views":144,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1638931046247,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70754016,
        "Question_title":"Creating a Vertex AI Workbench with a Non Organization Account and problems with constraints\/compute.vmExternalIpAccess",
        "Question_body":"<p>I'm trying to create a Vertex AI Workbench on GCP, but every time I try I get the following error:<\/p>\n<p><em>&lt;Workbench Name&gt; Constraint constraints\/compute.vmExternalIpAccess violated for project &lt;Project ID&gt;. Add instance &lt;Workbench ID&gt; to the constraint to use external IP with it.<\/em><\/p>\n<p>I went to the Organization Policies page to edit the constraint: <em>constraints\/compute.vmExternalIpAccess<\/em> and saw that is denied for all (which is odd because in the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints\" rel=\"nofollow noreferrer\">constraints documentation<\/a> it says that is should be enabled for all by default). Now, the problem is that when I go to edit the constraint, it says that it requires this set of permissions:<\/p>\n<ul>\n<li><em>orgpolicy.policies.create<\/em><\/li>\n<li><em>orgpolicy.policies.delete<\/em><\/li>\n<li><em>orgpolicy.policies.update<\/em><\/li>\n<li><em>orgpolicy.policy.get<\/em><\/li>\n<\/ul>\n<p>which are all part of the role: <em>roles\/orgpolicy.policyAdmin<\/em> that can only be granted at an organization level, and well, I have a Non Organization Account.<\/p>\n<p>Am I missing something?<\/p>\n<p>Thank for your time!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1642501704547,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|workbench|google-cloud-vertex-ai",
        "Question_view_count":333,
        "Owner_creation_time":1353379904267,
        "Owner_last_access_time":1653505313253,
        "Owner_location":null,
        "Owner_reputation":215,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70754016",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70623713,
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641570150640,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641588471210,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69566674,
        "Question_title":"Vertex AI endpoints timing out",
        "Question_body":"<p>I am using vertex-ai endpoints to serve a deep learning service.<\/p>\n<p>My service takes approximately 30s - 2 minutes to respond on CPU depending on the size of the input. I noticed that when the input size takes more than one minute to respond, the API fails, giving me this error:<\/p>\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n  &lt;meta charset=utf-8&gt;\n  &lt;meta name=viewport content=&quot;initial-scale=1, minimum-scale=1, width=device-width&quot;&gt;\n  &lt;title&gt;Error 502 (Server Error)!!1&lt;\/title&gt;\n  &lt;style&gt;\n    *{margin:0;padding:0}html,code{font:15px\/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* &gt; body{background:url(\/\/www.google.com\/images\/errors\/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/1x\/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat 0% 0%\/100% 100%;-moz-border-image:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  &lt;\/style&gt;\n  &lt;a href=\/\/www.google.com\/&gt;&lt;span id=logo aria-label=Google&gt;&lt;\/span&gt;&lt;\/a&gt;\n  &lt;p&gt;&lt;b&gt;502.&lt;\/b&gt; &lt;ins&gt;That\u2019s an error.&lt;\/ins&gt;\n  &lt;p&gt;The server encountered a temporary error and could not complete your request.&lt;p&gt;Please try again in 30 seconds.  &lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>\n<p>When I retry, I keep getting the same error. Once I decrease the input size, the API starts working again. For these reasons, I believe this is a timeout issue.<\/p>\n<p>So my question is: how can I change the timeout value in vertex-ai endpoints? I read through all the documentation, and it doesn't seem to be mentioned anywhere.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1634196565023,
        "Question_score":1,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_time":1407150503730,
        "Owner_last_access_time":1663843156893,
        "Owner_location":null,
        "Owner_reputation":387,
        "Owner_up_votes":37,
        "Owner_down_votes":2,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69566674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70657497,
        "Question_title":"GCP's Vertex AI - Restarting notebook <NAME>: <ZONE> does not have enough resources available to fulfill the request",
        "Question_body":"<p>I'm having issues starting \/ creating new user-managed notebooks in Vertex AI &gt; Workbench, where I'll end up with this error and can't even open JupyterLabs:<\/p>\n<blockquote>\n<p>[Vertex AI] - Restarting notebook NAME: ZONE does not have enough\nresources available to fulfill the request. Retry later or try another zone in your configurations.<\/p>\n<\/blockquote>\n<p>I didn't have this problem last month when I was still under the free trial. I also noticed this error only come up when I try to install the GPU in the notebook.<\/p>\n<p>So far, I've done the following:<\/p>\n<ul>\n<li>Change payment method (got an email that asked me to update payment settings at the end of the trial last month)<\/li>\n<li>Create new notebook in different zones and cluster over the past week, all give the same error when I attach a GPU.<\/li>\n<li>In Quotas, changed 'GPU (all regions)' from 1 to 0, and then back to 1.<\/li>\n<li>In Quotas, changed 'VM instances (default)' from 24 to 48, and then back to 24.<\/li>\n<li>Created several new projects, didn't work either.<\/li>\n<\/ul>\n<p>I also created a snapshot of both the data and boot of the notebook, but don't know how to use it to recreate it in Vertex AI again.<\/p>\n<p>Have you faced a similar issue before, how did you fix it? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1641840588547,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":480,
        "Owner_creation_time":1624939042350,
        "Owner_last_access_time":1654691815403,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1641991292413,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70657497",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73279871,
        "Question_title":"Proper Format of Vertex AI AutoML Action Recognition Data Labels",
        "Question_body":"<p>I'm trying to build an action recognition model in Vertex AI AutoML. I've studied the documentation thoroughly, but so far my model is not able to make any decent predictions in the wild. I've made three attempts so far, and my most recent attempt had a precision-recall curve that could be described as 'respectable', but the predictions are really awful. I'll try to explain my process below as best as I can.<\/p>\n<h2>The Raw Data<\/h2>\n<p>I recorded the same action in 34 ~3 min videos, with the number of actions in each video varying between 30 and 100. The actions themselves take &lt; 1 second. I recorded the data from 4 cameras at multiple angles, and because I was moving around a lot, there was plenty of variance in each action performed. While each raw video contains only one action, there are a total of six classes of action we hope to identify.<\/p>\n<h2>First Model Attempt<\/h2>\n<p>According to the Vertex AI documentation, it's expecting time segments for the actions The annotation JSONL\/CSV documentation says as much, but somewhere else in the documentation it says it's expecting the maximal point at which the action is performed if you wish to label the videos inside the console. Anyways, I created a labeling job and my team and I labeled all the time segments for the actions in the videos. The precision-recall curve alluded to some kind of data leakage, and when we inspected the batch prediction results we discovered that it appeared that the model was training on the 0th frame of the time segment. We were careful not to include any frames that weren't part of the action, but due to the nature of the actions, they all essentially start and end at a 'neutral' spot. At seemingly random intervals in the video, multiple or all actions would be labeled, but ONLY in those spots.<\/p>\n<h2>Second Attempt<\/h2>\n<p>We took the annotation data that we had built with the labeling job and chopped up the original videos into a series of subclips. We had all the labels and the time segments, so we did this with a simple script. We did not remove any of the frames of the video, so the neutral position in the beginning and end frames were still present. The precision-recall curve again looked suspicious, but slightly better. Inference in the wild yielded the same results.<\/p>\n<h2>Third Attempt<\/h2>\n<p>After further reviewing the documentation, Vertex AI appeared to contradict itself in what it expects in the data labels:<\/p>\n<blockquote>\n<p>When the action starts appearing that you want to identify, slowly\nprogress through till you find the center or the most representative\nmoment of the action using &quot;Next frame&quot; option.<\/p>\n<\/blockquote>\n<p>To avoid spending a ton of time on another labeling task (takes us about three days), we labeled a subset of the original subclip dataset according to this information and trained a model to analyze the precision-recall curve. FINALLY something much more respectable. However, the inferences in the wild were still terrible, suffering from the same.<\/p>\n<p>My question is: <strong>do I need to annotate negative action sequences?<\/strong> In the object tracking or object detection documentation it says that adding a <code>None_of_the_above<\/code> label would help the model to identify that which it doesn't need to focus on. And again in the action recognition documentation it points out a limitation in the labeling console:<\/p>\n<blockquote>\n<p>Limitation: There's a limitation when using the VAR labeling console,\nwhich means if you want to use the labeling tool to label actions, you\nmust label all the actions in that video.<\/p>\n<\/blockquote>\n<p>I can write a script to fill in the dead space in the video as a negative action sequence, but I'd like to know what the best practice is before going down that route and spending the money to train yet another terrible model.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1659970389697,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|automl|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_time":1360536048187,
        "Owner_last_access_time":1663951121943,
        "Owner_location":"Miami, FL, USA",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279871",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68614794,
        "Question_title":"How to create MLOps vertex ai pipeline with custom sklearn code?",
        "Question_body":"<p>I'm trying to build MLOps pipeline using vertex ai but failing to deploy it<\/p>\n<pre><code>@dsl.pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=&quot;pipeline-test-1&quot;,\n)\ndef pipeline(\nserving_container_image_uri: str = &quot;us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest&quot;\n):\n    dataset_op = get_data()\n    train_op = train_xgb_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    train_knn = knn_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    \n    eval_op = eval_model(\n        test_set=dataset_op.outputs[&quot;dataset_test&quot;],\n        xgb_model=train_op.outputs[&quot;model_artifact&quot;],\n        knn_model=train_knn.outputs['best_model_artifact']\n    )\n    \n    endpoint_op = gcc_aip.ModelDeployOp(\n    project=PROJECT_ID,\n    model=eval_op.outputs[&quot;model_artifacts&quot;],\n    machine_type=&quot;n1-standard-4&quot;,\n    )\n    \n    #endpoint_op.after(eval_op)\n    \ncompiler.Compiler().compile(pipeline_func=pipeline,\n        package_path='xgb_pipe.json')\n<\/code><\/pre>\n<p>gcc_aip.ModelDeployOp is throwing error that correct model id or name should be pass<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1627856634787,
        "Question_score":1,
        "Question_tags":"mlops|google-cloud-vertex-ai",
        "Question_view_count":154,
        "Owner_creation_time":1448438738667,
        "Owner_last_access_time":1655296770893,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68614794",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73141754,
        "Question_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Question_body":"<p>I successfully trained and deployed a Tensorflow Recommender model on Vertex AI.<\/p>\n<p>Everything is online and to predict the output. In the notebook I do:<\/p>\n<pre><code>loaded = tf.saved_model.load(path)\nscores, titles = loaded([&quot;doctor&quot;])\n<\/code><\/pre>\n<p>That returns:<\/p>\n<pre><code>Recommendations: [b'Nelly &amp; Monsieur Arnaud (1995)'\n b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\n<\/code><\/pre>\n<p>That is, the payload (input for the neural network) must be <code>[&quot;doctor&quot;]<\/code><\/p>\n<p>Then I generate the JSON for payload (the error is here):<\/p>\n<pre><code>!echo {&quot;\\&quot;&quot;instances&quot;\\&quot;&quot; : [{&quot;\\&quot;&quot;input_1&quot;\\&quot;&quot; : {[&quot;\\&quot;&quot;doctor&quot;\\&quot;&quot;]}}]} &gt; instances0.json\n<\/code><\/pre>\n<p>And submit to the endpoint:<\/p>\n<pre><code>!curl -X POST  \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\n-d @instances0.json &gt; results.json\n<\/code><\/pre>\n<p>... as seen here: <a href=\"https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd<\/a><\/p>\n<p>However, when I use this payload, I get error 400:<\/p>\n<pre><code>code: 400\nmessage: &quot;Invalid JSON payload received. Expected an object key or }. s&quot; : [{&quot;input_1&quot; : {[&quot;doctor&quot;]}}]} ^&quot;\nstatus: &quot;INVALID_ARGUMENT&quot;\n<\/code><\/pre>\n<p>This below don't work either:<\/p>\n<pre><code>!echo {&quot;inputs&quot;: {&quot;input_1&quot;: [&quot;doctor&quot;]}} &gt; instances0.json\n<\/code><\/pre>\n<p>Even with validated JSON Lint, it does not return the proper prediction.<\/p>\n<p>In another Stackoverflow question is suggested to remove the &quot; \\ &quot; in the payload, but this didn't work either.<\/p>\n<p>Running:<\/p>\n<pre><code>!saved_model_cli show --dir \/home\/jupyter\/model --all\n<\/code><\/pre>\n<p>I get:<\/p>\n<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:0\n    outputs['output_2'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:1\n  Method name is: tensorflow\/serving\/predict\n\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n<\/code><\/pre>\n<p>The point is: I'm passing an array and I'm not sure if it must be in b64 format.<\/p>\n<p>This Python code works, but returns a different result than expected:<\/p>\n<pre><code>import tensorflow as tf\nimport base64\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nimport numpy as np\nfrom google.cloud import aiplatform\nimport os\nvertex_model = tf.saved_model.load(&quot;gs:\/\/bucket\/model&quot;)\n\nserving_input = list(\n    vertex_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()\n)[0]\n\nprint(&quot;Serving input :&quot;, serving_input)\n\naip_endpoint_name = (\n    f&quot;projects\/my-project\/locations\/us-west1\/endpoints\/12345567&quot;\n)\nendpoint = aiplatform.Endpoint(aip_endpoint_name)\n\ndef encode_input(input):\n    return base64.b64encode(np.array(input)).decode(&quot;utf-8&quot;)\n\ninstances_list = [{serving_input: {&quot;b64&quot;: encode_input(np.array([&quot;doctor&quot;]))}}]\ninstances = [json_format.ParseDict(s, Value()) for s in instances_list]\n\nresults = endpoint.predict(instances=instances)\nprint(results.predictions[0][&quot;output_2&quot;])\n\n\n['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\n<\/code><\/pre>\n<p>Any ideas on how to fix \/ encode the payload ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658940444647,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":105,
        "Owner_creation_time":1475181309097,
        "Owner_last_access_time":1664074238173,
        "Owner_location":"Brazil",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Question_last_edit_time":1659415987720,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73141754",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70435505,
        "Question_title":"Vertex AI AutoML getting data about Model, Dataset, Training Job",
        "Question_body":"<p>I am using Vertex AI for AutoML Video classification and I would like to get some data that I'm seeing in Web UI (Cloud Console) (Model\/Dataset detail).\nI'm using AI platform Python SDK or REST API.<\/p>\n<p>For example Model API returns 'training videos' but not test videos (web Model detail, tab EVALUATE)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" alt=\"Vertex AI Model evaluation\" \/><\/a><\/p>\n<p>then for example in tab Model Properties on the web I can't obtain Training time, Total items, Algorithm, Objective, Total Items<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" alt=\"Vertex AI Model properties\" \/><\/a><\/p>\n<p>For Dataset detail, I would like to get number of labeled\/unlabeled videos, labels and correspoding number<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" alt=\"Dataset detail, labels\" \/><\/a><\/p>\n<p>This is code I'm using to get the data (as component in Vertex AI Pipeline):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_metadata(project_id, region, model_id):\n    import requests\n\n    import google.auth\n    import google.cloud.aiplatform as aip\n    from google.cloud import aiplatform_v1\n    from google.protobuf import json_format\n    from google.auth.transport import requests as grequests\n\n    aip.init(project=project_id, location=region)\n    API_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(region)\n    model = aip.Model(model_id)\n    model_dict = model.to_dict()\n    model_metadata = model_dict['metadata']\n\n    model_name = model_dict['displayName']\n    model_creation_date = model_dict['createTime']\n    model_type = model_metadata['modelType']\n    number_training = model_metadata['trainingDataItemsCount']\n\n    client_options = {\n        &quot;api_endpoint&quot;: API_ENDPOINT\n    }\n    model_path = model.resource_name\n    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)\n    list_eval_request = aiplatform_v1.types.ListModelEvaluationsRequest(parent=model_path)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n\n    eval_name = ''\n    for val in list_eval:\n        eval_name = val.name\n    get_eval_request = aiplatform_v1.types.GetModelEvaluationRequest(name=eval_name)\n    model_eval = client_model.get_model_evaluation(request=get_eval_request)\n    model_eval_data = json_format.MessageToDict(model_eval._pb)\n\n    model_metrics = model_eval_data['metrics']\n    average_precision = model_metrics.get('auPrc')\n    confidence_metrics = model_metrics['confidenceMetrics']\n    confidence_threshold = -1\n    f1_score = -1\n    precision = -1\n    recall = -1\n\n    for item in confidence_metrics:\n        confidence_threshold_temp = item['confidenceThreshold']\n        if confidence_threshold_temp &gt;= 0.5:\n            confidence_threshold = confidence_threshold_temp\n            f1_score = item['f1Score']\n            precision = item['precision']\n            recall = item['recall']\n            break\n    # auc_precision = precision\n    # auc_recall = recall\n\n    credentials, _ = google.auth.default()\n    r = grequests.Request()\n    credentials.refresh(r)\n    training_pipeline_resource_name = model_dict['trainingPipeline']\n\n    training_pipeline_url = f'https:\/\/{API_ENDPOINT}\/v1beta1\/{training_pipeline_resource_name}'\n    headers = {\n        'Authorization': f'Bearer {credentials.token}'\n    }\n    r = requests.get(training_pipeline_url, headers=headers)\n    training_pipeline_detail = r.json()\n    input_data_config = training_pipeline_detail.get('inputDataConfig', {})\n    dataset_id = input_data_config.get('datasetId', '')\n    fraction_split = input_data_config.get('fractionSplit', {})\n    test_fraction = fraction_split.get('testFraction')\n    training_fraction = fraction_split.get('trainingFraction')\n    data_split = f'{training_fraction}\/{test_fraction}'\n\n    dataset = aip.VideoDataset(dataset_id)\n    dataset_resource = json_format.MessageToDict(dataset.gca_resource._pb)\n    dataset_name = dataset_resource.get('displayName')\n    dataset_creation_date = dataset_resource.get('createTime')\n    labels = dataset_resource['labels']\n    dataset_type = labels.get('aiplatform.googleapis.com\/dataset_metadata_schema')\n\n    data = {\n        'model_id': model_id,\n        'model_name': model_name,\n        'model_creation_date': model_creation_date,\n        'model_type': model_type,\n        'number_training': number_training,\n        'average_precision': average_precision,\n        'precision': precision,\n        'recall': recall,\n        'data_split': data_split,\n        'dataset_name': dataset_name,\n        'dataset_type': dataset_type,\n        'dataset_id': dataset_id,\n        'dataset_creation_date': dataset_creation_date,        \n    }\n\n<\/code><\/pre>\n<p>Also for example what I found is that on training job when I created dataset, training model via WebUI I can obtain data split (training\/testing ratio), but when I'm doing this in Vertex AI Pipelines, I'm not explicitly setting data split for AutoMLVideoTrainingJobRunOp, I can't get data split from Training job detail, so it seems that it saved only when it's explicitly set.<\/p>\n<p>Other thing I noticed is when API requests are made for Cloud Console (inspecting Chrome Dev Tools) it returns more (richer) data (items) then for public Vertex AI APIs.<\/p>\n<p>I'm not sure if this is temporary or intentional\/permanent behaviour.<\/p>\n<p>I would appreciate thoughts\/comments\/help with this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1640089790567,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai|google-cloud-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":395,
        "Owner_creation_time":1372196724860,
        "Owner_last_access_time":1664040926440,
        "Owner_location":"Prague, Czech Republic",
        "Owner_reputation":276,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70435505",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72809603,
        "Question_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Question_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1656554846450,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":122,
        "Owner_creation_time":1614739500313,
        "Owner_last_access_time":1664046580507,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1656987570560,
        "Answer_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657366819400,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72809603",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70003299,
        "Question_title":"Vertex AI returns a different result from the local tflite model",
        "Question_body":"<p>I uploaded my tflite model on Vertex AI and made an endpoint, and I requested inference with some input value, but it returns a different result from my local tflite model's inference result.<\/p>\n<p>The input value is float32 array(actually sampled audio data) and I used <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this code<\/a> for the request. Although there was the same input array, the local tflite model and the model which is uploaded on Vertex AI returns quite big different result.<\/p>\n<p>Is there any possibility of distortion on the value while it transfers to the Vertex AI instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637147367180,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":175,
        "Owner_creation_time":1630024461567,
        "Owner_last_access_time":1660186387690,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1637188171083,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70003299",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70175440,
        "Question_title":"Google Cloud Platform - Vertex AI - is there a way to look at a chart of training performance over time?",
        "Question_body":"<p>I'd like to know how the training performance changes over the course of the training. Is there any way to access that via Vertex AI automl service?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638302912743,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":249,
        "Owner_creation_time":1392762853817,
        "Owner_last_access_time":1663862388447,
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70175440",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71979012,
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":1650713970783,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651402011487,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1651402328353,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70740897,
        "Question_title":"Specifying Machine Type in Vertex AI Pipeline",
        "Question_body":"<p>I have a pipeline component defined like this:<\/p>\n<pre><code>data_task = run_ssd_data_op(\n        labels_path=input_labels,\n        data_config=config_task.outputs[&quot;output_data_config&quot;],\n        training_config=config_task.outputs[&quot;output_training_config&quot;],\n        assets_json=dump_conversion_task.outputs[&quot;output_ssd_query&quot;]\n    )\ndata_task.execution_options.caching_strategy.max_cache_staleness = &quot;P0D&quot;\ndata_task.container.add_resource_request('cpu', cpu_request)\ndata_task.container.add_resource_request('memory', memory_request)\n<\/code><\/pre>\n<p>When I run the pipeline on VertexAI the above component runs on an E2 machine type which matches the CPU and RAM requirements.<\/p>\n<p>However, the component runs much more slowly on VertexAI than on the Kubeflow pipeline I setup using AIPlatform. I configured that cluster to use N1-highmem-32 machines for this job.<\/p>\n<p>I would like to request that this component is run on an <code>n1-highmem-32<\/code> machine, how can I do that?<\/p>\n<p>For the GPU component of the pipeline I could use the line:<\/p>\n<pre><code>training_task.add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'NVIDIA_TESLA_T4').set_gpu_limit(\n        gpu_request)\n<\/code><\/pre>\n<p>What is the equivalent <code>node_selector_constraint<\/code> that I need to apply to my <code>data_task<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1642420252480,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":603,
        "Owner_creation_time":1350304341050,
        "Owner_last_access_time":1663674484983,
        "Owner_location":"Morecambe, United Kingdom",
        "Owner_reputation":3827,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":573,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70740897",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72930772,
        "Question_title":"Using GCP's Vertex AI Image Classification exported (TF SavedModel) model for prediction",
        "Question_body":"<p>I've trained an Image Classification model via Google Cloud Platform's Vertex AI framework and liked the results. Due to that I then proceeded to export it in Tensorflow SavedModel format (shows up as 'Container' export) for custom prediction because I don't like neither the slowness of Vertex's batch prediction nor the high cost of using a Vertex endpoint.<\/p>\n<p>In my python code I used<\/p>\n<pre><code>model = tensorflow.saved_model.load(model_path)\ninfer =  model.signatures[&quot;serving_default&quot;]\n<\/code><\/pre>\n<p>When I tried to inspect what <code>infer<\/code> requires I saw that its input is two parameters: <code>image_bytes<\/code> and <code>key<\/code>. Both are string-type tensors.<\/p>\n<p>This question can be broken off into several sub-questions that then make a whole:<\/p>\n<ol>\n<li>Isn't inference done on multiple data instances? If so, why is it image_bytes and not images_bytes?<\/li>\n<li>Is image_bytes just the output of <code>open(&quot;img.jpg&quot;, &quot;rb&quot;).read()<\/code>? If so, don't I have to resize it first? To what size? How do I check that?<\/li>\n<li>What is key? I have absolutely no clue or guess regarding this one's meaning.<\/li>\n<\/ol>\n<p>The documentation for GCP is paid only and so I have decided to ask for help here. I tried to search for an answer on google for multiple days but found no relevant article.\nThank you for reading and your help would be greatly appreciated and maybe even useful to future readers.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657476634237,
        "Question_score":0,
        "Question_tags":"python|tensorflow|google-cloud-platform|image-classification|google-cloud-vertex-ai",
        "Question_view_count":118,
        "Owner_creation_time":1442687709540,
        "Owner_last_access_time":1663092654683,
        "Owner_location":null,
        "Owner_reputation":79,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1659779993553,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72930772",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72827960,
        "Question_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Question_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656671602857,
        "Question_score":0,
        "Question_tags":"ruby|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1656670919183,
        "Owner_last_access_time":1659105111753,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656947266917,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73345417,
        "Question_title":"Vertex AI Instance Out of Disk Space",
        "Question_body":"<p>I accidentally ran out of disk space on my Vertex AI instance. There's no way to connect to it by any means now. Using the jupyter failed as there's not enough disk space:\n<code>OSError: [Errno 28] No space left on device<\/code><\/p>\n<p>I tried to increase disk space for both boot and data disks using <code>gcloud compute disks resize<\/code>, but it still doesn't work despite disk space being shown as increased in the machine info panel.<\/p>\n<p>Also tried connecting through ssh but got timeouts. My guess is that it's still caused by disk space.<\/p>\n<p>So is there any ways to recover the instance without a hard reset?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660403531073,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":71,
        "Owner_creation_time":1660402958963,
        "Owner_last_access_time":1663710025030,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660403595093,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73345417",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71480531,
        "Question_title":"Hugging face code throwing error when running using vertex-ai in gcp",
        "Question_body":"<p>I am training a NLP Hugging face model in vertex-ai with custom image.<\/p>\n<p>The same code works in local machine.<\/p>\n<p>Here is my code and the error.<\/p>\n<pre><code>import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport transformers as tr\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM\nfrom transformers import DataCollatorForLanguageModeling\nfrom scipy.special import softmax\nimport scipy\nimport random\nimport pickle\nimport os\n\nprint(&quot;package imported completed&quot;)\n\nos.environ['TRANSFORMERS_OFFLINE']='1'\nos.environ['HF_MLFLOW_LOG_ARTIFACTS']='TRUE'\n\nprint(&quot;env setup completed&quot;)\nprint( tr.__version__)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(&quot;Using&quot;, device)\ntorch.backends.cudnn.deterministic = True  \n\ntr.trainer_utils.set_seed(0)\n\nprint(&quot;here&quot;)\n\ntokenizer = tr.XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-large&quot;,local_files_only=True)\nmodel = tr.XLMRobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-large&quot;, return_dict=True,local_files_only=True)\n\nmodel.to(device)\nprint(&quot;Model loaded successfully&quot;)\n\ndf=pd.read_csv(&quot;gs:\/\/****bucket***\/data.csv&quot;) \nprint(&quot;read csv&quot;)\n# ,engine='openpyxl',sheet_name=&quot;master_data&quot;\ntrain_df=df.text.tolist()\nprint(len(train_df))\n\ntrain_df=list(set(train_df))\ntrain_df = [x for x in train_df if str(x) != 'nan']\ntrain_df=train_df[:50]\n\nprint(&quot;Length of training data is \\n &quot;,len(train_df))\nprint(&quot;DATA LOADED successfully&quot;)\n\n\ntrain_encodings = tokenizer(train_df, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nprint(&quot;encoding done&quot;)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\nprint(&quot;data collector done&quot;)\n\nclass SEDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings[&quot;attention_mask&quot;])\n\ntrain_data = SEDataset(train_encodings)\n\nprint(&quot;train data created&quot;)\n\ntraining_args = tr.TrainingArguments(\n    output_dir='gs:\/\/****bucket***\/results_mlm_exp1', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n#     ,save_strategy=&quot;epoch&quot;\n#     ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n#     logging_dir='gs:\/\/****bucket***\/logs_mlm_exp1',            # directory for storing logs\n#     logging_steps=32000,\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n)\nprint(&quot;training to start&quot;)\ntrainer.train()\nprint(&quot;model training finished&quot;)\ntrainer.save_model(&quot;gs:\/\/****bucket***\/model_mlm_exp1&quot;)\n\nprint(&quot;training finished&quot;)\n<\/code><\/pre>\n<p>The error that I get is:<\/p>\n<pre><code>None    INFO    train data created\nNone    INFO    training to start\nNone    ERROR   0%| | 0\/8 [00:00&lt;?, ?it\/s]train.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nNone    ERROR     item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\nNone    ERROR   \/opt\/conda\/lib\/python3.7\/site-packages\/torch\/nn\/parallel\/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\nNone    ERROR     warnings.warn('Was asked to gather along dimension 0, but all '\n\/var\/sitecustomize\/sitecustomize.py INFO    None\nNone    ERROR   0%| | 0\/8 [00:09&lt;?, ?it\/s]\n<\/code><\/pre>\n<p>Most of them are warning but still my code stops with error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647339579240,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":153,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71480531",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69904211,
        "Question_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Question_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1636487376953,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":894,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1636603578603,
        "Answer_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1636603298293,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1636679717047,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72014493,
        "Question_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Question_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1650978304193,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|spacy-3|google-cloud-vertex-ai|spacy-transformers",
        "Question_view_count":234,
        "Owner_creation_time":1629385138957,
        "Owner_last_access_time":1663953209400,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Question_last_edit_time":1651090705967,
        "Answer_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651093924243,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68795098,
        "Question_title":"Error when trying to use CustomPythonPackageTrainingJobRunOp in VertexAI pipeline",
        "Question_body":"<p>I am using the google cloud pipeline component CustomPythonPackageTrainingJobRunOp in a VertexAI pipeline . I have been able to run this package successfully as a CustomTrainingJob before. I can see multiple (11) error messages in the logs but the only one that seems to make sense to me is, &quot;ValueError: too many values to unpack (expected 2) &quot; but I am unable to figure out the solution. I can add all the other error messages too if required. I am logging some messages at the start of the training code so I know the errors happen before the training code is executed. I am completely stuck on this. Links to samples where someone has used CustomPythonPackageTrainingJobRunOp in a pipeline would very helpful as well. Below is the pipeline code that I am trying to execute:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name)\ndef pipeline(\n    project: str = &quot;adsfafs-321118&quot;,\n    location: str = &quot;us-central1&quot;,\n    display_name: str = &quot;vertex_pipeline&quot;,\n    python_package_gcs_uri: str = &quot;gs:\/\/vertex\/training\/training-package-3.0.tar.gz&quot;,\n    python_module_name: str = &quot;trainer.task&quot;,\n    container_uri: str = &quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n    staging_bucket: str = &quot;vertex_bucket&quot;,\n    base_output_dir: str = &quot;gs:\/\/vertex_artifacts\/custom_training\/&quot;\n):\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=python_module_name,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=package_path\n)\n\napi_client = AIPlatformClient(project_id=project_id, region=region)\n\nresponse = api_client.create_run_from_job_spec(\n    package_path,\n    pipeline_root=pipeline_root_path\n)\n\n<\/code><\/pre>\n<p>In the documentation for CustomPythonPackageTrainingJobRunOp, the type of the argument &quot;python_module&quot; seems to be &quot;google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob&quot; instead of string, which seems odd. However, I tried to re-define the pipeline, where I have replaced argument python_module in CustomPythonPackageTrainingJobRunOp with a CustomPythonPackageTrainingJob object instead of a string, as below but still getting the same error:<\/p>\n<pre><code>def pipeline(\n    project: str = &quot;...&quot;,\n    location: str = &quot;...&quot;,\n    display_name: str = &quot;...&quot;,\n    python_package_gcs_uri: str = &quot;...&quot;,\n    python_module_name: str = &quot;...&quot;,\n    container_uri: str = &quot;...&quot;,\n    staging_bucket: str = &quot;...&quot;,\n    base_output_dir: str = &quot;...&quot;,\n):\n\n    job = aiplatform.CustomPythonPackageTrainingJob(\n        display_name= display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module_name=python_module_name,\n        container_uri=container_uri,\n        staging_bucket=staging_bucket\n    )\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=job,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>Added the args that I was passing and had forgotten to add here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629058946030,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1629151780473,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68795098",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73210389,
        "Question_title":"Vertex AI Executor gives NoSuchKernel",
        "Question_body":"<p>I have a simple hello-world ipynb file in a Vertex AI notebook instance that looks like this:<\/p>\n<pre><code>print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>When setting up an executor for this as shown below I receive the following error in the executor logs: <em>jupyter_client.kernelspec.NoSuchKernel: No such kernel named local-python3\nerror<\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The notebook has the following metadata<\/p>\n<pre><code>{\n    &quot;kernelspec&quot;: {\n        &quot;display_name&quot;: &quot;Python 3 (Local)&quot;,\n        &quot;language&quot;: &quot;python&quot;,\n        &quot;name&quot;: &quot;local-python3&quot;\n    },\n    &quot;language_info&quot;: {\n        &quot;codemirror_mode&quot;: {\n            &quot;name&quot;: &quot;ipython&quot;,\n            &quot;version&quot;: 3\n        },\n        &quot;file_extension&quot;: &quot;.py&quot;,\n        &quot;mimetype&quot;: &quot;text\/x-python&quot;,\n        &quot;name&quot;: &quot;python&quot;,\n        &quot;nbconvert_exporter&quot;: &quot;python&quot;,\n        &quot;pygments_lexer&quot;: &quot;ipython3&quot;,\n        &quot;version&quot;: &quot;3.7.12&quot;\n    }\n}\n<\/code><\/pre>\n<p>What would require to run this notebook successfully? I looked into the possibility of customer containers but that should be to much of a complex solution for such.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1659455503420,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_time":1587457219480,
        "Owner_last_access_time":1663764469243,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73210389",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73542706,
        "Question_title":"How to solve VertexAI prediction endpoint error?",
        "Question_body":"<p>I am trying to get predictions from an endpoint that is already created in VertexAI through UI.<\/p>\n<p>I am getting an error when I run the following code<\/p>\n<pre><code>from typing import Dict\nimport pandas as pd\nfrom google.cloud import aiplatform\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nfrom google.oauth2 import service_account\n\nkey_path = '..\/golden-tempest-xxxxx.json'\ncredentials = service_account.Credentials.from_service_account_file(key_path, scopes=[\n    &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;], )\n\n\naiplatform.init(\n    project='golden-tempest-xxxxx',\n    location='us-central1',\n    credentials=credentials,\n)\n\ndef predict_tabular_classification_sample(\n    project: str,\n    endpoint_id: str,\n    instance_dict: Dict,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n    instance = json_format.ParseDict(instance_dict, Value())\n    instances = [instance]\n    parameters_dict = {}\n    parameters = json_format.ParseDict(parameters_dict, Value())\n    endpoint = client.endpoint_path(\n        project=project, location=location, endpoint=endpoint_id\n    )\n    response = client.predict(\n        endpoint=endpoint, instances=instances, parameters=parameters\n    )\n    print(&quot;response&quot;)\n    print(&quot; deployed_model_id:&quot;, response.deployed_model_id)\n    # See gs:\/\/google-cloud-aiplatform\/schema\/predict\/prediction\/tabular_classification_1.0.0.yaml for the format of the predictions.\n    predictions = response.predictions\n    for prediction in predictions:\n        print(&quot; prediction:&quot;, dict(prediction))\n\n\ndf = pd.read_csv('..\/btc_test_classification_2.csv')\n\ndf_dict = df.to_dict('index')\n\npredict_tabular_classification_sample(\n    project=&quot;xxxxx&quot;,\n    endpoint_id=&quot;886215168080478208&quot;,\n    location=&quot;us-central1&quot;,\n    instance_dict={'instances': [df_dict[4]]}\n)\n<\/code><\/pre>\n<p><strong>Error<\/strong>:<\/p>\n<pre><code>InvalidArgument: 400 {&quot;error&quot;: &quot;Column prefix: . Error: Missing struct property: tick_count.&quot;}\n<\/code><\/pre>\n<p>My training data contains the same columns I have in test. <a href=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" alt=\"columns\" \/><\/a><\/p>\n<p>I am not sure why I am getting this error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1661863594927,
        "Question_score":0,
        "Question_tags":"google-api|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1426332042323,
        "Owner_last_access_time":1663922259243,
        "Owner_location":null,
        "Owner_reputation":1026,
        "Owner_up_votes":236,
        "Owner_down_votes":1,
        "Owner_views":256,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73542706",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70159582,
        "Question_title":"Google Cloud Platform - Vertex AI Image classification training fails with no specific error message",
        "Question_body":"<p>I'm doing an image classification task using Vertex AI and after about 3 hours of training it fails. The error message is nondescript. It says &quot;Training pipeline failed with error message: Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.&quot;<\/p>\n<p>It's done that for three of my models using the same image dataset (about 45k large). What could be the error here? How can I find out?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1638210471447,
        "Question_score":0,
        "Question_tags":"machine-learning|google-cloud-platform|computer-vision|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_time":1392762853817,
        "Owner_last_access_time":1663862388447,
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Question_last_edit_time":1638223011490,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70159582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68532457,
        "Question_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Question_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627312838200,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":1627313437400,
        "Answer_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1627331282973,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73816577,
        "Question_title":"How do I ingest float from BigQuery into featurestore?",
        "Question_body":"<p>I have a <code>float<\/code> type field in BQ. I try to ingest this into a <code>double<\/code> type field in featurestore but get:<\/p>\n<pre><code>Source schema does not match the expected schema for this import. \nMissing fields in source: feature_field. Type mismatches in source: \nExpected type and mode [STRING, NULLABLE] for BQ_field, but got [FLOAT, ].\n<\/code><\/pre>\n<p>How can I import it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663857615367,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":7,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73816577",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73362430,
        "Question_title":"Vertex AI executor with .env file",
        "Question_body":"<p>Normally when using environment variables in Docker you would specify them with ENV command in the Dockerfile or give the .env file with the --env-file option in a docker run command.<\/p>\n<p>When creating a Vertex AI notebook executor this latter option is not available while the first option requires maintenance of the Dockerfile when an env variable changes.<br \/>\nWe can specify the env variables as parameters in a .yaml when creating the executor, but that requires again an extra step editing in the process when a env variable changes.<\/p>\n<p>How can we pass the content of the .env file to the container at runtime with no extra effort in Vertex AI?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1660573840647,
        "Question_score":1,
        "Question_tags":"docker|environment-variables|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_time":1587457219480,
        "Owner_last_access_time":1663764469243,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73362430",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69203143,
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1631772554497,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_time":1449207605093,
        "Owner_last_access_time":1663813482530,
        "Owner_location":"Manila, NCR, Philippines",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632103988633,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68280940,
        "Question_title":"vertex ai: ResourceExhausted 429 received trailing metadata size exceeds limit",
        "Question_body":"<p>I am using google vertex AI online prediction:\nIn order to send an image it has to be in a JSON file in unit8 format which has to be less than 1.5 MB, when converting my image to uint8 it definitely exceeds 1.5MB.<\/p>\n<p>To go around this issue we can encode the unit8 file to b64, that makes the JSON file in KBs\nwhen running the prediction I get <code>Resource Exhausted: 429 received trailing metadata size exceeds limit<\/code>  Is there anyone who knows what's the problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1625638603360,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-ai-platform|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":875,
        "Owner_creation_time":1551350379477,
        "Owner_last_access_time":1663847801140,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1625646472767,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68280940",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69977440,
        "Question_title":"How to use kfp Artifact with sklearn?",
        "Question_body":"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:<\/p>\n<ol>\n<li>read data from a big query table<\/li>\n<li>create a pandas <code>DataFrame<\/code><\/li>\n<li>use the <code>DataFrame<\/code> to train a K-Means model<\/li>\n<li>deploy the model to an endpoint<\/li>\n<\/ol>\n<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]<\/code> as output because <code>pd.DataFrame<\/code> type that I found <a href=\"https:\/\/stackoverflow.com\/questions\/43890844\/pythonic-type-hints-with-pandas\">here<\/a> did not work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    df: Output[Artifact],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    df = client.list_rows(table).to_dataframe()\n<\/code><\/pre>\n<p>Here the code of the step 3:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])\ndef kmeans_training(\n        dataset: Input[Artifact],\n        model: Output[Model],\n        num_clusters: int,\n):\n    from sklearn.cluster import KMeans\n    model = KMeans(num_clusters, random_state=220417)\n    model.fit(dataset)\n<\/code><\/pre>\n<p>The run of the pipeline is stopped due to the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: float() argument must be a string or a number, not 'Artifact'\n<\/code><\/pre>\n<p>Is it possible to convert Artifact to <code>numpy array<\/code> or <code>Dataframe<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636992603783,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp",
        "Question_view_count":339,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69977440",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72881056,
        "Question_title":"Why does my dbt container hang in Vertex AI?",
        "Question_body":"<p>I am trying to follow <a href=\"https:\/\/datatonic.com\/insights\/dbt-vertex-ai-pipelines-google-cloud\/\" rel=\"nofollow noreferrer\">this<\/a> tutorial to run a dbt docker image as a Vertex AI component. When the pipeline runs the component just seems to sit there for ever. Is there any way of debugging the component?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657099839053,
        "Question_score":0,
        "Question_tags":"docker|dbt|google-cloud-vertex-ai|kfp",
        "Question_view_count":73,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You see the pipeline logs to get an idea regarding what is going on in the pipeline.\nFrom the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/logging\" rel=\"nofollow noreferrer\">doc<\/a><\/p>\n<blockquote>\n<p>After you define and build a pipeline, you can use Cloud Logging to create log entries to help you monitor events such as pipeline failures. You can create custom log-based metrics that send notifications when the rate of pipeline failures reaches a given threshold.<\/p>\n<\/blockquote>\n<p>You can also select a component inside Pipeline's runtime graph and then view detailed info and logs of that particular component.\n<a href=\"https:\/\/i.stack.imgur.com\/QXm02.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QXm02.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also you can hover your cursor on the component status area(green check or grey disabled icon) to check the current status of that component.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1657175196140,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657175516073,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72881056",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73113256,
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1658770653850,
        "Question_score":0,
        "Question_tags":"python-3.x|hyperparameters|google-cloud-vertex-ai|spacy-3",
        "Question_view_count":133,
        "Owner_creation_time":1629385138957,
        "Owner_last_access_time":1663953209400,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Question_last_edit_time":1660741967557,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1660669810153,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660850228733,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71137452,
        "Question_title":"control over Vertex AI annotation",
        "Question_body":"<p>In the Google Vertex AI platform, the configuration of labelling the data set is decided by the vertex AI itself, ie. for example the color which is given to the bounding box while labelling is decided by vertex ai, what if I want to assign the color as my preference? I couldn't see any option to change the annotation color and also tried to feed externally labelled files to the vertex ai and it shows an error as there is no field named color;<\/p>\n<pre><code>{\n  &quot;imageGcsUri&quot;: &quot;gs:\/\/cloud-ai-platform-5100f6e6-d2e6-4966-869e-ba22a09ef85a\/bg_MAX_0002.JPG&quot;,\n  &quot;boundingBoxAnnotations&quot;: [\n    {\n      &quot;displayName&quot;: &quot;abc&quot;,\n      &quot;xMin&quot;: 0.07510431154381085,\n      &quot;xMax&quot;: 0.34492350486787204,\n      &quot;yMin&quot;: 0.1022964509394572,\n      &quot;yMax&quot;: 0.4384133611691023,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    },\n    {\n      &quot;displayName&quot;: &quot;pqr&quot;,\n      &quot;xMin&quot;: 0.6801112656467315,\n      &quot;xMax&quot;: 0.9318497913769124,\n      &quot;yMin&quot;: 0.1503131524008351,\n      &quot;yMax&quot;: 0.6367432150313153,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    },\n    {\n      &quot;displayName&quot;: &quot;xyz&quot;,\n      &quot;xMin&quot;: 0.15438108484005564,\n      &quot;xMax&quot;: 0.6620305980528511,\n      &quot;yMin&quot;: 0.605427974947808,\n      &quot;yMax&quot;: 0.906054279749478,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    }\n  ],\n  &quot;dataItemResourceLabels&quot;: {\n    \n  }\n}\n<\/code><\/pre>\n<p>Given above is an example label generated by vertex ai and it doesn't contain any color information even though I've labelled in multiple colors.\nSo my question is how can we get control over the color feature of the labelling system in vertex AI?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644994320227,
        "Question_score":0,
        "Question_tags":"machine-learning|google-cloud-platform|annotations|artificial-intelligence|google-cloud-vertex-ai",
        "Question_view_count":173,
        "Owner_creation_time":1626179648183,
        "Owner_last_access_time":1646630602500,
        "Owner_location":"India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1657278784877,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71137452",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72378701,
        "Question_title":"How do I set overcommit_memory on Google Cloud notebook?",
        "Question_body":"<p>I am using a GCP Vertex managed notebook and I get a memory error which I think can be fixed by:<\/p>\n<pre><code>echo 1 &gt; \/proc\/sys\/vm\/overcommit_memory\n<\/code><\/pre>\n<p>but when I run this from a Jupyterlab terminal I am asked for a sudo password, which I do not know. What can I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653485896420,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|memory-management|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":76,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72378701",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70477987,
        "Question_title":"Vertex Ai issue when deploying a model using Java",
        "Question_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1640406502527,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":244,
        "Owner_creation_time":1369787017730,
        "Owner_last_access_time":1663945541287,
        "Owner_location":"Atlanta, Georgia",
        "Owner_reputation":55,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640820603630,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72783902,
        "Question_title":"Is it possible to run Vertex AI Workbench on Spot machines?",
        "Question_body":"<p>I'm trying to save budget on jupyter notebooks on Google Cloud but couldn't find a way to run Vertex AI Workbench (Notebooks) on spot machines.\nWhat are my alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656408332770,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-notebook|jupyter|google-cloud-vertex-ai",
        "Question_view_count":176,
        "Owner_creation_time":1322253579120,
        "Owner_last_access_time":1661153607010,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72783902",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70205432,
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1638473538253,
        "Question_score":0,
        "Question_tags":"visual-studio-code|google-cloud-platform|ssh|google-compute-engine|google-cloud-vertex-ai",
        "Question_view_count":230,
        "Owner_creation_time":1580840045043,
        "Owner_last_access_time":1661885424787,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1638550660460,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638843453690,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69925931,
        "Question_title":"Vertex AI model batch prediction failed with internal error",
        "Question_body":"<p>I have trained the AutoMl classification model on Vertex AI, unfortunately model does not work with batch predictions, whenever I try to score training dataset (same which was used for the successful model training) with batch predictions on Vertex AI I get a following error:<\/p>\n<p>&quot;Due to one or more errors, this training job was canceled on Nov 11, 2021 at 09:42AM&quot;.<\/p>\n<p>There is an option to get a details from this error and those say the following thing:<\/p>\n<p>&quot;Batch prediction job customer_value_label_cv_automl_gui encountered the following errors: INTERNAL&quot;<\/p>\n<p>Does anyone know what might be the reason for getting this kind of error? I am very surprised that the model cannot score the dataset that it was trained on. My dataset consists of 570 columns and about 300k of records. <a href=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636622783290,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":712,
        "Owner_creation_time":1551797759387,
        "Owner_last_access_time":1648564097343,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":77,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1636706508710,
        "Answer_body":"<p>We have been able to finally figure this out. As we were using model.batch_predict method described in the <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">official documentation<\/a> we unnecessary set the machine_type parameter. Finally, we were able to figure out that it was causing the issue, the machine was probably too weak. Once we removed this declaration this method started to use automatic resources and that solved the case. I wish Vertex AI errors were a little bit more informative because it took us a lot of trials and error to figure out.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637235864267,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69925931",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69857932,
        "Question_title":"Specify signature name on Vertex AI Predict",
        "Question_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1636138079960,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|vertex|google-ai-platform|tfx|google-cloud-vertex-ai",
        "Question_view_count":508,
        "Owner_creation_time":1606605180560,
        "Owner_last_access_time":1664067783153,
        "Owner_location":"Brazil",
        "Owner_reputation":98,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1636203038130,
        "Answer_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636439088550,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636453108543,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69857932",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73557938,
        "Question_title":"Vertex AI: Unknown Custom Training Job Error",
        "Question_body":"<p>While attempting to use the Vertex AI via Google Console to create a custom training as follows:<\/p>\n<ol>\n<li>Select Dataset<\/li>\n<li>Select Annotation set<\/li>\n<li>Select &quot;Custom training (advanced)&quot;<\/li>\n<li>Select &quot;Train new model&quot;<\/li>\n<li>Provide Name<\/li>\n<li>Select training options<\/li>\n<li>Select &quot;Custom container&quot;<\/li>\n<li>Browse and select Docker container in Artifact Registry<\/li>\n<li>Select &quot;Dataset export directory&quot; to GCS location<\/li>\n<li>No hyperparameters<\/li>\n<li>Select &quot;Region&quot; us-central1<\/li>\n<li>Select &quot;Machine type&quot; a2-highgpu-1g<\/li>\n<li>Select &quot;Accelerator type&quot; NVIDIA_TESLA_A100<\/li>\n<li>Select &quot;Accelerator count&quot; 1<\/li>\n<li>No &quot;Prediction container&quot;<\/li>\n<\/ol>\n<p>It results in the following error message:<\/p>\n<blockquote>\n<p>Unable to start training due to the following error: Unable to parse\n`training_pipeline.training_task_inputs` into custom task `inputs`\ndefined in the file:\ngs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml<\/p>\n<\/blockquote>\n<p>This error message does not provide enough information as to exactly what is the issue, and was wondering if anyone else know of a solution for this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1661958154070,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_time":1516951090470,
        "Owner_last_access_time":1663790014450,
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":1661967668820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73557938",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73612214,
        "Question_title":"Vertex AI model serving - gRPC access - code pointers \/ samples",
        "Question_body":"<p>We have created an endpoint in Vertex AI. We have got the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models\" rel=\"nofollow noreferrer\">client library<\/a> route working. However, we also want to figure out the gRPC route since that is closest to the gRPC route we had with self managed TF-Serving.\nCan someone provide a code pointer for Vertex AI model serving using gRPC (preferably in Python)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662394780720,
        "Question_score":1,
        "Question_tags":"python|grpc|google-cloud-vertex-ai",
        "Question_view_count":36,
        "Owner_creation_time":1280773677817,
        "Owner_last_access_time":1662955154927,
        "Owner_location":"Jersey City, NJ",
        "Owner_reputation":4339,
        "Owner_up_votes":16,
        "Owner_down_votes":1,
        "Owner_views":479,
        "Question_last_edit_time":1662401625170,
        "Answer_body":"<p>gRPC can be used through Vertex Prediction private endpoint, but it is not yet officially supported. See sample here: <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662511382057,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73612214",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71557442,
        "Question_title":"How combine results from multiple models in Google Vertex AI?",
        "Question_body":"<p>I have multiple models in Google Vertex AI and I want to create an endpoint to serve my predictions.\nI need to run aggregation algorithms, like the Voting algorithm on the output of my models.\nI have not found any ways of using the models together so that I can run the voting algorithms on the results.\nDo I have to create a new model, curl my existing models and then run my algorithms on the results?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647864699137,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":253,
        "Owner_creation_time":1372407778700,
        "Owner_last_access_time":1663592257430,
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":134,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":74,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no in-built provision to implement aggregation algorithms in Vertex AI. To <code>curl<\/code> results from the models then aggregate them, we would need to deploy all of them to individual endpoints. Instead, I would suggest the below method to deploy the models and the meta-model(aggregate model) to a single endpoint using <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom containers for prediction<\/a>. The custom container requirements can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>You can load the model artifacts from GCS into a custom container. If the same set of models are used (i.e) the input models to the meta-model do not change, you can package them inside the container to reduce load time. Then, a custom HTTP logic can be used to return the aggregation output like so. This is a sample custom flask server logic.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_models_from_gcs():\n    ## Pull the required model artifacts from GCS and load them here.\n    models = [model_1, model_2, model_3]\n    return models\n\ndef aggregate_predictions(predictions):\n    ## Your aggregation algorithm here\n    return aggregated_result\n\n\n@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    models = get_models_from_gcs()\n    predictions = []\n    \n    for model in models:\n        predictions.append(model.predict(preprocessed_inputs))\n\n    aggregated_result = aggregate_predictions(predictions)\n\n    return {&quot;aggregated_predictions&quot;: aggregated_result}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1647950025930,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1647950484410,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71557442",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71132848,
        "Question_title":"Google.Cloud.AIPlatform.V1 Received http2 header with status: 404",
        "Question_body":"<p>We are trying to call the Google.Cloud.AIPlatform.V1 predict API using the .Net client and keep getting the following error:   Received http2 header with status: 404<\/p>\n<p>We setup credentials using the API key and environment variable:  GOOGLE_APPLICATION_CREDENTIALS<\/p>\n<p>Here is the code to call the vertex AI predict API:<\/p>\n<pre><code>const string projectId = &quot;xxxxxx&quot;;\nconst string location = &quot;us-central1&quot;; \nconst string endpointId = &quot;xxxxxx&quot;;  \n\nPredictionServiceClient client = PredictionServiceClient.Create();\n\nvar structVal = Google.Protobuf.WellKnownTypes.Value.ForStruct(new Struct\n{\n    Fields =\n    {\n    [&quot;mimeType&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(&quot;text\/plain&quot;),\n    \/\/ Sample contents is a string constant defined in a separate file\n    [&quot;content&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(Consts.SampleContents)\n    }\n});\n\nPredictRequest req = new PredictRequest()\n{\n    EndpointAsEndpointName = EndpointName.FromProjectLocationEndpoint(projectId, location, endpointId),\n    Instances = { structVal }\n};\n\nPredictResponse response = client.Predict(req);\n<\/code><\/pre>\n<p>The full error returned:<\/p>\n<p>Status(StatusCode=&quot;Unimplemented&quot;, Detail=&quot;Received http2 header with status: 404&quot;, DebugException=&quot;Grpc.Core.Internal.CoreErrorDetailException: {&quot;created&quot;:&quot;@1644947338.412000000&quot;,&quot;description&quot;:&quot;Received http2 :status header with non-200 OK status&quot;,&quot;file&quot;:&quot;......\\src\\core\\ext\\filters\\http\\client\\http_client_filter.cc&quot;,&quot;file_line&quot;:134,&quot;grpc_message&quot;:&quot;Received http2 header with status: 404&quot;,&quot;grpc_status&quot;:12,&quot;value&quot;:&quot;404&quot;}&quot;)<\/p>\n<p>I validate the same call using CURL and was able to successfully make the call.<\/p>\n<pre><code>curl -X POST -H &quot;Authorization: Bearer XXXXX&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/us-central1-aiplatform.googleapis.com\/ui\/projects\/XXXXXX\/locations\/us-central1\/endpoints\/XXXXXX:predict -d @payload.json\n<\/code><\/pre>\n<p>Any help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644956294297,
        "Question_score":0,
        "Question_tags":"c#|google-cloud-vertex-ai",
        "Question_view_count":204,
        "Owner_creation_time":1545835825527,
        "Owner_last_access_time":1664041392653,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71132848",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68915712,
        "Question_title":"Google Auto ML taking huge time for forcasting",
        "Question_body":"<p>I have around 500 time series dataset for a period of 2.5 yrs with a granularity of 1 day for each series. This amounts to roughly 1 million data points.\nI want to forecast for 2 weeks in 1 day granularity for each of the time series. There might be correlation among these 500 time series.\nAfter ensuring that I have data for each timestamp, we are feeding these (500) time series to autoML where each time series is identified by \u201cseries identifier\u201d.\nSo, our input to the autoML (Forecasting) is timestamp, series identifier, features, and target value. I have 30 feature which are combination of categorical and numerical.\nWith this setup, if I feed to autoML, it is taking more than 20 hrs for training which is not cost effective for me.<\/p>\n<p>Please help me to optimized this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629854539447,
        "Question_score":0,
        "Question_tags":"time-series|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_time":1497882511323,
        "Owner_last_access_time":1649034469423,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68915712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71956892,
        "Question_title":"Training Pipeline fails after uploading model artifacts to Google Cloud Storage",
        "Question_body":"<p>Here's a snippet of my training code:<\/p>\n<pre><code>param_grid = {\n&quot;max_tokens&quot; : [100],\n&quot;max_len&quot; : [10],\n&quot;dropout&quot; : [0.1],\n}\ngs_model = GridSearchCV(KerasClassifier(build_model), param_grid, cv=3, scoring='accuracy')\ngs_model.fit(x_train, y_train, verbose = 1)\nbest_params = gs_model.best_params_\noptimized_model = build_model(max_tokens = best_params[&quot;max_tokens&quot;], max_len = best_params[&quot;max_len&quot;], dropout = best_params[&quot;dropout&quot;])\noptimized_model.fit(x_train, y_train, epochs = 3, validation_split = 0.2, callbacks = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose = 1))\nmodel_name = &quot;\/tmp\/custom-model-test&quot;\noptimized_model.save(model_name)\nprint('saved model to ', model_name)\nupload_from_directory(model_name, &quot;[redacted Bucket name]&quot;, &quot;custom-model-test&quot;)\ntry: \n    upload_blob(&quot;[redacted Bucket name]&quot;, &quot;goback-custom-train\/requirements.txt&quot;, &quot;custom-model-test\/requirements.txt&quot;)\nexcept:\n    print(traceback.format_exc())\n    print('Upload failed')\n<\/code><\/pre>\n<p>Which succeeds in uploading to Google Cloud Storage. It makes use of <code>model.save<\/code> from Keras, and saves the created directory to my Bucket, along with a <code>requirements.txt<\/code> file inside it. To be clear, once the code block above is ran, a directory <code>custom-model-test\/<\/code> is created in <code>gs:\/\/[redacted Bucket name]<\/code> with contents <code>requirements.txt<\/code> and <code>tmp\/<\/code>. Inside <code>tmp\/<\/code> is <code>keras-metadata.pb<\/code>, <code>saved_model.pb<\/code>, and <code>variables\/<\/code>.<\/p>\n<p>I run this container in the following codeblock in my Kubeflow Pipeline:<\/p>\n<pre><code>training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n    project = project,\n    display_name = display_name,\n    container_uri=training_container_uri,\n    model_serving_container_image_uri=model_serving_container_image_uri,\n    model_serving_container_predict_route = model_serving_container_predict_route,\n    model_serving_container_health_route = model_serving_container_health_route,\n    model_serving_container_ports = [8080],\n    service_account = &quot;[redacted service account]&quot;,\n    machine_type = &quot;n1-highmem-2&quot;,\n    accelerator_type =&quot;NVIDIA_TESLA_V100&quot;,\n    staging_bucket = BUCKET_NAME)\n<\/code><\/pre>\n<p>For some reason, after training and saving the model artifacts (the logs for the model training says it completed successfully) the pipeline fails with logs saying:<\/p>\n<pre><code>&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py&quot;, line 905, in _raise_failure &quot;\n&quot; raise RuntimeError(&quot;Training failed with:\\n%s&quot; % self._gca_resource.error) &quot;\n&quot;RuntimeError: Training failed with: &quot;\n&quot;code: 5\n&quot;message: &quot;There are no files under \\&quot;gs:\/\/[redacted Bucket name]\/aiplatform-custom-training-2022-04-21-14:04:46.151\/model\\&quot; to copy.&quot;\n&quot;\n<\/code><\/pre>\n<p>What's going on here? What's the fix?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1650554216863,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_time":1649861924490,
        "Owner_last_access_time":1663770236590,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1650554522460,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71956892",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73085293,
        "Question_title":"Library is not installed on PATH - How can I install on path?",
        "Question_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658518447040,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|installation|package|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_time":1621620820567,
        "Owner_last_access_time":1660057756750,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658596861407,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69373666,
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632907406933,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_time":1300443398977,
        "Owner_last_access_time":1645461116407,
        "Owner_location":"Switzerland",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Question_last_edit_time":1632914906017,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633278542093,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69373666",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72741757,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656062302777,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_time":1656061360900,
        "Owner_last_access_time":1659951704647,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656071944493,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1656343337227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72313973,
        "Question_title":"Vertex AI AutoML regression - batch predition error due to datatype mismatch",
        "Question_body":"<p>I trained a Vertex AI AutoML regression model (using the UI).\nI ran a Batch Prediction (alos with the UI) and it failed because of a datatype mismatch:\nThe Batch Prediction returned an error table in the export location in BigQuery.<\/p>\n<p>The Error :<\/p>\n<ul>\n<li><a href=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" alt=\"List item\" \/><\/a><\/li>\n<\/ul>\n<p><strong>In the output from the Batch Prediction, DISCOUNT_PCT is indeed a STRING<\/strong>:\n<a href=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>But in the table I loaded for the Batch Prediction, it is a NUMERIC<\/strong> (as it is in the data I used to train the model):\n<a href=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/q42iM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q42iM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the Batch Prediction process somehow changed the datatype of the table I loaded. Why is this happening and how can I solve it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653024740080,
        "Question_score":0,
        "Question_tags":"prediction|google-cloud-ml|type-mismatch|google-cloud-vertex-ai",
        "Question_view_count":65,
        "Owner_creation_time":1534984935633,
        "Owner_last_access_time":1661490030977,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1653031324513,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72313973",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70606958,
        "Question_title":"Use a model trained by Google Cloud Vertex AI accelerated with TRT on Jetson Nano",
        "Question_body":"<p>I am trying to standardize our deployment workflow for machine vision systems. So we were thinking of the following workflow.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lZiQs.png\" rel=\"nofollow noreferrer\">Deployment workflow<\/a><\/p>\n<p>So, we want to create the prototype for the same, so we followed the workflow. So, there is no problem with GCP operation whatsoever but when we try to export models, which we train on the <code>vertexAI<\/code> it will give three models as mentioned in the workflow which is:<\/p>\n<ol>\n<li>SaveModel<\/li>\n<li>TFLite<\/li>\n<li>TFJS<\/li>\n<\/ol>\n<p>and we try these models to convert into the ONNX model but we failed due to different errors.<\/p>\n<ol>\n<li>SaveModel - Always getting the same error with any parameter which is as follows\n<a href=\"https:\/\/i.stack.imgur.com\/HsBoa.png\" rel=\"nofollow noreferrer\">Error in savemodel<\/a>\nI tried to track the error and I identified that the model is not loading inside the TensorFlow only which is wired since it is exported from the <code>GCP vertexAI<\/code> which leverages the power of TensorFlow.<\/li>\n<li>TFLite - Successfully converted but again the problem with the <code>opset<\/code> of ONNX but with 15 <code>opset<\/code> it gets successfully converted but then NVIDIA tensorRT ONNXparser doesn't recognize the model during ONNX to TRT conversion.<\/li>\n<li>TFJS - yet not tried.<\/li>\n<\/ol>\n<p>So we are blocked here due to these problems.<\/p>\n<p>We can run these models exported directly from the <code>vertexAI<\/code> on the Jetson Nano device but the problem is <code>TF-TRT<\/code> and TensorFlow is not memory-optimized on the GPU so the system gets frozen after 3 to 4 hours of running.<\/p>\n<p>We try this workflow with google teachable machine once and it workout well all steps are working perfectly fine so I am really confused How I conclude this full workflow since it's working on a teachable machine which is created by Google and not working on vertexAI model which is again developed by same Company.<\/p>\n<p>Or am I doing Something wrong in this workflow?\nFor the background we are developing this workflow inside C++ framework for the realtime application in industrial environment.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641471385767,
        "Question_score":1,
        "Question_tags":"tensorflow|onnx|nvidia-jetson-nano|google-cloud-vertex-ai",
        "Question_view_count":189,
        "Owner_creation_time":1641468106990,
        "Owner_last_access_time":1663914585187,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1641962349420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70606958",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70648776,
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641797901177,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1641896853597,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1641810988040,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70664460,
        "Question_title":"Is it possible to create and train natural-language google-cloud model using python sdk?",
        "Question_body":"<p>I want to create a Vertex pipeline using KFP for training natural language model, and I can't find a python API to use for creating and training model, I know that there is the option of creating the model from the console, but I am looking for a way to do it on my git repository.<\/p>\n<p>any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641893769170,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|automl|google-cloud-vertex-ai|google-natural-language",
        "Question_view_count":13,
        "Owner_creation_time":1563265471333,
        "Owner_last_access_time":1663834240627,
        "Owner_location":"Israel",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70664460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72692781,
        "Question_title":"Error call Google Vertex AI endpoint from a python backend",
        "Question_body":"<p>I am trying to send an http post request to my google vertex ai endpoint for prediction. Though I do set the Bearer Token in the request header, the request still fails with the below error:<\/p>\n<pre><code>{\n&quot;error&quot;: {\n    &quot;code&quot;: 401,\n    &quot;message&quot;: &quot;Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.&quot;,\n    &quot;status&quot;: &quot;UNAUTHENTICATED&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.ErrorInfo&quot;,\n            &quot;reason&quot;: &quot;ACCESS_TOKEN_TYPE_UNSUPPORTED&quot;,\n            &quot;metadata&quot;: {\n                &quot;service&quot;: &quot;aiplatform.googleapis.com&quot;,\n                &quot;method&quot;: &quot;google.cloud.aiplatform.v1.PredictionService.Predict&quot;\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n<p>}<\/p>\n<p>Since I am making this call from a python backend, I'm not sure if OAuth 2 as suggested in the message would be wise and applicable choice.<\/p>\n<p>The model is already deployed and endpointed test on vertex ai and it worked fine. What I am trying to do is send same prediction task via an http post request using postman and this is what failed.<\/p>\n<p>The request url looks like this:<\/p>\n<pre><code>https:\/\/[LOCATION]-aiplatform.googleapis.com\/v1\/projects\/[PROJECT ID]\/locations\/[LOCATION]\/endpoints\/[ENDPOINT ID]:predict\n<\/code><\/pre>\n<p>Where token bearer is set in the potman authorization tab and instance set in request body.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1655757826930,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-endpoints|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":353,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1655940147090,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72692781",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73815721,
        "Question_title":"How do you specify multiple values for the inputTensorName key in INPUTMETADATA spec in Vertex Explainable AI for a Functional API model?",
        "Question_body":"<p>I want to add explanation to my model running in Vertex AI using the Vertex AI SDK.I get a silent error when running the batch prediction using ModelBatchPredictOp, where the ModelBatchPredictOp node runs infinitely.Here is my ModelBatchPredictOp definition;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ModelBatchPredictOp(\n    project=project_id,\n    job_display_name=&quot;tensorflow-ex-batch-prediction-job&quot;,\n    location=project_location,\n    model=champion_model.outputs[&quot;model&quot;],\n    instances_format=&quot;csv&quot;,\n    predictions_format=&quot;jsonl&quot;,\n    gcs_source_uris=gcs_source_uris,\n    gcs_destination_output_uri_prefix=gcs_destination_output_uri_prefix,\n    machine_type=batch_prediction_machine_type,\n    starting_replica_count=batch_prediction_min_replicas,\n    max_replica_count=batch_prediction_max_replicas,\n    generate_explanation=True,\n)\n<\/code><\/pre>\n<p>I have narrowed down the issue to inputTensorName key in the INPUTMETADATA spec.The 'inputTensorName' key in the INPUTMETADATA spec takes in a string for it's value(<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/ExplanationSpec#InputMetadata\" rel=\"nofollow noreferrer\">INPUTMETADATA spec<\/a>). In my case I have a tensorflow model defined using the functional API meaning it has multiple inputs, as shown below;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># numeric\/categorical features in Chicago trips dataset to be preprocessed\nNUM_COLS = [&quot;dayofweek&quot;, &quot;hourofday&quot;, &quot;trip_distance&quot;, &quot;trip_miles&quot;, &quot;trip_seconds&quot;]\nORD_COLS = [&quot;company&quot;]\nOHE_COLS = [&quot;payment_type&quot;]\n\ndef build_and_compile_model(dataset: Dataset, model_params: dict) -&gt; Model:\n        &quot;&quot;&quot;Build and compile model.\n\n        Args:\n            dataset (Dataset): training dataset\n            model_params (dict): model parameters\n\n        Returns:\n            model (Model): built and compiled model\n        &quot;&quot;&quot;\n\n        # create inputs (scalars with shape `()`)\n        num_ins = {\n            name: Input(shape=(), name=name, dtype=tf.float32) for name in NUM_COLS\n        }\n        ord_ins = {\n            name: Input(shape=(), name=name, dtype=tf.string) for name in ORD_COLS\n        }\n        cat_ins = {\n            name: Input(shape=(), name=name, dtype=tf.string) for name in OHE_COLS\n        }\n\n        # join all inputs and expand by 1 dimension. NOTE: this is useful when passing\n        # in scalar inputs to a model in Vertex AI batch predictions or endpoints e.g.\n        # `{&quot;instances&quot;: {&quot;input1&quot;: 1.0, &quot;input2&quot;: &quot;str&quot;}}` instead of\n        # `{&quot;instances&quot;: {&quot;input1&quot;: [1.0], &quot;input2&quot;: [&quot;str&quot;]}`\n        all_ins = {**num_ins, **ord_ins, **cat_ins}\n        exp_ins = {n: tf.expand_dims(i, axis=-1) for n, i in all_ins.items()}\n\n        # preprocess expanded inputs\n        num_encoded = [normalization(n, dataset)(exp_ins[n]) for n in NUM_COLS]\n        ord_encoded = [str_lookup(n, dataset, &quot;int&quot;)(exp_ins[n]) for n in ORD_COLS]\n        ohe_encoded = [str_lookup(n, dataset, &quot;one_hot&quot;)(exp_ins[n]) for n in OHE_COLS]\n\n        # ensure ordinal encoded layers is of type float32 (like the other layers)\n        ord_encoded = [tf.cast(x, tf.float32) for x in ord_encoded]\n\n        # concat encoded inputs and add dense layers including output layer\n        x = num_encoded + ord_encoded + ohe_encoded\n        x = Concatenate()(x)\n        for units, activation in model_params[&quot;hidden_units&quot;]:\n            x = Dense(units, activation=activation)(x)\n        x = Dense(1, name=&quot;output&quot;, activation=&quot;linear&quot;)(x)\n\n        model = Model(inputs=all_ins, outputs=x, name=&quot;nn_model&quot;)\n        model.summary()\n\n        logging.info(f&quot;Use optimizer {model_params['optimizer']}&quot;)\n        optimizer = optimizers.get(model_params[&quot;optimizer&quot;])\n        optimizer.learning_rate = model_params[&quot;learning_rate&quot;]\n\n        model.compile(\n            loss=model_params[&quot;loss_fn&quot;],\n            optimizer=optimizer,\n            metrics=model_params[&quot;metrics&quot;],\n        )\n\n        return model\n<\/code><\/pre>\n<p>As a consequence when getting the input layer, using;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>serving_input = list(\n        loaded_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()\n    )[0]\n  \nINPUT_METADATA = {\n   &quot;input_tensor_name&quot;: serving_input,\n   &quot;encoding&quot;: &quot;BAG_OF_FEATURES&quot;,\n   &quot;modality&quot;: &quot;numeric&quot;,\n   &quot;index_feature_mapping&quot;: cols,\n}\n\n<\/code><\/pre>\n<p>I only get one Input layer corresponding to one of the input tensors (cols) in either NUM_COLS, ORD_COLS or OHE_COLS. This causes an infinite run when running the ModelBatchPredictOp in the prediction pipeline as only the name to one input tensor is passed as the value to the inputTensorName. Running<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>list(model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys())\n<\/code><\/pre>\n<p>returns the a list of all the input layers corresponding to the input tensor names (cols) defined in NUM_COLS, ORD_COLS and OHE_COLS.<\/p>\n<p>How do I specify the value of inputTensorName in order to capture all the input layers? Or is there a work around to assign multiple input tensor names to inputTensorName?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663854073780,
        "Question_score":1,
        "Question_tags":"python|tensorflow|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines|xai",
        "Question_view_count":19,
        "Owner_creation_time":1461847578647,
        "Owner_last_access_time":1663929992527,
        "Owner_location":"Nairobi, Kenya",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663924909350,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73815721",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72907038,
        "Question_title":"Cost of deploying a TensorFlow model in GCP?",
        "Question_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657258379287,
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_time":1569457921527,
        "Owner_last_access_time":1664002308563,
        "Owner_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Owner_reputation":41,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1657258776587,
        "Answer_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657283230410,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72640182,
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655350561587,
        "Question_score":0,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines|mlops",
        "Question_view_count":107,
        "Owner_creation_time":1621620820567,
        "Owner_last_access_time":1660057756750,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1659256989790,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659257407333,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72314675,
        "Question_title":"Managing data drift when using w2vec embeddings on VertexAI",
        "Question_body":"<p>So I am looking into moving my models from GCP's AI Platform to Vertex AI, my main motivation for it being the fact that Vertex AI has automatic email notifications when your data skews or drifts (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring<\/a>).<\/p>\n<p>So if you start receiving dodgy data that doesn't resemble the training set, they send you an email telling you which features (columns) of the data you are trying to predict are drifting away from your training data.<\/p>\n<p>However, I am unsure how this would work in my case since my data is text data that has been encoded using word2vec embeddings. Therefore, my dataset has 300 columns but I don't know what feature each of the columns refers to.<\/p>\n<p>Is this sort of data drift analysis still useful in my particular case?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1653029251863,
        "Question_score":1,
        "Question_tags":"machine-learning|google-cloud-platform|word2vec|google-cloud-vertex-ai",
        "Question_view_count":133,
        "Owner_creation_time":1612345105200,
        "Owner_last_access_time":1663853054067,
        "Owner_location":"London, Reino Unido",
        "Owner_reputation":57,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72314675",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71101070,
        "Question_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Question_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644758810230,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":225,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644873889257,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71101070",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72440188,
        "Question_title":"Deploy Pretrained Model Test Error: The first dimension of paddings must be the rank of inputs[4,2]",
        "Question_body":"<p>I have a successfully trained and tested a custom instance segmentation model using pixellib Mask_RCNN model.  The model runs inferences fine locally, but when I try to serve predictions using vertex ai \/ google cloud platform I cannot get the predictions to serve correctly.<\/p>\n<h2>Model Signature:<\/h2>\n<pre><code>  inputs['input_anchors'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, 4)\n      name: serving_default_input_anchors:0\n  inputs['input_image'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, -1, 3)\n      name: serving_default_input_image:0\n  inputs['input_image_meta'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 14)\n      name: serving_default_input_image_meta:0\n<\/code><\/pre>\n<h2>JSON Requests - Snippet:<\/h2>\n<p>To test the model independent of any other code I use the test feature in google cloud console for vertex AI.  I enter JSON structured as follows.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;instances&quot;: \n    [\n    {\n     &quot;input_anchors&quot;: [[[-0.35, ...], ... ]],\n     &quot;input_image&quot;: [[[[-123.7, -116.8, -103.9], ... ]]], \n     &quot;input_image_meta&quot;: [[0.0, 240.0, ....]]\n    }\n    ]\n}\n<\/code><\/pre>\n<h2>Shapes of input<\/h2>\n<p>I can confirm that the shapes of the inputs are the correct dimensions for the model signature.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>np.array(instance['input_anchors']).shape -&gt; (1,1023,4)\nnp.array(instance['input_image']).shape -&gt; (1,64,64,3)\nnp.array(instance['input_image_meta'].shape -&gt; (1,14)\n<\/code><\/pre>\n<h2>Returns the error:<\/h2>\n<p>Testing the model returns the following error.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;The first dimension of paddings must be the rank of inputs[4,2] [1,1,64,64,3]\\\\n\\\\t [[{{node mask_rcnn\/zero_padding2d_1\/Pad}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>The message indicates that the model is looking for a 5D input <code>[1,1,64,64,3]<\/code>, but the model signature requires a 4D input <code>(-1,-1,-1,3)<\/code>.<\/p>\n<p>Using np.expand_dims to make the input_image 5D, it results in the same error message but now asking for a 6D input <code>[1,1,1,64,64,3]<\/code>.  A 6D input yields an error message asking for a 7D input...<\/p>\n<p>If I reduce the input_image to 3D (which should definitely be incorrect) <code>[64,64,3]<\/code> I get a different error:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;slice index 1 of dimension 0 out of bounds.\\\\n\\\\t [[{{node mask_rcnn\/roi_align_classifier\/strided_slice_8}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>Can someone help me understand if I'm structuring my inputs incorrectly for the model or if I'm running into a known bug?  It's strange that the error message always asks for +1 dimension than the dimension that I give it.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653946027497,
        "Question_score":0,
        "Question_tags":"python|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_time":1457431514813,
        "Owner_last_access_time":1662026934273,
        "Owner_location":null,
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72440188",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73573487,
        "Question_title":"Vertex AI model version using Python SDK",
        "Question_body":"<p>Vertex AI offers a very interesting Model Registry that allows you to store all trained models and track all their versions.<\/p>\n<p>However, I don't manage to create new versions of the same model using the Python SDK. In particular, I have a Vertex AI Pipeline that performs: 1) data preprocessing, 2) feature engineering, 3) feature store creation, and in the end, 4) train a model with AutoML Tabular.<\/p>\n<p>The code of the Pipeline component dedicated to the point 4 is:<\/p>\n<pre><code> automl_training_electric_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n    project=project_bq,\n    model_display_name=&quot;pred-model&quot;,\n    display_name=&quot;pred-model&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-prc&quot;,\n    budget_milli_node_hours=1000,\n    dataset=comp5a.outputs[&quot;dataset&quot;],\n    target_column=&quot;fault&quot;,\n    location=location\n)\n<\/code><\/pre>\n<p>In the Google documentation I didn't find anything that could help me in creating new versions of the &quot;pred-model&quot;, in fact, any time I run the pipeline, Vertex AI creates a new model with the same name.<\/p>\n<p>I would like that at each training, AutoML creates a new version of the same model. E.g., v1, v2, v3.<\/p>\n<p>Here, the current situation, in which the same model is replicated and not versioned:\n<a href=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662054058413,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":58,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73573487",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70070421,
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1637604275697,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-vertex-ai",
        "Question_view_count":319,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":1637686316647,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637647501513,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73659986,
        "Question_title":"Vertex pipeline model training component stuck running forever because of metadata issue",
        "Question_body":"<p>I'm attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.<\/p>\n<p>The error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:<\/p>\n<pre><code>Retryable error reported. System is retrying.\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=ABORTED, message=Specified Execution `etag`: `1662555654045` does not match server `etag`: `1662555533339`, cause=null System is retrying.\n<\/code><\/pre>\n<p>I've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:<\/p>\n<p><code>protoPayload.methodName=&quot;google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph&quot;<\/code><\/p>\n<p><code>protoPayload.resourceName=&quot;projects\/724306335858\/locations\/europe-west4\/metadataStores\/default<\/code><\/p>\n<p>Leading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.<\/p>\n<p>I've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/P8ViW.png\" rel=\"nofollow noreferrer\">screenshot of ui<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1662715406920,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|mlops|gcp-ai-platform-training|custom-training",
        "Question_view_count":51,
        "Owner_creation_time":1662714633560,
        "Owner_last_access_time":1663324616117,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1662729439980,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73659986",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71214828,
        "Question_title":"Vertex AI 504 Errors in batch job - How to fix\/troubleshoot",
        "Question_body":"<p>We have a Vertex AI model that takes a relatively long time to return a prediction.<\/p>\n<p>When hitting the model endpoint with one instance, things work fine.  But batch jobs of size say 1000 instances end up with around 150 504 errors (upstream request timeout). (We actually need to send batches of 65K but I'm troubleshooting with 1000).<\/p>\n<p>I tried increasing the number of replicas assuming that the # of instances handed to the model would be (1000\/# of replicas) but that doesn't seem to be the case.<\/p>\n<p>I then read that the default batch size is 64 and so tried decreasing the batch size to 4 like this from the python code that creates the batch job:<\/p>\n<p>model_parameters = dict(batch_size=4)<\/p>\n<pre><code>def run_batch_prediction_job(vertex_config):\n\n    aiplatform.init(\n        project=vertex_config.vertex_project, location=vertex_config.location\n    )\n\n    model = aiplatform.Model(vertex_config.model_resource_name)\n\n    model_params = dict(batch_size=4)\n    batch_params = dict(\n        job_display_name=vertex_config.job_display_name,\n        gcs_source=vertex_config.gcs_source,\n        gcs_destination_prefix=vertex_config.gcs_destination,\n        machine_type=vertex_config.machine_type,\n        accelerator_count=vertex_config.accelerator_count,\n        accelerator_type=vertex_config.accelerator_type,\n        starting_replica_count=replica_count,\n        max_replica_count=replica_count,\n        sync=vertex_config.sync,\n        model_parameters=model_params\n    )\n\n    batch_prediction_job = model.batch_predict(**batch_params)\n\n    batch_prediction_job.wait()\n\n    return batch_prediction_job\n<\/code><\/pre>\n<p>I've also tried increasing the machine type to n1-high-cpu-16 and that helped somewhat but I'm not sure I understand how batches are sent to replicas?<\/p>\n<p>Is there another way to decrease the number of instances sent to the model?\nOr is there a way to increase the timeout?\nIs there log output I can use to help figure this out?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1645492543853,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":194,
        "Owner_creation_time":1462581330170,
        "Owner_last_access_time":1663976709807,
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71214828",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73252066,
        "Question_title":"Query BQ table with Jupyter Notebook across owned projects",
        "Question_body":"<p>I have an issue about acessing data in a BigQuery table in one project using a VertexAI in another project.<\/p>\n<p>Now, I own both project and have service accounts in both project, which implies that I also have the key (credential.json in the project containing the data) which I can use to define my client:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('credentials.json')\nproject_id = 'cloud-billing-XXXX'\nclient = bigquery.Client(credentials= credentials,project=project_id)\n<\/code><\/pre>\n<p>which should be enough to run:<\/p>\n<pre><code>%%bigquery\nSELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>ERROR:\n 403 Access Denied: Table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export: User does not have permission to query table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export.\n<\/code><\/pre>\n<p>I can, from the project containing my notebook, query the table in BigQuery. This makes me think that the problem is a VertexAI permission issue. I read somewhere that the service account used when defining the notebook must match the service account in the project the data resides in. I tried to create a workbench notebook with the service account in the first project and it is created but when I try to open it it refuses to do so and get an error message.<\/p>\n<p>I've also tried to grant Editor and job user permissions across both project but that wouldn't work either.<\/p>\n<p>Any experiences and ideas on how to solve this would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659713227817,
        "Question_score":1,
        "Question_tags":"google-bigquery|jupyter-notebook|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":67,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":null,
        "Answer_body":"<ol>\n<li>As a mentioned in the comment: add your notebook service account to the first project (which contains your BigQuery data) and grant it with <strong>Bigquery Job User<\/strong> and <strong>BigQuery Data Viewer<\/strong> permissions.<\/li>\n<li>You can query data directly in you python code (without using &quot;magic&quot; %%bigquery). Just add the next two rows:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>pct_overlap_terms_by_days_apart = client.query(&quot;SELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100&quot;).to_dataframe()\npct_overlap_terms_by_days_apart.head()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check out table name. If table path is wrong, then you'll get the same error: <strong>403 Access Denied<\/strong>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659745527510,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73252066",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73368320,
        "Question_title":"Vertax AI pipeline quota",
        "Question_body":"<p>I got a custom_model_training_cpus error when runing a submitted pipeline on Vertex AI. I could not find any documents. And I am using the n1-standard-4 for the deployment machine, I do not see any issue. Any commnents would be much appriciated.<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus, cause=null; Failed to create custom job for the task.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgCJD.png\" rel=\"nofollow noreferrer\">DAG flow and error message<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660618735810,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":110,
        "Owner_creation_time":1660618422463,
        "Owner_last_access_time":1660874466550,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368320",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71016472,
        "Question_title":"Vertex AI forecasting AutoML datatype mismatch",
        "Question_body":"<p>I could train the vertex AI AutoML forecating model but when I do batch prediction I get following error<\/p>\n<blockquote>\n<p>Batch prediction job batch_prediction encountered the following\nerrors:<\/p>\n<pre><code>Column &quot;sales&quot; expects type: NUMBER, the actual type is: STRING.\n<\/code><\/pre>\n<\/blockquote>\n<p>Below is a sample of test set I am passing for batch prediction in big query.<\/p>\n<p>According to the documentation for batch prediction we have to send some training\/historical data and forecasting dates. I did just that.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1644227346773,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_time":1450288149287,
        "Owner_last_access_time":1661168365350,
        "Owner_location":null,
        "Owner_reputation":500,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1644237395457,
        "Answer_body":"<p>Google recommend you to use the same input format for ingraining and prediction. Seams you have trained your model using a input format here the column sales were a <code>numeric<\/code> type, and now in the prediction you a using a BigQuery table with the <code>sales<\/code> column as <code>string<\/code>.<\/p>\n<p>Delete this table and import the data again defining the schema <strong>manually<\/strong>, and set sales as a numeric field, as following:<\/p>\n<pre><code>date:DATE,\nstore_product_id:STRING,\nsales:NUMERIC\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644420672183,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71016472",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69779123,
        "Question_title":"Vertex AI Tensorboard trough user interface",
        "Question_body":"<p>I have been using the Vertex AI training service with a custom container for my own machine learning pipeline. I would like to get tensorboard logs into the experiments tab to see in real-time the metrics while the model is training.<\/p>\n<p>I was wondering if it is possible to set a custom training job in the user interface setting a <code>TENSORBOARD_INSTANCE_NAME<\/code>. It seems that this is only possible through a json-post-request.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635592687870,
        "Question_score":0,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_time":1628607500490,
        "Owner_last_access_time":1663182539140,
        "Owner_location":"Colombia",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69779123",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72580712,
        "Question_title":"kubeflow component - why so many ways to define a component and what are the differences?",
        "Question_body":"<p>Please help understand what are the meaningful\/significant differences among different ways to create kubeflow pipeline components and the reason for having so many ways?<\/p>\n<pre><code>from kfp.components import func_to_container_op\n\n@func_to_container_op\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.v2.dsl import component\n\n@component\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.components import create_component_from_func\n\n@create_component_from_func\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654904685283,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":77,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72580712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72677822,
        "Question_title":"How do i get the output from a custom container and pass to next pipeline in Vertex AI\/Kubeflow pipeline?",
        "Question_body":"<p>I am having difficulty trying to understand how to pass a result from a container as an output artifact. I understand that we need to write the output to a file but i need some example how to do it.<\/p>\n<p><a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk-v2\/component-development\/\" rel=\"nofollow noreferrer\">https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk-v2\/component-development\/<\/a><\/p>\n<p>This is the last part of the python container program where i save the <code>url<\/code> of model file on GCS onto <code>output.txt<\/code>.<\/p>\n<pre><code>with open('.\/output.txt', 'w') as f:\n    logging.info(f&quot;Model path url is in {'.\/output.txt'}&quot;)\n    f.write(model_path)\n<\/code><\/pre>\n<p>This is the component <code>.yaml<\/code> file<\/p>\n<pre><code>name: Dummy Model Training\ndescription: Train a dummy model and save to GCS\ninputs:\n  - name: input_url\n    description: 'Input csv url.'\n    type: String\n  - name: gcs_url\n    description: 'GCS bucket url.'\n    type: String\noutputs:\n  - name: gcs_model_path\n    description: 'Trained model path.'\n    type: String\nimplementation:\n    container:\n        image: ${CONTAINER_REGISTRY}\n        command: [\n          python, .\/app\/trainer.py,\n          --input_url, {inputValue: input_url},\n          --gcs_url, {inputValue: gcs_url},\n        ]\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655650023283,
        "Question_score":1,
        "Question_tags":"python|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":130,
        "Owner_creation_time":1352353242573,
        "Owner_last_access_time":1663165937033,
        "Owner_location":"Singapore",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72677822",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69540618,
        "Question_title":"Kubeflow, passing Python dataframe across components?",
        "Question_body":"<p>I am writing a Kubeflow component which reads an input query and creates a <code>dataframe<\/code>, roughly as:<\/p>\n<pre><code>from kfp.v2.dsl import component \n\n@component(...)\ndef read_and_write():\n    # read the input query \n    # transform to dataframe \n    sql.to_dataframe()\n<\/code><\/pre>\n<p>I was wondering how I can pass this dataframe to the next operation in my Kubeflow pipeline.\nIs this possible? Or do I have to save the dataframe in a csv or other formats and then pass the output path of this?\nThank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634041967303,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|tfx|google-cloud-vertex-ai",
        "Question_view_count":879,
        "Owner_creation_time":1624352292607,
        "Owner_last_access_time":1649173178490,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69540618",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72174602,
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1652110521003,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652193787553,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68892701,
        "Question_title":"How to structure container logs in Vertex AI?",
        "Question_body":"<p>I have a model in Vertex AI, from the logs it seems that Vertex AI has ingested the log into <code>message<\/code> field within <code>jsonPayload<\/code> field, but i would like to structure the <code>jsonPayload<\/code> field such that every key in <code>message<\/code> will be a field within <code>jsonPayload<\/code>, i.e: flatten\/extract <code>message<\/code> <a href=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1629721604810,
        "Question_score":0,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":342,
        "Owner_creation_time":1608712056580,
        "Owner_last_access_time":1663849917143,
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68892701",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69269073,
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632228068913,
        "Question_score":2,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":269,
        "Owner_creation_time":1519630645620,
        "Owner_last_access_time":1663095146407,
        "Owner_location":"Northampton, UK",
        "Owner_reputation":333,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632376096770,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632482492493,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71738221,
        "Question_title":"How to get preprocess\/postprocess steps from model created using Google Vertex AI?",
        "Question_body":"<p>A client of mine wants to run their Google Vertex AI model on NVIDIA Jetson boards using TensorRT as accelerator. The problem with this is that their model uses certain operators (DecodeJpeg) that are not supported by ONNX. I've been able to isolate the feature extrator subgraph from the model, so everything supported by ONNX is being used, while the preprocess and postprocess will be written separate from the model.<\/p>\n<p>I'm asking because I need to be provided the pre\/postprocess of the model so I could implement them separately, so is there a way to get pre\/postprocess from Google Vertex AI console?<\/p>\n<p>I've tried running a loop that rescales the image to a squared tile from 0 to 512, but none of those gave the adequate result.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1649079409933,
        "Question_score":0,
        "Question_tags":"tensorflow|onnx|tensorrt|google-cloud-vertex-ai",
        "Question_view_count":107,
        "Owner_creation_time":1616415228997,
        "Owner_last_access_time":1653485900393,
        "Owner_location":null,
        "Owner_reputation":16,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738221",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70341414,
        "Question_title":"How to view Vertex AI Matching Engine Deployed Index logs",
        "Question_body":"<p>I have deployed an index in Vertex AI IndexEndpoint. According to the docs for <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1beta1.types.DeployedIndex\" rel=\"nofollow noreferrer\">DeployedIndex<\/a>, I have set the attribute <code>enable_access_logging<\/code> to <code>True<\/code> to enable private endpoints access logs.<\/p>\n<blockquote>\n<p><strong>enable_access_logging<\/strong><br \/>\nOptional. If true, private endpoint's access logs are sent to StackDriver Logging. These logs are like standard server access logs, containing information like timestamp and latency for each MatchRequest. Note that Stackdriver logs may incur a cost, especially if the deployed index receives a high queries per second rate (QPS). Estimate your costs before enabling this option.<\/p>\n<\/blockquote>\n<p>However, in cloud logging I only see Vertex AI audit logs and no access logs. Where can I find logs that contain information for timestamp and latency for each request?<\/p>\n<p>Deployed Index Configuration<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-12-13T16:37:27.030230Z'\n  deploymentGroup: default\n  displayName: glove_brute_force_deployed_V1\n  enableAccessLogging: true\n  id: glove_brute_force_deployed_V1\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXXXXXXXXX\n  indexSyncTime: '2021-12-13T20:19:00.874937Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.0.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yNMD_AR3V6LIrGln9Ye5PuWWYAOoJwxgSHs2T2Xt8iwAPv1mLOZTfaDMLFTAaBC\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/NETWORK_ID\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Cloud Logging<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639432575870,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":290,
        "Owner_creation_time":1463607987530,
        "Owner_last_access_time":1651418190167,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1639484817827,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70341414",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70803996,
        "Question_title":"How to set environment variables for a User-managed notebook in Vertex AI",
        "Question_body":"<p>I am trying to set some environment variables for a user managed notebook in google cloud Vertex AI. I don't want to set this from a jupyter notebook itself because I want these environment variables to be available to anyone who opens a jupyter notebook from this notebook instance. This is what I have tried so far but nothing has worked:<\/p>\n<ol>\n<li>I have an existing user managed notebook. I ssh'd into the notebook vm and then set a environment variable, <code>export TEST_VAR=TEST_VARIABLE_WAS_SET<\/code> there. However, when I open a jupyter notebook from the console and do <code>os.environ[&quot;TEST_VAR&quot;]<\/code>, it gives a key error. So, I am assuming that this has something to do with the fact that the jupyter lab session that Vertex AI starts is in a different shell session or something similar. I also tried to add the following two metadata keys to the vm, and then restarted the vm, but it did not work:<\/li>\n<\/ol>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata startup-script-url=$GCS_BUCKET_NAME\/script.sh<\/code><\/p>\n<p>where script.sh is:<\/p>\n<pre><code>#!\/bin\/bash\n\nexport TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<p>AND<\/p>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata container-env-file=$GCS_BUCKET_NAME\/notebook-env.txt<\/code><\/p>\n<p>where notebook-env.txt is<\/p>\n<pre><code>TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<ol start=\"2\">\n<li>I also tried to create a new instance of a user managed notebook from the cloud console. In that I tried to provide a script in the &quot;Select a script to run after creation&quot; and also through the &quot;Metadata&quot; option by providing, the key as <code>startup-script-url<\/code> and the value as the script location on google cloud storage. The script was the same startup script earlier.<\/li>\n<\/ol>\n<p>So, how do I achieve this, for existing user managed notebooks and when I create new ones?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642780081007,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-ai-platform|google-cloud-vertex-ai|google-notebook",
        "Question_view_count":851,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1642975658733,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70803996",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70296482,
        "Question_title":"AutoML Tables Batch Prediction- Produces an empty table in google big query",
        "Question_body":"<p>I am new to google cloud platform's Vertex AI. In Vertex AI I have created a new batch prediction and would like a Bigquery output table. However when I create the new batch prediction the output table is empty. I am not sure what the issue is. Any advise please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1639082614653,
        "Question_score":1,
        "Question_tags":"google-bigquery|prediction|forecasting|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1523972080007,
        "Owner_last_access_time":1661442395577,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1640052565217,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70296482",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70173096,
        "Question_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Question_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638291169620,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|grpc|google-cloud-ml|grpc-python|google-cloud-vertex-ai",
        "Question_view_count":350,
        "Owner_creation_time":1463607987530,
        "Owner_last_access_time":1651418190167,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1639486727367,
        "Answer_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638293279417,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70670669,
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641920680143,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":168,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641974198663,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69806432,
        "Question_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Question_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635836251457,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":665,
        "Owner_creation_time":1353151867410,
        "Owner_last_access_time":1663836030397,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Question_last_edit_time":1636016613530,
        "Answer_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635870534303,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635872119870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69219230,
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631862868123,
        "Question_score":2,
        "Question_tags":"java|google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_time":1227171471293,
        "Owner_last_access_time":1664047108080,
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Question_last_edit_time":1631987478697,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1631875853753,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631888060710,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73586832,
        "Question_title":"How to Access Managed Dataset in Vertex AI using Custom Container",
        "Question_body":"<p>In the google cloud documentation below:<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application<\/a><\/p>\n<p>It says that the following environment variables are sent to the training container:<\/p>\n<pre><code>AIP_DATA_FORMAT: The format that your dataset is exported in. Possible values include: jsonl, csv, or bigquery.\nAIP_TRAINING_DATA_URI: The location that your training data is stored at.\nAIP_VALIDATION_DATA_URI: The location that your validation data is stored at.\nAIP_TEST_DATA_URI: The location that your test data is stored at.\n<\/code><\/pre>\n<p>Where each of the URI values are wildcards that annotate training, validation, and test data files in <code>.jsonl<\/code> format as such:<\/p>\n<pre><code>gs:\/\/bucket_name\/path\/training-*\ngs:\/\/bucket_name\/path\/validation-*\ngs:\/\/bucket_name\/path\/test-*\n<\/code><\/pre>\n<p><strong>Now, in your custom container that contains the python code, how do you actually access the contents of each of the files?<\/strong><\/p>\n<p>I've tried splitting the URI string using the following regex to obtain the <code>bucket_name<\/code> and the <code>prefix<\/code> info, and attempted the grab it using <code>bucket.list_blobs(delimiter='\/', prefix=prefix[:-1])<\/code> but it returns nothing when the files are definitely there. Here is a minimal example of the attempted code:<\/p>\n<pre><code>import os\nimport re\nfrom google.cloud import storage\n\naip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\nmatch = re.match('gs:\/\/(.*?)\/(.*)', aip_training_data_uri)\nbucket_name, prefix = match.groups()\n\nclient = storage.Client()\nbucket = client.bucket(bucket_name)\nblobs = bucket.list_blobs(delimiter='\/', prefix=prefix[:-1]) # &quot;[:-1]&quot; to remove wildcard asterisks\n\nfor blob in blobs:\n   print(blob.download_as_string()) # This returns an empty string\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1662144278260,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_time":1516951090470,
        "Owner_last_access_time":1663790014450,
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73586832",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70968460,
        "Question_title":"ModelUploadOp step failing with custom prediction container",
        "Question_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643879294320,
        "Question_score":0,
        "Question_tags":"python|google-cloud-ml|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":273,
        "Owner_creation_time":1512301540763,
        "Owner_last_access_time":1663934781440,
        "Owner_location":"Milan, Italy",
        "Owner_reputation":1427,
        "Owner_up_votes":93,
        "Owner_down_votes":16,
        "Owner_views":113,
        "Question_last_edit_time":1644239389000,
        "Answer_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643965835693,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71158453,
        "Question_title":"module 'google.cloud.logging_v2' has no attribute 'MetricsServiceV2Client' Vertex WorkBench",
        "Question_body":"<p>I am trying to set up a locust based framework for ML load test and need to create custom metrics and logs for which the example that I am following is using 'MetricsServiceV2Client' in 'google.cloud.logging_v2' lib.\nIn the Vertex Workbench on GCP inspite being on v3.0 of the google-cloud-logging lib I am getting an issue of import<\/p>\n<p>from google.cloud import logging_v2\nfrom google.cloud.logging_v2 import MetricsServiceV2Client<\/p>\n<p>error: cannot import name 'MetricsServiceV2Client' from 'google.cloud.logging_v2' (\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/logging_v2\/<strong>init<\/strong>.py)<\/p>\n<p>Interestingly when I test the import in  google's cloud console I am able to import without any issue. What could be the issue ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645102105297,
        "Question_score":0,
        "Question_tags":"google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":68,
        "Owner_creation_time":1615961068170,
        "Owner_last_access_time":1664041311570,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>from google.cloud.logging_v2.services.metrics_service_v2 import MetricsServiceV2Client this works !!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645126649433,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71158453",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72094768,
        "Question_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Question_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1651551486133,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines|argoproj|kfp",
        "Question_view_count":520,
        "Owner_creation_time":1562750927333,
        "Owner_last_access_time":1663926437127,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":803,
        "Owner_up_votes":123,
        "Owner_down_votes":7,
        "Owner_views":73,
        "Question_last_edit_time":1654867910903,
        "Answer_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655447541937,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70838510,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643047926007,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_time":1598873976143,
        "Owner_last_access_time":1663530379503,
        "Owner_location":"Versailles, France",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1643080918690,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643081751060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71600291,
        "Question_title":"GPU quotas increased but error on the excess of my quota",
        "Question_body":"<p>I try to make a managed notebook in Vertex AI. I requested and obtained an increase in my quota but when I create the notebook it keeps giving me error on the excess of my quota. I don't know why.<\/p>\n<p>&quot;QUOTA_EXCEEDED error message&quot;<\/p>\n<p>As required:\n<a href=\"https:\/\/i.stack.imgur.com\/M2o4f.png\" rel=\"nofollow noreferrer\">The error is about the exceed of my quotas of Nvidia GPU_P100<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/OZxfb.png\" rel=\"nofollow noreferrer\">A you can see i have a quota of 2 GPU_P100<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1648115810510,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gpu|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":68,
        "Owner_creation_time":1517346608423,
        "Owner_last_access_time":1664035493327,
        "Owner_location":null,
        "Owner_reputation":86,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1648135240783,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71600291",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73434003,
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1661081410723,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_time":1639312590127,
        "Owner_last_access_time":1663617607140,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661266806083,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73434003",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71300217,
        "Question_title":"Not able to run gcloud run deploy from Vertex AI Pipelines",
        "Question_body":"<p>I have been trying to deploy a model to cloud run but I am continuously getting a permission error. Tried using all the available suggestions and answers but had no success. Finally gave the owner access to the service accounts but no success.<\/p>\n<p>screenshot of what I am trying to do.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8AUzB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8AUzB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the code<\/p>\n<pre><code>storage_client = storage.Client()\n\nbucket = storage_client.get_bucket(bucket_name)\nblobs = bucket.list_blobs(prefix=prefix)  # Get list of files\nfor blob in blobs:\n    filename = blob.name\n    if not filename.endswith(&quot;\/&quot;):\n        blob.download_to_filename(dl_dir+filename.split(&quot;\/&quot;)[-1])  # Download\n\nbucket = storage_client.get_bucket(tar_path.split(&quot;\/&quot;)[0])\nblob = bucket.blob(&quot;\/&quot;.join(tar_path.split(&quot;\/&quot;)[1:]))\nblob.download_to_filename(&quot;\/tmp\/model_serving\/model.pth.tar&quot;)\n\nos.chdir(&quot;\/tmp\/model_serving\/&quot;)\nprint(&quot;############### &quot;, os.listdir())\n\nos.system(&quot;gcloud run deploy --port 5050 --allow-unauthenticated --region us-central1 darts-rnn-from-pipeline --source .&quot;)   \n<\/code><\/pre>\n<p>error I am getting<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tjc72.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tjc72.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am able to run the command outside the pipeline without any errors.\nI am also able to run the storage and bq clients inside the pipeline without any errors.\nBut only this command I am not able to run.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1646077052303,
        "Question_score":1,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-run|google-cloud-iam|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1646112654983,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71300217",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73727583,
        "Question_title":"IDtoken retrieval in Vertex AI pipeline fails randomly",
        "Question_body":"<p>In vertex AI pipelines we have created a custom component that trigger a cloud run.\nIn order to do so we need to fetch the Id Token Credentials of the running identity.\nThis work most of the time but fails randomly every 5 to 10 hours.<\/p>\n<p>here is our code :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>auth_req = google.auth.transport.requests.Request()\nid_token = google.oauth2.id_token.fetch_id_token(auth_req, url_dbt_server)\nreturn id_token\n<\/code><\/pre>\n<p>the error message is:<\/p>\n<pre><code>google.auth.exceptions.RefreshError: (&quot;Failed to retrieve http:\/\/metadata.google.internal\/computeMetadata\/v1\/instance\/service-accounts\/default\/identity?audience=&lt;Our audience&gt;&amp;format=full from the Google Compute Enginemetadata service. Status: 404 Response:\\nb'Not Found\\\\n'&quot;, &lt;google.auth.transport.requests._Response object at 0x7f8f0d78aca0&gt;)\n<\/code><\/pre>\n<p>How can I solve (or at least go around) this problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663229049140,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-run|google-cloud-vertex-ai",
        "Question_view_count":16,
        "Owner_creation_time":1663227968907,
        "Owner_last_access_time":1663943402967,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663229581173,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727583",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73130582,
        "Question_title":"How to query \/ flatten from vertex ml results saved to bigquery",
        "Question_body":"<p>This is driving me bonkers so any help greatly appreciated.<\/p>\n<p>I am using Google's Vertex ML. I have exported a batch prediction to BigQuery.<\/p>\n<p>The schema is I believe a record with repeat fields.<\/p>\n<p>So I think it would like this in JSON:<\/p>\n<pre><code>[{&quot;category&quot;:true,&quot;score&quot;:.9999},{&quot;category&quot;:false,&quot;score&quot;,.05}]\n<\/code><\/pre>\n<p>I can not figure out how to either unnest or narrow a search where a category is true.<\/p>\n<p>I need to have a flat select that has the correct category column and score value<\/p>\n<pre><code>123 | true | .9999\n123 | false | .05\n<\/code><\/pre>\n<p>or a select with a where clause to only get true values<\/p>\n<pre><code>123 | .9999\n<\/code><\/pre>\n<p>The following unnests everything but it creates four rows joining both the true and false to both the scores.<\/p>\n<pre><code>SELECT\n  row_id,\n  classes,\n  scores\nFROM\n  `database`\ncross JOIN\n  UNNEST(exported.classes) AS classes,\n  UNNEST(exported.scores) AS scores\nLIMIT\n  10\n<\/code><\/pre>\n<p>creates rows like:<\/p>\n<pre><code>123 | true | .9999\n123 | false | .9999\n123 | true | .05\n123 | false | .05\n<\/code><\/pre>\n<p>This does select the values I need but it's still a nested field...<\/p>\n<pre><code>select\nrow_id,\nclasses.classes,\nclasses.scores\nfrom (\nSELECT\n  voter_id,\n  ARRAY_CONCAT([predicted_results]) as the_results\nFROM\n  `data`\nLIMIT\n  10\n),\nunnest(the_results) as classes\n<\/code><\/pre>\n<p>creates rows like<\/p>\n<pre><code>123 | [true:.9999,false:.05]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658876367830,
        "Question_score":0,
        "Question_tags":"sql|google-bigquery|flatten|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1282610737523,
        "Owner_last_access_time":1663981942420,
        "Owner_location":null,
        "Owner_reputation":185,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Question_last_edit_time":1659331589370,
        "Answer_body":"<pre><code>select \n primary_key, \n  predicted_supports.classes[SAFE_OFFSET(index)] as class,\n  predicted_supports.scores[SAFE_OFFSET(index)] as score,\nFROM `database`,\nunnest(generate_array(0,array_length(predicted_supports.classes)-1)) as index\n<\/code><\/pre>\n<p>Here is the ouput:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1659415712443,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1659486136643,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73130582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71763407,
        "Question_title":"Set artifact name when using kfp dsl.importer",
        "Question_body":"<p>When importing an artifact using the kfp <code>dsl.importer()<\/code> function, the imported artifact gets the default (display) name <code>artifact<\/code>. I would like to give it a custom name to make the pipeline and lineage tracking more clear. I checked the <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.dsl.html#kfp.dsl.importer\" rel=\"nofollow noreferrer\">documentation<\/a>, but I can't seem to find a way to change the name of the artifact that the <code>dsl.importer()<\/code> function produces.<\/p>\n<p>Example code <code>dsl.importer()<\/code>:<\/p>\n<pre><code>    load_dataset_step = dsl.importer(\n        artifact_uri=input_data_uri,\n        artifact_class=dsl.Dataset,\n        reimport=False\n    ).set_display_name(&quot;Load Dataset&quot;)\n<\/code><\/pre>\n<p>Visualisation of the <code>dsl.importer()<\/code> step:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" alt=\"pipelines visualisation\" \/><\/a><\/p>\n<p>I'm making use of Google Cloud Vertex AI Pipelines.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1649233510080,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_time":1353508925387,
        "Owner_last_access_time":1663854791917,
        "Owner_location":"Belgium",
        "Owner_reputation":383,
        "Owner_up_votes":100,
        "Owner_down_votes":8,
        "Owner_views":132,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71763407",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71990757,
        "Question_title":"How can GCP Automl handle overfitting?",
        "Question_body":"<p>I have created a Vertex AI AutoML image classification model. How can I assess it for overfitting? I assume I should be able to compare training vs validation accuracy but these do not seem to be <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">available<\/a>.<\/p>\n<p>And if it is overfitting,can I tweak regularization parameters? Is it already doing cross validation? Anything else that can be done? (More data,early stopping, dropouts ie how can these be done?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650821429613,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":27,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Deploy it to endpoint and test result with sample images by uploading to endpoint. If it's overfitting you can see the stats in analysis. You can increase the training sample and retrain your model again to get better result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650850146067,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69658459,
        "Question_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Question_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634804848290,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":330,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1635245663990,
        "Answer_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1636443694643,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69577270,
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634245049023,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":1634248660770,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1634271217373,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72427594,
        "Question_title":"Questions on json and GCP",
        "Question_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653862009460,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|endpoint|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_time":1632597456473,
        "Owner_last_access_time":1661080359213,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1653863593763,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1653864193883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68737342,
        "Question_title":"Model evaluation predictions from VertexAI AutoMLTabularTrainingJob",
        "Question_body":"<p>I am following python api documentation to create a AutoMLTabularTrainingJob : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular<\/a>. It trains successfully with a dataset that consists of train\/valid\/test splits. I can also get the model evaluation metrics after following this documentation : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models<\/a><\/p>\n<p>However, I am unable to find a way to get the raw predictions that are used to generate the model evaluation metrics. Any pointers would be much appreciated !<\/p>\n<p>Do I have to request for batch predictions again ? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_time":1628665036657,
        "Question_score":0,
        "Question_tags":"google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":53,
        "Owner_creation_time":1308505624643,
        "Owner_last_access_time":1663893908543,
        "Owner_location":null,
        "Owner_reputation":1876,
        "Owner_up_votes":165,
        "Owner_down_votes":2,
        "Owner_views":201,
        "Question_last_edit_time":1628666618203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68737342",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70614514,
        "Question_title":"Google Cloud Vertex AI Notebook Scheduled Runs Aren't Running Code?",
        "Question_body":"<p>I've followed their <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-schedule-execution-console\" rel=\"nofollow noreferrer\">instructions<\/a> to how to set up a managed Jupyter notebook and schedule a run, and I tossed in some pretty standard parameters and my bucket.<\/p>\n<p>After setting up the schedule, however, the run just comes out as &quot;Failed&quot;, and when I get &quot;view results&quot;, I just get my code back (with no output indication). For some reason it's just not running. Ideas?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/r0vra.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r0vra.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>[2]<a href=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641509078660,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":457,
        "Owner_creation_time":1523235961717,
        "Owner_last_access_time":1642750774450,
        "Owner_location":null,
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":1641509284197,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70614514",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71245000,
        "Question_title":"Vertex AI Pipeline Failed Precondition",
        "Question_body":"<p>I have been following this video:\n<a href=\"https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s<\/a><\/p>\n<p>Code located at:\n<a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a>\n(I have done the last two steps as per the video which isn't an issue for google_cloud_pipeline_components version: 0.1.1)<\/p>\n<p>I have created a pipeline in vertex ai which ran and used the following code to create the pipeline (from video not code extract in link above):<\/p>\n<pre><code>#run pipeline\nresponse = api_client.create_run_from_job_spec(\n    &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n    parameter_values = {\n    &quot;project&quot; : PROJECT_ID,\n    &quot;display_name&quot; : DISPLAY_NAME\n    }\n)\n    \n<\/code><\/pre>\n<p>and in the GCP logs I get the following error:<\/p>\n<pre><code>&quot;google.api_core.exceptions.FailedPrecondition: 400 BigQuery Dataset location `eu` must be in the same location as the service location `us-central1`.\n<\/code><\/pre>\n<p>I get the error at the dataset_create_op stage:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project = project, display_name = display_name, bq_source = bq_source\n)\n<\/code><\/pre>\n<p>My dataset is configured in EU (the whole region) so I don't understand where us-central1 is coming from (or what the service location is?).<\/p>\n<p>Here is the all the code I have used:<\/p>\n<pre><code> PROJECT_ID = &quot;marketingtown&quot;\n BUCKET_NAME = f&quot;gs:\/\/lookalike_model&quot;\n from typing import NamedTuple\n import kfp\n from kfp import dsl\n from kfp.v2 import compiler\n from kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                            OutputPath, ClassificationMetrics, \n Metrics, component)\n from kfp.v2.components.types.artifact_types import Dataset\n from kfp.v2.google.client import AIPlatformClient\n from google.cloud import aiplatform\n from google_cloud_pipeline_components import aiplatform as gcc_aip\n\n #set environment variables\n PATH = %env PATH\n %env PATH = (PATH):\/\/home\/jupyter\/.local\/bin\n REGION = &quot;europe-west2&quot;\n    \n #cloud storage path where artifact is created by pipeline\n PIPELINE_ROOT = f&quot;{BUCKET_NAME}\/pipeline_root\/&quot;\n PIPELINE_ROOT\n import time\n DISPLAY_NAME = f&quot;lookalike_model_pipeline_{str(int(time.time()))}&quot;\n print(DISPLAY_NAME)\n \n@kfp.dsl.pipeline(name = &quot;lookalike-model-training-v2&quot;, \npipeline_root = PIPELINE_ROOT)\n\ndef pipeline(\n    bq_source : str = f&quot;bq:\/\/{PROJECT_ID}.MLOp_pipeline_temp.lookalike_training_set&quot;,\n    display_name : str = DISPLAY_NAME,\n    project : str = PROJECT_ID,\n    gcp_region : str = &quot;europe-west2&quot;,\n    api_endpoint : str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str : str = '{&quot;auPrc&quot; : 0.3}'\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project = project, display_name = display_name, bq_source = bq_source\n    )\n    \n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;], #dataset from previous step\n        target_column=&quot;sale&quot;,\n    )\n    \n    #outputted evaluation metrics\n    model_eval_task = classification_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n    )\n    \n    #if deployment threshold is mean, deploy\n    with dsl.Condition(\n        model_eval_task.outputs[&quot;dep_decision&quot;] == &quot;true&quot;,\n        name=&quot;deploy_decision&quot;,\n    ):\n        \n    endpoint_op = gcc_aip.EndpointCreateOp(\n        project=project,\n        location=gcp_region,\n        display_name=&quot;train-automl-beans&quot;,\n    )\n        \n    #deploys model to an endpoint\n    gcc_aip.ModelDeployOp(\n        model=training_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_op.outputs[&quot;endpoint&quot;],\n        min_replica_count=1,\n        max_replica_count=1,\n        machine_type=&quot;n1-standard-4&quot;,\n        )\n   \n\n     compiler.Compiler().compile(\n        pipeline_func = pipeline, package_path = &quot;tab_classif_pipeline.json&quot;\n    )\n\n    #run pipeline\n    response = api_client.create_run_from_job_spec(\n        &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n        &quot;project&quot; : PROJECT_ID,\n        &quot;display_name&quot; : DISPLAY_NAME\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1645657042940,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|pipeline|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":420,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1645697298213,
        "Answer_body":"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project=project,\n    display_name=display_name, \n    bq_source=bq_source,\n    location = gcp_region\n)\n<\/code><\/pre>\n<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646828083057,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71245000",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70464072,
        "Question_title":"Optionally use component functions added in VertexAI python SDK",
        "Question_body":"<p>I am using vertex ai's python SDK and it's built on top of Kubeflow pipelines. In it, you supposedly can do this:<\/p>\n<pre><code>train_op = (sklearn_classification_train(\n        train_data = data_op.outputs['train_out']\n    ).\n    set_cpu_limit(training_cpu_limit).\n    set_memory_limit(training_memory_limit).\n    add_node_selector_constraint(training_node_selector).\n    set_gpu_limit(training_gpu_limit)\n)\n<\/code><\/pre>\n<p>where you can add these functions (<code>set_cpu_limit<\/code>, <code>set_memory_limit<\/code>, <code>add_node_selector<\/code>, and <code>set_gpu_limit<\/code>) onto your component. I've haven't used this syntax before.<\/p>\n<p>How I can optionally use each 'sub function' only if the variables are specified each function?<\/p>\n<p>For example, if <code>training_gpu_limit<\/code> isn't set, I don't want to execute <code>set_gpu_limit<\/code> on the component.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640272812233,
        "Question_score":0,
        "Question_tags":"python-3.x|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":51,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70464072",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71598020,
        "Question_title":"Internal error on batch prediction job. Reproduction conditions are unidentified",
        "Question_body":"<p>I trained some tabular forecasting model and executed batch prediction jobs with Vertex AI, then sometimes the following error occurred 30-60 minutes after the start of the jobs.<\/p>\n<pre><code>Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.\n<\/code><\/pre>\n<p>As the message mentioned, I retried the next day with same data and then I got the prediction job succeed.\nHowever, I have seen this error from time to time with Vertex AI.\nI cannot identify the reproduction conditions.\nI cannot find the logs of the job execution.<\/p>\n<p>What causes this error in Vertex AI?\nHow can I avoid this errror?<\/p>\n<p>Resource name:\nprojects\/832409671062\/locations\/us-central1\/batchPredictionJobs\/3813678620929425408<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648103893343,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":124,
        "Owner_creation_time":1641394229917,
        "Owner_last_access_time":1650802845457,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71598020",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72237600,
        "Question_title":"GCP Vertex AI Managed Notebook cannot use custom container",
        "Question_body":"<p>In GCP Vertex AI, I created a Managed Notebook by specifying one of our custom containers which work perfectly with User-Managed Notebook kernels.\nThe Managed Notebook starts, and Jupyter Lab seems to work without any signs of error.<\/p>\n<p>Unfortunately, if I look at the available kernels in Jupyter Lab, only the default kernels are listed but not my custom kernel.<\/p>\n<p>An activity log entry on the right shows a spinning wheel &quot;Loading kernel from [custom container]&quot; which never disappears.<br \/>\nTaking a look at the terminal,<\/p>\n<p><em>docker image ls<\/em><\/p>\n<p>does not show the custom container either; obviously, it was not even pulled to the Managed Notebook.<\/p>\n<p>If I perform<\/p>\n<p><em>docker pull [custom container]<\/em><\/p>\n<p>in the terminal, to test connectivity to the Artifact Registry then it pulls the container correctly as expected.\nHowever, the custom kernel is still not visible in Jupyter Lab (even after a notebook restart).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1652506277780,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_time":1652504586737,
        "Owner_last_access_time":1652551986367,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72237600",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70984360,
        "Question_title":"Vertex AI Pipeline is not using the GPU",
        "Question_body":"<p>I am building a customized pipeline with the following step:<\/p>\n<pre><code>     trainer_task = (trainer(download_task.output).set_cpu_request(&quot;16&quot;).set_memory_request(&quot;60G&quot;).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', &quot;NVIDIA_TESLA_K80&quot;).set_gpu_limit(2))\n<\/code><\/pre>\n<p>However, when I check the number of GPU's available it says zero, and the only visible device is a CPU device. I am migrating a project from Kubeflow, and this is the first time using Vertex AI, so I am not pretty sure why this is happening.<\/p>\n<p>The involved step is component that loads a docker image from the Artifact Registry and installs Tensorflow-gpu==2.4.1.<\/p>\n<p>Am I missing something? Why is not enabling the specified GPUs?<\/p>\n<p>Any help will be highly appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1643968333550,
        "Question_score":1,
        "Question_tags":"tensorflow|google-cloud-platform|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":250,
        "Owner_creation_time":1643967668103,
        "Owner_last_access_time":1654434560203,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70984360",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73623659,
        "Question_title":"Is it possible to import custom source files into Kubeflow components?",
        "Question_body":"<p>I know that Kubeflow only modifies the container with the specified libraries to be installed. But I want to use my custom module in the training Component section of the pipeline.<\/p>\n<p>So let me clarify my case; I'm deploying a GCP Vertex AI pipeline which exists of preprocessing and training  steps. And there is also custom library that I created using some libraries like scikit. My main issue is that I want to re-use that library objects within my training step which looks like;<\/p>\n<pre><code>    packages_to_install = [\n        &quot;pandas&quot;,\n        &quot;sklearn&quot;,\n        &quot;mycustomlibrary?&quot;\n    ],\n)\ndef train_xgb_model(\n    dataset: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \n    from MyCustomLibrary import XGBClassifier\n    import pandas as pd\n    \n    data = pd.read_csv(dataset.path)\n\n    model = XGBClassifier(\n        objective=&quot;binary:logistic&quot;\n    )\n    model.fit(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    score = model.score(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    model_artifact.metadata[&quot;train_score&quot;] = float(score)\n    model_artifact.metadata[&quot;framework&quot;] = &quot;XGBoost&quot;\n    \n    model.save_model(model_artifact.path)``` \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662474254327,
        "Question_score":1,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":31,
        "Owner_creation_time":1529909367513,
        "Owner_last_access_time":1662970375840,
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73623659",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73300381,
        "Question_title":"Documentation on Vertex AI Feature Store online serving architecture?",
        "Question_body":"<p>There's documentation on <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/serving-online\" rel=\"nofollow noreferrer\">Vertex AI online serving<\/a>, but no mention of the underlying system being used other than <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">&quot;online serving nodes&quot;<\/a>. Is it <a href=\"https:\/\/cloud.google.com\/datastore\" rel=\"nofollow noreferrer\">Datastore<\/a>? Something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660103073257,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":21,
        "Owner_creation_time":1386714193850,
        "Owner_last_access_time":1663802361323,
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73300381",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72073763,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1651375335713,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":358,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652802212843,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68156788,
        "Question_title":"Google Vertex AI AutoML - cannot specify schema for CSV Dataset",
        "Question_body":"<p>I have created Tabular datasets in Vertex AI \/ Datasets based on some CSV files. However when I try to use these datasets in AutoML for training and prediction, there is no way to specify the data types of the fields. In <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#csv\" rel=\"nofollow noreferrer\">the docs<\/a> I could not find how to do the &quot;transformations&quot;. In theory it supports the following types:<\/p>\n<ul>\n<li>Text<\/li>\n<li>Categorical<\/li>\n<li>Numeric<\/li>\n<li>Timestamp<\/li>\n<\/ul>\n<p>In case of BigQuery tables it is pretty obvious to get the data types as it is explicitely specified by the schema of the table. However in case of a CSV file sometimes it is not obvious to find out the type of a field and indeed in my case sometimes AutoML guesses incorrectly. Any ideas how to specify the data types explicitely for CSV files?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624846088910,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":443,
        "Owner_creation_time":1457019461140,
        "Owner_last_access_time":1653612331473,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1624940922617,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68156788",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69106055,
        "Question_title":"Is there anyway to train a classification model with Google's AutoML, with mixed (language and tabular) data?",
        "Question_body":"<p>I want to train a NLP classification model with Google's AutoML. The inputs of the model are tabular data and a text field which is the main field for the classification task. Without the tabular data the classification error gets huge. I'm aware that it can be done with a custom model using Keras or PyTorch. Could it be done using Google's AutoML?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1631115758097,
        "Question_score":0,
        "Question_tags":"keras|nlp|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":46,
        "Owner_creation_time":1500240803887,
        "Owner_last_access_time":1645585994337,
        "Owner_location":"Medell\u00edn, Medellin, Antioquia, Colombia",
        "Owner_reputation":38,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1631204087533,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69106055",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70023419,
        "Question_title":"Vertex AI - cannot create managed notebook instance",
        "Question_body":"<p>I'm following all the (easy) steps in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-create-console\" rel=\"nofollow noreferrer\">documentation<\/a>, but I'm stuck at clicking the &quot;CREATE&quot; button. When I click it, the process runs for a few seconds, then the button re-appears, like I never clicked it.<\/p>\n<p>If I go back to the &quot;Managed Notebooks&quot; page, no instance is present.<\/p>\n<p>Am I missing something basic? Has someone the same problem as mine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1637252864337,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":281,
        "Owner_creation_time":1450277518267,
        "Owner_last_access_time":1663928482250,
        "Owner_location":"Milano, MI, Italia",
        "Owner_reputation":947,
        "Owner_up_votes":625,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70023419",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69040163,
        "Question_title":"Vertex ai custom model training for pyspark ml model",
        "Question_body":"<p>Is it possible to train a spark\/pyspark ML lib model using VertexAI custom container model building? I couldn't find any reference in the vertex ai documents regarding spark model training. For distributed processing model building only options available are PyTorch or TensorFlow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630648846963,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|apache-spark-mllib|machine-learning-model|google-cloud-vertex-ai",
        "Question_view_count":432,
        "Owner_creation_time":1630648341403,
        "Owner_last_access_time":1663926044083,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69040163",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68465990,
        "Question_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Question_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626854597743,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":742,
        "Owner_creation_time":1466977784157,
        "Owner_last_access_time":1664005111393,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1626858370043,
        "Answer_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627666181500,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661471987217,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68465990",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69936296,
        "Question_title":"Vertex AI custom container batch prediction",
        "Question_body":"<p>I have created a custom container for prediction and successfully uploaded the model to Vertex AI. I was also able to deploy the model to an endpoint and successfully request predictions from the endpoint. Within the custom container code, I use the <code>parameters<\/code> field as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">here<\/a>, which I then supply later on when making an online prediction request.\nMy questions are regarding requesting batch predictions from a custom container for prediction.<\/p>\n<ol>\n<li><p>I cannot find any documentation that describes what happens when I request a batch prediction. Say, for example, I use the <code>my_model.batch_predict<\/code> function from the Python SDK and set the <code>instances_format<\/code> to &quot;csv&quot; and provide the <code>gcs_source<\/code>. Now, I have setup my custom container to expect prediction requests at <code>\/predict<\/code> as described in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">documentation<\/a>. Does Vertex AI make a POST request to this path, converting the cvs data into the appropriate POST body?<\/p>\n<\/li>\n<li><p>How do I specify the <code>parameters<\/code> field for batch prediction as I did for online prediction?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636674290160,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":808,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69936296",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69904968,
        "Question_title":"How to set the run name of a PipelineJob",
        "Question_body":"<p>I have this code to start a VertexAI pipeline job:<\/p>\n<pre><code>import google.cloud.aiplatform as vertexai\n\nvertexai.init(project=PROJECT_ID,staging_bucket=PIPELINE_ROOT)\n\njob = vertexai.PipelineJob(\n    display_name='pipeline-test-1',\n    template_path='xgb_pipe.json'\n)\n\njob.run()\n<\/code><\/pre>\n<p>which works nicely, but the <code>run name<\/code> label is a random number.  How can I specify the <code>run name<\/code>?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2Fkom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2Fkom.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636491344913,
        "Question_score":0,
        "Question_tags":"python|google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":302,
        "Owner_creation_time":1262684260990,
        "Owner_last_access_time":1664043379267,
        "Owner_location":"Transylvania, Romania",
        "Owner_reputation":200885,
        "Owner_up_votes":4092,
        "Owner_down_votes":148,
        "Owner_views":12738,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69904968",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72064796,
        "Question_title":"GCP Vertex AI - what is the return value of gcloud ai endpoints create defined?",
        "Question_body":"<p>Where is the return value of <code>gcloud ai endpoints create<\/code> defined and documented?<\/p>\n<p>I am following GCP coursera Vertex AI model monitoring <a href=\"https:\/\/www.coursera.org\/learn\/art-science-ml\/gradedLti\/Ru8lM\/lab-vertex-ai-model-monitoring\" rel=\"nofollow noreferrer\">Lab: Vertex AI Model Monitoring<\/a> and the task is asking to get the model endpoint ID from the <code>gcloud ai endpoints create<\/code> command output.<\/p>\n<pre><code># Deploy your model to the endpoint\nENDPOINT_NAME = &quot;churn&quot;\noutput = !gcloud --quiet beta ai endpoints create --display-name=$ENDPOINT_NAME --format=&quot;value(name)&quot;\nprint(&quot;endpoint output: &quot;, output)\n\nENDPOINT = output[-1]\nENDPOINT_ID = # TODO: Your code goes here\n<\/code><\/pre>\n<p><a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/create\" rel=\"nofollow noreferrer\">gcloud ai endpoints create<\/a> documentation shows no information of its return value.<\/p>\n<p>Where is it documented?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651278069133,
        "Question_score":0,
        "Question_tags":"gcloud|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72064796",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73830357,
        "Question_title":"issues with passing command line args to custom training job vertex AI",
        "Question_body":"<p>I am trying to run basic custom training job<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n display_name='testjob-name',\n container_uri='gcr.io\/prj-id\/image-name:latest',\n project=project_id,\n credentials= credentials,\n staging_bucket= 'stage-bucket'\n)\n<\/code><\/pre>\n<p>using below code to run the job<\/p>\n<pre><code>job.run(\n        args=['--data_dir', '\/gcs\/bucket\/folder',\n        '--model_dir', '\/gcs\/bucket\/model',\n        '--configs', 'internal-config.yml'],\n        replica_count=1,\n        sync=True\n    )\n\n<\/code><\/pre>\n<p>Training job is getting exited with code 127 when I am passing args with job.run(). kindly help, what is the correct way to send command line arguments to custom training python script.\nThere isn\u2019t much coming up in logs either.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663948945487,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|custom-training",
        "Question_view_count":22,
        "Owner_creation_time":1433508703627,
        "Owner_last_access_time":1664083750830,
        "Owner_location":null,
        "Owner_reputation":130,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73830357",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70379395,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":346,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":1639726542407,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1639667359983,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1639750456897,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73300297,
        "Question_title":"Vertex AI Feature Store limits",
        "Question_body":"<p>Vertex AI has <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/quotas#featurestore\" rel=\"nofollow noreferrer\">quotas and limits<\/a>. Other than submitting very high quota requests, is there documentation on hard limits for all of the quotas? Things like &quot;online serving requests per minute&quot;, &quot;Concurrent batch jobs&quot;, and &quot;Entity types across all featurestores&quot; are pretty key constraints to know before committing to Vertex.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1660102046707,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":56,
        "Owner_creation_time":1386714193850,
        "Owner_last_access_time":1663802361323,
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73300297",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71038823,
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644344024883,
        "Question_score":0,
        "Question_tags":"python|protocol-buffers|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_time":1519933306780,
        "Owner_last_access_time":1664076383160,
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644385932890,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644387344880,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73649262,
        "Question_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Question_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1662640804323,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":85,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1662967888203,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73458040,
        "Question_title":"How do I enable_logging for ModelMonitoringAlertConfig in GCP?",
        "Question_body":"<p>I am trying to <code>enable_logging<\/code> in <code>ModelMonitoringAlertConfig<\/code> I have tried:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as vertex_ai_beta\n...\n    alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\n    enable_logging=True,\n    email_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n        user_emails=NOTIFY_EMAILS\n    )\n)\n<\/code><\/pre>\n<p>gives:<\/p>\n<pre><code> Unknown field for ModelMonitoringAlertConfig: enable_logging\n<\/code><\/pre>\n<p>but <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.ModelMonitoringAlertConfig\" rel=\"nofollow noreferrer\">this<\/a> suggests it should work. What am I missing?<\/p>\n<p>(I have also tried <code>aiplatform_v1beta1<\/code>.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661254728657,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":19,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458040",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72988019,
        "Question_title":"Vertex AI deploy custom model to an endpoint",
        "Question_body":"<p>After I run<\/p>\n<pre><code>gcloud beta ai endpoints deploy-model (ENDPOINT : --region=REGION) --display-name=DISPLAY_NAME --model=MODEL [--accelerator=[count=COUNT],[type=TYPE]] [--deployed-model-id=DEPLOYED_MODEL_ID] [--disable-container-logging] [--enable-access-logging] [--machine-type=MACHINE_TYPE] [--max-replica-count=MAX_REPLICA_COUNT] [--min-replica-count=MIN_REPLICA_COUNT] [--service-account=SERVICE_ACCOUNT] [--traffic-split=[DEPLOYED_MODEL_ID=VALUE,\u2026]] [GCLOUD_WIDE_FLAG \u2026]\n<\/code><\/pre>\n<p>I got error &quot;(gcloud.beta.ai.endpoints.deploy-model) INVALID_ARGUMENT: AUTOMATIC_RESOURCES is not one of the supported deployment resources types for Model projects\/...\/locations\/us-central1\/models\/...<\/p>\n<p>What does this mean?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657846565487,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":63,
        "Owner_creation_time":1617317930923,
        "Owner_last_access_time":1658305735803,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72988019",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71575080,
        "Question_title":"Vertex AI Predictions using a .NET Core webapi Custom Container",
        "Question_body":"<p>I'm getting some strange behavior out of Vertex AI when calling Predictions on a custom container that is a .NET core based application.<\/p>\n<p>A little background.  I'm doing a small proof of concept to test the viability of using .NET core based Docker images as custom containers in Vertex AI.<\/p>\n<p>I have created a simple .NET core webapi that mocks out the prediction endpoint by returning what vertex-ai expects as a prediction result from the endpoint.  The Controller code is below:<\/p>\n<pre><code>[ApiController]\n[Route(&quot;api\/[controller]&quot;)]\n[Produces(&quot;application\/json&quot;)]\npublic class PredictionController : ControllerBase\n{\n    private readonly ILogger&lt;PredictionController&gt; logger;\n    public PredictionController(ILogger&lt;PredictionController&gt; logger)\n    {\n        this.logger = logger;\n    }\n\n    [HttpPost]\n    public Prediction PostPrediction(dynamic data) \n    {\n        Prediction prediction = new Prediction();\n\n        List&lt;double&gt; predictions = new List&lt;double&gt;();\n\n        predictions.Add(0.82);\n\n        prediction.Predictions = predictions;\n\n        return prediction;\n         \n    }\n}\n<\/code><\/pre>\n<p>The Dockerfile used to host the .NET core application is:<\/p>\n<pre><code>FROM mcr.microsoft.com\/dotnet\/sdk:6.0-alpine as build\nWORKDIR \/app\nCOPY . .\nRUN dotnet restore\nRUN dotnet publish -o \/app\/published-app\n\nFROM mcr.microsoft.com\/dotnet\/aspnet:6.0-alpine as runtime\nWORKDIR \/app\nCOPY --from=build \/app\/published-app \/app\nENTRYPOINT [ &quot;dotnet&quot;, &quot;\/app\/dotnet-poc.dll&quot; ]\n<\/code><\/pre>\n<p>Here is an image of hitting the dockerized .NET core app endpoint with the expected vertex-ai input and the output of the endpoint.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" alt=\"docker prediction\" \/><\/a><\/p>\n<p>Following the standard documentation for deploying a model using a custom container for Vertex AI predictions, I have uploaded my docker image to Artifact Registry and imported the model using the custom container in Vertex AI.  I then deploy the model to an endpoint and run a prediction test...<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you can see Vertex is returning a 502 without much detail.  I cannot see any issue with the return from the prediction endpoint that should cause any error.<\/p>\n<p>Thanks for reading and looking forward to any answers that might shed light on the issue.<\/p>\n<p>Other things to note:<\/p>\n<p>Health endpoint is returning 200<\/p>\n<p>Vertex AI Endpoint is healthy<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647964299573,
        "Question_score":0,
        "Question_tags":".net-core|google-cloud-vertex-ai",
        "Question_view_count":72,
        "Owner_creation_time":1321330022043,
        "Owner_last_access_time":1663959945940,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71575080",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68650973,
        "Question_title":"Automl SDK code with file location from bigquery but having issue while predicting",
        "Question_body":"<p>I was creating a model using GCP automl sdk module in AI Platform (using bigquery table as input for training and predicting) and predicting using batch_prediction. The issue is that the code runs fine but the output table of predictions is empty and error table has all series from prediction dataframe and addition column stating error code 3 and error is &quot;The time series has no values to predict. The time series has been excluded from predictions.&quot;.<\/p>\n<p>Code which I have used for model training:<\/p>\n<pre><code>job = aiplatform.AutoMLForecastingTrainingJob(\n    display_name='train-sdk-automl_tst1',\n    optimization_objective='minimize-mae',    \n    column_transformations=[\n        {&quot;timestamp&quot;: {&quot;column_name&quot;: &quot;Date&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Price&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Grammage&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMax&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMin&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Consumer_promo&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Promo_Value&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Trade_Promotion&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Holiday&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Sales&quot;}},\n    ]\n)\n\n# This will take around an hour to run\nmy_model = job.run(\n    dataset=ds,\n    target_column='Sales',\n    time_column='Date',\n    time_series_identifier_column='SKU',\n    available_at_forecast_columns=['Date', 'Price','Grammage'\n                                   ,'apparentTemperatureMax','apparentTemperatureMin','Consumer_promo',\n                                   &quot;Promo_Value&quot;,&quot;Trade_Promotion&quot;,&quot;Holiday&quot;],\n    unavailable_at_forecast_columns=['Sales'],\n    forecast_horizon=21.0,\n    data_granularity_unit='week',\n    data_granularity_count=1,\n    weight_column=None,\n    budget_milli_node_hours=1000,\n    model_display_name='sdk_tsting_bq-forecast-model', \n    predefined_split_column_name=None\n)\n<\/code><\/pre>\n<p>Code for predictions:<\/p>\n<pre><code>BATCH_PREDICT_SOURCE = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool.test_data_sdk1'\nBATCH_PREDICT_DESTINATION_PREFIX = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool' \nmy_model.batch_predict(\n   bigquery_source=BATCH_PREDICT_SOURCE,\n   instances_format='bigquery',\n   bigquery_destination_prefix = BATCH_PREDICT_DESTINATION_PREFIX,\n   predictions_format='bigquery',\n   job_display_name='predict_sdk_tst')\n<\/code><\/pre>\n<p>Please suggest what might be going wrong here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628078794493,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-bigquery|sdk|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_time":1628078248313,
        "Owner_last_access_time":1637841402880,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68650973",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70916238,
        "Question_title":"AlphaFold on VertexAI - Stuck in setting up notebook for 2 hours",
        "Question_body":"<p>I am trying to run AlphaFold on VertexAI as explained <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/running-alphafold-on-vertexai\" rel=\"nofollow noreferrer\">here<\/a>. However, my instance creation is stuck in this state for roughly two hours now. There is no error message either. I am wondering if something has gone wrong or this is just the expected time it will take to setup a new instance?<\/p>\n<p>I actually tried with two different notebooks. One is the <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/raw\/main\/community-content\/alphafold_on_workbench\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">default one<\/a> linked in the above article and the other is <a href=\"https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb<\/a><\/p>\n<p>Both are in the same state for roughly the same time.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643557381820,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_time":1354268805153,
        "Owner_last_access_time":1663957595997,
        "Owner_location":"Islamabad Capital Territory, Pakistan",
        "Owner_reputation":388,
        "Owner_up_votes":411,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70916238",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72439167,
        "Question_title":"Vertex AI Endpoint creation failes",
        "Question_body":"<p>When I try to deploy a google auto ml trained model to a google vertex ai endpoint (which worked 3 days ago) I suddely get an error message:<\/p>\n<p>Hello Vertex AI Customer,<\/p>\n<p>Due to an error, Vertex AI was unable to create endpoint &quot;test2&quot;.\nAdditional Details:\nOperation State: Failed with errors\nResource Name:\nprojects\/\/######\/locations\/europe-west4\/endpoints\/\/#####\nError Messages: Machine type temporarily unavailable, please deploy with a\ndifferent machine type or retry<\/p>\n<p>To view the error on Cloud Console, go back to your endpoint using\n<a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/locations\/europe-west4\/models\/\/#####\/versions\/1\/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=\/######\" rel=\"nofollow noreferrer\">https:\/\/console.cloud.google.com\/vertex-ai\/locations\/europe-west4\/models\/\/#####\/versions\/1\/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=\/######<\/a><\/p>\n<p>Sincerely,\nThe Google Cloud AI TeamHello Vertex AI Customer,<\/p>\n<p>Does anyone have an idea why the endpoint deployment suddely does not work anymore?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653938723073,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":217,
        "Owner_creation_time":1653938429847,
        "Owner_last_access_time":1659630685180,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72439167",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71896741,
        "Question_title":"Adding label in AutoML for text classification",
        "Question_body":"<p>I am trying to create a text dataset in a <code>Pipeline<\/code> for a text classification but I believe I am doing it the wrong way or at least I don't get it. The csv passing only contains two columns <code>message<\/code> and <code>label<\/code> which is true or false.<\/p>\n<p>Inside my pipeline I am creating dataset like this which I am not very sure how dataset is recognizing that column <code>label<\/code> is the independent variable.<\/p>\n<pre><code>dataset = gcp_aip.TextDatasetCreateOp(\n    project = project # my project id,\n    display_name = display_name # reference name,\n    gcs_source  = src_uris # path to my data in gcs,\n    import_schema_uri = aiplatform.schema.dataset.ioformat.text.single_label_classification, \n)\n<\/code><\/pre>\n<p>once created the dataset, i do training like this within the <code>Pipeline<\/code><\/p>\n<pre><code># training\nmodel = gcp_aip.AutoMLTextTrainingJobRunOp(\n    project = project,\n    display_name = display_name,\n    prediction_type = &quot;classification&quot;,\n    multi_label = False,   \n    dataset = dataset.outputs[&quot;dataset&quot;],\n)\n<\/code><\/pre>\n<p>Not sure if creation and training is doing correctly since I never specified that <code>label<\/code> is my label column and needs to use <code>message<\/code> as a feature.<\/p>\n<p>In vertex ai the dataset created look like this<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Puts.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Puts.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But in my training section the results from the AutML, looks like this, dont know why, label with 0% is there, which makes me doubt about the insertion of the data<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650136092427,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|text|vertex|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_time":1505748144073,
        "Owner_last_access_time":1663921056407,
        "Owner_location":null,
        "Owner_reputation":1102,
        "Owner_up_votes":108,
        "Owner_down_votes":1,
        "Owner_views":140,
        "Question_last_edit_time":1650356093117,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71896741",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73661090,
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":62,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663223342093,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73351866,
        "Question_title":"Vertex AI pipelines import custom modules",
        "Question_body":"<p>I'm developing a complex pipeline in Vertex AI using Pipelines and components. I would like to import some custom modules and functions I developed for this use case.\nUnfortunately, I cannot figure out how to import those custom functions in my code without creating ad-hoc Docker images or without publishing my code on public repositories like PyPi.<\/p>\n<p>There are two pain points in pasting those custom functions' code in each component:<\/p>\n<ol>\n<li>The code becomes huge and difficult read<\/li>\n<li>The function's code completely loses the maintenability because at each small change, I have to replicate it for each component.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660482276650,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":132,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73351866",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72732651,
        "Question_title":"how to configure proxy in .Net application",
        "Question_body":"<p>I am facing problem while calling <strong>gcp api's (Vertex Ai &amp; Bigquery)<\/strong>. we have been using these api's from last few months. The behavior is unpredictable, sometimes we get api response successfully and sometimes its failing.<\/p>\n<p>what we noticed that the same host <strong>us-central1-aiplatform.googleapis.com<\/strong> communication is disrupted.<\/p>\n<p>Currently we are connecting with <strong>GOOGLE_APPLICATION_CREDENTIALS<\/strong>.<\/p>\n<p>I want to go through proxy. any suggestions ? or any other way I can solve this ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655997589337,
        "Question_score":1,
        "Question_tags":"c#|.net|google-cloud-platform|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732651",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72591967,
        "Question_title":"Vertex workbench - how to run BigQueryExampleGen in Jupyter notebook",
        "Question_body":"<h1>Problem<\/h1>\n<p>Tried to run <a href=\"https:\/\/www.tensorflow.org\/tfx\/api_docs\/python\/tfx\/v1\/extensions\/google_cloud_big_query\/BigQueryExampleGen\" rel=\"nofollow noreferrer\">BigQueryExampleGen<\/a> in the<\/p>\n<pre><code>InvalidUserInputError: Request missing required parameter projectId [while running 'InputToRecord\/QueryTable\/ReadFromBigQuery\/Read\/SDFBoundedSourceReader\/ParDo(SDFBoundedSourceDoFn)\/SplitAndSizeRestriction']\n<\/code><\/pre>\n<h2>Steps<\/h2>\n<p><a href=\"https:\/\/www.tensorflow.org\/tfx\/api_docs\/python\/tfx\/v1\/extensions\/google_cloud_big_query\/BigQueryExampleGen\" rel=\"nofollow noreferrer\">BigQueryExampleGen<\/a>\nSetup the GCP project and the interactive TFX context.<\/p>\n<pre><code>import os\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = &quot;path_to_credential_file&quot;\n\n\nfrom tfx.v1.extensions.google_cloud_big_query import BigQueryExampleGen\nfrom tfx.v1.components import (\n    StatisticsGen,\n    SchemaGen,\n)\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\ncontext = InteractiveContext(pipeline_root='.\/data\/artifacts')\n<\/code><\/pre>\n<p>Run the BigqueryExampleGen.<\/p>\n<pre><code>query = &quot;&quot;&quot;\nSELECT \n    * EXCEPT (trip_start_timestamp, ML_use)\nFROM \n    {PROJECT_ID}.public_dataset.chicago_taxitrips_prep\n&quot;&quot;&quot;.format(PROJECT_ID=PROJECT_ID)\n\nexample_gen = context.run(\n    BigQueryExampleGen(query=query)\n)\n<\/code><\/pre>\n<p>Got the error.<\/p>\n<pre><code>InvalidUserInputError: Request missing required parameter projectId [while running 'InputToRecord\/QueryTable\/ReadFromBigQuery\/Read\/SDFBoundedSourceReader\/ParDo(SDFBoundedSourceDoFn)\/SplitAndSizeRestriction']\n<\/code><\/pre>\n<h1>Data<\/h1>\n<p>See <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">mlops-with-vertex-ai\/01-dataset-management.ipynb<\/a> to setup the BigQuery dataset for CThe Chicago Taxi Trips dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655034967900,
        "Question_score":0,
        "Question_tags":"google-bigquery|google-cloud-vertex-ai|tfx|tensorflow-extended",
        "Question_view_count":125,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72591967",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71885282,
        "Question_title":"Constructing a Vertex AI Pipeline with a custom training container and a model serving container",
        "Question_body":"<p>I'd like to be able to train a model with a training app container that I've made and saved to my artifact registry. I want to be able to deploy a model with a flask app and with a \/predict route that can handle some logic -- not necessarily just predicting an input json. It'll also need a \/healthz route I understand. So basically I want a pipeline that performs a training job on a model training container that I make, and deploys the model with a flask app with a model serving container that I make. Looking around on Overflow, I wonder if <a href=\"https:\/\/stackoverflow.com\/questions\/68075940\/vertex-pipeline-custompythonpackagetrainingjobrunop-not-supplying-workerpoolspe\">this<\/a> question's pipeline has the correct layout I'll eventually want to have. So, something like this:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline():\n        training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n            project=project_id,\n            display_name=training_job_name,\n            model_display_name=model_display_name,\n            python_package_gcs_uri=python_package_gcs_uri,\n            python_module=python_module,\n            container_uri=container_uri,\n            staging_bucket=staging_bucket,\n            model_serving_container_image_uri=model_serving_container_image_uri)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project_id,\n            display_name=model_display_name,\n            artifact_uri=output_dir,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n        model_deploy_op = gcc_aip.ModelDeployOp(\n            project=project_id,\n            model=model_upload_op.outputs[&quot;model&quot;],\n            endpoint=aiplatform.Endpoint(\n                endpoint_name='0000000000').resource_name,\n            deployed_model_display_name=model_display_name,\n            machine_type=&quot;n1-standard-2&quot;,\n            traffic_percentage=100)\n\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_spec_path)\n<\/code><\/pre>\n<p>I'm hoping that <code>model_serving_container_image_uri<\/code> and <code>serving_container_image_uri<\/code> both refer to the URI for the model serving container I'm going to make. I've already made a training container that trains a model and saves <code>saved_model.pb<\/code> to Google Cloud Storage. Other than having a flask app that handles the prediction and health check routes and a Dockerfile that exposes a port for the flask app, what else will I need to do to ensure the model serving container works in this pipeline? Where in the code do I install the model from GCS? In the Dockerfile? How is the model serving container meant to work so that everything will go swimmingly in the construction of the pipeline? I'm having trouble finding any tutorials or examples of precisely what I'm trying to do anywhere even though this seems like a pretty common scenario.<\/p>\n<p>To that end, I attempted this with the following pipeline:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline(\n        project: str = [redacted project ID],\n        display_name: str = &quot;custom-pipe&quot;,\n        model_display_name: str = &quot;test_model&quot;,\n        training_container_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-training-test&quot;,\n        model_serving_container_image_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-model-serving-test&quot;,\n        model_serving_container_predict_route: str = &quot;\/predict&quot;,\n        model_serving_container_health_route: str = &quot;\/healthz&quot;,\n        model_serving_container_ports: str = &quot;8080&quot;\n):\n        training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n            display_name = display_name,\n            container_uri=training_container_uri,\n            model_serving_container_image_uri=model_serving_container_image_uri,\n            model_serving_container_predict_route = model_serving_container_predict_route,\n            model_serving_container_health_route = model_serving_container_health_route,\n            model_serving_container_ports = model_serving_container_ports)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project,\n            display_name=model_display_name,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n#        model_deploy_op = gcc_aip.ModelDeployOp(\n#            project=project,\n#            model=model_upload_op.outputs[&quot;model&quot;],\n#            endpoint=aiplatform.Endpoint(\n#                endpoint_name='0000000000').resource_name,\n#            deployed_model_display_name=model_display_name,\n#            machine_type=&quot;n1-standard-2&quot;,\n#            traffic_percentage=100)\n<\/code><\/pre>\n<p>Which is failing with<\/p>\n<pre><code>google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.trainingPipelines.create' denied on resource '\/\/aiplatform.googleapis.com\/projects\/u15c36a5b7a72fabfp-tp\/locations\/us-central1' (or it may not exist).\n<\/code><\/pre>\n<p>Despite the fact that my service account has the Viewer and Kubernetes Engine Admin roles needed to work AI Platform pipelines. My training container uploads my model to Google Cloud Storage and my model serving container I've made downloads it and uses it for serving at <code>\/predict<\/code>.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650032900060,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":421,
        "Owner_creation_time":1649861924490,
        "Owner_last_access_time":1663770236590,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1650387987397,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71885282",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71813169,
        "Question_title":"Run a Vertex AI model locally",
        "Question_body":"<p>Using the Vertex AI product at GCP training was very easy, I uploaded a data set and it returned a model which is saved in a gcp bucket, I downloaded the files and the tree has these files<\/p>\n<pre><code>\u251c\u2500\u2500 environment.json\n\u251c\u2500\u2500 feature_attributions.yaml\n\u251c\u2500\u2500 final_model_structure.pb\n\u251c\u2500\u2500 instance.yaml\n\u251c\u2500\u2500 predict\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 001\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 PVC_vocab\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets.extra\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tf_serving_warmup_requests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 saved_model.pb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 variables\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 variables.index\n\u251c\u2500\u2500 prediction_schema.yaml\n\u251c\u2500\u2500 tables_server_metadata.pb\n\u2514\u2500\u2500 transformations.pb\n<\/code><\/pre>\n<p>I would like to serve this model locally from a dockerized python application, but I don't know enough TF to do this and I am very confused about which <code>.pb<\/code> file is the actual one that has the neural network I need.<\/p>\n<p>Thanks for any tips.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1649554815260,
        "Question_score":0,
        "Question_tags":"python|docker|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":261,
        "Owner_creation_time":1483057875593,
        "Owner_last_access_time":1664081219423,
        "Owner_location":"St. Louis, MO, USA",
        "Owner_reputation":710,
        "Owner_up_votes":43,
        "Owner_down_votes":16,
        "Owner_views":175,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71813169",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72593351,
        "Question_title":"Jupyter Lab instance crashes with 502 error",
        "Question_body":"<p>I am using a JupyterLab virtual notebook instance from GCP Vertex AI Workbench.<\/p>\n<p>I am reading 2 billion rows of data where each row is comprised of 3 columns of\n8 bytes each.<\/p>\n<p>I am reading 100 million rows of data at a time and concatenating it to Pandas dataframe.<\/p>\n<p>All of sudden, the notebook becomes unresponsive with 502 error.<\/p>\n<p>I realize that the virtual machine crashed.<\/p>\n<p>Here is the spec to the virtual machine:\nn1-standard 64  240GB RAM\n100 GB drive<\/p>\n<p>One time, I was successful to reach 2 billion rows.\nBut all of sudden, to my dismay, it crashed with that error.<\/p>\n<p>Google doc just mentions to restart the kernel.\nThat is not so easy when it took more than 1 hour to read 2 billion rows of data.\nThis means more than 1 hour of work just got wasted.<\/p>\n<p>What is causing this error?\nWhy the error occurs so inconsistently?\nWhere is the error message for this to crash?\nOr is this an error related to pandas dataframe?\nI am creating a dataframe that have 2 billion rows.\nIf pandas cannot handle rows of this magnitude, it should simply\ncause a run time error, not crashing a virtual machine.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655046019523,
        "Question_score":1,
        "Question_tags":"pandas|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_time":1468198346110,
        "Owner_last_access_time":1664078356887,
        "Owner_location":"overland park, kansas",
        "Owner_reputation":932,
        "Owner_up_votes":192,
        "Owner_down_votes":10,
        "Owner_views":367,
        "Question_last_edit_time":1655255619327,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593351",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73360734,
        "Question_title":"Programmatically enable installed extensions in Vertex AI Managed Notebook instance",
        "Question_body":"<p>I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. When the instance is created, there are a number of JupyterLab extensions that are installed by default. In the web GUI, one can click the puzzle piece icon and enable\/disable all extensions with a single button click. I currently run a post-startup bash script to manage environments and module installations, and I would like to add to this script whatever commands would turn on the existing extensions. My understanding is that I can do this with<\/p>\n<pre><code># Status of extensions\njupyter labextension list\n# Enable\/disable some extension\njupyter labextension enable extensionIdentifierHere\n<\/code><\/pre>\n<p>However, when I test the enable\/disable command in an instance Terminal window, I receive, for example<\/p>\n<pre><code>[Errno 13] Permission denied: '\/opt\/conda\/etc\/jupyter\/labconfig\/page_config.json'\n<\/code><\/pre>\n<p>If I try to run this with <code>sudo<\/code>, I am asked for a password, but have no idea what that would be, given that I just built the environment and didn't set any password.<\/p>\n<p>Any insights on how to set this up, what the command(s) may be, or how else to approach this, would be appreciated.<\/p>\n<p>Potentially relevant:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/65950610\/not-able-to-install-jupyterlab-extensions-on-gcp-ai-platform-notebooks\">Not able to install Jupyterlab extensions on GCP AI Platform Notebooks<\/a><\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52753205\/unable-to-sudo-to-deep-learning-image\">Unable to sudo to Deep Learning Image<\/a><\/p>\n<p><a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions\" rel=\"nofollow noreferrer\">https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions<\/a><\/p>\n<p>Edit 1:\nAdding more detail in response to answers and comments (@gogasca, @kiranmathew). My goal is to use ipyleaft-based mapping, through the geemap and earthengine-api python modules, within the notebook. If I create a Managed Notebook instance (service account, Networks shared with me, Enable terminal, all other defaults), launch JupyterLab, open the Terminal from the Launcher, and then run a bash script that creates a venv virtual environment, exposes a custom kernel, and performs the installations, I can use geemap and ipywidgets to visualize and modify (e.g., widget sliders that change map properties) Google Earth Engine assets in a Notebook. If I try to replicate this using a Docker image, it seems to break the connection with ipyleaflet, such that when I start the instance and use a Notebook, I have access to the modules (they can be imported) but can't use ipyleaflet to do the visualization. I thought the issue was that I was not properly enabling the extensions, per the &quot;Error displaying widget: model not found&quot; error, addressed in <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">this<\/a>, etc. -- hence the title of my post. I tried using and modifying @TylerErickson 's Dockerfile that modifies a Google deep learning container and should handle all of this (<a href=\"https:\/\/github.com\/gee-community\/ee-jupyter-contrib\/blob\/master\/docker\/gcp_ai_deep_learning_platform\/Dockerfile\" rel=\"nofollow noreferrer\">here<\/a>), but both the original and modifications break the ipyleaflet connection when booting the Managed Notebook instance from the Docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660564868170,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":202,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1660731534620,
        "Answer_body":"<p>Google Managed Notebooks do not support third-party JL extensions.  Most of these extensions require a rebuild of the JupyterLab static assets bundle. This requires root access which our Managed Notebooks do not support.<\/p>\n<p>Untangling this limitation would require a significant change to the permission and security model that Managed Notebooks provides. It would also have implications for the supportability of the product itself since a user could effectively break their Managed Notebook by installing something rogue.<\/p>\n<p>I would suggest to use User Managed Notebooks.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1660628912063,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660634341887,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73360734",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70428593,
        "Question_title":"Vertex AI seems to think a deployed model input shape is different then when predicting locally",
        "Question_body":"<p>I'm getting what I see as strange behavior out of a deployed model in vertex ai.  I have a CNN model built with tensorflow\/keras version 2.7.  My input data is a 3 dimensional array with the follow shape (1, 570, 33).  When I pass the input data to the model locally I have a correct response.<\/p>\n<pre><code>model = keras.models.load_model('model')\nx = model.predict(input_data) # input_data is a numpy array of shape (1, 570, 33)   \nprint(x)\n[[0.1259355  0.9124526  0.65782744 0.2628207 ]]\n<\/code><\/pre>\n<p>This is a correct prediction and the model does what it is trained to do.  No problems<\/p>\n<p>When I upload the model to Vertex AI using the prebuilt Tensorflow 2.7 docker container with no extra settings (no acceleration for example) and deploy that model to an endpoint this is what I get when I call predict with the same input_data formatted for Vertex AI.<\/p>\n<pre><code>resp = client.predict(\n    endpoint=endpoint_path,\n    instances=input_data.toList(),\n    parameters=parameters,\n)\n\ninput must be 4-dimensional[1,570,33]\\n\\t [[{{function_node __inference__wrapped_model_28143}}{{node sequential\/conv2d\/BiasAdd}}]]\n<\/code><\/pre>\n<p>Here is the summary of of the model<\/p>\n<pre><code>Model: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)             (None, 570, 33, 32)       320       \n                                                                \nbatch_normalization (BatchN  (None, 570, 33, 32)      128       \normalization)                                                   \n                                                                \nactivation (Activation)     (None, 570, 33, 32)       0         \n                                                                \nconv2d_1 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_1 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_1 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_2 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_2 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_2 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_3 (Conv2D)           (None, 285, 17, 64)       18496     \n                                                                \nbatch_normalization_3 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_3 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_4 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_4 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_4 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_5 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_5 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_5 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_6 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_6 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_6 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_7 (Conv2D)           (None, 143, 9, 96)        55392     \n                                                                \nbatch_normalization_7 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_7 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_8 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_8 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_8 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_9 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_9 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_9 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_10 (Conv2D)          (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_10 (Bat  (None, 143, 9, 96)       384       \nchNormalization)                                                \n                                                                \nactivation_10 (Activation)  (None, 143, 9, 96)        0         \n                                                                \nconv2d_11 (Conv2D)          (None, 72, 5, 128)        110720    \n                                                                \nbatch_normalization_11 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_11 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_12 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_12 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_12 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_13 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_13 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_13 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_14 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_14 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_14 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_15 (Conv2D)          (None, 36, 3, 160)        184480    \n                                                                \nbatch_normalization_15 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_15 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_16 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_16 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_16 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_17 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_17 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_17 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_18 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_18 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_18 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_19 (Conv2D)          (None, 18, 2, 192)        276672    \n                                                                \nbatch_normalization_19 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_19 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_20 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_20 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_20 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_21 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_21 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_21 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_22 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_22 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_22 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_23 (Conv2D)          (None, 9, 1, 224)         387296    \n                                                                \nbatch_normalization_23 (Bat  (None, 9, 1, 224)        896       \nchNormalization)                                                \n                                                                \nactivation_23 (Activation)  (None, 9, 1, 224)         0         \n                                                                \nreshape (Reshape)           (None, 9, 224)            0         \n                                                                \nmasking (Masking)           (None, 9, 224)            0         \n                                                                \nlambda (Lambda)             (None, 224)               0         \n                                                                \ndense (Dense)               (None, 4)                 900       \n                                                                \n=================================================================\nTotal params: 3,554,532\nTrainable params: 3,548,772\nNon-trainable params: 5,760\n<\/code><\/pre>\n<p>I've got a classic case of 'It works on my machine' here and could use any input or help :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640038358063,
        "Question_score":1,
        "Question_tags":"python|numpy|tensorflow|keras|google-cloud-vertex-ai",
        "Question_view_count":178,
        "Owner_creation_time":1321330022043,
        "Owner_last_access_time":1663959945940,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":1640046263673,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70428593",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71755724,
        "Question_title":"Cannot use tensorboard with Vertex AI Custom job",
        "Question_body":"<p>I'm trying to launch a custom training job using Vertex AI through <a href=\"https:\/\/github.com\/deepmind\/xmanager\" rel=\"nofollow noreferrer\">XManager<\/a>. When running Custom jobs with tensorboard enabled I get a tensorboard instance in <code>experiments -&gt; tensorboard instances<\/code> and a button on the custom job page that says <code>OPEN TENSORBOARD<\/code>. However, this leads to an empty page that says <code>Not found: TensorboardExperiment<\/code>.<\/p>\n<ul>\n<li>I observed this behaviour when running my own custom job and when running XManager's example <a href=\"https:\/\/github.com\/deepmind\/xmanager\/tree\/main\/examples\/cifar10_tensorflow\" rel=\"nofollow noreferrer\">cifar10_tensorflow<\/a>. Note that in both cases the job runs to completion without problems.<\/li>\n<li>I can visualise the logs locally via the standard tensorboard package and passing as <code>log_dir<\/code> the cloud storage directory containing the experiments logs.<\/li>\n<li>I can upload experiment logs to Vertex AI tensorboard manually using<\/li>\n<\/ul>\n<pre><code>tb-gcp-uploader --tensorboard_resource_name \\\n  TENSORBOARD_INSTANCE_NAME \\\n  --logdir=LOG_DIR \\\n  --experiment_name=TB_EXPERIMENT_NAME --one_shot=True\n<\/code><\/pre>\n<ul>\n<li>For more details check out the discussion: <a href=\"https:\/\/github.com\/deepmind\/xmanager\/issues\/15\" rel=\"nofollow noreferrer\">https:\/\/github.com\/deepmind\/xmanager\/issues\/15<\/a><\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1649178281173,
        "Question_score":1,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1451919318690,
        "Owner_last_access_time":1661428260770,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1649180370237,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71755724",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71941672,
        "Question_title":"How to build custom pipeline in GCP using Vertex AI",
        "Question_body":"<p>I was exploring the vertex AI AutoML feature in GCP, which lets users import datasets, train, deploy and predict ML models. My use case is to do the data pre-processing on my own (I didn't get satisfied with AutoML data preprocessing) and want to feed that data directly to a pipeline where it trains and deploys the model.\nAlso, I want to feed the new data to the dataset. It should take care of the entire pipeline (from data preprocessing to deploying the latest model).\nI want insight as to how to approach this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1650465785530,
        "Question_score":2,
        "Question_tags":"machine-learning|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training",
        "Question_view_count":264,
        "Owner_creation_time":1549305339163,
        "Owner_last_access_time":1664020879733,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":454,
        "Owner_up_votes":43,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Question_last_edit_time":1650551747143,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71941672",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68998065,
        "Question_title":"Read vertex ai datasets in jupyter notebook",
        "Question_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1630410227747,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":737,
        "Owner_creation_time":1616070829583,
        "Owner_last_access_time":1643884535477,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631634873007,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73033284,
        "Question_title":"How to extract model file(s) from Vertex AI?",
        "Question_body":"<p>I have been trying to import model file(s) from Vertex AI to Workbench environment. My pipeline consists of preprocessing, training and batch predictions. Sometimes the training fails due to unknown reasons yet I want the batch predictions from the latest model in that case. Is there a way to access the trained models using Python?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658217944860,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":34,
        "Owner_creation_time":1649934777040,
        "Owner_last_access_time":1663995816517,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1658221416540,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73033284",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72481717,
        "Question_title":"Google AutoML Video Tracking Architecture",
        "Question_body":"<p>I'm developing an object tracking system using Google's Vertex AI AutoML Video Tracking. We currently have an accurate model that identifies objects per frame (as a picture) and I'm exploring models that may be able to gain further insight and accuracy by using a collection of frames (video) for the classification and tracking purposes. I want to learn more about the architecture used in the AutoML Object Tracking, but all I can find is articles hyping up the dynamic nature of the architecture. Mainly, I'm trying to answer the following 3 questions:<\/p>\n<ol>\n<li>What methods does the AutoML Object Tracking use to classify the objects and track them? Are the classifications done frame to frame, with a Euclidean distance tracker mapping objects together? Or are the objects identified and classified across multiple frames a recurrent network in space (image) and time (frame to frame). Something like a LSTM.<\/li>\n<li>What performance can object tracking in AutoML achieve that is better than their image object identification models?<\/li>\n<li>Where can I go to learn more about the model architectures on Vertex AI? It's hard to know which google publications are associated with their current platform.<\/li>\n<\/ol>\n<p>Any feedback is greatly appreciated!!!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654200034790,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|google-cloud-automl|google-cloud-vertex-ai|object-tracking",
        "Question_view_count":27,
        "Owner_creation_time":1654199427997,
        "Owner_last_access_time":1654825517947,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72481717",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72746403,
        "Question_title":"How to call a Google Vertex AI endpoint with an Api Key with c# and curl",
        "Question_body":"<p>from a c# program i want to call my <strong>vertex ai endpoint<\/strong> for prediction with an <strong>api key<\/strong> (via <strong>&quot;Google Cloud&quot;\/Credentials\/API Keys&quot;<\/strong> )). I gave the api key access to Vertex AI and as a test everything else too.<\/p>\n<p>calling it with curl or c# i get the error that the service expects an &quot;OAuth 2 access token &quot; or something else.<\/p>\n<p><strong>CURL:<\/strong>\ncurl -X POST\n-H &quot;apikey=..mykey...&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/.....googleapis.com\/v1\/projects\/[PROJECT_ID]\/locations\/europe-west4\/endpoints\/[ENDPOINT_ID]:predict\n-d @image.json<\/p>\n<p><strong>ERROR:<\/strong>\n&quot;error&quot;: {\n&quot;code&quot;: 401,\n&quot;message&quot;: &quot;Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. ...\n}<\/p>\n<p><strong>MY QUESTION:<\/strong>\nIs there a way to use the Apikey for authentication to vertex ai?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656085117850,
        "Question_score":0,
        "Question_tags":"c#|authentication|curl|api-key|google-cloud-vertex-ai",
        "Question_view_count":229,
        "Owner_creation_time":1656083586047,
        "Owner_last_access_time":1662660715730,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72746403",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69721067,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1635242363740,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-api-python-client|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":357,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1636446864247,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1636364178673,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1638471768683,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69316032,
        "Question_title":"Custom Container deployment in vertex ai",
        "Question_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632490921653,
        "Question_score":3,
        "Question_tags":"flask|dockerfile|google-cloud-vertex-ai",
        "Question_view_count":629,
        "Owner_creation_time":1631092954063,
        "Owner_last_access_time":1652934780303,
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1632806171387,
        "Answer_score":4.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1632810969573,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69316032",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72254372,
        "Question_title":"GCP's Vertex AI(AI Platform) PipelineServiceClient gives unimplemented error",
        "Question_body":"<p>When trying to list pipelines with <code>PipelineServiceClient<\/code> <code>list_pipeline_jobs<\/code> method as given <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.services.pipeline_service.PipelineServiceClient#google_cloud_aiplatform_v1_services_pipeline_service_PipelineServiceClient_list_pipeline_jobs\" rel=\"nofollow noreferrer\">here<\/a>, I get the following error:<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNIMPLEMENTED\ndetails = &quot;Received http2 header with status: 404&quot;\n...\n<\/code><\/pre>\n<p>How is the API unimplemented, how do I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652677608807,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_time":1550380195090,
        "Owner_last_access_time":1663747273577,
        "Owner_location":null,
        "Owner_reputation":434,
        "Owner_up_votes":106,
        "Owner_down_votes":34,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72254372",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73078858,
        "Question_title":"VERTEX AI, BATCH PREDICTION",
        "Question_body":"<p>Does anyone know how I can choose the option &quot;Files on Cloud Storage (file list) in the program? How can I [enter image description here choose the list format?<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/aIBA9.png\" alt=\"1\" \/><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658485663377,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_time":1658484570563,
        "Owner_last_access_time":1658815412120,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1658603284180,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73078858",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70915437,
        "Question_title":"Use a model trained on image to detect objects in videos?",
        "Question_body":"<p>Using Google Vertex AI, I trained a model to detect some specific objects in images.<\/p>\n<p>Can i use this trained model to detect sames objects, but in videos ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643551863927,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":49,
        "Owner_creation_time":1427410714920,
        "Owner_last_access_time":1647194808140,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70915437",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70940773,
        "Question_title":"Misstated quota exceed errors on managed notebooks in GCP",
        "Question_body":"<p>I am migrating some of my notebooks from the soon to be deprecated AI-Platform to the new Vertex platform in GCP. In Vertex I am using the &quot;Managed Notebooks&quot;, and all seemed to be working fine, but then suddenly I got this strange quota exceed error, when I am still far below my limits (I am currently at 4-5% of limit for the given APIs). See the error below:<\/p>\n<p>*Restarting notebook prototyping-notebook: Quota of &quot;::internal: operation &quot;projects\/1096432937575\/locations\/us-central1\/operations\/start-92b393a0-6c2e-4e11-be32-0172418d33c11643717891501967646&quot; completed with error: %!w(<em>status.Status=&amp;{{{} [] [] } 13 INTERNAL: operation name: operation-1643717891545-5d6f3e5095a3c-9aa08cdd-4bd8b9cd error code: QUOTA_EXCEEDED error message: Quota &quot; exceeded limit: 40 in region us-central1.<\/em><\/p>\n<p>I know it is still in preview, but anyone faced the same error or have any experience in how to fix this? Is there some combination of configurations that seems to be more stable than others?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1643719337910,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_time":1445972391513,
        "Owner_last_access_time":1663852274613,
        "Owner_location":null,
        "Owner_reputation":181,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70940773",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68331232,
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625948141620,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1455786472727,
        "Owner_last_access_time":1663964690547,
        "Owner_location":"Varna, Bulgaria",
        "Owner_reputation":405,
        "Owner_up_votes":36,
        "Owner_down_votes":1,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626781802423,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71351821,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1646398826330,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":1298,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646582537360,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73529912,
        "Question_title":"vertex ai pipeline: label not populating on billing",
        "Question_body":"<p>I'm trying to trace how much each pipeline I run on vertex costs. I read about adding labels that lets me filter my billing report based on the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/creating-managing-labels\" rel=\"nofollow noreferrer\">labels<\/a>.<\/p>\n<p>It says that vertex ai is supported and the api shows the same with a labels kwarg.<\/p>\n<pre><code>job = aiplatform.PipelineJob(display_name = 'inference',\n                                    template_path = tmpdirname + '\/' + &quot;inference.json&quot;,\n                                    enable_caching = True,\n                                    project = 'project id',\n                                    location = &quot;europe-west4&quot;,    \n                                    parameter_values=params,\n                                    credentials=service_account.Credentials.from_service_account_file('service.json'),\n                                    labels={'pipeline':job_id}\n<\/code><\/pre>\n<p>The pipeline starts and runs through without issue. The label is on the job and I can within the vertex AI pipelines console filter for the job as well. On the billing dashboard it still doesn't exist, when I export the data to bigquery it doesn't exist, I can see the cost for those pipelines I ran but I cannot see those labels and filter on them.<\/p>\n<p>has anyone managed to get the label filter to work for vertex ai so that you can see the cost of a pipeline job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661781168010,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":42,
        "Owner_creation_time":1634554256497,
        "Owner_last_access_time":1663952930450,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73529912",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68793294,
        "Question_title":"How to schedule repeated runs of a custom training job in Vertex AI",
        "Question_body":"<p>I have packaged my training code as a python package and then am able to run it as a custom training job on Vertex AI. Now, I wanted to be able to schedule this job to run, say every 2 weeks, and re-train the model. The Scheduling settings in the CustomJoBSpec allow only 2 fields, &quot;timeout&quot; and &quot;restartJobOnWorkerRestart&quot; so it's not possible using the scheduling settings in the CustomJobSpec. One way to achieve this I could think of was to create a Vertex AI pipeline with a single step using the &quot;CustomPythonPackageTrainingJobRunOp&quot; Google Cloud Pipeline Component and then scheduling the pipeline to run as I see fit. Are there better alternatives to achieve this?<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>I was able to schedule the custom training job using Cloud Scheduler, but I found using the create_schedule_from_job_spec method in the AIPlatformClient very easy to use in the Vertex AI pipeline. The steps I took to schedule the custom job using Cloud Scheduler in gcp are as follows, <a href=\"https:\/\/cloud.google.com\/scheduler\/docs\/http-target-auth#setting_up_the_service_account\" rel=\"nofollow noreferrer\">link<\/a> to google docs:<\/p>\n<ol>\n<li>Set target type to HTTP<\/li>\n<li>For the url to specify the custom job, I followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#curl\" rel=\"nofollow noreferrer\">this<\/a> link to get the url<\/li>\n<li>For the authentication, under Auth header, I selected the &quot;Add OAauth token&quot;<\/li>\n<\/ol>\n<p>You also need to have a &quot;Cloud Scheduler service account&quot; with  a &quot;Cloud Scheduler Service Agent role granted to it&quot; in your project. Although the docs ay this should have been set up automatically if you enabled the Cloud Scheduler API after March 19, 2019, this was not the case for me and had to add the service account with the role manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1629043963683,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":2669,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1629147961557,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68793294",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69267589,
        "Question_title":"Setting test\/train column Google AutoML",
        "Question_body":"<p>I have written some code to train a model using Google's AutoML in VertexAI. A nuance of my problem is that I need to set the test train column manually. The documentation for the method, set_test_train_column, says:<\/p>\n<pre><code>&quot;&quot;&quot;Sets the test\/train (ml_use) column which designates which data\nbelongs to the test and train sets. This column must be categorical.&quot;&quot;&quot;\n<\/code><\/pre>\n<p>My test\/train column is called 'set' and consists of three values, namely, 'TEST', 'TRAIN', and 'VALIDATE'. The dtype of this column is 'object' where each cell takes a string value. I have included 'VALIDATE' as this is required when setting the test\/train column when training a model in the automl section of VertexAI.<\/p>\n<p>The piece of code that implements this is:<\/p>\n<pre><code>dataset_display_name = 'dataset_1'\ntable_client.set_test_train_column(dataset_display_name=dataset_display_name,\n                                   column_spec_display_name='set')\n<\/code><\/pre>\n<p>Could someone please help me understand whether I have implemented this code correctly? Are the values in the test\/train column written correctly? Does it matter that I have not used pd.Categorical to explicitly say that the test\/train column is categorical? Should 'VALIDATE' be included in the test\/train column?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1632221342757,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69267589",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73811793,
        "Question_title":"How do I undeploy a model from an endpoint without knowing its id in Vertex AI?",
        "Question_body":"<p>I have managed to undeploy a model from an endpoint using <code>UndeployModelRequest<\/code>:<\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)\n    deployed_models_info = model_info.deployed_models\n    deployed_model_id=model_info.deployed_models[0].deployed_model_id       \n    \n    undeploy_request = aiplatform_v1.types.UndeployModelRequest\n                       (endpoint=end_point, deployed_model_id=deployed_model_id)\n\n    client.undeploy_model(request=undeploy_request)\n<\/code><\/pre>\n<p>but all this depends on knowing <code>model_id<\/code>. I want to be able to just undeploy a model from an endpoint without knowing the model's id (there will only be one model per endpoint ever). Is that possible or can I get the model id from the endpoint somehow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663836585407,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":14,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73811793",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70729582,
        "Question_title":"Batch prediction Input",
        "Question_body":"<p>I have a tensorflow model deployed on Vertex AI of Google Cloud. The model definition is:<\/p>\n<pre><code>item_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=item_vocab, mask_token=None),\n  tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dim)\n])\n\nuser_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=user_vocab, mask_token=None),\n  # We add an additional embedding to account for unknown tokens.\n  tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dim)\n])\n\n\nclass NCF_model(tf.keras.Model):\n    def __init__(self,user_model, item_model):\n        super(NCF_model, self).__init__()\n        # define all layers in init\n        \n        self.user_model = user_model\n        self.item_model  = item_model\n        self.concat_layer   = tf.keras.layers.Concatenate()\n        self.feed_forward_1 = tf.keras.layers.Dense(32,activation= 'relu')\n        self.feed_forward_2 = tf.keras.layers.Dense(64,activation= 'relu')\n        self.final = tf.keras.layers.Dense(1,activation= 'sigmoid')\n\n\n    def call(self, inputs ,training=False):\n        user_id , item_id = inputs[:,0], inputs[:,1]\n        x = self.user_model(user_id)\n        y = self.item_model(item_id)\n\n        x = self.concat_layer([x,y])\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = self.final(x)\n\n\n        return x\n<\/code><\/pre>\n<p>The model has two string inputs and it outputs a probability value.\nWhen I use the following input in the batch prediction file, I get an empty prediction file.\nSample of csv input file:<\/p>\n<pre><code>userid,itemid\nyuu,190767\nyuu,364\nyuu,154828\nyuu,72998\nyuu,130618\nyuu,183979\nyuu,588\n<\/code><\/pre>\n<p>When I use a jsonl file with the following input.<\/p>\n<pre><code>{&quot;input&quot;:[&quot;yuu&quot;, &quot;190767&quot;]}\n<\/code><\/pre>\n<p>I get the following error.<\/p>\n<pre><code>('Post request fails. Cannot get predictions. Error: Exceeded retries: Non-OK result 400 ({\\n    &quot;error&quot;: &quot;Failed to process element: 0 key: input of \\'instances\\' list. Error: INVALID_ARGUMENT: JSON object: does not have named input: input&quot;\\n}) from server, retry=3.', 1)\n<\/code><\/pre>\n<p>What seems to be going wrong with these inputs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642332344157,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":323,
        "Owner_creation_time":1429532538823,
        "Owner_last_access_time":1664028982667,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70729582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72251787,
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1652644857243,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push",
        "Question_view_count":5722,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652667678670,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1652683203067,
        "Answer_score":23.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654468583460,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73638322,
        "Question_title":"Vertex AI Pipelines (Kubeflow) skip step with dependent outputs on later step",
        "Question_body":"<p>I\u2019m trying to run a Vertex AI Pipelines job where I skip a certain pipeline step if the value of a certain pipeline parameter (in this case <code>do_task1<\/code>) is <code>False<\/code>. But because there is another step that runs unconditionally and expects the output of the first potentially skipped step, I get the following error, independently of do_task1 being <code>True<\/code> or <code>False<\/code>:<\/p>\n<pre><code>AssertionError: component_input_artifact: pipelineparam--task1-output_path not found. All inputs: parameters {\n  key: &quot;do_task1&quot;\n  value {\n    type: STRING\n  }\n}\nparameters {\n  key: &quot;task1_name&quot;\n  value {\n    type: STRING\n  }\n}\n<\/code><\/pre>\n<p>It seems like the compiler just cannot find the output <code>output_path<\/code> from <code>task1<\/code>. So I wonder if there is any way to have some sort of placeholders for the outputs of those steps that are under a <code>dsl.Condition<\/code> , and thus they get filled with default values unless the actual steps run and fill them with the non-default values.\nThe code below represents the problem and is easily reproducible.<\/p>\n<p>I'm using <code>google-cloud-aiplatform==1.14.0<\/code> and <code>kfp==1.8.11<\/code><\/p>\n<pre><code>from typing import NamedTuple\n\nfrom kfp import dsl\nfrom kfp.v2.dsl import Dataset, Input, OutputPath, component\nfrom kfp.v2 import compiler\n\nfrom google.cloud.aiplatform import pipeline_jobs\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task1(\n    # inputs\n    task1_name: str,\n    # outputs\n    output_path: OutputPath(&quot;Dataset&quot;),\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;output_1&quot;, str), (&quot;output_2&quot;, int)]):\n\n    import pandas as pd\n    \n    output_1 = task1_name + &quot;-processed&quot;\n    output_2 = 2\n\n    df_output_1 = pd.DataFrame({&quot;output_1&quot;: [output_1]})\n    df_output_1.to_csv(output_path, index=False)\n\n    return (output_1, output_2)\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task2(\n    # inputs\n    task1_output: Input[Dataset],\n) -&gt; str:\n\n    import pandas as pd\n\n    task1_input = pd.read_csv(task1_output.path).values[0][0]\n\n    return task1_input\n\n@dsl.pipeline(\n    pipeline_root='pipeline_root',\n    name='pipelinename',\n)\ndef pipeline(\n    do_task1: bool,\n    task1_name: str,\n):\n\n    with dsl.Condition(do_task1 == True):\n\n        task1_op = (\n            task1(\n                task1_name=task1_name,\n            )\n        )\n\n    task2_op = (\n        task2(\n            task1_output=task1_op.outputs[&quot;output_path&quot;],\n        )\n    )\n\n\nif __name__ == '__main__':\n    \n    do_task1 = True # &lt;------------ The variable to modify ---------------\n\n    # compile pipeline\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path='pipeline.json')\n\n    # create pipeline run\n    pipeline_run = pipeline_jobs.PipelineJob(\n        display_name='pipeline-display-name',\n        pipeline_root='pipelineroot',\n        job_id='pipeline-job-id',\n        template_path='pipelinename.json',\n        parameter_values={\n            'do_task1': do_task1, # pipeline compilation fails with either True or False values\n            'task1_name': 'Task 1',\n        },\n        enable_caching=False\n    )\n    \n    # execute pipeline run\n    pipeline_run.run()\n<\/code><\/pre>\n<p>Any help is much appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662565503980,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":39,
        "Owner_creation_time":1518297015053,
        "Owner_last_access_time":1663947750400,
        "Owner_location":"Spain",
        "Owner_reputation":21,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73638322",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73224036,
        "Question_title":"Google cloud vertex AI workbench notebook stuck on \"starting\"",
        "Question_body":"<p>I'm running a Jupyter Notebook on the Vertex AI workbench and getting the error below. It loads for an hour or two, then reverts back to a stopped state. It's a managed notebook, so I can't <code>ssh<\/code> to the notebook to recover my code, which is all I want to do since there's a lot of work there I haven't backed up (I didn't expect a big Cloud service to randomly fail like this...) Any ideas on how to get this back up and running? I am happy to delete this and start a new instance onc I recover the code on this.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cardr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cardr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1659540977210,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":275,
        "Owner_creation_time":1564327169643,
        "Owner_last_access_time":1662223953267,
        "Owner_location":"Cambridge, MA, USA",
        "Owner_reputation":123,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73224036",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70535237,
        "Question_title":"How to update Python on Vertex AI notebooks?",
        "Question_body":"<p>I am working in notebooks provided in the Workbench section of Vertex AI. I need an updated version of Python, but I only have access to Python 3.7 in these notebooks. I have successfully followed <a href=\"https:\/\/stackoverflow.com\/a\/62831268\/8565438\">these steps<\/a> and if I run <code>python3.8 --version<\/code> in terminal, I get <code>Python 3.8.2<\/code>, which is good, but <code>python --version<\/code> still returns <code>Python 3.7.12<\/code>. If, following <a href=\"https:\/\/stackoverflow.com\/questions\/40694528\/how-to-know-which-python-is-running-in-jupyter-notebook\">this answer<\/a> and restarting notebook's kernel, I run<\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>in a notebook, and I get <code>3.7.12<\/code>.<\/p>\n<p><strong>How do I get a notebook in Vertex AI supporting an up-to-date Python version?<\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640887731833,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1736,
        "Owner_creation_time":1504639661230,
        "Owner_last_access_time":1664053782077,
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":5748,
        "Owner_up_votes":2565,
        "Owner_down_votes":761,
        "Owner_views":969,
        "Question_last_edit_time":1658927831667,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70535237",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71578582,
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647981155950,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":128,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648004091847,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72316498,
        "Question_title":"Authenticate Custom Training Job in Vertex AI with Service Account",
        "Question_body":"<p>I am trying to run a Custom Training Job to deploy my model in Vertex AI directly from a Jupyterlab. This Jupyterlab is instantiated from a Vertex AI Managed Notebook where I already specified the service account.<\/p>\n<p>My aim is to deploy the training script that I specify to the method <code>CustomTrainingJob<\/code> directly from the cells of my notebook. This would be equivalent to pushing an image that contains my script to <strong>container registry<\/strong> and deploying the Training Job manually from the UI of Vertex AI (in this way, by specifying the service account, I was able to corectly deploy the training job). However, I need everything to be executed from the same notebook.<\/p>\n<p>In order to specify the credentials to the <code>CustomTrainingJob<\/code> of aiplatform, I execute the following cell, where all variables are correctly set:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>import google.auth\nfrom google.cloud import aiplatform\nfrom google.auth import impersonated_credentials\n\nsource_credentials = google.auth.default()\ntarget_credentials = impersonated_credentials.Credentials(\nsource_credentials=source_credentials,\ntarget_principal='SERVICE_ACCOUNT.iam.gserviceaccount.com',\ntarget_scopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform'])\n\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n\njob = aiplatform.CustomTrainingJob(\n    display_name=JOB_NAME,\n    script_path=SCRIPT_PATH,\n    container_uri=MODEL_TRAINING_IMAGE,\n    credentials=target_credentials\n)\n<\/code><\/pre>\n<p>When after the <code>job.run()<\/code> command is executed it seems that the credentials are not correctly set. In particular, the following error is returned:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>\/opt\/conda\/lib\/python3.7\/site-packages\/google\/auth\/impersonated_credentials.py in _update_token(self, request)\n    254 \n    255         # Refresh our source credentials if it is not valid.\n--&gt; 256         if not self._source_credentials.valid:\n    257             self._source_credentials.refresh(request)\n    258 \n\nAttributeError: 'tuple' object has no attribute 'valid'\n<\/code><\/pre>\n<p>I also tried different ways to configure the credentials of my service account but none of them seem to work. In this case it looks like the tuple that contains the source credentials is missing the 'valid' attribute, even if the method <code>google.auth.default()<\/code> only returns two values.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653038432043,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_time":1648140384823,
        "Owner_last_access_time":1664054562907,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72316498",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69930186,
        "Question_title":"Google Cloud Platform Vertex AI logs not showing in custom job",
        "Question_body":"<p>I have written a python package that trains a neural network. I then package it up using the below command.<\/p>\n<pre><code>python3 setup.py sdist --formats=gztar\n<\/code><\/pre>\n<p>When I run this job through the GCP console, and manually click through all the options, I get logs from my program as expected (see example below)<\/p>\n<p><strong>Example successful logs:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/xGFkY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xGFkY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>However, when I run the exact same job programmatically, no logs appear. Only the final error (if one occurs):<\/p>\n<p><strong>Example logs missing:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/3oXfV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3oXfV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In both cases, the program is running - I just cant see any of the outputs. What could the reason for this be? For reference, I have also included the code I used to programmatically start the training process:<\/p>\n<pre><code>ENTRY_POINT = &quot;projects.yaw_correction.yaw_correction&quot;\nTIMESTAMP = datetime.datetime.strftime(datetime.datetime.now(),&quot;%y%m%d_%H%M%S&quot;)\nPROJECT = &quot;yaw_correction&quot;\nGCP_PROJECT = &quot;our_gcp_project_name&quot;\nLOCATION = &quot;europe-west1&quot;\nBUCKET_NAME = &quot;our_bucket_name&quot;\nDISPLAY_NAME = &quot;Training_Job_&quot; + TIMESTAMP\nCONTAINER_URI = &quot;europe-docker.pkg.dev\/vertex-ai\/training\/pytorch-xla.1-9:latest&quot;\nMODEL_NAME = &quot;Model_&quot; + TIMESTAMP\nARGS = [f&quot;\/gcs\/fotokite-training-data\/yaw_correction\/&quot;, &quot;--cloud&quot;, &quot;--gpu&quot;]\nTENSORBOARD = &quot;projects\/&quot;our_gcp_project_name&quot;\/locations\/europe-west4\/tensorboards\/yaw_correction&quot;\n\nMACHINE_TYPE = &quot;n1-standard-4&quot;\nREPLICA_COUNT = 1\nACCELERATOR_TYPE = &quot;ACCELERATOR_TYPE_UNSPECIFIED&quot;\nACCELERATOR_COUNT = 0\nSYNC = False\n\n#Delete existing source distributions\ndef deleteDist():\n    dirpath = Path('dist')\n    if dirpath.exists() and dirpath.is_dir():\n        shutil.rmtree(dirpath)\n\n# Copy distribution to the cloud bucket storage\ndeleteDist()\nsubprocess.run(&quot;python3 setup.py sdist --formats=gztar&quot;, shell=True)\nfilename = [x for x in Path('dist').glob('*')]\nif len(filename) != 1:\n    raise Exception(&quot;More than one distribution was found&quot;)\nprint(str(filename[0]))\nPACKAGE_URI = f&quot;gs:\/\/{BUCKET_NAME}\/distributions\/&quot;\nsubprocess.run(f&quot;gsutil cp {str(filename[0])} {PACKAGE_URI}&quot;, shell=True)\nPACKAGE_URI += str(filename[0].name)\ndeleteDist()\n\n# Initialise the compute instance\naiplatform.init(project=GCP_PROJECT, location=LOCATION, staging_bucket=BUCKET_NAME)\n\n# Schedule the job\njob = aiplatform.CustomPythonPackageTrainingJob(\n    display_name=DISPLAY_NAME,\n    #script_path=&quot;trainer\/test.py&quot;,\n    python_package_gcs_uri=PACKAGE_URI,\n    python_module_name=ENTRY_POINT,\n    #requirements=['tensorflow_datasets~=4.2.0', 'SQLAlchemy~=1.4.26', 'google-cloud-secret-manager~=2.7.2', 'cloud-sql-python-connector==0.4.2', 'PyMySQL==1.0.2'],\n    container_uri=CONTAINER_URI,\n)\n\nmodel = job.run(\n    dataset=None,\n    #base_output_dir=f&quot;gs:\/\/{BUCKET_NAME}\/{PROJECT}\/Train_{TIMESTAMP}&quot;,\n    base_output_dir=f&quot;gs:\/\/{BUCKET_NAME}\/{PROJECT}\/&quot;,\n    service_account=&quot;vertex-ai-fotokite-service-acc@fotokite-cv-gcp-exploration.iam.gserviceaccount.com&quot;,\n    environment_variables=None,\n    args=ARGS,\n    replica_count=REPLICA_COUNT,\n    machine_type=MACHINE_TYPE,\n    accelerator_type=ACCELERATOR_TYPE,\n    accelerator_count=ACCELERATOR_TYPE,\n    #tensorboard=TENSORBOARD,\n    sync=SYNC\n)\nprint(model)\nprint(&quot;JOB SUBMITTED&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636641674037,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":436,
        "Owner_creation_time":1636640830170,
        "Owner_last_access_time":1652193537447,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1636706361063,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69930186",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73540815,
        "Question_title":"how to pass threshold to BigQuery ML model when deployed to vertex ai",
        "Question_body":"<p>I am exporting a BigQuery ML model to cloud storage and then importing the resulting tensorflow model to vertex ai.<\/p>\n<p>The deployed model uses a threshold of 0.5.<\/p>\n<p>If I were doing prediction in BigQuery, I would have used ML.predict with STRUCT(0.05 as THRESHOLD). How can I tell Vertex AI to similarly pass THRESHOLD to all input requests?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661854797090,
        "Question_score":0,
        "Question_tags":"tensorflow|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":60,
        "Owner_creation_time":1400250266617,
        "Owner_last_access_time":1663549773980,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":3566,
        "Owner_up_votes":39,
        "Owner_down_votes":3,
        "Owner_views":732,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73540815",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72986981,
        "Question_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Question_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657835239540,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":62,
        "Owner_creation_time":1254829817773,
        "Owner_last_access_time":1663965241687,
        "Owner_location":"Germany",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657854766997,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657863259570,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73690729,
        "Question_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Question_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662992136247,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663048009750,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73690729",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72989531,
        "Question_title":"WebScrapping error in Google Cloud - Vertex AI Workbench (using Python3 & Selenium)",
        "Question_body":"<p>I am trying to use Workbench Managed Notebooks to schedule some Jupyter notebooks to run Selenium for webscrapping some pages.<\/p>\n<p>My code is below:<\/p>\n<pre><code>from get_gecko_driver import GetGeckoDriver\nget_driver = GetGeckoDriver()\nget_driver = GetGeckoDriver()\nget_driver.install()\n\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\n\nopts = FirefoxOptions()\nopts.add_argument(&quot;--headless&quot;)\nbrowser = webdriver.Firefox(options=opts)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nSessionNotCreatedException                Traceback (most recent call last)\n\/tmp\/ipykernel_1\/3371773802.py in &lt;module&gt;\n      4 opts = FirefoxOptions()\n      5 opts.add_argument(&quot;--headless&quot;)\n----&gt; 6 browser = webdriver.Firefox(options=opts)\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/firefox\/webdriver.py in __init__(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\n    178             command_executor=executor,\n    179             options=options,\n--&gt; 180             keep_alive=True)\n    181 \n    182         self._is_remote = False\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in __init__(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\n    275         self._authenticator_id = None\n    276         self.start_client()\n--&gt; 277         self.start_session(capabilities, browser_profile)\n    278 \n    279     def __repr__(self):\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in start_session(self, capabilities, browser_profile)\n    368         w3c_caps = _make_w3c_caps(capabilities)\n    369         parameters = {&quot;capabilities&quot;: w3c_caps}\n--&gt; 370         response = self.execute(Command.NEW_SESSION, parameters)\n    371         if 'sessionId' not in response:\n    372             response = response['value']\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in execute(self, driver_command, params)\n    433         response = self.command_executor.execute(driver_command, params)\n    434         if response:\n--&gt; 435             self.error_handler.check_response(response)\n    436             response['value'] = self._unwrap_value(\n    437                 response.get('value', None))\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/errorhandler.py in check_response(self, response)\n    245                 alert_text = value['alert'].get('text')\n    246             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 247         raise exception_class(message, screen, stacktrace)\n    248 \n    249     def _value_or_default(self, obj: Mapping[_KT, _VT], key: _KT, default: _VT) -&gt; _VT:\n\nSessionNotCreatedException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line\n<\/code><\/pre>\n<p>It seems that I need to setup somehow the location of Firefox, but I don't know if this is possible and how to do it (or if there are some alternatives that are easier to setup e.g. chromium).<\/p>\n<p><em>Please Note: this code is running perfectly fine when I run Jupyter notebooks in VM instances but I cannot schedule those notebooks to run automatically so I guess my only option is to go with the Vertex AI &gt; Workbench &gt; Managed Notebooks solution.<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657864043243,
        "Question_score":0,
        "Question_tags":"python-3.x|selenium|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":137,
        "Owner_creation_time":1623914392097,
        "Owner_last_access_time":1658465175717,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72989531",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70782169,
        "Question_title":"Vertex AI batch prediction location",
        "Question_body":"<p>When I initiate a batch prediction job on Vertex AI of google cloud, I have to specify a cloud storage bucket location. Suppose I provided the bucket location, <code>'my_bucket\/prediction\/'<\/code>, then the prediction files are stored in something like: <code>gs:\/\/my_bucket\/prediction\/prediction-test_model-2022_01_17T01_46_39_898Z<\/code>, which is a subdirectory within the bucket location I provided. The prediction files are stored within that subdirectory and are named:<\/p>\n<pre><code>prediction.results-00000-of-00002\nprediction.results-00001-of-00002\n<\/code><\/pre>\n<p>Is there any way to programmatically get the final export location from the batch prediction name, id or any other parameter as shown below in the details of the batch prediction job?\n<a href=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1642663428603,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":554,
        "Owner_creation_time":1429532538823,
        "Owner_last_access_time":1664028982667,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70782169",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73403004,
        "Question_title":"Using ipyleaflet within a Vertex AI Managed Notebook running on a Docker image",
        "Question_body":"<p>TL;DR How does one get ipyleaflet to work in a Vertex AI Managed Notebook booted from a custom Docker image?<\/p>\n<p><strong>Objective<\/strong><\/p>\n<p>Following on <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">this thread<\/a>, I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. I am trying to supply a custom Docker image, such that when a Jupyter Lab notebook is launched from the running instance, it (a) contains some modules for performing and visualizing cartographic analysis, and, importantly, (b) permits visualization with ipyleaflet and associated modules.<\/p>\n<p><strong>What I've tried<\/strong><\/p>\n<p>Thus far, I have succeed in creating a Docker image (derivative of <a href=\"https:\/\/cloud.google.com\/deep-learning-containers\/docs\/choosing-container\" rel=\"nofollow noreferrer\">Google image<\/a>) that supplies, in the running Jupyter Lab, a dedicated environment (explicitly exposed kernel) with the correct modules (in particular, geemap, earthengine-api, ipyleaflet, ipywidgets). The modules are all importable and appear sound. However, so far as I can tell, supplying a custom Docker image during the build process, effectively breaks the ipyleaflet (and presumably widgets, events, etc) connection that Google's Jupyter Lab base image has if one creates a Managed Notebook <em>without<\/em> supplying a Docker image. Attempts to create map visualizations returns, &quot;Error displaying widget: model not found&quot;, discussed at <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">2<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">3<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">4<\/a>. In other words, if one creates a Managed Notebook <em>without<\/em> a Docker image, starts the notebook instance, launches Jupyter Lab, opens a notebook, and then uses <code>%pip install xyz<\/code> for the modules of interest, ipyleaflet-based mapping works fine. I suspect that the nuance of difference here, is that the latter method (<code>%pip install<\/code>ing from within the notebook), is being layered on top of a fully formed base Jupyter Lab container (per @gogasca's comment <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">here<\/a>, that the Google Managed Jupyter Lab runs as a container that is not customizable).<\/p>\n<p><strong>Questions<\/strong><\/p>\n<p>So, what I would love to know is:<\/p>\n<p><strong>(1)<\/strong> How does one retain ipyleaflet (and associated modules) functionality in a Managed Notebook that is based on a user supplied Docker image?<\/p>\n<p><strong>(2)<\/strong> Is there a way to effectively replicate the <code>%pip install<\/code> approach when using a custom Docker image, such that commands specified in the Docker file are layered on top of a fully formed base Google image.<\/p>\n<p>To question (2), I suspect that when the gcloud sdk for Managed Notebooks is available (currently under the impression that this is a work in progress), it will be possible to provide a post-startup-script, as in <a href=\"https:\/\/medium.com\/@gogasca_\/ai-platform-notebooks-with-voila-c3c57d4e8e\" rel=\"nofollow noreferrer\">this example<\/a>. I am aware that there are REST and Terraform build options available that might satisfy my Managed Notebook needs. These require quite a bit more legwork though, so I am interested in simpler solutions, if they exist.<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>Adding content for partial reproducibility. Steps to reproduce:<\/p>\n<ol>\n<li>A custom Docker image was created using the interface at <a href=\"https:\/\/shell.cloud.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/shell.cloud.google.com\/<\/a>.<\/li>\n<li>Within the cloud shell, from the terminal, set gcloud configuration parameters with <code>gcloud config set project yourProjectIDHere<\/code><\/li>\n<li>Create a file named Dockerfile with the following content:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>FROM python:3.7.4-buster \nENV VIRTUAL_ENV=\/env\/testEnvironment \nRUN python3 -m venv $VIRTUAL_ENV --system-site-packages \nENV PATH=&quot;$VIRTUAL_ENV\/bin:$PATH&quot; \n# Install dependencies: \nCOPY requirements.txt . \nRUN $VIRTUAL_ENV\/bin\/pip install -r requirements.txt\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"4\">\n<li>Create a file named requirements.txt with the following content<\/li>\n<\/ol>\n<blockquote>\n<pre><code>ipython\nipykernel\ngeemap\nearthengine-api\nipyleaflet\nfolium\nvoila\nipywidgets\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"5\">\n<li>From the cloud shell terminal, build your Docker image (this takes a long time to build. We will use a Google deep learning library derivative once we get our workflow sorted):\n<code>docker build . -f Dockerfile -t &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Push container to GCP Container Registry\n<code>docker push &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Move to Google cloud console and create a new Managed Notebook within your GCP.<\/li>\n<li>Select the defaults except for (a) use Service account permissions; (b) use &quot;Networks shared with me&quot; associated with the host project, including both a Network and Shared subnetwork (presuming that this isn't going to change installation components, so probably doesn't matter); (c) uncheck &quot;Enable external IPs&quot;; (d) check &quot;Enable terminal&quot;; and (e) check &quot;Provide custom docker images&quot;.<\/li>\n<li>From the &quot;Provide custom docker images&quot; dialog, select the image you just created.<\/li>\n<li>Create the Managed Notebook, and once created, open Jupyter Lab.<\/li>\n<li>The Docker image should expose a kernel in the Jupyter Lab environment. Open a new notebook using the kernel.<\/li>\n<li>Load and test modules within the notebook<\/li>\n<\/ol>\n<blockquote>\n<pre><code># Import and initialize the earthengine-api by whatever means fit your use case\nimport ee\nee.Initialize( ... )\n# Test authentication pathways\nprint(ee.Image(&quot;NASA\/NASADEM_HGT\/001&quot;).get(&quot;title&quot;).getInfo())\n# Provided you have your authentication set-up correctly...\n\n# Test import of various other modules   \nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport geemap\nfrom IPython.display import display, HTML, Image\nimport ipywidgets as widgets\nfrom ipywidgets import Layout\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Button, HBox, VBox\nfrom ipywidgets import HTML\nimport voila\n\n# Test inline mapping using ipyleaflet-based map     \nMap = geemap.Map(center=(-0.2557968807155925, 119.46629773460036), zoom=5)\nMap\n\n# Receive error.\n\n# This is also replicable without earthengine-api or geemap, by just trying to make a basic ipyleaflet map\nfrom ipyleaflet import Map, basemaps, basemap_to_tiles\nm = Map(basemap=basemap_to_tiles(basemaps.OpenStreetMap.Mapnik), center=(48.204793, 350.121558), zoom=3)\nm\n<\/code><\/pre>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1660825300527,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":148,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1662058794213,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73403004",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69681031,
        "Question_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Question_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634924192977,
        "Question_score":1,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":312,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637263566353,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71556469,
        "Question_title":"Vertex AI Endpoints - Failed to create endpoint",
        "Question_body":"<p>I'm trying to deploy a custom model to an endpoint with Vertex AI. I run the custom training and the model was correctly created in my bucket using Tensorflow 2 <code>export_saved_model<\/code> for estimators. In this bucket there is the <code>saved_model.pb<\/code> file with the folder <code>variables<\/code>.\nHowever, when I try to create an endpoint selecting the path to the saved model, the following error occurs:<\/p>\n<p><code>Failed to create endpoint &quot;endpoint_name&quot; due to error: APPLICATION_ERROR; google.cloud.ml.v1\/ModelService.CreateVersion;Field: version.deployment_uri Error: Deployment directory gs:\/\/different_bucket\/artifacts\/ is expected to contain exactly one of: [saved_model.pb, saved_model.pbtxt].<\/code><\/p>\n<p>It seems it is searching the .pb file in a bucket that is not the one I set.\nSuggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1647860330843,
        "Question_score":1,
        "Question_tags":"google-cloud-endpoints|google-cloud-vertex-ai",
        "Question_view_count":345,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71556469",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73715783,
        "Question_title":"Why can't I access Output from Vertex pipeline kfp component?",
        "Question_body":"<p>In a Vertex AI pipeline (google_cloud_pipeline_components version: 1.0.19\nkfp version: 1.8.13), I try:<\/p>\n<pre><code>does_endpoint_exist_op = does_endpoint_exist(project=project, \n    location=location, endpoint_name_in=endpoint_name) \nendpoint_name=does_endpoint_exist_op.outputs['endpoint_name']\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>AssertionError: component_input_parameter: pipelineparam--does-endpoint-exist-endpoint_name not found.\n<\/code><\/pre>\n<p>The component is defined:<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;, \n                         &quot;google-cloud-pipeline-components==1.0.19&quot;],\n    output_component_file=&quot;does_endpoint_exist_component.yaml&quot;,\n)\n\ndef does_endpoint_exist(project: str, \n                    location: str,\n                    endpoint_name_in: str, endpoint: Output[Artifact], \n                    endpoint_name: Output[Artifact]) -&gt; str:\n<\/code><\/pre>\n<p>I can do:<\/p>\n<pre><code>endpoint_name=does_endpoint_exist_op.outputs['Output'] \n<\/code><\/pre>\n<p>OK, so why can't I access <code>endpoint_name<\/code>?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663153516447,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":17,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73715783",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70610069,
        "Question_title":"Is there any parameter in any SDK for enabling access logging in GCP Vertex AI?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/kV1Vb.png\" rel=\"nofollow noreferrer\">enabling access logging on the UI<\/a>seeking for some support in terms of enabling logging via the SDK(Vertex AI or AI platform or any other).Just as we enable it on the UI(pls refer attached file) &amp; other times via gcloud command like this-&gt;\ngcloud ai endpoints deploy-model ENDPOINT_ID--region=LOCATION --model=MODEL_ID --display-name=DEPLOYED_MODEL_NAME --machine-type=MACHINE_TYPE --accelerator=count=2,type=nvidia-tesla-t4 --disable-container-logging --enable-access-logging<\/p>\n<p>Does AI platform or vertex ai or any other SDK comprise of any API\/parameter which would allow us to enable access logging? If yes, could you please point in that direction?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641485506960,
        "Question_score":0,
        "Question_tags":"logging|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":24,
        "Owner_creation_time":1632752693853,
        "Owner_last_access_time":1658209784610,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70610069",
        "Question_exclusive_tag":"Vertex AI"
    }
]