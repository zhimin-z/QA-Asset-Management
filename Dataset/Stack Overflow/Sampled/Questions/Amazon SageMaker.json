[
    {
        "Question_id":48014413,
        "Question_title":"Connection timed out - using sqlalchemy to access AWS usaspending data",
        "Question_body":"<p>I created an instance of the usaspending.gov database in my AWS RDS. A description of this database can be found here: <a href=\"https:\/\/aws.amazon.com\/public-datasets\/usaspending\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/public-datasets\/usaspending\/<\/a><\/p>\n\n<p>The data are available as a PostgreSQL snapshot, and I would like to access the database using Python's sqlalchemy package within a Jupyter notebook within Amazon SageMaker.<\/p>\n\n<p>I tried to set up my database connection with the code below, but I'm getting a Connection timed out error. I'm pretty new to AWS and Sagemaker, so maybe I messed up my sqlalchemy engine? I think my VPC security settings are OK (it looks like they accept inbound and outbound requests).<\/p>\n\n<p>Any ideas what I could be missing?<\/p>\n\n<p>engine = create_engine(\u2018postgresql:\/\/root:password@[my endpoint]\/[DB instance]<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" alt=\"connection timed out\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" alt=\"VPC inbound settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" alt=\"VPC outbound settings\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-12-28 21:08:41.563 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|postgresql|amazon-web-services|sqlalchemy|amazon-sagemaker",
        "Question_view_count":1528,
        "Owner_creation_date":"2017-05-25 00:08:25.787 UTC",
        "Owner_last_access_date":"2018-12-27 20:23:47.16 UTC",
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48014413",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71389083,
        "Question_title":"Is it possible to trigger Sagemaker notebook from AWS Lambda function?",
        "Question_body":"<p>I am trying to trigger Sagemaker notebook or Sagemaker Studio notebook from AWS Lambda when data is available in S3 bucket. I want to know if this is possible and if yes, how?<\/p>\n<p>All I want is once data is uploaded in S3, the lambda function should be able to spin up the Sagemaker notebook with a standard CPU cluster.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-08 01:35:17.017 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":621,
        "Owner_creation_date":"2020-06-29 19:21:19.09 UTC",
        "Owner_last_access_date":"2022-09-14 18:02:24.907 UTC",
        "Owner_reputation":82,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71389083",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69795424,
        "Question_title":"Amazon Sagemaker Failed: Please check if you have a directory that has same name as the git repo?",
        "Question_body":"<p>I was working in Sagemaker, and noticed that my notebook instance was behind my github repo as I had just pushed to it outside of working in Sagemaker. I couldn't seem to pull, so I deleted the directory within Jupyter and git cloned my updated repo. It worked fine, but once I was done working I haven't since been able to reinitialize the notebook. Sagemaker simpy says<\/p>\n<blockquote>\n<p><strong>Failure reason<\/strong><\/p>\n<p>Please check if you have a directory that has same name as the git repo.<\/p>\n<\/blockquote>\n<p>I cloned from the same repo, so I don't imagine that the directory name changed. Maybe the directory is in the wrong place? Either way, how do I go in and change things if I can't open the notebook? Not sure what to do about this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-01 10:12:09.64 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"github|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_date":"2017-10-24 13:47:52.37 UTC",
        "Owner_last_access_date":"2022-04-01 15:10:20.05 UTC",
        "Owner_reputation":95,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":68,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69795424",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68549349,
        "Question_title":"'ResolvedOutputS3Uri' key missing from describe_feature_group() dict",
        "Question_body":"<p>I'm having this issue when describing a feature group from a pipeline script.<\/p>\n<p>The dictionary retrieved by describe_feature_group() doesn't have the 'ResolvedOutputS3Uri' key and I'm trying to figure out why. The feature group exists and has data (actually, the description works perfectly fine from a SageMaker notebook)<\/p>\n<p>Anyone have a hint on where to start looking?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-27 17:26:03.183 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":24,
        "Owner_creation_date":"2018-01-02 16:31:19.387 UTC",
        "Owner_last_access_date":"2021-07-30 02:34:48.55 UTC",
        "Owner_reputation":173,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68549349",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64126327,
        "Question_title":"Load Amazon Sagemaker NTM model locally for inference",
        "Question_body":"<p>I have trained a Sagemaker <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/introduction-to-the-amazon-sagemaker-neural-topic-model\/\" rel=\"nofollow noreferrer\">NTM<\/a> model which is a neural topic model, directly on the AWS sagemaker platform. Once training is complete you are able to download the <code>mxnet<\/code> model files. Once unpacked the files contain:<\/p>\n<ul>\n<li>params<\/li>\n<li>symbol.json<\/li>\n<li>meta.json<\/li>\n<\/ul>\n<p>I have followed the docs on mxnet to load the model and have the following code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sym, arg_params, aux_params = mx.model.load_checkpoint('model_algo-1', 0)\nmodule_model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())\n\nmodule_model.bind(\n    for_training=False,\n    data_shapes=[('data', (1, VOCAB_SIZE))]\n)\n\nmodule_model.set_params(arg_params=arg_params, aux_params=aux_params, allow_missing=True) # must set allow missing true here or receive an error for a missing n_epoch var\n<\/code><\/pre>\n<p>I now try and use the model for inference using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>module_model.predict(x) # where x is a numpy array of size (1, VOCAB_SIZE)\n<\/code><\/pre>\n<p>The code runs, but the result is just a single value, where I expect a distribution over topics:<\/p>\n<pre><code>[11.060672]\n&lt;NDArray 1 @cpu(0)&gt;\n<\/code><\/pre>\n<p>EDIT:<\/p>\n<p>I have tried to load it using the Symbol API, but still no luck:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    deserialized_net = gluon.nn.SymbolBlock.imports('model_algo-1-symbol.json', ['data'], 'model_algo-1-0000.params', ctx=mx.cpu())\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>AssertionError: Parameter 'n_epoch' is missing in file: model_algo-1-0000.params, which contains parameters: 'logsigma_bias', 'enc_0_bias', 'projection_bias', ..., 'enc_1_weight', 'enc_0_weight', 'mean_bias', 'logsigma_weight'. Please make sure source and target networks have the same prefix.\n<\/code><\/pre>\n<p>Any help would be great!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-29 19:25:42.533 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-01 15:31:53.1 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|nlp|amazon-sagemaker|mxnet",
        "Question_view_count":149,
        "Owner_creation_date":"2015-05-13 15:21:55.873 UTC",
        "Owner_last_access_date":"2022-09-22 08:59:46.38 UTC",
        "Owner_reputation":2763,
        "Owner_up_votes":203,
        "Owner_down_votes":35,
        "Owner_views":264,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, United Kingdom",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64126327",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60910863,
        "Question_title":"The difference between boto3.client and pymysql.connect",
        "Question_body":"<p>I am the novice in using AWS-RDS-mysql, Now I have a problem when connecting this database<\/p>\n\n<p>Here I have two ways to connect to my AWS-RDS-mysql, one is to use 'boto3' package and another is 'pymysql' package.<\/p>\n\n<p>For boto3, four paramaters are required, they are:<\/p>\n\n<pre><code>1) type='rds', \n2) region,\n3) aws_access_key_id\n4) aws_secret_access_key\n<\/code><\/pre>\n\n<p>The function I use is:<\/p>\n\n<pre><code>boto3.client(type, region, aws_access_key_id=key_id, aws_secret_access_key=access_key)\n<\/code><\/pre>\n\n<p>For pymysql, I need 5 parameters, they are:<\/p>\n\n<pre><code>1) host = \"databaseinstancename.cxxxxxxxxxx.ca-central-1.rds.amazonaws.com\"\n2) port = 3306\n3) dbname = \"xxx\"\n4) user = \"admin\"\n5) password = \"ssssss\"\n<\/code><\/pre>\n\n<p>The method I use is:<\/p>\n\n<pre><code>pymysql.connect(host, user=user, port=port, passwd=password, db=dbname)\n<\/code><\/pre>\n\n<p>All of them could connect to the database from my pycharm successfully.<\/p>\n\n<p>My question is <\/p>\n\n<p>1) what's the difference between the first method(using boto3) and the second one(using pymysql)? There are must be something difference because I provide two different kinds of information to connect.<\/p>\n\n<p>2) When I use jupyter notebook in AWS sagemaker to edit python3 code to try to connect to the database, I find only the first method(using boto3) could work, the second method(using pymysql) cannot connect successfully with a error saying that 'time out'. Why does this happen when one method could be accessible to the database but another one fails? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-29 06:24:44.537 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-03-29 06:34:46.46 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|boto3|amazon-rds|pymysql|amazon-sagemaker",
        "Question_view_count":936,
        "Owner_creation_date":"2020-03-29 02:35:02.723 UTC",
        "Owner_last_access_date":"2022-01-02 15:46:55.597 UTC",
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60910863",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54923674,
        "Question_title":"Using cloudwatch api get_metric_data to get sagemaker endpoints invocation metrics",
        "Question_body":"<p>I am trying to use aws cloudwatch api get_metric_data to get sagemaker endpoint invocation metrics in python, and it returns me empty timestamps and Values, but there are some invocations between the time I specified, so there is something going wrong. Below is the code I write in python.<\/p>\n\n<pre><code>cloudwatch.get_metric_data(\nMetricDataQueries=[\n    {\n        'Id': 'm1',\n        'MetricStat': {\n            'Metric': {\n                'Namespace': 'AWS\/SageMaker',\n                'MetricName': 'Invocations',\n                'Dimensions': [\n                    {\n                        'Name': 'EndpointName',\n                        'Value': 'users-hcl-2',\n                    },\n                    {\n                        'Name': 'VariantName',\n                        'Value': 'AllTraffic',\n                    },\n                ]\n            },\n            'Period': 3600,\n            'Stat': 'Sum',\n            'Unit': 'None'\n        },\n        'ReturnData': True,\n    },\n],\nStartTime=datetime(2019, 2, 1),\nEndTime=datetime(2019,2,13),\n)\n<\/code><\/pre>\n\n<p>And it returns below:<\/p>\n\n<pre><code>    {'MetricDataResults': [{'Id': 'm1',\n   'Label': 'Invocations',\n   'Timestamps': [],\n   'Values': [],\n   'StatusCode': 'Complete'}],\n 'ResponseMetadata': {'RequestId': '8dd847eb-3b43-11e9-b50f-5f6fedb3e07d',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '8dd847eb-3b43-11e9-b50f-5f6fedb3e07d',\n   'content-type': 'text\/xml',\n   'content-length': '494',\n   'date': 'Thu, 28 Feb 2019 10:28:13 GMT'},\n  'RetryAttempts': 0}}\n<\/code><\/pre>\n\n<p>As I said, the timestamp and values shouldn't be empty, can you help me to sort out where I did wrong, I found some useful links below:<\/p>\n\n<p>cloudwatch concepts: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/monitoring\/cloudwatch_concepts.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/monitoring\/cloudwatch_concepts.html<\/a><\/p>\n\n<p>invocation metrics info:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html<\/a><\/p>\n\n<p>cloudwatch get_metric_data api:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/<\/a><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/cloudwatch.html#CloudWatch.Client.get_metric_data\" rel=\"nofollow noreferrer\">cloudwatch.html#CloudWatch.Client.get_metric_data<\/a><\/p>\n\n<p>For what I have already tried, changing \"Period\" entity to different values, but it does no help. Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-28 10:39:36.037 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":2925,
        "Owner_creation_date":"2019-02-28 09:49:41.9 UTC",
        "Owner_last_access_date":"2020-02-28 20:24:25.61 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Berlin, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54923674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67570694,
        "Question_title":"How to build YOLACT++ using Docker?",
        "Question_body":"<p>I have to build yolact++ in docker enviromment (i'm using sagemaker notebook). Like this<\/p>\n<pre><code>ARG PYTORCH=&quot;1.3&quot;\nARG CUDA=&quot;10.1&quot;\nARG CUDNN=&quot;7&quot;\n \nFROM pytorch\/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel\n<\/code><\/pre>\n<p>And i want to run this<\/p>\n<pre><code>COPY yolact\/external\/DCNv2\/setup.py \/opt\/ml\/code\/external\/DCNv2\/setup.py\nRUN cd \/opt\/ml\/code\/external\/DCNv2 &amp;&amp; \\\npython setup.py build develop\n<\/code><\/pre>\n<p>But i got this error :<\/p>\n<pre><code>No CUDA runtime is found, using CUDA_HOME='\/usr\/local\/cuda'\nTraceback (most recent call last):\nFile &quot;setup.py&quot;, line 64, in &lt;module&gt;\next_modules=get_extensions(),\nFile &quot;setup.py&quot;, line 41, in get_extensions\nraise NotImplementedError('Cuda is not available')\nNotImplementedError: Cuda is not available\n<\/code><\/pre>\n<p>But the enviromment supports CUDA. Anyone have an idea where is the problem ?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2021-05-17 13:30:22.833 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-05-20 13:59:11.353 UTC",
        "Question_score":0,
        "Question_tags":"docker|pytorch|dockerfile|amazon-sagemaker",
        "Question_view_count":486,
        "Owner_creation_date":"2020-04-16 20:44:06.163 UTC",
        "Owner_last_access_date":"2021-10-17 13:17:39.293 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>SOLUTION :<\/p>\n<p>i edit the \/etc\/docker\/daemon.json with content:<\/p>\n<pre><code>{\n&quot;runtimes&quot;: {\n    &quot;nvidia&quot;: {\n        &quot;path&quot;: &quot;\/usr\/bin\/nvidia-container-runtime&quot;,\n        &quot;runtimeArgs&quot;: []\n     } \n},\n&quot;default-runtime&quot;: &quot;nvidia&quot; \n}\n<\/code><\/pre>\n<p>Then i Restart docker daemon:<\/p>\n<pre><code>sudo system restart docker\n<\/code><\/pre>\n<p>it solved my problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-20 13:58:49.333 UTC",
        "Answer_score":1.0,
        "Owner_location":"France",
        "Answer_last_edit_date":"2021-05-20 14:04:39.057 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67570694",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61037520,
        "Question_title":"AWS Sagemaker custom training job container emit loss metric",
        "Question_body":"<p>I have created a customer docker container using an Amazon tensorflow container as a starting point:<\/p>\n\n<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com\/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04\n<\/code><\/pre>\n\n<p>inside the container I run a custom keras (with TF backend) training job from the docker SAGEMAKER_PROGRAM. I can access the training data ok (from an EFS mount) and can generate output into \/opt\/ml\/model that gets synced back to S3. So input and output is good: what I am missing is real-time monitoring.<\/p>\n\n<p>A Sagemaker training job emits system metrics like cpu and gpu loads which you can conveniently view in real-time on the Sagemaker training job console. But I cannot find a way to emit metrics about the progress of the training job. i.e. loss, accuracy etc from my python code.  <\/p>\n\n<p>Actually, ideally I would like to use Tensorboard but as Sagemaker doesn't expose the instance on the EC2 console I cannot see how I can find the IP address of the instance to connect to for Tensorboard. <\/p>\n\n<p>So the fallback is try and emit relevant metrics from the training code so that we can monitor the job as it runs.<\/p>\n\n<p>The basic question is how do I real-time monitor key metrics for my custom training job runnning in a container on Sagemaker training job:\n- Is a tensorboard solution possible? If so how?\n- If not how do I emit metrics from my python code and have them show up in the training job console or as cloudwatch metrics directly?<\/p>\n\n<p>BTW: so far I have failed to be able to get sufficient credentials inside the training job container to access either s3 or cloudwatch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-05 03:06:51.97 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|docker|tensorflow|amazon-sagemaker|amazon-cloudwatch-metrics",
        "Question_view_count":771,
        "Owner_creation_date":"2020-04-05 02:46:34.86 UTC",
        "Owner_last_access_date":"2020-07-26 01:21:12.213 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61037520",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58400838,
        "Question_title":"Deploy an externally trained tensorflow model artefact",
        "Question_body":"<p>I would like to host a tensorflow model (trained on my local PC) on Sagemaker and I followed this article: <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a><\/p>\n\n<p>However I'm not able to perform the inference. My model is an object detection model, locally it has generated frozen_inference_graph.pb, model.ckpt.data-00000-of-00001, model.ckpt.index and model.ckpt.meta, all this files have been converted into save_model.pb and variables.data-00000-of-00001, variable.index, then into .tar.gz by respecting the folder structure export\/Servo\/version\/...<\/p>\n\n<p>.tar.gz has been manually uploaded to s3 and I have successfully created an endpoint.<\/p>\n\n<p>But when I try to perform inference, I have an error:\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"{\"error\": \"Unsupported Media Type: Unknown\"}\"<\/p>\n\n<p>My input datas are images. I need help. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2019-10-15 18:41:28.07 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":372,
        "Owner_creation_date":"2019-10-03 12:56:42.463 UTC",
        "Owner_last_access_date":"2019-11-06 07:56:17.54 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Paris, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58400838",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71178934,
        "Question_title":"How do I get embeddings from Huggingface(in sagemaker) instead of features?",
        "Question_body":"<p>I have a text classifier model that depends on embeddings from a certain huggingface model<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\nencodings = model.encode(&quot;guckst du bundesliga&quot;)\n<\/code><\/pre>\n<p>this has a shape of (768,)<\/p>\n<p>tldr: is there a clean simple way to do this on sagemaker (hopefully using the images it provides) ?<\/p>\n<p>context:\nlooking at docs of this <a href=\"https:\/\/huggingface.co\/T-Systems-onsite\/cross-en-de-roberta-sentence-transformer?text=guckst%20du%20bundesliga\" rel=\"nofollow noreferrer\">huggingface model<\/a> the only sagemaker option I see is feature extraction<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n    'HF_MODEL_ID':'T-Systems-onsite\/cross-en-de-roberta-sentence-transformer',\n    'HF_TASK':'feature-extraction'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1, # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n    'inputs': &quot;Today is a sunny day and I'll get some ice cream.&quot;\n})\n<\/code><\/pre>\n<p>this gives my the features which has a shape (9, 768)<\/p>\n<p>there is a connection between these two values, which is seen from a another code sample<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef embeddings(feature_envelope, attention_mask):\n    features = feature_envelope[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(features.size()).float()\n    sum_embeddings = torch.sum(features * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings \/ sum_mask\n\n#Sentences we want sentence embeddings for\nsentences = ['guckst du bundesliga']\n\n#Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\nmodel = AutoModel.from_pretrained('T-Systems-onsite\/cross-en-de-roberta-sentence-transformer')\n\n#Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n\n#Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n#     print(model_output)\n\n#Perform pooling. In this case, mean pooling\nsentence_embeddings = embeddings(model_output, encoded_input['attention_mask'])\nsentence_embeddings.shape, sentence_embeddings\n<\/code><\/pre>\n<p>But as you can see cant derive embedding given only features<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-18 19:32:28.743 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|sentence-transformers",
        "Question_view_count":415,
        "Owner_creation_date":"2016-11-21 18:54:42.24 UTC",
        "Owner_last_access_date":"2022-09-13 10:44:42.097 UTC",
        "Owner_reputation":349,
        "Owner_up_votes":23,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71178934",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70727988,
        "Question_title":"How to pre deploy the instances used by sagemaker for training?",
        "Question_body":"<p>Sagemaker takes ~3 mins for <code>preparing the instances for training<\/code> and around ~1 min for the actual training and artifact publishing.<\/p>\n<p>I want to pre-deploy some ec2 instances which it can use for training. However I couldn't find an option to specify pre deployed instances in the <code>ResourceConfig<\/code> part of the <code>createTrainingJob<\/code>.<\/p>\n<p>Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-16 06:57:58.607 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":30,
        "Owner_creation_date":"2019-03-04 12:06:56.487 UTC",
        "Owner_last_access_date":"2022-09-23 07:48:34.613 UTC",
        "Owner_reputation":735,
        "Owner_up_votes":167,
        "Owner_down_votes":15,
        "Owner_views":106,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70727988",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64372199,
        "Question_title":"Deploying directly from model artifacts",
        "Question_body":"<p>I want to deploy a pretrained neural network as an endpoint at Sagemaker. All my model artifacts are stored in S3.<\/p>\n<p>In the Sagemacer documentation (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html\" rel=\"nofollow noreferrer\">here<\/a>) I found this solution:<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlowModel \n\nmodel = TensorFlowModel(model_data='s3:\/\/mybucket\/model.tar.gz', role='MySageMakerRole')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n<\/code><\/pre>\n<p>I adapted the TensorFlowModel() function like this:<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlowModel\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\ntensorflow_model = TensorFlowModel(model_data='s3:\/\/my-bucket-name\/path_to_the_model\/model.tar.gz',\n                                   role=role,\n                                   framework_version='2.1.0')\n<\/code><\/pre>\n<p>This resulted in the error:\nTypeError: <strong>init<\/strong>() missing 1 required positional argument: 'entry_point'<\/p>\n<p>The documentation of sagemaker.tensorflow.model.TensorFlowModel() (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">here<\/a>) says about the argument entry point:<\/p>\n<blockquote>\n<p>entry_point (str) \u2013 Path (absolute or relative) to the Python source file which should be executed as the entry point to model hosting. If source_dir is specified, then entry_point must point to a file located at the root of source_dir.<\/p>\n<\/blockquote>\n<p>Is the documentation wrong or am I missing a major difference between my code and the example from the documentation?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-15 13:03:51.413 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2020-11-17 15:39:30.36 UTC",
        "Question_score":2,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":227,
        "Owner_creation_date":"2020-10-06 16:10:59.123 UTC",
        "Owner_last_access_date":"2021-06-16 10:58:35.157 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64372199",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71126832,
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 12:56:47.89 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2022-02-15 13:25:22.733 UTC",
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_date":"2021-02-18 15:25:28.947 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:08.913 UTC",
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-22 21:59:21.52 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61566816,
        "Question_title":"How to serve daily precomputed predictions in aws sagemaker?",
        "Question_body":"<p>I'm trying to use Sagemaker to serve precomputed predictions. The predictions are in the following format in a python dictionary.<\/p>\n\n<pre><code>customer_group prediction\n1              50\n2              60\n3              25\n4              30\n...\n<\/code><\/pre>\n\n<p>Currently the docker <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">serve API code<\/a> goes to s3 and downloads the data daily.<\/p>\n\n<p>The problem is that downloading the data blocks the api from responding to the Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests\" rel=\"nofollow noreferrer\">health endpoint calls<\/a>.<\/p>\n\n<p>This a case study of <a href=\"https:\/\/aws.amazon.com\/solutions\/case-studies\/zappos-case-study\/\" rel=\"nofollow noreferrer\">how zappos did it<\/a> using Amazon DynamoDB. However, is there a way to do it in Sagemaker? <\/p>\n\n<p>Where and how can I add the s3 download function to avoid interrupting the health check?<\/p>\n\n<p>Could this work? -> <a href=\"https:\/\/github.com\/seomoz\/s3po\" rel=\"nofollow noreferrer\">https:\/\/github.com\/seomoz\/s3po<\/a>\n<a href=\"https:\/\/blog.miguelgrinberg.com\/post\/the-flask-mega-tutorial-part-x-email-support\" rel=\"nofollow noreferrer\">https:\/\/blog.miguelgrinberg.com\/post\/the-flask-mega-tutorial-part-x-email-support<\/a><\/p>\n\n<pre><code>app = flask.Flask(__name__)\n\n@app.route('\/ping', methods=['GET'])\ndef ping():\n    \"\"\"Determine if the container is working and healthy. In this sample container, we declare\n    it healthy if we can load the model successfully.\"\"\"\n    health = ScoringService.get_model() is not None  # You can insert a health check here\n\n    status = 200 if health else 404\n    return flask.Response(response='\\n', status=status, mimetype='application\/json')\n\n@app.route('\/invocations', methods=['POST'])\ndef transformation():\n    \"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n    just means one prediction per line, since there's a single column.\n    \"\"\"\n    data = None\n\n    # Convert from CSV to pandas\n    if flask.request.content_type == 'text\/csv':\n        data = flask.request.data.decode('utf-8')\n        s = StringIO.StringIO(data)\n        data = pd.read_csv(s, header=None)\n    else:\n        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text\/plain')\n\n    print('Invoked with {} records'.format(data.shape[0]))\n\n    # Do the prediction\n    predictions = ScoringService.predict(data)\n\n    # Convert from numpy back to CSV\n    out = StringIO.StringIO()\n    pd.DataFrame({'results':predictions}).to_csv(out, header=False, index=False)\n    result = out.getvalue()\n\n    return flask.Response(response=result, status=200, mimetype='text\/csv')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-02 21:53:43.127 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-05-03 11:33:04.957 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_date":"2016-07-14 00:43:23.417 UTC",
        "Owner_last_access_date":"2022-09-14 08:41:23.24 UTC",
        "Owner_reputation":163,
        "Owner_up_votes":152,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61566816",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59375896,
        "Question_title":"How to assign users in SageMaker Studio?",
        "Question_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-17 14:01:36.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-12-18 03:52:59.673 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1029,
        "Owner_creation_date":"2015-12-26 21:09:38.85 UTC",
        "Owner_last_access_date":"2022-01-19 17:52:30.627 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-27 00:21:55.467 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bulgaria",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71056275,
        "Question_title":"Terabytes of data on AWS S3, want to do data processing followed by modeling on AWS SageMaker",
        "Question_body":"<p>I have terabytes of data stored in S3 bucket in parquet format. I want to develop prototype followed by scaling. The data is really huge. Everyday, it is 50gb incremental. It is structured conventional data (is not image, video or audio). The history for prototype isn't decided yet but if it is 3 months, then it will be 90days x 50gb = 4500gb for prototype or (9000 gb for 6 months).<\/p>\n<p>I want to do data processing, derive some new variables, EDA followed by modelling (feature engineering &amp; implement unsupervised deep learning algorithms). Can anyone suggest me the best way here? For e.g. Use sagemaker notebook, write data processing python scripts there, save processed data to S3 folder, and then apply algorithms? or use EMR for data processing followed by SageMaker for EDA+Modeling. Or any other best way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-09 20:27:53.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-10 05:53:47.657 UTC",
        "Question_score":0,
        "Question_tags":"amazon-s3|bigdata|amazon-emr|amazon-sagemaker|data-processing",
        "Question_view_count":160,
        "Owner_creation_date":"2022-02-09 20:08:32.23 UTC",
        "Owner_last_access_date":"2022-07-04 20:11:54.337 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71056275",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59921196,
        "Question_title":"Can you load a SageMaker trained model into Keras?",
        "Question_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-26 18:02:24.52 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"keras|object-detection|amazon-sagemaker",
        "Question_view_count":212,
        "Owner_creation_date":"2018-08-05 10:39:44.553 UTC",
        "Owner_last_access_date":"2022-09-23 09:26:07.4 UTC",
        "Owner_reputation":341,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-01-27 10:11:23.43 UTC",
        "Answer_score":2.0,
        "Owner_location":"Bucharest, Romania",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73040175,
        "Question_title":"not able to update trust policy for a role",
        "Question_body":"<p>I am trying to create featureGroup using sagemaker API in ec2 instance.\ngot below error while running python script which creates featureGroup.<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateFeatureGroup operation: The execution role ARN is invalid. Please ensure that the role exists and that its trust relationship policy allows the action 'sts:AssumeRole' for the service principal 'sagemaker.amazonaws.com'.\n<\/code><\/pre>\n<p>I observed that the role I am using doesn't have &quot;sagemaker.amazonaws.com&quot; as a Trusted entity so I tried to add that however getting error &quot;<code>user: arn:aws:sts::xxxxxx11:assumed-role\/engineer\/abcUser is not authorized to perform: iam:UpdateAssumeRolePolicy on resource: role app-role-12345 with an explicit deny in an identity-based policy<\/code>&quot;<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;Service&quot;: [&quot;ec2.amazonaws.com&quot;,&quot;sagemaker.amazonaws.com&quot;]\n            },\n            &quot;Action&quot;: &quot;sts:AssumeRole&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I tried through terraform as well<\/p>\n<pre><code>data\u00a0&quot;aws_iam_policy_document&quot;\u00a0&quot;instance-assume-role-policy&quot;\u00a0{\n\u00a0\u00a0statement\u00a0{\n\u00a0\u00a0\u00a0\u00a0actions\u00a0=\u00a0[&quot;sts:AssumeRole&quot;]\n\u00a0\u00a0\u00a0\u00a0principals\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0type\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0=\u00a0&quot;Service&quot;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0identifiers\u00a0=\u00a0[&quot;ec2.amazonaws.com&quot;, &quot;sagemaker.amazonaws.com&quot;]\n\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0}\n}\n\nresource\u00a0&quot;aws_iam_role&quot;\u00a0&quot;instance&quot;\u00a0{\n\u00a0\u00a0name\u00a0 =\u00a0&quot;engineer-12345&quot;\n\u00a0\u00a0assume_role_policy\u00a0=\u00a0data.aws_iam_policy_document.instance-assume-role-policy.json\n}\n<\/code><\/pre>\n<p>however its not working. Got access denied error.<\/p>\n<p>Can anyone help to resolve this?<\/p>\n<p>code used:<\/p>\n<pre><code>import pandas as pd\nimport sagemaker\nfrom time import gmtime, strftime, sleep\nfrom sagemaker.feature_store.feature_group import FeatureGroup\nimport time\n\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_region_name\nrole = sagemaker.get_execution_role()\n\nprint(&quot;role : &quot;,role)\n\n\nprint(&quot;start&quot;)\n\ntry:\n    customer_data = pd.read_csv(&quot;data.csv&quot;,dtype={'customer_id': int,'city_code': int, 'state_code': int, 'country_code': int, 'eventtime': float })\n\n    customers_feature_group_name = &quot;customers-fg-01&quot;\n    customers_feature_group = FeatureGroup(name=customers_feature_group_name, sagemaker_session=sagemaker_session\n                                           )\n\n    current_time_sec = int(round(time.time()))\n\n    record_identifier_feature_name = &quot;customer_id&quot;\n\n    customers_feature_group.load_feature_definitions(data_frame=customer_data)\n\n    customers_feature_group.create(\n        s3_uri=&quot;s3:\/\/xxxx\/sagemaker-featurestore\/&quot;,\n        record_identifier_name=record_identifier_feature_name,\n        event_time_feature_name=&quot;eventtime&quot;,\n        role_arn='arn:aws:iam::1234:role\/role-1234',\n        enable_online_store=True,\n        online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'\n    )\nexcept Exception as e:\n    print(str(e))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":9,
        "Question_creation_date":"2022-07-19 16:00:41.833 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-26 14:47:51.487 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|assume-role|aws-iam-policy",
        "Question_view_count":117,
        "Owner_creation_date":"2013-04-10 05:09:01.22 UTC",
        "Owner_last_access_date":"2022-09-14 22:34:07.097 UTC",
        "Owner_reputation":519,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":576,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73040175",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59357737,
        "Question_title":"aws Sagemaker autoscaling with instance metrics per instance",
        "Question_body":"<p>I am using aws Sagemaker endpoint for inference. Based upon amount of traffic, endpoint should scale up and down by adding more instance into the endpoint. I am trying to use instance metrics (CPUUtilization, MemoryUtilization or DiskUtilization) as metric for sagemaker endpoint autoscaling. These are the predefined metrics as defined here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-logs-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-logs-metrics.html<\/a><\/p>\n\n<p>The problem is that the instance metrics for a given endpoint are sum of all the running instances within an endpoint. For example in the following endpoint runtime settings:\n<a href=\"https:\/\/i.stack.imgur.com\/bKgYi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bKgYi.png\" alt=\"Example of aws sagemaker endpoint runtime settings\"><\/a><\/p>\n\n<p>Current running instances are 5 then the the value of CPUUtilization can range from 0 to 500%. Based upon the number of instances running the maximum value will change hence autoscaling policy should be changed. \nQuestion is: Is there any way to find out Metric per instance i.e. CPUUtilizationPerInstance without explicitly calculating them or through custom metric? \nAutoscaling policy of scaling up and down by setting a threshold on per instance CPUUtilization seems the right way. Is there any other similar option on aws?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2019-12-16 13:36:13.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-cloudwatch|autoscaling|amazon-sagemaker",
        "Question_view_count":1436,
        "Owner_creation_date":"2017-12-01 13:47:29.647 UTC",
        "Owner_last_access_date":"2021-07-15 00:07:03.79 UTC",
        "Owner_reputation":169,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Berlin, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59357737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68647008,
        "Question_title":"Calling PyTorch Estimator Hyperparameter in Training Script in SageMaker",
        "Question_body":"<p>In [PyTorch Estimator for SageMaker][1], it says as below.<\/p>\n<blockquote>\n<p>hyperparameters (dict) \u2013 Hyperparameters that will be used for\ntraining (default: None). The hyperparameters are made accessible as a\ndict[str, str] to the training code on SageMaker. For convenience,\nthis accepts other types for keys and values, but str() will be called\nto convert them before training.<\/p>\n<\/blockquote>\n<pre><code>estimator = PyTorch(entry_point='test_trainer.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 1,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 1,\n                                     &quot;num_labels&quot;: 2,\n                                     &quot;backend&quot;: &quot;gloo&quot;\n                         }}\n<\/code><\/pre>\n<p>So, should I declare my estimator as above and fit the estimator via my test_trainer.py, I should be able to access these values of hyperparameter within my test_trainer.py. But how exactly should I call this hyperparmeter in order to access these hyperparam values ?<\/p>\n<p>Any resource would be greatly appreciated.\n[1]: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-04 07:23:12.38 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":227,
        "Owner_creation_date":"2020-09-21 20:00:48.277 UTC",
        "Owner_last_access_date":"2022-09-16 06:59:29.497 UTC",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seoul, South Korea",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68647008",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59500449,
        "Question_title":"AWS SageMaker EndPoint returns 415",
        "Question_body":"<p>I have trained a multiclass classification model on the wine quality dataset and I have deployed the model.\nAfter deploying the model I got EndPoint URL like:<\/p>\n\n<p><a href=\"https:\/\/runtime.sagemaker.region.amazonaws.com\/endpoints\/experiment\/invocations\" rel=\"nofollow noreferrer\">https:\/\/runtime.sagemaker.region.amazonaws.com\/endpoints\/experiment\/invocations<\/a><\/p>\n\n<p>And I am invoking the URL after passing AWS credentials and body like:<\/p>\n\n<p><code>{\n  \"instances\": [7.4,0.7,0,1.9,0.076,11,34,0.9978,3.51,0.56,9.4]\n}<\/code><\/p>\n\n<p>But I am getting below error:<\/p>\n\n<p><code>{\n    \"ErrorCode\": \"CLIENT_ERROR_FROM_MODEL\",\n    \"LogStreamArn\": \"\",\n    \"OriginalMessage\": \"'application\/json' is an unsupported content type.\",\n    \"OriginalStatusCode\": 415\n}<\/code><\/p>\n\n<p>I tried looking for the trace logs in the cloud watch but no traces there as well. Anyone could guide me on this?<\/p>\n\n<p>I have trained a model using Sage Maker Studion.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-27 11:57:00.647 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-12-27 12:46:22.78 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|awsdeploy",
        "Question_view_count":795,
        "Owner_creation_date":"2014-06-12 07:28:21.253 UTC",
        "Owner_last_access_date":"2022-09-24 14:09:54.573 UTC",
        "Owner_reputation":962,
        "Owner_up_votes":116,
        "Owner_down_votes":239,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Hyderabad, Telangana, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59500449",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70191668,
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-01 21:50:43.06 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mlops",
        "Question_view_count":716,
        "Owner_creation_date":"2020-01-05 18:52:39.257 UTC",
        "Owner_last_access_date":"2022-09-18 18:55:51.313 UTC",
        "Owner_reputation":197,
        "Owner_up_votes":53,
        "Owner_down_votes":7,
        "Owner_views":49,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2021-12-01 22:01:10.903 UTC",
        "Answer_score":2.0,
        "Owner_location":"Amsterdam",
        "Answer_last_edit_date":"2022-06-02 19:24:10.167 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54172907,
        "Question_title":"Amazon Sagemaker. AccessDeniedException when calling the InvokeEndpoint operation",
        "Question_body":"<p>I have deployed an Endpoint on Amazon SageMaker.\nNow I am trying to Invoke it.<\/p>\n\n<p>If I run this code in Sagemaker's Jupyter Notebook: <\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>it works properly.<\/p>\n\n<p>But if I run the same code, with added credentials for boto3 client, from my machine:<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime', \n                       aws_access_key_id=ACCESS_ID,\n                       aws_secret_access_key= ACCESS_KEY)\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>I get this error:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::249707424405:user\/yury.logachev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/demo-xgboostendpoint-2018-12-12-22-07-28 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>If I run the latter piece of code (with added credentials as a parameters of client) on Sagemaker's Jupyter Notebook, I also get the same error.<\/p>\n\n<p>I understand that the solution should be linked with roles, policies etc, but could not find out it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-13 20:25:52.043 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-01-16 18:40:21.823 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|web|amazon-sagemaker",
        "Question_view_count":2435,
        "Owner_creation_date":"2011-12-27 12:21:49.37 UTC",
        "Owner_last_access_date":"2022-09-22 11:58:08.16 UTC",
        "Owner_reputation":1593,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":93,
        "Answer_body":"<p>The problem was with the MFA autharization. \nWhen I invoked the model from inside the model, the MFA was passed. \nBut when I tried to invoke the model from my machine, the MFA was not passed, so the access was denied.<\/p>\n\n<p>I created special user without MFA to debug the model, and that solved my problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-16 18:44:23.5 UTC",
        "Answer_score":1.0,
        "Owner_location":"Moscow, Russia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54172907",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54621379,
        "Question_title":"Correct parameters for training AWS Sagemaker with multiple classes per image",
        "Question_body":"<p>I've found consistently that \"multi_label\" to \"1\" for image classification jobs, they crash with the following error:<\/p>\n\n<pre><code>Algorithm Error: Internal Server Error\n[15:56:08] \/opt\/brazil-pkg-cache\/packages\/MXNetECL\/MXNetECL-master.657.0\/AL2012\/generic-flavor\/src\/src\/operator\/custom\/custom.cc:418: Check failed: reinterpret_cast&lt;CustomOpFBFunc&gt;(params.info-&gt;callbacks[kCustomOpBackward])( ptrs.size(), const_cast&lt;void**&gt;(ptrs.data()), const_cast&lt;int*&gt;(tags.data()), reinterpret_cast&lt;const int*&gt;(req.data()), static_cast&lt;int&gt;(ctx.is_train), params.info-&gt;contexts[kC\uf141\n15:56:08 Stack trace returned 7 entries:\n15:56:08 [bt] (0) \/opt\/amazon\/lib\/libaialgsdataiter.so(dmlc::StackTrace()+0x3d) [0x7f85e19f179d]\n15:56:08 [bt] (1) \/opt\/amazon\/lib\/libaialgsdataiter.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x1a) [0x7f85e19f1a3a] \uf141\n15:56:08 [bt] (2) \/opt\/amazon\/lib\/libmxnet.so(+0x26da8fd) [0x7f85d0edb8fd]\n15:56:08 [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(std::thread::_Impl&lt;std::_Bind_simple&lt;mxnet::op::custom::CustomOperator::CustomOperator()::{lambda()#1} ()&gt; &gt;::_M_run()+0x12f) [0x7f85d0ede0ef]\n15:56:08 [bt] (4) \/opt\/amazon\/lib\/libstdc++.so.6(+0xce440) [0x7f85cc9ea440]\n15:56:08 [bt] (5) \/lib64\/libpthread.so.0(+0x7dc5) [0x7f85e31e1dc5]\n15:56:08 [bt] (6) \/lib64\/libc.so.6(clone+0x6d) [0x7f85e25de6ed]\n15:56:08 Algorithm Error: Internal Server Error\n<\/code><\/pre>\n\n<p>Based my understanding of the documentation, this parameter should let you assign multiple tags to each image - is there a trick to get it to work, or to debugging these stack traces? (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/IC-Hyperparameter.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/IC-Hyperparameter.html<\/a>)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-10 21:43:13.73 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":142,
        "Owner_creation_date":"2012-09-23 23:56:52.63 UTC",
        "Owner_last_access_date":"2022-09-22 19:11:36.75 UTC",
        "Owner_reputation":346,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54621379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63153563,
        "Question_title":"Problem populating prior labels in adjustment job [AWS Groundtruth labeling]",
        "Question_body":"<p>I'm using the console CLI commands to create new as well adjustment jobs.<br \/>\nI was successfully able to create new jobs for a folder where images are lying but <strong>having issues<\/strong> while creating adjustment job to a completed job.<\/p>\n<p>I am using the below command to create the adjustment job<\/p>\n<pre><code>aws sagemaker create-labeling-job --labeling-job-name &quot;test-insulator-relabeling-29-7-2020-v1&quot; --label-attribute-name &quot;insulator-1&quot; --input-config DataSource={S3DataSource={ManifestS3Uri=&quot;s3:\/\/s3-resized-images\/C73\/P878813J\/OUTPUT\/P878813J-objectdetection-2\/manifests\/output\/output.manifest&quot;}},DataAttributes={ContentClassifiers=[&quot;&quot;]} --output-config S3OutputPath=&quot;s3:\/\/prasanna-test-bucket\/Sample images folder\/OUTPUT\/&quot; --role-arn &quot;confidential&quot; --label-category-config-s3-uri &quot;s3:\/\/s3-resized-images\/Consolidated-Labels\/Transformer-Insulator-labels.json&quot; --stopping-conditions MaxPercentageOfInputDatasetLabeled=100 --human-task-config WorkteamArn=&quot;confidential&quot;,UiConfig={UiTemplateS3Uri=&quot;s3:\/\/venki-test-raw-images\/template1.liquid&quot;},PreHumanTaskLambdaArn=&quot;arn:aws:lambda:us-east-1:432418664414:function:PRE-AdjustmentBoundingBox&quot;,TaskKeywords=&quot;Images&quot;,&quot;bounding boxes&quot;,&quot;object detection&quot;,&quot;adjustment&quot;,TaskTitle=&quot;test-insulator-relabeling-29-7-2020-v1&quot;,TaskDescription=&quot;test-insulator-relabeling-29-7-2020-v1&quot;,TaskTimeLimitInSeconds=3600,NumberOfHumanWorkersPerDataObject=1,AnnotationConsolidationConfig={AnnotationConsolidationLambdaArn=&quot;arn:aws:lambda:us-east-1:432418664414:function:ACS-AdjustmentBoundingBox&quot;}\n<\/code><\/pre>\n<p>As soon as I launch the command the job is getting auto completed without the option to adjust the previous jobs labels.<br \/>\nI was able to succesfully created adjustment jobs in AWS UI through browser however the disadvantage there is that i am restricted to previous jobs labels and i cant add any new.<\/p>\n<p>I am trying to figure where I'm going wrong since I made sure all the paramenters required for the adjustment job are given correctly the moment adjustment job launched using CLI as per the above command the job status is getting auto completed.<\/p>\n<p>Can someone help what needs to be understood here to launch to correctly and be able to see previous labels performed in prior job?\nI am taking care of the config as below<\/p>\n<p><strong>Input<\/strong> : previous jobs output manifest address in S3<\/p>\n<p><strong>label attribute name<\/strong> : same as the prior job<\/p>\n<p><strong>PreHumanTaskLambdaArn<\/strong>=&quot;arn:aws:lambda:us-east-1:432418664414:function:PRE-AdjustmentBoundingBox&quot;<\/p>\n<p><strong>AnnotationConsolidationLambdaArn<\/strong>=&quot;arn:aws:lambda:us-east-1:432418664414:function:ACS-AdjustmentBoundingBox&quot;<\/p>\n<p><strong>TaskKeywords<\/strong>=&quot;Images&quot;,&quot;bounding boxes&quot;,&quot;object detection&quot;,&quot;adjustment&quot;<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-29 12:25:55.877 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-30 06:53:52.287 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker|labeling",
        "Question_view_count":214,
        "Owner_creation_date":"2020-07-29 12:11:35.48 UTC",
        "Owner_last_access_date":"2020-09-03 12:46:41.283 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63153563",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52244963,
        "Question_title":"Impossible to invoke endpoint with sagemaker",
        "Question_body":"<p>I am using aws sagemaker to invoke the endpoint : <\/p>\n\n<pre><code>payload = pd.read_csv('payload.csv', header=None)\n\n&gt;&gt; payload\n\n\n    0   1   2   3   4\n0   setosa  5.1     3.5     1.4     0.2\n1   setosa  5.1     3.5     1.4     0.2\n<\/code><\/pre>\n\n<p>with this code :<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>But I got this problem : <\/p>\n\n<pre><code>ParamValidationError                      Traceback (most recent call last)\n&lt;ipython-input-304-f79f5cf7e0e0&gt; in &lt;module&gt;()\n      1 response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n      2                                    ContentType='text\/csv',\n----&gt; 3                                    Body=payload)\n      4 \n      5 result = json.loads(response['Body'].read().decode())\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    584         }\n    585         request_dict = self._convert_to_request_dict(\n--&gt; 586             api_params, operation_model, context=request_context)\n    587 \n    588         handler, event_response = self.meta.events.emit_until_response(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _convert_to_request_dict(self, api_params, operation_model, context)\n    619             api_params, operation_model, context)\n    620         request_dict = self._serializer.serialize_to_request(\n--&gt; 621             api_params, operation_model)\n    622         prepare_request_dict(request_dict, endpoint_url=self._endpoint.host,\n    623                              user_agent=self._client_config.user_agent,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/validate.py in serialize_to_request(self, parameters, operation_model)\n    289                                                     operation_model.input_shape)\n    290             if report.has_errors():\n--&gt; 291                 raise ParamValidationError(report=report.generate_report())\n    292         return self._serializer.serialize_to_request(parameters,\n    293                                                      operation_model)\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value:         0    1    2    3    4\n0  setosa  5.1  3.5  1.4  0.2\n1  setosa  5.1  3.5  1.4  0.2, type: &lt;class 'pandas.core.frame.DataFrame'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I am just using the same code\/step like in the aws tutorial .  <\/p>\n\n<p>Can you help me to resolve this problem please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-09-09 13:24:52.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":4668,
        "Owner_creation_date":"2018-02-14 14:17:32.857 UTC",
        "Owner_last_access_date":"2019-10-30 10:34:04.507 UTC",
        "Owner_reputation":495,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":"<p>The payload variable is a Pandas' DataFrame, while <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">invoke_endpoint()<\/a> expects  <code>Body=b'bytes'|file<\/code>.<\/p>\n\n<p>Try something like this (coding blind):<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=open('payload.csv'))\n<\/code><\/pre>\n\n<p>More on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-inference.html\" rel=\"nofollow noreferrer\">expected formats here<\/a>. \nMake sure the file doesn't include a header.<\/p>\n\n<p>Alternatively, convert your DataFrame to bytes, <a href=\"https:\/\/stackoverflow.com\/questions\/34666860\/converting-pandas-dataframe-to-bytes\">like in this example<\/a>, and pass those bytes instead of passing a DataFrame.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-09-11 19:13:14.577 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52244963",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57010184,
        "Question_title":"XGBoost Model in AWS-Sagemaker Fails with no error message",
        "Question_body":"<p>I'm trying to get a model using the XGBoost classifier in AWS-Sagemaker.  I'm following the abalone example, but when I run it to build the training job it states InProgress 3 times and then just states Failed.  Where do I go to find why it failed?  <\/p>\n\n<p>I've double checked the parameters and made sure the input and output files and directories in S3 were correct.  I know there is permission to read and write because when setting up the data for train\/validate\/test I read and write to S3 with no problems.<\/p>\n\n<pre><code>print(status)\nwhile status !='Completed' and status!='Failed':\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n    print(status)\n<\/code><\/pre>\n\n<p>That is the code where the print statements come from.  Is there something I can add to receive a better error message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-07-12 15:44:33.023 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":204,
        "Owner_creation_date":"2015-05-29 03:50:33.557 UTC",
        "Owner_last_access_date":"2019-12-30 21:07:55.38 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-30 13:44:54.317 UTC",
        "Answer_score":0.0,
        "Owner_location":"Abilene, TX, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57010184",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62422682,
        "Question_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Question_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-17 06:25:29.67 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":350,
        "Owner_creation_date":"2011-07-15 03:14:53.433 UTC",
        "Owner_last_access_date":"2022-09-22 06:31:23.927 UTC",
        "Owner_reputation":2889,
        "Owner_up_votes":79,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Answer_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-06-23 01:37:54.34 UTC",
        "Answer_score":-1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73817379,
        "Question_title":"How can I preprocess inputs sent to a hugging face estimator?",
        "Question_body":"<p>I've been reviewing tutorials like the one found <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">here<\/a> which detail how to train a huggingface estimator (specifically, a transformer model) and then deploy it to sagemaker.<\/p>\n<p>The tutorial linked above trains an estimator via:<\/p>\n<pre><code>huggingface_estimator = HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py37',\n                            role=role,\n                            hyperparameters = {'epochs': 1,\n                                               'train_batch_size': 32,\n                                               'model_name':'distilbert-base-uncased'\n                                                })\n<\/code><\/pre>\n<p>Where <code>train.py<\/code> outlines a training script which tokenizes input data, and then fine-tunes a transformer model on the training data. <em>Note: in this example, the training data is hard coded into <code>train.py<\/code>, but that is not the source of the issue I'm encountering.<\/em><\/p>\n<p>The model is fit using<\/p>\n<pre><code>huggingface_estimator.fit()\n<\/code><\/pre>\n<p>and is then deployed using<\/p>\n<pre><code>predictor = huggingface_estimator.deploy(1,&quot;ml.g4dn.xlarge&quot;)\n<\/code><\/pre>\n<p>But then this deployed model is used to make a prediction via:<\/p>\n<pre><code>sentiment_input= {&quot;inputs&quot; : &quot;I love using the new Inference DLC.&quot;}\npredictor.predict(sentiment_input)\n<\/code><\/pre>\n<p>The problem is that no one specifed <em>anywhere<\/em> how this input is to be preprocessed: the model does not work on raw text, the text must be tokenized. Even the official <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs<\/a> for sagemaker don't seem to outline how preprocessing for a deployed model is handled.<\/p>\n<p>How can I specify a preprocessing step for my model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 15:40:14.277 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":8,
        "Owner_creation_date":"2019-06-17 15:38:47.3 UTC",
        "Owner_last_access_date":"2022-09-25 02:14:02.827 UTC",
        "Owner_reputation":4613,
        "Owner_up_votes":1267,
        "Owner_down_votes":633,
        "Owner_views":401,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Ontario, Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73817379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56263995,
        "Question_title":"Does the library multprocesing works on sagemaker conda-python3?",
        "Question_body":"<p>Does the library multprocesing works well on sagemaker conda-python3?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-22 19:49:24.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|python-multiprocessing|amazon-sagemaker",
        "Question_view_count":723,
        "Owner_creation_date":"2016-03-19 19:45:07.413 UTC",
        "Owner_last_access_date":"2022-08-25 22:03:06.273 UTC",
        "Owner_reputation":617,
        "Owner_up_votes":74,
        "Owner_down_votes":0,
        "Owner_views":117,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Ecuador, Quito",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56263995",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49548422,
        "Question_title":"Training data in S3 in AWS Sagemaker",
        "Question_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-29 05:07:08.073 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|mxnet|amazon-sagemaker",
        "Question_view_count":1353,
        "Owner_creation_date":"2015-12-02 04:47:49.083 UTC",
        "Owner_last_access_date":"2022-09-23 18:14:54.043 UTC",
        "Owner_reputation":777,
        "Owner_up_votes":119,
        "Owner_down_votes":2,
        "Owner_views":103,
        "Answer_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-03-29 06:37:55.727 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49299079,
        "Question_title":"Cloudpickle.dump(pyspark_Alsmodel_object),getting error py4j.Py4JException: Method __getnewargs__([]) does not exist?",
        "Question_body":"<p>After creating ALS model object,using pyspark. <\/p>\n\n<p>Sample code example:<\/p>\n\n<pre><code>from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\n\nlines = spark.read.text(\"data\/mllib\/als\/sample_movielens_ratings.txt\").rdd\nparts = lines.map(lambda row: row.value.split(\"::\"))\nratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n                                     rating=float(p[2]), timestamp=long(p[3])))\nratings = spark.createDataFrame(ratingsRDD)\n(rating_data, test) = ratings.randomSplit([0.8, 0.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\n\n    als_model = als_spec.fit(rating_data)\n<\/code><\/pre>\n\n<p>here i am just creating ALS model and making cloudepickel.\nif we are using fit then also need to do transform?<\/p>\n\n<p>I am trying pickel the my als_model object using the below code :<\/p>\n\n<pre><code>with open(os.path.join(model_path, 'als-als-model.pkl'), 'w') as out:\n                cloudpickle.dump(als_model, out)\n<\/code><\/pre>\n\n<p>I am getting error like below:<\/p>\n\n<pre><code>  File \"\/usr\/local\/spark\/python\/lib\/py4j-0.10.6-src.zip\/py4j\/protocol.py\", line 324, in get_return_value\n    format(target_id, \".\", name, value))\nPy4JError: An error occurred while calling o224.__getnewargs__. Trace:\npy4j.Py4JException: Method __getnewargs__([]) does not exist\n#011at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n#011at \n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-124-8c94f4ee0de9&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 tree.fit(data_location)\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name)\n    152         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    153         if wait:\n--&gt; 154             self.latest_training_job.wait(logs=logs)\n    155         else:\n    156             raise NotImplemented('Asynchronous fit not available')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2018-03-15 12:06:49.45 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-03-16 11:08:40.33 UTC",
        "Question_score":0,
        "Question_tags":"python|apache-spark|pyspark|pyspark-sql|amazon-sagemaker",
        "Question_view_count":255,
        "Owner_creation_date":"2015-04-03 14:45:43.217 UTC",
        "Owner_last_access_date":"2022-01-14 00:51:53.803 UTC",
        "Owner_reputation":1015,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":304,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49299079",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62341557,
        "Question_title":"How do I verify that an interface VPC endpoint is functioning well?",
        "Question_body":"<p>I am pretty new to AWS, and here is my scenario:\nI create a sagemaker endpoint that can be called with AWS SDK IAmazonSageMakerRuntime::InvokeEndpointAsync() from a EC2 web server in a private subnet over public internet via NAT instance. <\/p>\n\n<pre><code>using (MemoryStream ms = new MemoryStream(Encoding.UTF8.GetBytes(mydata)))\n{\n   InvokeEndpointRequest req = new InvokeEndpointRequest()\n   {\n      EndpointName = \"MyEndpoint\",\n      ContentType = \"application\/json\",\n      Body = ms\n   };\n   var resp = await _sageMakerRuntime.InvokeEndpointAsync(req);\n}\n<\/code><\/pre>\n\n<p>And to further secure the connection and avoid traffic over public internet, I set up a interface VPC endpoint(the associated security group inbound rule allows the cidr ip range in my VPC with port 80\/443) for SageMakerRuntime service to take advantage of the benefits from using AWS PrivteConnection. <\/p>\n\n<pre><code>EndpointSageMakerSecurityGroup:\nType: AWS::EC2::SecurityGroup\nProperties:\n  GroupDescription: Enable access from app to interface VPC endpoint for sagemaker runtime\n  VpcId: !Ref VPC\n  SecurityGroupIngress:\n  - CidrIp: 10.50.0.0\/16\n    IpProtocol: tcp\n    ToPort: 80\n    FromPort: 80\n  - CidrIp: 10.50.0.0\/16\n    IpProtocol: tcp\n    ToPort: 443\n    FromPort: 443\n  Tags:\n  - Key: Name\n    Value: !Sub \"${AWS::StackName}-EndpointSageMakerSecurityGroup\"\n\nEndpointSageMakerRuntime:\n    Type: AWS::EC2::VPCEndpoint\n    Properties:\n      PolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Principal: '*'\n            Action:\n              - sagemaker:InvokeEndpoint\n            Resource: !Sub arn:${AWS::Partition}:sagemaker:${AWS::Region}:${AWS::AccountId}:endpoint\/MyEndpoint      \n      SubnetIds: !Ref PrivateSubnets\n      SecurityGroupIds:\n      - !Ref EndpointSageMakerSecurityGroup\n      ServiceName: !Sub com.amazonaws.${AWS::Region}.sagemaker.runtime #runtime.sagemaker.${AWS::Region}.amazonaws.com\n      PrivateDnsEnabled: true\n      VpcEndpointType: Interface\n      VpcId: !Ref VPC\n<\/code><\/pre>\n\n<p>However, I really have no idea whether the interface VPC endpoint does take effect and my function call InvokeEndpointAsync() indeed calls to the interface VPC endpoint into AWS SageMakerRuntime service. I tried creating a notification in the interface VPC endpoint to publish message to a SNS topic and subscribing that SNS topic with my email, but I was not able to get any email from it. I guess there is something wrong but can't not figure out how to get the interface VPC endpoint right. Any ideas would be appreciated. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2020-06-12 09:40:08.8 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-12 09:49:52.603 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_date":"2013-08-08 02:27:50.1 UTC",
        "Owner_last_access_date":"2021-09-22 01:44:14.14 UTC",
        "Owner_reputation":145,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62341557",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52100549,
        "Question_title":"Procedural Generation of Feature Columns for use with Sagemaker Tensorflow Instance",
        "Question_body":"<p>Have exhausted myself on this one so any help would be appreciated.<\/p>\n\n<p>I am trying to set up hosting my tensorflow model with Amazon Sagemaker and following the example found <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>This example uses hard coded feature columns with known dimensionality. <\/p>\n\n<pre><code>feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n<\/code><\/pre>\n\n<p>I need to avoid this as my dataset changes often.<\/p>\n\n<h1>Local Machine Set Up<\/h1>\n\n<p>Now on my local machine, I define a list of columns <\/p>\n\n<pre><code>my_feature_columns = []\n<\/code><\/pre>\n\n<p>With the following strategy<\/p>\n\n<pre><code>#Define placeholder nodes based on datatype being inserted\n\nfor key in train_x.keys():\n<\/code><\/pre>\n\n<p>Where train_x is the dataset without labels.<\/p>\n\n<p>'OBJECTS' become hashed buckets as there are many possible categories<\/p>\n\n<pre><code>    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n<\/code><\/pre>\n\n<p>'INT64' become categorical columns as there are only two possible categories (I have recoded booleans to 0\/1)<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n<\/code><\/pre>\n\n<p>'FLOATS' become continuous columns<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n<\/code><\/pre>\n\n<p>On the local machine this yields a nice list of all of my features that can be given as an argument when instantiating a tf.estimator.DNNClassifier. As more categories are added to each OBJECT column, this is handled by<\/p>\n\n<pre><code>hash_bucket_size = len(train_x[key].unique())\n<\/code><\/pre>\n\n<h1>Sagemaker<\/h1>\n\n<p>From the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">Docs<\/a><\/p>\n\n<p><em>Preparing the TensorFlow training script\nYour TensorFlow training script must be a Python 2.7 source file. The SageMaker TensorFlow docker image uses this script by calling specifically-named functions from this script.<\/em><\/p>\n\n<p><em>The training script must contain the following:<\/em><\/p>\n\n<p><em>Exactly one of the following:\nmodel_fn: defines the model that will be trained.\nkeras_model_fn: defines the tf.keras model that will be trained.\nestimator_fn: defines the tf.estimator.Estimator that will train the model.<\/em><\/p>\n\n<p><em>train_input_fn: preprocess and load training data.<\/em><\/p>\n\n<p><em>eval_input_fn: preprocess and load evaluation data.<\/em><\/p>\n\n<p>Again, from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">example<\/a><\/p>\n\n<pre><code>def train_input_fn(training_dir, params):\n\"\"\"Returns input function that would feed the model during training\"\"\"\nreturn _generate_input_fn(training_dir, 'iris_training.csv')\n<\/code><\/pre>\n\n<p>This function is called by the sagemaker docker image, which adds its own argument for <strong>training_dir<\/strong>, it is not a global parameter.<\/p>\n\n<p>When trying to access my training data from the estimator_fn to build a my_feature_columns list<\/p>\n\n<pre><code>NameError: global name 'training_dir' is not defined\n<\/code><\/pre>\n\n<h1>I would love to be able to do something like this.<\/h1>\n\n<pre><code>def estimator_fn(run_config, params):\n\nmy_feature_columns = []\n\ntrain_x , _ , _ , _ = datasplitter(os.path.join(training_dir, 'leads_test_frame.csv'))\n\nfor key in train_x.keys():\n    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n\n    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n\nreturn tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n                                  hidden_units=[10, 20, 10],\n                                  n_classes=2,\n                                  config=run_config)\n<\/code><\/pre>\n\n<p>Thanks to anyone who can help in any way. Will happily give more info if needed but feel like 4 pages is probably enough :-S<\/p>\n\n<p>Cheers!\nClem<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-30 15:23:35.98 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":165,
        "Owner_creation_date":"2017-04-05 19:23:13.25 UTC",
        "Owner_last_access_date":"2021-11-03 10:16:50.537 UTC",
        "Owner_reputation":163,
        "Owner_up_votes":81,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Answer_body":"<p><strong>training_dir<\/strong> points to your training channel, i.e. <em>\/opt\/ml\/input\/data\/training<\/em>. You can hardcode this location inside your <strong>estimation_fn<\/strong>.<\/p>\n\n<p>When training starts, SageMaker makes the data for the channel available in the <em>\/opt\/ml\/input\/data\/<strong>channel_name<\/em><\/strong> directory in the Docker container.<\/p>\n\n<p>You can find more information here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-09-25 01:00:58.77 UTC",
        "Answer_score":1.0,
        "Owner_location":"Windsor, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52100549",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53660590,
        "Question_title":"PySpark Using collect_list to collect Arrays of Varying Length",
        "Question_body":"<p>I am attempting to use collect_list to collect arrays (and maintain order) from two different data frames.<\/p>\n\n<p>Test_Data and Train_Data have the same format.<\/p>\n\n<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy('Group').orderBy('date')\n\n# Train_Data has 4 data points\n# Test_Data has 7 data points\n# desired target array:         [1, 1, 2, 3]\n# desired MarchMadInd array:    [0, 0, 0, 1, 0, 0, 1]\n\nsorted_list_diff_array_lens = train_data.withColumn('target', \nF.collect_list('target').over(w)\n                                  )\\\ntest_data.withColumn('MarchMadInd', F.collect_list('MarchMadInd').over(w))\\\n   .groupBy('Group')\\\n   .agg(F.max('target').alias('target'), \n    F.max('MarchMadInd').alias('MarchMadInd')\n)\n<\/code><\/pre>\n\n<p>I realize the syntax is incorrect with \"test_data.withColumn\", but I want to select the array for the <em>MarchMadInd<\/em> from the <strong>test_date<\/strong>, but the array for the <em>target<\/em> from the <strong>train_data<\/strong>. The desired output would look like the following:<\/p>\n\n<pre><code>{\"target\":[1, 1, 2, 3], \"MarchMadInd\":[0, 0, 0, 1, 0, 0, 1]}\n<\/code><\/pre>\n\n<p>Context: this is for a DeepAR time series model (using AWS) that requires dynamic features to include the prediction period, but the target should be historical data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-12-06 22:22:48.25 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-12-11 13:26:45.89 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|amazon-sagemaker",
        "Question_view_count":345,
        "Owner_creation_date":"2017-09-28 19:40:12.487 UTC",
        "Owner_last_access_date":"2022-09-23 21:00:56.663 UTC",
        "Owner_reputation":582,
        "Owner_up_votes":211,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":"<p>The solution involves using a join as recommended by pault. <\/p>\n\n<ol>\n<li>Create a dataframe with dynamic features of length equal to Training + Prediction period<\/li>\n<li>Create a dataframe with target values of length equal to just the Training period.<\/li>\n<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together<\/li>\n<\/ol>\n\n<p>Now, using collect_list will create the desired result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-18 15:14:43.377 UTC",
        "Answer_score":0.0,
        "Owner_location":"Denver, CO, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53660590",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61953977,
        "Question_title":"AWS Sagemaker Lifecycle Configuration",
        "Question_body":"<p>I am working with AWS Sagemaker Notebook, now every time I start the notebook I should install packages I am working with like Librosa ( this one takes forever to be installed) so I look for a way to keep my installed packages in the notebook instance there where I found lifecycle configuration for instance and I have try every possiblity I found in net without any success always give long time error, please if some have a real good working solution help me, and thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2020-05-22 11:28:43.84 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pip|jupyter-notebook|data-science|lifecycle|amazon-sagemaker",
        "Question_view_count":240,
        "Owner_creation_date":"2017-04-02 19:11:15.13 UTC",
        "Owner_last_access_date":"2022-09-23 17:01:57.79 UTC",
        "Owner_reputation":1181,
        "Owner_up_votes":58,
        "Owner_down_votes":40,
        "Owner_views":281,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Meknes, Morocco",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61953977",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72015623,
        "Question_title":"How to write if-else statements in a lifecycle configuration script",
        "Question_body":"<p>I have a sagemaker notebook instance having two jupyter notebook ipynb files. When I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.<\/p>\n<p>Now I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.<\/p>\n<p>The trigger is file uploading into S3. Based on what location the file is added, the corresponding jupyter notebook should run<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-26 14:21:00.397 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-29 13:11:51.09 UTC",
        "Question_score":0,
        "Question_tags":"aws-lambda|amazon-sagemaker",
        "Question_view_count":76,
        "Owner_creation_date":"2020-01-03 17:09:09.303 UTC",
        "Owner_last_access_date":"2022-09-20 13:22:32.15 UTC",
        "Owner_reputation":415,
        "Owner_up_votes":105,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Hyderabad India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72015623",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69884834,
        "Question_title":"How to set SAGEMAKER_SUBMIT_DIRECTORY environment variable in sagemaker",
        "Question_body":"<p>I am trying to deploy a model that i have registered. I registered the model using the following code:<\/p>\n<pre><code>step_register = RegisterModel(\n    name=&quot;RegisterCustomModel&quot;,\n    estimator=estimator,\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n    transform_instances=[&quot;ml.m5.large&quot;],\n    model_package_group_name=model_package_group_name,\n    approval_status=model_approval_status,\n    model_metrics=model_metrics,\n)\n<\/code><\/pre>\n<p>However, I am getting an error when i deploy this model which I believe is because the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY<\/code> is not set.<\/p>\n<p>My question is, can I set the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY<\/code> in the <code>RegisterModel<\/code> function and if I can, how do I do that?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-08 14:09:01.73 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":444,
        "Owner_creation_date":"2018-03-22 16:45:24.657 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747 UTC",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Milton Keynes",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69884834",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57599389,
        "Question_title":"how to fix pop from empty list error when using keras.layers.Dot in GPU mode",
        "Question_body":"<p>I'm trying to train a simple model with embeddings on Sagemaker, using Mxnet backhand and aiming to run across multiple GPUs.<\/p>\n\n<p>The line below is causing an error:<\/p>\n\n<pre><code>   preprod = keras.layers.Dot(axes=1, normalize=False)([bet_vec, user_vec])\n<\/code><\/pre>\n\n<p>Whenever I use keras.layers.Dot I get the following error msg:<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/keras\/backend\/mxnet_backend.py\", line 2292, in squeeze\n    assert shape.pop(axis) == 1, 'Can only squeeze size 1 dimension'\nIndexError: pop from empty list<\/p>\n\n<p>The code runs fine locally but it does not work using Mxnet\/GPUs...<\/p>\n\n<p>If anyone could help it would be much appreciated.<\/p>\n\n<p>Thank you :-)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2019-08-21 21:31:40.957 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-08-22 13:27:42.637 UTC",
        "Question_score":1,
        "Question_tags":"keras|deep-learning|gpu|amazon-sagemaker|mxnet",
        "Question_view_count":487,
        "Owner_creation_date":"2015-10-29 14:52:26.223 UTC",
        "Owner_last_access_date":"2022-09-22 13:36:12.89 UTC",
        "Owner_reputation":461,
        "Owner_up_votes":43,
        "Owner_down_votes":0,
        "Owner_views":74,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57599389",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49826004,
        "Question_title":"AWS S3 bucket write error",
        "Question_body":"<p>I created AWS S3 bucket and tried sample kmeans example on Jupyter notebook.\nBeing account owner I have read\/write permissions but I am unable to write logs with following error, <\/p>\n\n<pre><code> ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>here's the kmeans sample code, <\/p>\n\n<pre><code> from sagemaker import get_execution_role\n role = get_execution_role()\n bucket='testingshk' \n\n import pickle, gzip, numpy, urllib.request, json\nurllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", \"mnist.pkl.gz\")\n with gzip.open('mnist.pkl.gz', 'rb') as f:\n train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n\n from sagemaker import KMeans\n data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\n output_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\n print('training data will be uploaded to: {}'.format(data_location))\n print('training artifacts will be uploaded to: {}'.format(output_location))\n\n kmeans = KMeans(role=role,\n            train_instance_count=2,\n            train_instance_type='ml.c4.8xlarge',\n            output_path=output_location,\n            k=10,\n            data_location=data_location)\n kmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-13 22:36:17.623 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":915,
        "Owner_creation_date":"2014-05-17 06:10:37.673 UTC",
        "Owner_last_access_date":"2019-03-06 14:28:20.027 UTC",
        "Owner_reputation":83,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>Even if you have all the access to the bucket, you need to provide access key and secret in order to put some object in bucket if it is private. Or if you make bucket access public to all then you can push object to bucket without any problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-13 23:21:40.917 UTC",
        "Answer_score":1.0,
        "Owner_location":"Austria",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49826004",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67123040,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-04-16 09:53:03.027 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-16 14:57:49.787 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1216,
        "Owner_creation_date":"2013-05-22 21:25:42.213 UTC",
        "Owner_last_access_date":"2022-09-21 16:01:33.95 UTC",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-16 10:18:25.233 UTC",
        "Answer_score":2.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72592998,
        "Question_title":"Is it possible to perform local dev on a CPU-only machine on HF\/sagemaker?",
        "Question_body":"<p>I'm trying to dev locally on <code>sagemaker.huggingface.HuggingFace<\/code> before moving to sagemaker for actual training. I set up a<\/p>\n<p><code>HF_estimator = HuggingFace(entry_point='train.py', instance_type='local' ...)<\/code><\/p>\n<p>And called <code>HF_estimator.fit()<\/code><\/p>\n<p>In <code>train.py<\/code> im simply printing and exiting to see if it will work. However I ran into this:<\/p>\n<pre><code>ValueError: Unsupported processor: cpu. You may need to upgrade your SDK version (pip install -U sagemaker) for newer processors. Supported processor(s): gpu.\n<\/code><\/pre>\n<p>Is it possible to bypass this for local development?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-12 14:13:13.093 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|huggingface",
        "Question_view_count":133,
        "Owner_creation_date":"2013-05-23 22:24:43.58 UTC",
        "Owner_last_access_date":"2022-09-24 14:44:15.853 UTC",
        "Owner_reputation":5626,
        "Owner_up_votes":211,
        "Owner_down_votes":1,
        "Owner_views":872,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Portland, OR",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72592998",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70181884,
        "Question_title":"SageMaker Speed in different region",
        "Question_body":"<p>I am working on AWS SageMaker service.  First, I ran my notebook file under the region , &quot;Hong Kong&quot;, this work cost me around 5 minutes. However, after I switched to &quot;Tokyo&quot;, the same script spend 8 minutes for executing.<\/p>\n<p>Does region effect script's runtime ?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2021-12-01 09:34:39.287 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-03 01:46:41.613 UTC",
        "Question_score":1,
        "Question_tags":"runtime|amazon-sagemaker",
        "Question_view_count":32,
        "Owner_creation_date":"2021-09-14 03:53:11.133 UTC",
        "Owner_last_access_date":"2022-02-08 01:35:13.383 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70181884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59949130,
        "Question_title":"Install Jupytext plugin on AWS Sagemaker",
        "Question_body":"<p><a href=\"https:\/\/github.com\/mwouts\/jupytext\" rel=\"nofollow noreferrer\">Jupytext<\/a> allows you to save your notebook as a plain python or markdown file. One of the advantages is that you can do an easy git diff in merge requests.<\/p>\n\n<p>How can you install the jupytext plugin on the jupyter\/jupyterlab environment on AWS Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-28 12:48:38.597 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2020-01-29 07:16:49.97 UTC",
        "Question_score":2,
        "Question_tags":"jupyter-notebook|jupyter|amazon-sagemaker|jupyter-lab",
        "Question_view_count":754,
        "Owner_creation_date":"2012-10-24 12:12:59.277 UTC",
        "Owner_last_access_date":"2022-09-25 05:49:23.95 UTC",
        "Owner_reputation":3126,
        "Owner_up_votes":1817,
        "Owner_down_votes":2,
        "Owner_views":262,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Leuven, Belgium",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59949130",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61052173,
        "Question_title":"Is there a limit on input json string for aws sagemaker endpoint?",
        "Question_body":"<p>I have ~5MB json string that I want to send to my endpoint. I am using boto3.client to invoke the endpoint from my python client. It throws ConnectionResetError. <\/p>\n\n<pre><code>    File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1229, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 92, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1275, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1224, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 119, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 203, in send\n    return super(AWSConnection, self).send(str)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 977, in send\n    self.sock.sendall(data)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 1012, in sendall\n    v = self.send(byte_view[count:])\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 981, in send\n    return self._sslobj.write(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>\n\n<p>Looking at the trace, I am guessing it is due to json string size. Could someone please help me how to get around this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-06 02:35:41.57 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-04-06 04:14:50.857 UTC",
        "Question_score":1,
        "Question_tags":"json|python-3.x|http-post|amazon-sagemaker|urllib3",
        "Question_view_count":1281,
        "Owner_creation_date":"2020-03-17 16:09:29.093 UTC",
        "Owner_last_access_date":"2022-09-19 12:58:16.57 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.<\/p>\n\n<p>From the SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Maximum payload size for endpoint invocation |    5 MB<\/p>\n<\/blockquote>\n\n<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-04-09 08:39:09.19 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61052173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71827884,
        "Question_title":"create sagemaker notebook instance via Terraform",
        "Question_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2022-04-11 12:30:47.017 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":338,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-11 14:38:53.13 UTC",
        "Answer_score":2.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63960011,
        "Question_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Question_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-18 17:06:23.673 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":494,
        "Owner_creation_date":"2010-09-25 01:47:51.58 UTC",
        "Owner_last_access_date":"2022-04-04 12:09:39.223 UTC",
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Answer_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-29 23:01:49.39 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71706292,
        "Question_title":"AWS MLOps - Issue with SageMaker pipeline to deploy new version of model to existing endpoint",
        "Question_body":"<p>I have a problem using SageMaker pipeline for MLOps, I have followed<a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/hugging-face-lambda-step\/sm-pipelines-hugging-face-lambda-step.ipynb\" rel=\"nofollow noreferrer\"> this example<\/a>, they seems to have only example of one time deployment, my project requires to retrain model weekly, and it will be error if retrain and deploy the model again, I check on AWS document too, I cannot find any example to update model version of running endpoint, my workaround is to delete and recreate the endpoint again, but it will cause down-time<\/p>\n<p>Any suggested solution to update new model without downtime?<\/p>\n<p>Here is my code below :<\/p>\n<p>scheduler code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\n    sklearn_preprocessor = SKLearn(\n                entry_point=script_path,\n                role=role,\n                framework_version=&quot;0.23-1&quot;,\n                base_job_name=&quot;test-model&quot;,\n                instance_type=env.TRAIN_INSTANCE_TYPE,\n                sagemaker_session=sagemaker_session,\n            )\n    \n            train_step = TrainingStep(\n                name=&quot;TrainingStep&quot;,\n                display_name=&quot;Traning Step&quot;,\n                estimator=sklearn_preprocessor,\n                inputs={&quot;train&quot;: train_input},\n            )\n    \n            model = Model(\n                image_uri=sklearn_preprocessor.image_uri,\n                model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,  # pylint: disable=no-member\n                sagemaker_session=sagemaker_session,\n                role=role,\n                name=&quot;test-model&quot;,\n            )\n    \n            step_register_pipeline_model = RegisterModel(\n                name=&quot;RegisterModelStep&quot;,\n                display_name=&quot;Register Model Step&quot;,\n                model=model,\n                content_types=[&quot;text\/csv&quot;],\n                response_types=[&quot;text\/csv&quot;],\n                inference_instances=[env.TRAIN_INSTANCE_TYPE],\n                transform_instances=[env.INFERENCE_INSTANCE_TYPE],\n                model_package_group_name=&quot;test-model-group&quot;,\n                approval_status=&quot;Approved&quot;,\n            )\n    \n            inputs = CreateModelInput(\n                instance_type=env.INFERENCE_INSTANCE_TYPE,\n            )\n    \n            step_create_model = CreateModelStep(\n                name=&quot;CreateModelStep&quot;, display_name=&quot;Create Model Step&quot;, model=model, inputs=inputs\n            )\n    \n            lambda_fn = Lambda(\n                function_arn=&quot;arn:aws:lambda:ap-southeast-1:xxx:function:model-deployment&quot;\n            )\n    \n            step_deploy_lambda = LambdaStep(\n                name=&quot;DeploymentStep&quot;,\n                display_name=&quot;Deployment Step&quot;,\n                lambda_func=lambda_fn,\n                inputs={\n                    &quot;model_name&quot;: &quot;test-model&quot;,\n                    &quot;endpoint_config_name&quot;: &quot;test-model&quot;,\n                    &quot;endpoint_name&quot;: &quot;test-endpoint&quot;,\n                    &quot;model_package_arn&quot;: step_register_pipeline_model.steps[\n                        0\n                    ].properties.ModelPackageArn,\n                    &quot;role&quot;: &quot;arn:aws:iam::xxx:role\/service-role\/xxxx-role&quot;\n                },\n            )\n    \n            pipeline = Pipeline(\n                name=&quot;sagemaker-pipeline&quot;,\n                steps=[train_step, step_register_pipeline_model, step_deploy_lambda],\n            )\n            pipeline.upsert(\n                role_arn=&quot;arn:aws:iam::xxx:role\/service-role\/xxxx-role&quot;\n            )\n            pipeline.start()\n<\/code><\/pre>\n<p>lambda function for deployment:<\/p>\n<pre><code>import json\nimport boto3\n\ndef lambda_handler(event, context):\n    model_name = event[&quot;model_name&quot;]\n    model_package_arn = event[&quot;model_package_arn&quot;]\n    endpoint_config_name = event[&quot;endpoint_config_name&quot;]\n    endpoint_name = event[&quot;endpoint_name&quot;]\n    role = event[&quot;role&quot;]\n    \n    sm_client = boto3.client(&quot;sagemaker&quot;)\n    container = {&quot;ModelPackageName&quot;: model_package_arn}\n    create_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n                &quot;InitialInstanceCount&quot;: 1,\n                &quot;ModelName&quot;: model_name,\n                &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n            }\n        ]\n    )\n\n    create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Done!')\n    }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-01 11:35:54.767 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2022-04-01 11:41:00.253 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mlops|amazon-machine-learning",
        "Question_view_count":353,
        "Owner_creation_date":"2018-10-17 15:15:14.223 UTC",
        "Owner_last_access_date":"2022-09-24 06:02:17.707 UTC",
        "Owner_reputation":677,
        "Owner_up_votes":74,
        "Owner_down_votes":6,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Phnom Penh, Cambodia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71706292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53640440,
        "Question_title":"How to use boto3 cloudwatch for SageMaker submitted training jobs?",
        "Question_body":"<p>I have submitted a few training jobs from AWS SageMaker. I want to use boto3 cloudwatch api to fetch the cloudwatch data to be displayed within jupyter notebook instead of using CloudWatch UI. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-05 20:43:57.003 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|boto3|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":588,
        "Owner_creation_date":"2017-02-28 23:26:56.977 UTC",
        "Owner_last_access_date":"2019-03-28 02:25:59.547 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53640440",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73216926,
        "Question_title":"Load estimator from model artifact in s3 bucket aws",
        "Question_body":"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code<\/p>\n<pre><code>estimator = PyTorch(\n    entry_point=&quot;train_deploy.py&quot;,\n    source_dir=&quot;code&quot;,\n    role=role,\n    framework_version=&quot;1.3.1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,  # this script only support distributed training for GPU instances.\n    instance_type=&quot;ml.m5.12xlarge&quot;,\n    output_path=output_path,\n    hyperparameters={\n        &quot;epochs&quot;: 1,\n        &quot;num_labels&quot;: 7,\n        &quot;backend&quot;: &quot;gloo&quot;,\n    },\n    disable_profiler=False, # disable debugger\n)\nestimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})\n<\/code><\/pre>\n<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below<\/p>\n<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-03 06:06:27.267 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker|bert-language-model",
        "Question_view_count":28,
        "Owner_creation_date":"2013-05-05 08:07:44.647 UTC",
        "Owner_last_access_date":"2022-09-19 08:48:50.963 UTC",
        "Owner_reputation":169,
        "Owner_up_votes":23,
        "Owner_down_votes":2,
        "Owner_views":30,
        "Answer_body":"<p>I was able to solve this by the following steps<\/p>\n<pre><code>model_data=output_path\nfrom sagemaker.pytorch.model import PyTorchModel \n\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=&quot;1.3.1&quot;,\n                             source_dir=&quot;code&quot;,\n                             py_version=&quot;py3&quot;,\n                             entry_point=&quot;train_deploy.py&quot;)\n\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)\nprint(&quot;predicted class: &quot;, np.argmax(result, axis=1))\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-08-03 07:29:00.763 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73216926",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62978658,
        "Question_title":"What sagemaker algorithm can be trained by the minimal instance?",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"nofollow noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker. The problem is ml.t2.medium instance fail to satisfy the constraint of K-means algorithm. I can't use the instances that satisfy the constraints of the algorithm(I will ask support for allocation of this resource). My question is there any algorithm from sagemaker that one can train on the MNIST dataset with the minimal instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-19 09:46:55.87 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|instance|amazon-sagemaker",
        "Question_view_count":150,
        "Owner_creation_date":"2011-07-16 13:02:36.88 UTC",
        "Owner_last_access_date":"2022-09-24 20:19:39.59 UTC",
        "Owner_reputation":14913,
        "Owner_up_votes":307,
        "Owner_down_votes":1,
        "Owner_views":1093,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Slovenia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62978658",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52198660,
        "Question_title":"Which is lower cost, Sagemaker or EC2?",
        "Question_body":"<p>For example, ml.p2.8xlarge for training job at ap-northeast on Sagemaker takes 16.408 USD \/ hour, but p2.8xlarge for on-demand at ap-northeast on Ec2 takes 12.336 USD\/hour. Is it cheap to just train the DL models on Ec2 rather than Sagemaker if we only use it for training?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_date":"2018-09-06 07:26:08.193 UTC",
        "Question_favorite_count":5.0,
        "Question_last_edit_date":"2020-10-16 11:18:38.677 UTC",
        "Question_score":19,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":15796,
        "Owner_creation_date":"2017-07-10 06:54:27.27 UTC",
        "Owner_last_access_date":"2021-06-24 10:42:33.08 UTC",
        "Owner_reputation":321,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"\u65e5\u672cKanagawa-ken",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52198660",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 08:38:22.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-07-28 15:35:17.217 UTC",
        "Owner_last_access_date":"2022-03-28 15:52:24.487 UTC",
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-14 22:27:30.467 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72379755,
        "Question_title":"How to use a csv with header for sagemaker batch transform?",
        "Question_body":"<p>I am performing a sagemaker batch transform using a transformer created out of an xgboost estimator. The csv input for prediction\/batch transform has both, an ID column and a header (with names of columns). For example, something like this:<\/p>\n<p>Name |Age |Height|Weight<\/p>\n<p>Sam  |10  |2     |3<\/p>\n<p>John |20  |3     |4<\/p>\n<p>Jane |30  |4     |5<\/p>\n<p>Of course, what needs to be passed is just the model inputs without the index (in this case, Name) or header (first row)<\/p>\n<p>We can exclude the index (i.e. 0th) column by using the InputFilter argument when creating the job as follows:<\/p>\n<pre><code>DataProcessing = { \n      &quot;InputFilter&quot;: &quot;$[1:]&quot;}\n<\/code><\/pre>\n<p>My question is how do we exclude the header? What JSONPath can be used for that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 14:44:48.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|data-science|amazon-sagemaker|jsonpath|json-path-expression",
        "Question_view_count":287,
        "Owner_creation_date":"2022-05-25 14:16:41.497 UTC",
        "Owner_last_access_date":"2022-05-31 04:14:48.747 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Revere, MA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72379755",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70512043,
        "Question_title":"ClientError: Failed to download data. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file",
        "Question_body":"<p>How are you?<\/p>\n<p>I'm trying to execute a sagemaker job but i get this error:<\/p>\n<pre><code>ClientError: Failed to download data. Cannot download s3:\/\/pocaaml\/sagemaker\/xsell_sc1_test\/model\/model_lgb.tar.gz, a previously downloaded file\/folder clashes with it. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file.\n<\/code><\/pre>\n<p>I'm have that model_lgb.tar.gz on that s3 path as you can see here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" alt=\"s3 bucket with model in it\" \/><\/a><\/p>\n<p>This is my code:<\/p>\n<pre><code>project_name = 'xsell_sc1_test'\ns3_bucket = &quot;pocaaml&quot;\nprefix = &quot;sagemaker\/&quot;+project_name\naccount_id = &quot;029294541817&quot;\ns3_bucket_base_uri = &quot;{}{}&quot;.format(&quot;s3:\/\/&quot;, s3_bucket)\ndev = &quot;dev-{}&quot;.format(strftime(&quot;%y-%m-%d-%H-%M&quot;, gmtime()))\n\nregion = sagemaker.Session().boto_region_name\nprint(&quot;Using AWS Region: {}&quot;.format(region))\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\nboto3.setup_default_session(region_name=region)\n\nboto_session = boto3.Session(region_name=region)\n\ns3_client = boto3.client(&quot;s3&quot;, region_name=region)\n\nsagemaker_boto_client = boto_session.client(&quot;sagemaker&quot;) #este pinta?\n\nsagemaker_session = sagemaker.session.Session(\n    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n)\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;, role=role, instance_type='ml.m5.4xlarge', instance_count=1\n)\n\nPREPROCESSING_SCRIPT_LOCATION = 'funciones_altas.py'\n\npreprocessing_input_code = sagemaker_session.upload_data(\n    PREPROCESSING_SCRIPT_LOCATION,\n    bucket=s3_bucket,\n    key_prefix=&quot;{}\/{}&quot;.format(prefix, &quot;code&quot;)\n)\n\npreprocessing_input_data = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;data&quot;)\npreprocessing_input_model = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;model&quot;)\npreprocessing_output = &quot;{}\/{}\/{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, dev, &quot;preprocessing&quot; ,&quot;output&quot;)\n\nprocessing_job_name = params[&quot;project_name&quot;].replace(&quot;_&quot;, &quot;-&quot;)+&quot;-preprocess-{}&quot;.format(strftime(&quot;%d-%H-%M-%S&quot;, gmtime()))\n\nsklearn_processor.run(\n    code=preprocessing_input_code,\n    job_name = processing_job_name,\n    inputs=[ProcessingInput(input_name=&quot;data&quot;,\n                            source=preprocessing_input_data, \n                            destination=&quot;\/opt\/ml\/processing\/input\/data&quot;),\n           ProcessingInput(input_name=&quot;model&quot;,\n                           source=preprocessing_input_model, \n                           destination=&quot;\/opt\/ml\/processing\/input\/model&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;output&quot;, \n                         destination=preprocessing_output,\n                         source=&quot;\/opt\/ml\/processing\/output&quot;)],\n    wait=False,\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe()\n<\/code><\/pre>\n<p>and on funciones_altas.py i'm using ohe_altas.tar.gz and not model_lgb.tar.gz making this error super weird.<\/p>\n<p>can you help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-28 20:10:45.71 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jobs|amazon-sagemaker",
        "Question_view_count":310,
        "Owner_creation_date":"2019-12-31 13:11:19.88 UTC",
        "Owner_last_access_date":"2022-09-24 16:18:02.34 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70512043",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63781356,
        "Question_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Question_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-09-07 16:39:14.993 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2020-09-08 07:02:35.527 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":636,
        "Owner_creation_date":"2009-10-06 11:50:17.773 UTC",
        "Owner_last_access_date":"2022-09-23 20:34:01.687 UTC",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Answer_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-09-16 12:35:22.303 UTC",
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66545823,
        "Question_title":"Not able to perform batch inferencing using fast-bert sagemaker model over entire test dataset",
        "Question_body":"<p>it would be appreciated if someone can help me on fast-bert batch transform using sagemaker trained model.<\/p>\n<p>we were able to get the output for single JSON record, but when applying over the entire test data(in JSON format) it is giving the following error. <strong>&quot;TypeError: string indices must be integers&quot;<\/strong>\ntest data: {&quot;text&quot;: &quot;test text data 1&quot;}\n{&quot;text&quot;: &quot;test text data 2&quot;}<\/p>\n<p><strong>trained model:<\/strong><\/p>\n<pre><code>estimator = \nsagemaker.estimator.Estimator(image,role,train_instance_count=1,train_instance_type='ml.p3.8xlarge', output_path=output_path, base_job_name=trn_config.get(&quot;base_job_name&quot;), hyperparameters=hyperparameters,sagemaker_session=session\n\n                                     )\n<\/code><\/pre>\n<p>batch transform function used:<\/p>\n<pre><code>bert_transformer = estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m5.large', \n    output_path=batch_output,\n    accept = &quot;application\/jsonlines&quot;\n)\n\nbert_transformer.transform(\n    data=batch_input,\n    content_type='application\/json',\n    split_type='Line'\n)\n<\/code><\/pre>\n<p>Single record passed to the model: {&quot;text&quot;: &quot;sample text description&quot;}\noutput: [[&quot;30428&quot;, 0.9663759469985962], [&quot;28135&quot;, 0.008436146192252636], [&quot;27615&quot;, 0.0028499893378466368], [&quot;30416&quot;, 0.001644121715798974], [&quot;29071&quot;, 0.001503797248005867], [&quot;30816&quot;, 0.001284519792534411], [&quot;30276&quot;, 0.0009555158321745694], [&quot;27343&quot;, 0.0007992553873918951], [&quot;27206&quot;, 0.0007786208298057318], [&quot;26316&quot;, 0.00075926398858428]]<\/p>\n<p>above mentioned batch transform function is working for single JSON record like shown above, but giving error for multiple records. I tried passing different formats of JSON data, didn't work out.<\/p>\n<p>Your help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-09 11:13:45.19 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-10 04:24:06.757 UTC",
        "Question_score":1,
        "Question_tags":"python|prediction|amazon-sagemaker|inference",
        "Question_view_count":108,
        "Owner_creation_date":"2013-12-04 08:50:36.08 UTC",
        "Owner_last_access_date":"2021-04-20 04:05:23.937 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66545823",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63005122,
        "Question_title":"Sagemaker Studio errors a loading screen to clear workspace every-time when connected \"open studio\"",
        "Question_body":"<p>On resuming or restarting the sagemaker-studio I have the below message pop-up.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/OXyNb.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OXyNb.png\" alt=\"Sagemaker-Studio Loading.. clear workspace error screen\" \/><\/a><\/p>\n<p>Even after clearing the workspace, it won't open. After few such retries, the Jupiter notebook (sagemaker studio \/ IDE) opens. I get this message every time I reconnect and approx 15 mins are wasted each time :(<\/p>\n<p>Am I doing something wrong while stopping\/closing the sagemaker studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-20 23:16:06.11 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-21 03:43:40.387 UTC",
        "Question_score":10,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1544,
        "Owner_creation_date":"2012-11-08 21:05:10.997 UTC",
        "Owner_last_access_date":"2021-09-07 14:37:25.787 UTC",
        "Owner_reputation":630,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63005122",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56876981,
        "Question_title":"Is there a way to export or view a classifier created in sagemaker so that we can see what weights\/constants are used in model evaluation",
        "Question_body":"<p>I created a simple linear learner model using sagemaker, and although I can deploy it on a test data set, I would like to be able to get the actual equation that the model uses to classify values (ie for linear regression the equation of the line).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-03 19:43:35.333 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2015-10-31 23:05:25.74 UTC",
        "Owner_last_access_date":"2020-12-27 15:50:38.097 UTC",
        "Owner_reputation":129,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pristina",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56876981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68858365,
        "Question_title":"AWS Sagemaker fatal: could not read Username for 'https:\/\/gitlab.com\/my\/repo.git': terminal prompts disabled",
        "Question_body":"<p>I want to integrate my private gitlab repository into AWS Sagemaker.<\/p>\n<p>I added git repository on Sagemaker using https protocol (it allows only this protocol) and saved secrets(username and password of my gitlab account) for git repo.<\/p>\n<p>When I run notebook instance by linking git repo, it failed with following message.<\/p>\n<p><code>fatal: could not read Username for 'https:\/\/gitlab.com\/my\/repo.git': terminal prompts disabled<\/code><\/p>\n<p>Is there any step I am missing?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-20 07:24:08.45 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-20 22:35:26.86 UTC",
        "Question_score":4,
        "Question_tags":"git|amazon-web-services|jupyter-notebook|gitlab|amazon-sagemaker",
        "Question_view_count":197,
        "Owner_creation_date":"2016-03-16 06:06:26.687 UTC",
        "Owner_last_access_date":"2022-09-24 21:55:35.777 UTC",
        "Owner_reputation":349,
        "Owner_up_votes":45,
        "Owner_down_votes":2,
        "Owner_views":50,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Europe",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68858365",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60728443,
        "Question_title":"AWS Python3 kernel dies immediately when launched",
        "Question_body":"<p>I recently used a suggestion from <a href=\"https:\/\/stackoverflow.com\/questions\/46510192\/change-the-theme-in-jupyter-notebook\">Change the Theme in Jupyter Notebook?<\/a>, installing jupyterthemes. It has caused the kernel to die immediately when started - how can I remove this package if I cannot run any commands through the kernel? I am working on a Jupyter notebook within Amazon Sagemaker but I am entirely new to the service.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-17 18:41:43.033 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":234,
        "Owner_creation_date":"2020-03-17 18:34:30.043 UTC",
        "Owner_last_access_date":"2020-08-20 12:05:46.807 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60728443",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64256639,
        "Question_title":"SyntaxError (amazon-sagemaker-object-has-no-attribute)",
        "Question_body":"<p>I'm running the code cell below, on SageMaker Notebook instance.<\/p>\n<pre><code>pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('train.csv')\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n<\/code><\/pre>\n<p>And if I hit, the following error is appearing:<\/p>\n<pre><code>AttributeError: 'SageMaker' object has no attribute 's3_input'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2020-10-08 06:02:55.053 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-10 01:02:37.197 UTC",
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":5138,
        "Owner_creation_date":"2020-10-08 05:58:44.43 UTC",
        "Owner_last_access_date":"2020-10-15 09:35:57.617 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64256639",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62309772,
        "Question_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Question_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-10 17:40:31.397 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":400,
        "Owner_creation_date":"2013-06-03 03:45:11.26 UTC",
        "Owner_last_access_date":"2022-09-18 12:32:59.83 UTC",
        "Owner_reputation":5295,
        "Owner_up_votes":351,
        "Owner_down_votes":4,
        "Owner_views":455,
        "Answer_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-08 23:49:20.05 UTC",
        "Answer_score":1.0,
        "Owner_location":"Toronto, ON, Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59988417,
        "Question_title":"Sagemaker batch transform \"ValueError: could not convert string to float\"",
        "Question_body":"<p>I'm running a local transformer using sagemaker and using batch transform. However, it seems like the transform is not calling my custom code.<\/p>\n\n<p>The following is SKlearn init<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\nsource_dir = 'train'\nscript_path = 'train.py'\n\nsklearn = SKLearn(\n    entry_point=script_path,\n    train_instance_type=\"local_gpu\",\n    source_dir=source_dir,\n    role=role,\n    sagemaker_session=sagemaker_session)\nsklearn.fit({'train': \"file:\/\/test.csv\"})\n<\/code><\/pre>\n\n<p>train.py is a python script that loads the training data, and saves the model to S3<\/p>\n\n<p>the batch transform is:<\/p>\n\n<pre><code>transformer = sklearn.transformer(instance_count=1,\n                                  entry_point=source_dir+\"\/\"+script_path,\n                                  instance_type='local_gpu',\n                                  strategy='MultiRecord',\n                                  assemble_with='Line'\n                                  )\ntransformer.transform(\"file:\/\/test_messages\", content_type='text\/csv', split_type='Line')\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p><code>file:\/\/test_messages<\/code> contains a csv that is a list of strings<\/p>\n\n<p>The full error is<\/p>\n\n<pre><code>algo-1-6c5rl_1  | 172.18.0.1 - - [30\/Jan\/2020:14:14:30 +0000] \"GET \/ping HTTP\/1.1\" 200 0 \"-\" \"-\"\nalgo-1-6c5rl_1  | 172.18.0.1 - - [30\/Jan\/2020:14:14:30 +0000] \"GET \/execution-parameters HTTP\/1.1\" 404 232 \"-\" \"-\"\nalgo-1-6c5rl_1  | 2020-01-30 14:14:30,846 ERROR - train - Exception on \/invocations [POST]\nalgo-1-6c5rl_1  | Traceback (most recent call last):\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_functions.py\", line 93, in wrapper\nalgo-1-6c5rl_1  |     return fn(*args, **kwargs)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 56, in default_input_fn\nalgo-1-6c5rl_1  |     return np_array.astype(np.float32) if content_type in content_types.UTF8_TYPES else np_array\nalgo-1-6c5rl_1  | ValueError: could not convert string to float: 'IMPORTANT - You could be entitled up to \ufffd3,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out.'\nalgo-1-6c5rl_1  | \nalgo-1-6c5rl_1  | During handling of the above exception, another exception occurred:\nalgo-1-6c5rl_1  | \nalgo-1-6c5rl_1  | Traceback (most recent call last):\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/app.py\", line 2446, in wsgi_app\nalgo-1-6c5rl_1  |     response = self.full_dispatch_request()\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/app.py\", line 1951, in full_dispatch_request\nalgo-1-6c5rl_1  |     rv = self.handle_user_exception(e)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/app.py\", line 1820, in handle_user_exception\nalgo-1-6c5rl_1  |     reraise(exc_type, exc_value, tb)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/_compat.py\", line 39, in reraise\nalgo-1-6c5rl_1  |     raise value\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/app.py\", line 1949, in full_dispatch_request\nalgo-1-6c5rl_1  |     rv = self.dispatch_request()\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/flask\/app.py\", line 1935, in dispatch_request\nalgo-1-6c5rl_1  |     return self.view_functions[rule.endpoint](**req.view_args)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_transformer.py\", line 200, in transform\nalgo-1-6c5rl_1  |     self._model, request.content, request.content_type, request.accept\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_transformer.py\", line 227, in _default_transform_fn\nalgo-1-6c5rl_1  |     data = self._input_fn(content, content_type)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_functions.py\", line 95, in wrapper\nalgo-1-6c5rl_1  |     six.reraise(error_class, error_class(e), sys.exc_info()[2])\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 692, in reraise\nalgo-1-6c5rl_1  |     raise value.with_traceback(tb)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_functions.py\", line 93, in wrapper\nalgo-1-6c5rl_1  |     return fn(*args, **kwargs)\nalgo-1-6c5rl_1  |   File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 56, in default_input_fn\nalgo-1-6c5rl_1  |     return np_array.astype(np.float32) if content_type in content_types.UTF8_TYPES else np_array\nalgo-1-6c5rl_1  | sagemaker_containers._errors.ClientError: could not convert string to float: 'IMPORTANT - You could be entitled up to \ufffd3,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out.'\nalgo-1-6c5rl_1  | 172.18.0.1 - - [30\/Jan\/2020:14:14:30 +0000] \"POST \/invocations HTTP\/1.1\" 500 290 \"-\" \"-\"\n.Waiting for transform job: sagemaker-scikit-learn-2020-01-30-14-14-30-490\n<\/code><\/pre>\n\n<p>It seems that it is not able to process my string. I do have code in train.py to convert the string using TfidfVectorizer, but that code is not getting called<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-30 14:37:52.733 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":1534,
        "Owner_creation_date":"2010-09-25 01:47:51.58 UTC",
        "Owner_last_access_date":"2022-04-04 12:09:39.223 UTC",
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59988417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49103679,
        "Question_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Question_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2018-03-05 04:53:58.01 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-09-03 22:01:31.09 UTC",
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|machine-learning|neural-network|amazon-sagemaker",
        "Question_view_count":3742,
        "Owner_creation_date":"2015-09-05 03:59:38.58 UTC",
        "Owner_last_access_date":"2022-09-24 07:31:53.813 UTC",
        "Owner_reputation":433,
        "Owner_up_votes":72,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-03-10 21:57:41.83 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-02-22 03:25:18.227 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63457857,
        "Question_title":"What is called within a sagemaker custom (training) container?",
        "Question_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-17 20:02:39.337 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-17 20:57:57.66 UTC",
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_date":"2017-01-19 15:07:44.573 UTC",
        "Owner_last_access_date":"2022-09-22 14:55:11.743 UTC",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Answer_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-16 16:04:09.597 UTC",
        "Answer_score":0.0,
        "Owner_location":"Amsterdam, Nederland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57027808,
        "Question_title":"Semantic segmentation with amazon AWS and S3 instance",
        "Question_body":"<p>This is probably a easy question but I have been stuck now for a time.\nI want to train an FCN an Amazon AWS. For this I want to use the procedure used in this example ( <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb<\/a> ) with my own datase.<\/p>\n\n<p>In contrast to that procedure I have my train and annotation images (as .png) saved in one S3 bucket with four folders (Training, TrainingAnnotation, Validation, ValidationAnnotaion).The files in the folder for Training and Annotation have the same name.<\/p>\n\n<p>I trained my model with followong code:<\/p>\n\n<pre><code>%%time\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n\nbucket = sess.default_bucket()  \nprefix = 'semantic-segmentation'\nprint(bucket)\n\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ntraining_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\nprint (training_image)\n\ns3_output_location = 's3:\/\/{}\/{}\/output'.format(bucket, prefix)\nprint(s3_output_location)\n\n# Create the sagemaker estimator object.\nss_model = sagemaker.estimator.Estimator(training_image,\n                                         role, \n                                         train_instance_count = 1, \n                                         train_instance_type = 'ml.p2.xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = 360000,\n                                         output_path = s3_output_location,\n                                         base_job_name = 'ss-notebook-demo',\n                                         sagemaker_session = sess)\nnum_training_samples=5400\n# Setup hyperparameters \nss_model.set_hyperparameters(backbone='resnet-50', \n                             algorithm='fcn',                   \n                             use_pretrained_model='True', \n                             crop_size=248, .                             \n                             num_classes=4, \n                             epochs=10, \n                             learning_rate=0.0001,                             \n                             optimizer='rmsprop', 'adam', 'rmsprop', 'nag', 'adagrad'.\n                             lr_scheduler='poly', 'cosine' and 'step'.                           \n                             mini_batch_size=16, \n                             validation_mini_batch_size=16,\n                             early_stopping=True, \n                             early_stopping_patience=2, \n                             early_stopping_min_epochs=10,    \n                             num_training_samples=num_training_samples) \n# Create full bucket names\n\nbucket1 = 'imagelabel1' \ntrain_channel = 'Training'\nvalidation_channel = 'Validation'\ntrain_annotation_channel = 'TrainingAnnotation'\nvalidation_annotation_channel =  'ValidataionAnnotation'\n\n\ns3_train_data = 's3:\/\/{}\/{}'.format(bucket1, train_channel)\ns3_validation_data = 's3:\/\/{}\/{}'.format(bucket1, validation_channel)\ns3_train_annotation = 's3:\/\/{}\/{}'.format(bucket1, train_annotation_channel)\ns3_validation_annotation  = 's3:\/\/{}\/{}'.format(bucket1, validation_annotation_channel)\n\n\n\ndistribution = 'FullyReplicated'\n# Create sagemaker s3_input objects\ntrain_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n                                        content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n                                        content_type='image\/png', s3_data_type='S3Prefix')\ntrain_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n                                        content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n                                        content_type='image\/png', s3_data_type='S3Prefix')\n\ndata_channels = {'train': train_data, \n                 'validation': validation_data,\n                 'train_annotation': train_annotation, \n                 'validation_annotation':validation_annotation}\ns3:\/\/imagelabel1\/Training\nss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The Errror Message is:<\/p>\n\n<p>ValueError: Error for Training job ss-notebook-demo-2019-07-15-06-42-25-784: Failed Reason: ClientError: train channel is empty.<\/p>\n\n<p>Does someone know what is wrong in this Code?<\/p>\n\n<p>Thank you <\/p>\n\n<p>Simon<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-14 13:22:22.1 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-07-15 18:06:18.413 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":204,
        "Owner_creation_date":"2019-07-14 13:06:48.897 UTC",
        "Owner_last_access_date":"2022-09-15 18:46:01.81 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Munich",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57027808",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58321545,
        "Question_title":"Running an .ipynb file from Lambda function",
        "Question_body":"<p>I'm New on AWS. I want to run a .ipynb file on Sagemaker notebook instance. I'm using runipy package on lambda to run a .ipynb file \"\/home\/ec2-user\/SageMaker\/xxxxxxxxxx.ipynb\" from Lambda function with s3 trigger but failed. \nCan someone suggest how to run .ipynb file from labdda in anyways possible.<\/p>\n\n<p><strong>Error<\/strong><\/p>\n\n<pre><code>[ERROR] FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/SageMaker\/xxxxxxxx.ipynb' Traceback (most recent call last): File \"\/var\/task\/classifier.py\", line 10, in lambda_handler notebook = read(open(\"\/home\/ec2-user\/SageMaker\/xxxxx.ipynb\"), 'json')\n\n\/var\/task\/IPython\/nbformat.py:13: ShimWarning: The IPython.nbformat package has been deprecated since IPython 4.0. You should import from nbformat instead\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2019-10-10 11:11:28.04 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-10-10 13:11:36.2 UTC",
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":1131,
        "Owner_creation_date":"2019-08-25 12:47:45.403 UTC",
        "Owner_last_access_date":"2019-11-21 13:41:50.473 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58321545",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70599052,
        "Question_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Question_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 20:10:29.643 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-iot",
        "Question_view_count":48,
        "Owner_creation_date":"2020-03-16 17:16:50.35 UTC",
        "Owner_last_access_date":"2022-04-26 10:55:17.747 UTC",
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-06 14:38:47.65 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70599052",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60934522,
        "Question_title":"Sagemaker Error for HyperParameterTuning job",
        "Question_body":"<p>I checked the aws forums and here at SO and I can't find a solution to this error or an explanation to what it is.<\/p>\n\n<p>The full error is:<\/p>\n\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job sagemaker-xgboost-200330-1544: Failed. Reason: No objective metrics found after running 5 training jobs. Please ensure that the custom algorithm is emitting the objective metric as defined by the regular expression provided.\n\n<\/code><\/pre>\n\n<p>and it is respective to the following code:<\/p>\n\n<pre><code>xgbt = sagemaker.estimator.Estimator(container, #name of training container\n                                    role, # IAM role to use\n                                    train_instance_count = 1, # number of instances yo use for training\n                                    train_instance_type = 'ml.m4.xlarge', #type of virtual machine to use\n                                    output_path = 's3:\/\/{}\/{}\/output'.format(session.default_bucket(),\n                                                                            prefix),\n                                    sagemaker_session = session) #current sagemaker session\n\n\nxgbt.set_hyperparameters(\n    max_depth = 5,\n    eta = 0.1,\n    eval_metric='auc',\n    objective='binary:logistic',\n    early_stopping_rounds=500,\n    rate_drop=0.1,\n    colsample_bytree=0.8,\n    subsample=0.75,\n    min_child_weight=0,\n    num_round = 500)\n\n\nxgbt.fit({'train': s3_input_train})\n\n<\/code><\/pre>\n\n<p>and then the hyperparameter tuning is where it croaks:<\/p>\n\n<pre><code>from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n\nxgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgbt, # The estimator object to use as the basis for the training jobs.\n                                               objective_metric_name = 'validation:auc', # The metric used to compare trained models.\n                                               objective_type = 'Maximize', # Whether we wish to minimize or maximize the metric.\n                                               max_jobs = 20, # The total number of models to train\n                                               max_parallel_jobs = 3, # The number of models to train in parallel\n                                               hyperparameter_ranges = {\n                                                    'max_depth': IntegerParameter(3, 12),\n                                                    'eta'      : ContinuousParameter(0.01, 0.5),\n                                                    'min_child_weight': IntegerParameter(2, 8),\n                                                    'subsample': ContinuousParameter(0.5, 0.9),\n                                                    'gamma': ContinuousParameter(0, 10),\n                                               })\n\nxgb_hyperparameter_tuner.fit({'train': s3_input_train})\n\nxgb_hyperparameter_tuner.wait()\n\n<\/code><\/pre>\n\n<p>I think I have the metrics correctly defined so I don't know what it wants from me.<\/p>\n\n<p>Thank you so much for checking this out.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2020-03-30 15:59:05.573 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|xgboost|amazon-sagemaker",
        "Question_view_count":1313,
        "Owner_creation_date":"2018-02-06 15:55:07.093 UTC",
        "Owner_last_access_date":"2022-03-31 15:50:16.033 UTC",
        "Owner_reputation":331,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60934522",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64945483,
        "Question_title":"How to store a .tar.gz formatted model to AWS SageMaker and use it as a deployed model?",
        "Question_body":"<p>I have a pre-trained BERT model which was trained on Google Cloud Platform, and the model is stored in a .tar.gz formatted file, I wanted to deploy this model to SageMaker and also be able to trigger the model via API, how can I achieve this?<\/p>\n<p>I found <a href=\"https:\/\/stackoverflow.com\/questions\/54916866\/with-aws-sagemaker-is-it-possible-to-deploy-a-pre-trained-model-using-the-sagem\">this question<\/a> is a little bit related to what I'm asking here, but it's for a scikit-learn model, I'm new to this area, can someone give me some guidance regarding this? Many thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2020-11-21 16:37:01.92 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|deployment|amazon-sagemaker|bert-language-model",
        "Question_view_count":649,
        "Owner_creation_date":"2018-10-30 17:35:56.27 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883 UTC",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"United Kingdom",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64945483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52939732,
        "Question_title":"Installing Torch on Amazon Linux",
        "Question_body":"<p>This took me an excruciatingly long time for me to figure out so I decided to save people the trouble.<\/p>\n\n<p>Trying to install torch on an ec2 instance with Amazon Linux(redhat) failed with the following command:<\/p>\n\n<pre><code>git clone https:\/\/github.com\/torch\/distro.git ~\/torch --recursive\ncd ~\/torch; bash install-deps;\n.\/install.sh\n<\/code><\/pre>\n\n<p>It gave me the error:<\/p>\n\n<pre><code>OpenBLAS Failed to compile\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-23 01:10:24.407 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-ec2|torch|amazon-sagemaker",
        "Question_view_count":478,
        "Owner_creation_date":"2014-06-24 20:18:14.843 UTC",
        "Owner_last_access_date":"2022-05-25 01:28:11.31 UTC",
        "Owner_reputation":754,
        "Owner_up_votes":127,
        "Owner_down_votes":3,
        "Owner_views":38,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Cambridge, MA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52939732",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57661142,
        "Question_title":"AWS S3 and Sagemaker: No such file or directory",
        "Question_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-26 15:49:42.57 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":4622,
        "Owner_creation_date":"2015-09-20 06:42:05.233 UTC",
        "Owner_last_access_date":"2022-04-07 05:34:25.537 UTC",
        "Owner_reputation":187,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-07 04:07:48.83 UTC",
        "Answer_score":2.0,
        "Owner_location":"Hyderabad",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72827239,
        "Question_title":"amazon-sagemaker-lab:: libXrender.so.1 package",
        "Question_body":"<p>I am trying to use the amazon sagemaker lab environment and the package libXrender is not installed.<\/p>\n<p>sudo privileges are removed and it's not possible to install it with:<\/p>\n<p><code>apt-get install libxrender1<\/code><\/p>\n<p>Is there an easy fix or do I have to contact their support to install the package in their docker container?<\/p>\n<p>Thanks in advance!<\/p>\n<p>Error results from this piece of code:<\/p>\n<pre><code>from rdkit.Chem.Draw import rdMolDraw2D\nfrom rdkit.Chem.Draw.rdMolDraw2D import *\n\nImportError: libXrender.so.1: cannot open shared object file: No such file or directory\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-01 09:32:28.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|rdkit",
        "Question_view_count":77,
        "Owner_creation_date":"2016-06-14 13:45:51.74 UTC",
        "Owner_last_access_date":"2022-09-22 10:07:51.313 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Cambridge, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72827239",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53782956,
        "Question_title":"Read bytes file from AWS S3 into AWS SageMaker conda_python3",
        "Question_body":"<p>Good morning,\nYesterday I saved a file from SageMaker conda_python3 to S3 like this:<\/p>\n\n<pre><code>s3 = boto3.client(\n            's3',\n            aws_access_key_id='XXXX',\n            aws_secret_access_key='XXXX'\n        )\ny = pandas.DataFrame(df.tag_factor,index = df.index)\ns3.put_object(Body = y.values.tobytes(), Bucket='xxx', Key='xxx')\n<\/code><\/pre>\n\n<p>Today I am trying to open it with conda_python3 as a pandas.Series or as a numpy.array object, with this code:<\/p>\n\n<pre><code>s3 = boto3.client(\n            's3',\n            aws_access_key_id='XXX',\n            aws_secret_access_key='XXX'\n        )\ny_bytes = s3.get_object(Bucket='xxx', Key='xxx')\ny = numpy.load(io.BytesIO(y_bytes['Body'].read()))\n<\/code><\/pre>\n\n<p>but I am getting this error: OSError: Failed to interpret file &lt;_io.BytesIO >object at 0x7fcb0b403258> as a pickle<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>y = numpy.fromfile(io.BytesIO(y_bytes['Body'].read()))\n<\/code><\/pre>\n\n<p>and I get this error:<\/p>\n\n<blockquote>\n  <p>UnsupportedOperation: fileno<\/p>\n<\/blockquote>\n\n<p>I tried this:<\/p>\n\n<pre><code>y = pd.read_csv(io.BytesIO(y_bytes['Body'].read()), sep=\" \", header=None)\n<\/code><\/pre>\n\n<p>and I get this error:<\/p>\n\n<blockquote>\n  <p>EmptyDataError: No columns to parse from file<\/p>\n<\/blockquote>\n\n<p>How can I read this file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-12-14 15:50:47.203 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-18 13:57:52.2 UTC",
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-s3|amazon-sagemaker",
        "Question_view_count":1398,
        "Owner_creation_date":"2016-03-19 19:45:07.413 UTC",
        "Owner_last_access_date":"2022-08-25 22:03:06.273 UTC",
        "Owner_reputation":617,
        "Owner_up_votes":74,
        "Owner_down_votes":0,
        "Owner_views":117,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Ecuador, Quito",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53782956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71241651,
        "Question_title":"SageMaker DataWrangler's one-hot-encoding into vector convert into suitable form using pandas",
        "Question_body":"<p>Using SageMaker DataWrangler's one-hot-encoding I have a column whose datapoints are of form<\/p>\n<pre><code>&quot;(26,[2],[1.0])&quot;\n&quot;(26,[0],[1.0])&quot;\n<\/code><\/pre>\n<p>how can I convert into suitable form using pandas to train a model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-23 17:35:47.667 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pandas|amazon-sagemaker",
        "Question_view_count":69,
        "Owner_creation_date":"2020-01-03 17:09:09.303 UTC",
        "Owner_last_access_date":"2022-09-20 13:22:32.15 UTC",
        "Owner_reputation":415,
        "Owner_up_votes":105,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Hyderabad India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71241651",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63035151,
        "Question_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Question_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-22 13:19:35.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":91,
        "Owner_creation_date":"2017-05-26 00:24:51.317 UTC",
        "Owner_last_access_date":"2022-04-19 19:57:14.917 UTC",
        "Owner_reputation":138,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Answer_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-23 21:28:32.143 UTC",
        "Answer_score":2.0,
        "Owner_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53182436,
        "Question_title":"SageMaker: visualizing training statistics",
        "Question_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-07 01:18:04.34 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|data-visualization|tensorboard|amazon-sagemaker",
        "Question_view_count":415,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-23 22:07:36.183 UTC",
        "Answer_score":3.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53182436",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62012264,
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-25 23:23:03.843 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"json|amazon-web-services|deployment|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_date":"2013-01-24 21:12:57.54 UTC",
        "Owner_last_access_date":"2020-12-26 02:54:29.537 UTC",
        "Owner_reputation":427,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-18 17:09:51.413 UTC",
        "Answer_score":0.0,
        "Owner_location":"New York, NY, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62255062,
        "Question_title":"Reinstalling conda_python2 \/ conda_python3 kernel on AWS",
        "Question_body":"<p>I've encountered a weird issue. My conda_python2 and conda_python3 in AWS Sagemaker are giving weird results, deviating to a large extent from what I had got earlier. Also, the output is getting generated much slower than usual. I guess the kernel is broken. Can someone please guide me on how to reset the kernel \/ reinstall it? Thanks in advance! <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-08 04:32:49.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|kernel|jupyter|conda|amazon-sagemaker",
        "Question_view_count":31,
        "Owner_creation_date":"2020-06-08 04:27:40.687 UTC",
        "Owner_last_access_date":"2020-12-11 14:13:57.083 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62255062",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68940126,
        "Question_title":"How can I update my custom build model in sagemaker everytime I create new training job?",
        "Question_body":"<p>I build a custom model on sagemaker and deployed the endpoint by creating a model. Now I want to retrain it every week and also I have to update same endpoint.<\/p>\n<p>I scheduled the training job with lambda function but It is creating new artifact location. This is not I want. I want to update same model.<\/p>\n<p>Does anyone know How can we do that?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-08-26 14:08:29.293 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":93,
        "Owner_creation_date":"2018-07-26 07:55:25.75 UTC",
        "Owner_last_access_date":"2022-09-23 13:58:07.923 UTC",
        "Owner_reputation":69,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pune, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68940126",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69944250,
        "Question_title":"AWS Sagemaker Pytorch does not run properly",
        "Question_body":"<p>I am currently trying to train a model using pytorch on AWS Sagemaker but can't get it to run properly. My main question now is: Is there some workflow step I'm missing? Any help is greatly appreciated.<\/p>\n<p>I managed to get the code running on colab or on a local machine for example but not on sagemaker.<\/p>\n<p>In short the program should: Setup a pytorch model, load the train data from a file system and perform train epochs.<\/p>\n<p>For this, I am trying the following:\nThe code files (dataloaders\/help functions etc) with the &quot;entry point&quot; are stored at Sagemaker Studio in the folder &quot;code&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xqHi3.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>The train files are stored in a s3 bucket and are transfered in &quot;file mode&quot;.<\/p>\n<p>I then call the estimator in a python notebook as this:<\/p>\n<pre><code>estimator = PyTorch(entry_point='entry.py',\n                    role=role,\n                    py_version='py3',\n                    source_dir = &quot;code&quot;,\n                    output_path = &quot;s3:\/\/XXXXX\/XXXXXX\/XXXX&quot;,\n                    framework_version='1.3.1',\n                    instance_count=1,\n                    instance_type='ml.g4dn.2xlarge',\n                    hyperparameters={\n                        'epochs': 5,\n                        'backend': 'gloo'\n                    })\n\ninputs = &quot;s3:\/\/XXXXX\/XXXXX&quot;\nestimator.fit({'training': inputs})\n<\/code><\/pre>\n<p>In the output I can see, that the train instance is prepared and the data is downloaded but then the problem arises:<\/p>\n<p>For some reason the program jumps right into the train method. The outputs of the first steps which should take place before a train epoch, Network whitening for example, are shown after or during the train step. After one train epoch the program freezes without any error message until I manually stop the instance.<\/p>\n<p>Thanks for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-12 14:11:39.757 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":328,
        "Owner_creation_date":"2021-11-12 13:38:43.13 UTC",
        "Owner_last_access_date":"2022-09-19 13:22:58.87 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69944250",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65611925,
        "Question_title":"sagmaker deploy() model gives error exec: \"serve\": executable file not found in $PATH",
        "Question_body":"<p>I have fitted a model succefully in Sagemaker with <code>classifier.fit()<\/code>, I have got success message saying that <code>Finished saving the model.Finished training the model. Script Status - Finished<\/code><\/p>\n<p>Now, I tried to deploy the model (make endpoint with the following code.<\/p>\n<pre><code>classifier.deploy(initial_instance_count=1, instance_type='ml.m4.4xlarge')\n<\/code><\/pre>\n<p>It initially gave me error.<\/p>\n<blockquote>\n<p>Error hosting endpoint\nkeras-seq-modelling2021-11-2021-01-07-11-26-56-705: Failed. Reason:\nThe primary container for production variant AllTraffic did not pass\nthe ping health check. Please check CloudWatch logs for this\nendpoint..<\/p>\n<\/blockquote>\n<p>When I looked in logs (cloud watch log stream), It is giving following error.<\/p>\n<blockquote>\n<p>exec: &quot;serve&quot;: executable file not found in $PATH<\/p>\n<\/blockquote>\n<p>I know it is relating to making some file executable with granting right permision in some docker container, but the point is that it should be handled by sagemaker and all we need to do according to tutorial is <code>model.deploy()<\/code><\/p>\n<p>Am I doing something wrong. Any idea how to resolved this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-01-07 11:54:15.803 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_date":"2014-05-27 17:36:45.023 UTC",
        "Owner_last_access_date":"2022-09-24 22:00:51.297 UTC",
        "Owner_reputation":19335,
        "Owner_up_votes":345,
        "Owner_down_votes":77,
        "Owner_views":1053,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, Uk",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65611925",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47847736,
        "Question_title":"AWS Sagemaker Neural Topic Model",
        "Question_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-16 16:29:08.563 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|neural-network|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2013-07-23 09:40:39.353 UTC",
        "Owner_last_access_date":"2022-09-21 17:49:28.25 UTC",
        "Owner_reputation":2177,
        "Owner_up_votes":523,
        "Owner_down_votes":113,
        "Owner_views":262,
        "Answer_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-01-06 17:35:39.877 UTC",
        "Answer_score":1.0,
        "Owner_location":"Poland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47847736",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66142193,
        "Question_title":"AWS Sagemaker training job stuck in progress state",
        "Question_body":"<p>I have created a training job yesterday, same as usual, just adding few more training data. I didn't have any problem with this in the last 2 years (the same exact procedure and code). This time after 14 hours more or less simply stalled.\nTraining job is still &quot;in processing&quot;, but cloudwatch is not logging anything since then. Right now 8 more hours passed and no new entry is in the logs, no errors no crash.\nCan someone explain this ? Unfortunately I don't have any AWS support plan.\nAs you can see from the picture below after 11am there is nothing..<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hswD7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hswD7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The training job is supposed to complete in the next couple of hours, but now I'm not sure if is actually running (in this case would be a cloudwatch problem) or not..<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>Suddenly the training job failed, without any further log. The reason is<\/p>\n<blockquote>\n<p>ClientError: Artifact upload failed:Error 7: The credentials received\nhave been expired<\/p>\n<\/blockquote>\n<p>But there is still nothing in the logs after 11am. Very weird.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-02-10 17:44:33.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-11 08:10:17.147 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":755,
        "Owner_creation_date":"2014-11-18 21:32:30.293 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437 UTC",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-25 15:04:37.673 UTC",
        "Answer_score":1.0,
        "Owner_location":"Jesi, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66142193",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64754032,
        "Question_title":"Sagemaker Script Mode Training: How to import custom modules in training script?",
        "Question_body":"<p>I am trying to use SageMaker script mode for training a model on image data. I have multiple scripts for data preparation, model creation, and training. This is the content of my working directory:<\/p>\n<pre><code>WORKDIR\n|-- config\n|   |-- hyperparameters.json\n|   |-- lossweights.json\n|   `-- lr.json\n|-- dataset.py\n|-- densenet.py\n|-- resnet.py\n|-- models.py\n|-- train.py\n|-- imagenet_utils.py\n|-- keras_utils.py\n|-- utils.py\n`-- train.ipynb\n<\/code><\/pre>\n<p>The training script is <code>train.py<\/code> and it makes use of other scripts. To run the training script, I'm using the following code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bucket='ashutosh-sagemaker'\ndata_key = 'training'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nprint(data_location)\ninputs = {'data':data_location}\nprint(inputs)\n\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(entry_point='train.py',\n                       role=role,\n                       train_instance_count=1,\n                       train_instance_type='ml.p2.xlarge',\n                       framework_version='1.14',\n                       py_version='py3',\n                       script_mode=True,\n                       hyperparameters={\n                           'epochs': 10\n                       }        \n                      )\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>On running this code, I get the following output:<\/p>\n<pre><code>2020-11-09 10:42:07 Starting - Starting the training job...\n2020-11-09 10:42:10 Starting - Launching requested ML instances......\n2020-11-09 10:43:24 Starting - Preparing the instances for training.........\n2020-11-09 10:44:43 Downloading - Downloading input data....................................\n2020-11-09 10:51:08 Training - Downloading the training image...\n2020-11-09 10:51:40 Uploading - Uploading generated training model\n\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 5, in &lt;module&gt;\n    from dataset import WatchDataSet\nModuleNotFoundError: No module named 'dataset'\nWARNING: Logging before flag parsing goes to stderr.\nE1109 10:51:37.525632 140519531874048 _trainer.py:94] ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.6 train.py --epochs 10 --model_dir s3:\/\/sagemaker-ap-northeast-1-485707876195\/tensorflow-training-2020-11-09-10-42-06-234\/model&quot;\n\n2020-11-09 10:51:47 Failed - Training job failed\n<\/code><\/pre>\n<p>What should I do to remove the <code>ModuleNotFoundError<\/code>? I tried to look for solutions but didn't find any relevant resources.<\/p>\n<p>The contents of <code>train.py<\/code> file:<\/p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom dataset import WatchDataSet\nfrom models import BCNN\nfrom utils import image_generator, val_image_generator\nfrom utils import BCNNScheduler, LossWeightsModifier\nfrom utils import restore_checkpoint, get_epoch_key\n\nimport argparse\nfrom collections import defaultdict\nimport json\nimport keras\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.backend import tensorflow_backend\nfrom keras.callbacks import LearningRateScheduler, TensorBoard\nfrom math import ceil\nimport numpy as np\nimport os\nimport glob\nfrom sklearn.model_selection import train_test_split\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', type=int, default=100, help='number of epoch of training')\nparser.add_argument('--batch_size', type=int, default=32, help='size of the batches')\nparser.add_argument('--data', type=str, default=os.environ.get('SM_CHANNEL_DATA'))\n\nopt = parser.parse_args()\n\ndef main():\n\n    csv_config_dict = {\n        'csv': opt.data + 'train.csv',\n        'image_dir': opt.data + 'images',\n        'xlabel_column': opt.xlabel_column,\n        'brand_column': opt.brand_column,\n        'model_column': opt.model_column,\n        'ref_column': opt.ref_column,\n        'encording': opt.encoding\n    }\n\n    dataset = WatchDataSet(\n        csv_config_dict=csv_config_dict,\n        min_data_ref=opt.min_data_ref\n    )\n\n    X, y_c1, y_c2, y_fine = dataset.X, dataset.y_c1, dataset.y_c2, dataset.y_fine\n    brand_uniq, model_uniq, ref_uniq = dataset.brand_uniq, dataset.model_uniq, dataset.ref_uniq\n\n    print(&quot;ref. shape: &quot;, y_fine.shape)\n    print(&quot;brand shape: &quot;, y_c1.shape)\n    print(&quot;model shape: &quot;, y_c2.shape)\n\n    height, width = 224, 224\n    channel = 3\n\n    # get pre-trained weights\n    if opt.mode == 'dense':\n        WEIGHTS_PATH = 'https:\/\/github.com\/keras-team\/keras-applications\/releases\/download\/densenet\/densenet121_weights_tf_dim_ordering_tf_kernels.h5'\n    elif opt.mode == 'res':\n        WEIGHTS_PATH = 'https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/download\/v0.2\/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n    weights_path, current_epoch, checkpoint = restore_checkpoint(opt.ckpt_path, WEIGHTS_PATH)\n\n    # split train\/validation\n    y_ref_list = np.array([ref_uniq[np.argmax(i)] for i in y_fine])\n    index_list = np.array(range(len(X)))\n    train_index, test_index, _, _ = train_test_split(index_list, y_ref_list, train_size=0.8, random_state=23, stratify=None)\n\n    print(&quot;Train&quot;)\n    model = None\n    bcnn = BCNN(\n        height=height,\n        width=width,\n        channel=channel,\n        num_classes=len(ref_uniq),\n        coarse1_classes=len(brand_uniq),\n        coarse2_classes=len(model_uniq),\n        mode=opt.mode\n    )\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-11-09 14:58:44.14 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-09 15:27:44.7 UTC",
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|tensorflow|deep-learning|amazon-sagemaker",
        "Question_view_count":790,
        "Owner_creation_date":"2017-03-12 05:37:02.35 UTC",
        "Owner_last_access_date":"2022-09-08 08:55:56.28 UTC",
        "Owner_reputation":109,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"IIT Guwahati, Guwahati, Assam, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64754032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70222875,
        "Question_title":"IOPub data rate exceeded in SageMaker?",
        "Question_body":"<p>Does anyone know where to find the config file to edit the <code>c.NotebookApp.iopub_data_rate_limit<\/code>? I am not working with the AWS CLI, but rather I am doing everything in the AWS Console. I have a SageMaker notebook running and I would like to change the data rate limit, but essentially don't have access to a terminal, unless someone could explain how to access the terminal within the console?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-04 03:43:22.183 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|jupyter-lab|amazon-sagemaker",
        "Question_view_count":57,
        "Owner_creation_date":"2018-09-08 18:30:47.58 UTC",
        "Owner_last_access_date":"2022-04-26 16:40:12.66 UTC",
        "Owner_reputation":113,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70222875",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-03 16:58:59.29 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_date":"2020-01-13 16:18:39.743 UTC",
        "Owner_last_access_date":"2022-09-23 12:14:46.19 UTC",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-03 17:38:04.32 UTC",
        "Answer_score":2.0,
        "Owner_location":"Ireland",
        "Answer_last_edit_date":"2022-03-08 17:40:23.947 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71222258,
        "Question_title":"Error in AWS SageMaker Ground Truth labeled job creation",
        "Question_body":"<p>I'm using AWS SageMaker Ground Truth for labeling images. I have uploaded the data into s3 bucket, create the IAM role to access 'S3,SageMaker,Groundtruth, and IAM'. When I am trying to create labeling job, it give me this error:<\/p>\n<blockquote>\n<p>NetworkingError: Network Failure - The S3 bucket 'sm-gt-s3-enron' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-22 13:32:32.887 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-23 06:52:47.9 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":225,
        "Owner_creation_date":"2015-07-11 06:11:42.337 UTC",
        "Owner_last_access_date":"2022-04-21 09:13:10.72 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71222258",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62319753,
        "Question_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Question_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-06-11 08:04:30.733 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-30 07:20:23.133 UTC",
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":1063,
        "Owner_creation_date":"2015-10-05 09:06:23.57 UTC",
        "Owner_last_access_date":"2022-06-29 08:50:22.493 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-30 07:22:18.047 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-10-07 08:27:00.017 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62313532,
        "Question_title":"xgboost on Sagemaker notebook import fails",
        "Question_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-06-10 21:37:23.587 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|conda|xgboost|amazon-sagemaker",
        "Question_view_count":1532,
        "Owner_creation_date":"2014-02-05 20:36:11.72 UTC",
        "Owner_last_access_date":"2022-09-22 15:03:56.253 UTC",
        "Owner_reputation":1248,
        "Owner_up_votes":97,
        "Owner_down_votes":1,
        "Owner_views":137,
        "Answer_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-19 12:59:49.163 UTC",
        "Answer_score":3.0,
        "Owner_location":"Tel Aviv-Yafo, Israel",
        "Answer_last_edit_date":"2020-06-19 13:36:09.99 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60122070,
        "Question_title":"AWS SageMaker PyTorch: no module named 'sagemaker'",
        "Question_body":"<p>I have deployed a PyTorch model on AWS with SageMaker, and I try to send a request to test the service. However, I got a very vague error message saying \"no module named 'sagemaker'\". I have tried to search online, but cannot find posts about similar message.<\/p>\n\n<p>My client code: <\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchPredictor\n\nENDPOINT = '&lt;endpoint name&gt;'\n\npredictor = PyTorchPredictor(ENDPOINT)\npredictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n<\/code><\/pre>\n\n<p>Detailed error message:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"client.py\", line 7, in &lt;module&gt;\n    predictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py\", line 110, in predict\n    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 276, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 586, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'sagemaker'\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/&lt;endpoint name&gt; in account xxxxxxxxxxxxxx for more information.\n<\/code><\/pre>\n\n<p>This bug is because I merge both the serving script and my deploy script together, see below<\/p>\n\n<pre><code>import os\nimport torch\nimport numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\n\ndef model_fn(model_dir):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model = resnet50()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f, map_location=device))\n    return model.to(device)\n\ndef predict_fn(input_data, model):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model.eval()\n    with torch.no_grad():\n        return model(input_data.to(device))\n\n\nif __name__ == '__main__':\n    pytorch_model = PyTorchModel(model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n                                    entry_point='serve.py', role='jiashenC-sagemaker',\n                                    py_version='py3', framework_version='1.3.1')\n    predictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n    print(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>\n\n<p>The root cause is the 4th line in my code. It tries to import sagemaker, which is an unavailable library. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-02-07 22:41:06.463 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-02-09 15:07:22.87 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":5568,
        "Owner_creation_date":"2017-03-16 03:15:12.74 UTC",
        "Owner_last_access_date":"2022-09-25 01:39:19.96 UTC",
        "Owner_reputation":1653,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":"<p><em>(edit 2\/9\/2020 with extra code snippets)<\/em><\/p>\n\n<p>Your serving code tries to use the <code>sagemaker<\/code> module internally. The <code>sagemaker<\/code> module (also called <a href=\"http:\/\/sagemaker.readthedocs.io\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a>, one of the numerous orchestration SDKs for SageMaker) is not designed to be used in model containers, but instead out of models, to orchestrate their activity (train, deploy, bayesian tuning, etc). In your specific example, you shouldn't include the deployment and model call code to server code, as those are actually actions that will be conducted from outside the server to orchestrate its lifecyle and interact with it. For model deployment with the Sagemaker Pytorch container, your entry point script just needs to contain the required <code>model_fn<\/code> function for model deserialization, and optionally an <code>input_fn<\/code>, <code>predict_fn<\/code> and <code>output_fn<\/code>, respectively for pre-processing, inference and post-processing (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#the-sagemaker-pytorch-model-server\" rel=\"nofollow noreferrer\">detailed in the documentation here<\/a>). This logic is beautiful :) : you don't need anything else to deploy a production-ready deep learning server! (MMS in the case of Pytorch and MXNet, Flask+Gunicorn in the case of sklearn).<\/p>\n\n<p>In summary, this is how your code should be split:<\/p>\n\n<p>An entry_point script <code>serve.py<\/code> that contains model serving code and looks like this:<\/p>\n\n<pre><code>import os\n\nimport numpy as np\nimport torch\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\ndef model_fn(model_dir):\n    # TODO instantiate a model from its artifact stored in model_dir\n    return model\n\ndef predict_fn(input_data, model):\n    # TODO apply model to the input_data, return result of interest\n    return result\n<\/code><\/pre>\n\n<p>and some orchestration code to instantiate a SageMaker Model object, deploy it to a server and query it. This is run from the orchestration runtime of your choice, which could be a SageMaker Notebook, your laptop, an AWS Lambda function, an Apache Airflow operator, etc - and with the SDK for your choice; don't need to use python for this.<\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\n\npytorch_model = PyTorchModel(\n    model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n    entry_point='serve.py',\n    role='jiashenC-sagemaker',\n    py_version='py3',\n    framework_version='1.3.1')\n\npredictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n\nprint(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-08 19:23:03.9 UTC",
        "Answer_score":4.0,
        "Owner_location":"Atlanta, GA, USA",
        "Answer_last_edit_date":"2020-02-09 17:29:35.957 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60122070",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65882662,
        "Question_title":"Clustering sharing between Spark kernel notebooks",
        "Question_body":"<p>I'm looking to understanding conceptually how several Jupyter notebooks running on Spark kernels (such as SparkMagic) can share a cluster of worker nodes.<\/p>\n<p>If User A persists or caches a large RDD (whether on disk or on memory) in a cell, and then goes away for the weekend but does not stop his\/her notebook, will this degrade other users' ability to run their jobs while User A's notebook is running?<\/p>\n<p>That is, all the Spark notebooks sharing the cluster will be able to submit jobs at the same time (do not have to run sequentially), but the resources will be divided up, right?<\/p>\n<p>This is a general question, but for us we're running on an AWS Sagemaker and EMR environment in an US region, in case it makes a difference.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-25 10:10:05.597 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|apache-spark|jupyter-notebook|amazon-emr|amazon-sagemaker",
        "Question_view_count":143,
        "Owner_creation_date":"2019-08-03 06:10:05.233 UTC",
        "Owner_last_access_date":"2022-08-09 10:08:43.907 UTC",
        "Owner_reputation":465,
        "Owner_up_votes":23,
        "Owner_down_votes":3,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65882662",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66242546,
        "Question_title":"Time Series Forecasting in SageMaker - which instance should I choose?",
        "Question_body":"<p>I am currently trying to forecasting hourly values with a weekly seasonality. In the <code>SARIMAX<\/code> model that means that I have to set m = 168 (24 hours a day and 7 days a week -&gt; 24*7).<\/p>\n<p>My model looks like this:<\/p>\n<pre><code>model = SARIMAX(df['IdCount'],order=(4,1,3),seasonal_order=(2,2,1,168))\nresults = model.fit()\n<\/code><\/pre>\n<p>And I think because of the relatively big <code>m=168<\/code> it takes a very long time to fit the model. When doing this locally, the kernel died.<\/p>\n<p>Therefore, I chose to try it in SageMaker and I am currently using ml.t2.medium instance. In order to perform this task fast and without any issues which instance would you recommend and why? Which is the best one only in regards to performance and which is the best one regarding cost benefit ratio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-17 13:07:25.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|performance|amazon-sagemaker|arima",
        "Question_view_count":148,
        "Owner_creation_date":"2019-05-29 11:58:00.073 UTC",
        "Owner_last_access_date":"2022-09-23 09:10:17.557 UTC",
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66242546",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60644771,
        "Question_title":"How to parse AWS Sagemaker SM_USER_ARGS with argparse into an argparse Namespace?",
        "Question_body":"<p>AWS Sagemaker uses <code>SM_USER_ARGS<\/code> (as documented <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#sm-user-args\" rel=\"nofollow noreferrer\">here<\/a>) as an environment variable in which it contains a string (list) of arguments as they are passed by the user. So the environment variable value looks like this: <code>'[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]'<\/code>.<\/p>\n\n<p>With <code>json.loads()<\/code> I am capable of transforming that string into a python list. Although, I want to create an abstract module that returns an <strong>argparse Namespace<\/strong> in a way that rest of the code remains intact whether I run it locally or in the AWS Sagemaker service.<\/p>\n\n<p>So, basically, what I want is a code that receives the input <code>[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]<\/code> and output <code>Namespace(test_size=0.2, random_seed='42', not_optmize=True, &lt;other_arguments&gt;... ])<\/code>.<\/p>\n\n<p>Does python <strong>argparse<\/strong> package helps me with that? I am trying to figure out a way that I do not need to re implement the argparse parser.<\/p>\n\n<p>Here is an example, I have this config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>I have this Argparser class:<\/p>\n\n<pre><code>import argparse\nimport configparser\nimport datetime\nimport json\nimport multiprocessing\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nfrom .files import JsonFile, YAMLFile\n\n\nclass ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass AWSArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        &lt;parse self.args.user_args&gt;\n\n        return self.args\n<\/code><\/pre>\n\n<p>then I have my <code>train<\/code> script:<\/p>\n\n<pre><code>from utils.arg_parser import AWSArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    if os.environ[\"ENVIRON\"] == \"Sagemaker\":\n        arg_parser = AWSArgParser()\n        args = arg_parser.get_arguments()\n    else:\n        args = &lt;normal local parse&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2020-03-11 21:40:23.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-11 22:13:11.167 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|argparse|amazon-sagemaker",
        "Question_view_count":782,
        "Owner_creation_date":"2016-02-14 10:09:50.637 UTC",
        "Owner_last_access_date":"2022-09-21 20:43:14.553 UTC",
        "Owner_reputation":903,
        "Owner_up_votes":463,
        "Owner_down_votes":28,
        "Owner_views":94,
        "Answer_body":"<p>Following <a href=\"https:\/\/stackoverflow.com\/users\/1126841\/chepner\">@chepner<\/a>'s comment an example solution would be something like this:<\/p>\n\n<p>config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optimize\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>A <code>TrainArgParser<\/code> class like this:<\/p>\n\n<pre><code>class ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass TrainArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        # Not in AWS Sagemaker arguments\n        self.parser.add_argument(\n            '--test_size',\n            default=0.2,\n            type=float,\n            help=\"Test dataset size (default: '0.2')\",\n        )\n        self.parser.add_argument(\n            '--random_seed',\n            default=42,\n            type=int,\n            help=\"Random number for initialization (default: '42')\",\n        )\n        self.parser.add_argument(\n            '--secrets',\n            type=YAMLFile.parse_string,\n            default='',\n            help=\"An yaml formated string (default: '')\"\n        )\n        self.parser.add_argument(\n            '--bucket_name',\n            type=str,\n            default='',\n            help=\"Bucket name of a remote storage (default: '')\"\n        )\n        self.args = self.parser.parse_args(self.args.user_args)\n\n        return self.args\n<\/code><\/pre>\n\n<p>and a entry_script for <code>train<\/code> would start like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\n\nfrom utils.arg_parser import TrainArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    arg_parser = TrainArgParser()\n    args = arg_parser.get_arguments()\n    print(args)\n<\/code><\/pre>\n\n<p>This should output something like this:<\/p>\n\n<pre><code>Namespace(bucket_name='', channel_names=['training'], environment='Docker', input_config_dir=PosixPath('\/opt\/ml\/input\/config'), input_dir='\/opt\/ml\/input', model_dir='\/opt\/ml\/model', num_cpus=8, num_gpus=1, output_dir=PosixPath('\/opt\/ml\/output'), random_seed=42, run_tag='2020-03-11-22-18-21', secrets={}, test_size=0.2, user_args=['--test_size', '0.2', '--random_seed', '42'])\n<\/code><\/pre>\n\n<p>But that is useless if AWS Sagemaker treats <code>SM_USER_ARGS<\/code> and <code>SM_HPS<\/code> as the same thing. :(<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-11 22:30:47.987 UTC",
        "Answer_score":0.0,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60644771",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828741,
        "Question_title":"Accelerate BERT training with HuggingFace Model Parallelism",
        "Question_body":"<p>I am currently using SageMaker to train BERT and trying to improve the BERT training time. I use PyTorch and Huggingface on AWS g4dn.12xlarge instance type.<\/p>\n<p>However when I run parallel training it is far from achieving linear improvement. I'm looking for some hints on distributed training to improve the BERT training time in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 13:47:56.633 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|huggingface-transformers|bert-language-model|distributed-training",
        "Question_view_count":12,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66692579,
        "Question_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Question_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-03-18 13:56:00.093 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-18 14:20:30.863 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|error-handling|amazon-sagemaker",
        "Question_view_count":1327,
        "Owner_creation_date":"2019-05-29 11:58:00.073 UTC",
        "Owner_last_access_date":"2022-09-23 09:10:17.557 UTC",
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Answer_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-03-18 15:24:06.907 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62274977,
        "Question_title":"What is Quotas in Amazon web service context",
        "Question_body":"<p>what is AWS Service Utilization Quota (Limits) ? Based on the definition provided in docs it says \" AWS implements quotas to provide highly available and reliable service to all customers, and protect you from unintentional spend. Each quota starts with an AWS default value. \" . <\/p>\n\n<p>Basically my question here is what are resources in that definition ?  Are they computers , nodes , cluster or some sort of distributed computation nodes or it is something else ?<\/p>\n\n<p>Secondly , if they are different nodes or computation resources, they should be distributed, right ? <\/p>\n\n<p>I really appreciate if someone can simplify this def for me ? <\/p>\n\n<p>For instance what happens in a Sage Maker service with quota of 20 , does it mean in the training batches can be distributed over 20 nodes for fwd and back prop training ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-06-09 04:20:01.5 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|quota",
        "Question_view_count":139,
        "Owner_creation_date":"2020-04-30 04:20:29.97 UTC",
        "Owner_last_access_date":"2022-09-14 06:58:40.45 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62274977",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64477093,
        "Question_title":"Customising model in AWS sagemaker",
        "Question_body":"<p>I have a python script which I wrote using tensorflow python 3.6 AWS sagemaker jupyter notebook inside AWS sagemaker instance. I have to use sagemaker debugger for my Deep Learning model. I can see many links suggesting that first dockerise the algorithm image and then use it over sagemaker. Can anyone please suggest that is there any available alternative such that Tensorflow-1 docker image is available and I can include some other packages via pip in this image and then run my model on sagemaker ? I am using keras 2.3.0 with tensorflow 1.15 .Please guide and share necessary references.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-22 07:16:02.16 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-22 08:49:45.23 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|docker|amazon-sagemaker|keras-2|tensorflow1.15",
        "Question_view_count":181,
        "Owner_creation_date":"2016-09-14 05:43:44.143 UTC",
        "Owner_last_access_date":"2022-08-16 12:07:05.64 UTC",
        "Owner_reputation":348,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64477093",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56853463,
        "Question_title":"Creating Sagemaker training job with terraform?",
        "Question_body":"<p>I am new to terraform and was looking through the documentation. From what I can tell, there's nothing in terraform with regards to create a training job that has the model artifacts. Does this mean I can't use terraform to set up the full sagemaker pipeline? It seems to me you would have to first create the training job in some way, and then you can use terraform to create a model enpoint that uses what is there (but you can't do the training job itself with terraform).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-07-02 13:20:42.21 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":938,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56853463",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-05 20:19:23.53 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_date":"2011-01-10 02:55:08.597 UTC",
        "Owner_last_access_date":"2022-09-22 22:11:16.413 UTC",
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-06-06 00:42:46.69 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71947311,
        "Question_title":"AMT Crowd Entity Annotation",
        "Question_body":"<p>I'm creating a task on AMT using the Crowd HTML Elements that let reviewers  label each word in a text block. Here is how the code looks:<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n\n    \n    &lt;crowd-entity-annotation\n        name=&quot;crowd-entity-annotation&quot;\n        header=&quot;Watch the video and label parts of the text below to indicate whether each word\/phrase is correctly reflected in the video.&quot;\n        labels=&quot;[{'label': 'included in the video', 'shortDisplayName': 'corr', 'fullDisplayName': 'correct'}, {'label': 'not included in the video', 'shortDisplayName': 'incorr', 'fullDisplayName': 'incorrect'}]&quot;\n        text=&quot;This is a test block of text.&quot;\n    &gt;\n        &lt;full-instructions header=&quot;Do the text and the video match&quot;&gt;\n            Apply labels to words or phrases based on whether they are correctly reflected in the video or not. \n\n            &lt;ol&gt;\n                &lt;li&gt;In this task you will read a text and watch a video.&lt;\/li&gt;\n                &lt;li&gt;&lt;strong&gt;Read&lt;\/strong&gt; the text and watch the video carefully and determine words\/phrases in the text the are correcly reflected in the video.&lt;\/li&gt;\n                \n                \n            &lt;\/ol&gt;\n        &lt;\/full-instructions&gt;\n\n        &lt;short-instructions&gt;\n            Apply labels to words or phrases based on whether they are correctly reflected in the video or not. \n\n            &lt;ol&gt;\n                &lt;li&gt;In this task you will read a text and watch a video that depicts what is described in the text.&lt;\/li&gt;\n                &lt;li&gt;&lt;strong&gt;Read&lt;\/strong&gt; the text and watch the video carefully and determine words\/phrases in the text the are correcly reflected in the video.&lt;\/li&gt;\n                \n            &lt;\/ol&gt;\n            \n        &lt;\/short-instructions&gt;\n\n\n        &lt;div id=&quot;additionalQuestions&quot; style=&quot;margin-top: 20px&quot;&gt;\n                \n                &lt;video width=&quot;400&quot; height=&quot;400&quot; controls&gt;\n                         &lt;source src=&quot;${video}&quot; type=&quot;video\/mp4&quot;&gt;\n                &lt;\/video&gt;\n                &lt;p&gt;&lt;\/p&gt;\n                &lt;h4&gt;Overall how would you rate the alignment between the video and text description?&lt;\/h4&gt;\n                &lt;crowd-radio-group&gt;\n                    &lt;crowd-radio-button name=&quot;very poor&quot; value=&quot;1&quot;&gt;very poor&lt;\/crowd-radio-button&gt;\n                    &lt;crowd-radio-button name=&quot;poor&quot; value=&quot;2&quot;&gt;poor&lt;\/crowd-radio-button&gt;\n                    &lt;crowd-radio-button name=&quot;average&quot; value=&quot;3&quot;&gt;average&lt;\/crowd-radio-button&gt;\n                    &lt;crowd-radio-button name=&quot;good&quot; value=&quot;4&quot;&gt;good&lt;\/crowd-radio-button&gt;\n                    &lt;crowd-radio-button name=&quot;excellent&quot; value=&quot;5&quot;&gt;excellent&lt;\/crowd-radio-button&gt;\n                &lt;\/crowd-radio-group&gt;\n            &lt;\/div&gt;\n\n       \n \n&lt;\/crowd-entity-annotation&gt;\n\n\n&lt;script&gt;\n\n  document.addEventListener('all-crowd-elements-ready', () =&gt; {\n    document\n      .querySelector('crowd-entity-annotation')\n      .shadowRoot\n      .querySelector('crowd-form')\n      .form\n      .appendChild(additionalQuestions);\n  });\n&lt;\/script&gt;\n<\/code><\/pre>\n<p>Right now the job can be submitted if at least one character in the text is labeled. I want to ensure that the reviewer selects a label for each word\/character in the text before submitting the job.\nWhat I am trying is to use <code>onsubmit<\/code> and a validation function in the script section to check whether the length of the labeled entities added together is equal to the total length of the text block but I can't access the output of the crowd entity annotation. This is what I tried but failed:<\/p>\n<pre><code>const answers = document.querySelector('crowd-entity-annotation').value.entities \n<\/code><\/pre>\n<p>I appreciate any help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-20 23:44:34.417 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon|amazon-sagemaker|mechanicalturk",
        "Question_view_count":100,
        "Owner_creation_date":"2022-04-20 23:08:26.817 UTC",
        "Owner_last_access_date":"2022-09-22 18:02:40.387 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71947311",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63536595,
        "Question_title":"Connect Sagemaker to RDS db",
        "Question_body":"<p>I am new to AWS. All I know is that the Postgre database is hosted in AWS RDS. I want to build an ML model using AWS Sagemaker. I am not sure how to get the data from AWS RDS so that I can use it for building the ML model.\nI will be thankful for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-22 13:16:39.987 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-30 13:39:14.893 UTC",
        "Question_score":2,
        "Question_tags":"amazon-rds|amazon-sagemaker",
        "Question_view_count":2550,
        "Owner_creation_date":"2020-08-22 13:11:51.76 UTC",
        "Owner_last_access_date":"2022-09-04 07:41:29.103 UTC",
        "Owner_reputation":153,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63536595",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66196815,
        "Question_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Question_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-02-14 15:15:31.557 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2021-02-24 12:09:13.343 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|logistic-regression|recurrent-neural-network|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_date":"2019-08-26 17:17:56.033 UTC",
        "Owner_last_access_date":"2022-01-09 18:01:35.493 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-02-27 18:21:16.54 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-02-27 18:46:22.833 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66196815",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56569605,
        "Question_title":"AWS Sagemaker BlazingText Multiple Training Files",
        "Question_body":"<p>Trying to find out if you can use multiple files for your dataset in Amazon Sagemaker BlazingText. <\/p>\n\n<p>I am trying to use it in Text Classification mode. <\/p>\n\n<p>It appears that it's not possible, certainly not in File mode, but wondering about whether Pipe mode supports it. I don't want to have all my training data in 1 file, because if it's generated by an EMR cluster I would need to combine it afterwards which is clunky. <\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-12 20:02:43.12 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-19 14:24:33.957 UTC",
        "Question_score":3,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":568,
        "Owner_creation_date":"2010-01-28 15:36:44.5 UTC",
        "Owner_last_access_date":"2020-10-30 13:38:18.957 UTC",
        "Owner_reputation":924,
        "Owner_up_votes":35,
        "Owner_down_votes":4,
        "Owner_views":139,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Florida",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56569605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65545350,
        "Question_title":"How to create hyperparameter tuning job with custom docker container on Amazon Sagemaker?",
        "Question_body":"<p>I created a custom docker container to run Catboost on Amazon Sagemaker, followed this demo (<a href=\"https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\/blob\/master\/Catboost_container_for_SageMaker.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\/blob\/master\/Catboost_container_for_SageMaker.ipynb<\/a>). I now want to do hyperparameter tuning with this custom container, but this is not a built-in or pre-built Sagemaker container, so I am not sure if I could or how to create hyperparameter tuning job on Sagemaker with a custom container. I didn't find any official documentation or official examples about using custom docker container to do HYT.<\/p>\n<p>So my question is: how to create hyperparameter tuning with a custom container on Amazon Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-02 23:51:32.08 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_date":"2021-01-02 23:03:09.943 UTC",
        "Owner_last_access_date":"2021-03-09 20:23:20.67 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Kansas City, MO, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65545350",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70492796,
        "Question_title":"What framework does Sagemaker use to train a model?",
        "Question_body":"<p>Models trained on Sagemaker produced <code>model.tar.gz<\/code> files as output.<br \/>\nI want to use these files for prediction on my local machine.<\/p>\n<p>I'm not sure which framework is being used by Sgaemaker to train these models.<\/p>\n<p>I guess I'd have to use the same framework to load the model artifacts for prediction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-27 07:38:08.077 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_date":"2019-03-04 12:06:56.487 UTC",
        "Owner_last_access_date":"2022-09-23 07:48:34.613 UTC",
        "Owner_reputation":735,
        "Owner_up_votes":167,
        "Owner_down_votes":15,
        "Owner_views":106,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70492796",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51433339,
        "Question_title":"Is it a bug in fastparquet module",
        "Question_body":"<p>I am using AWS sagemaker Jupiter notebook and getting following error:<\/p>\n\n<pre><code>&lt;ipython-input-42-14cd1ee49f9c&gt; in &lt;module&gt;()\n      1 import s3fs\n----&gt; 2 import fastparquet as fp\n      3 s3 = s3fs.S3FileSystem()\n      4 fs = s3fs.core.S3FileSystem()\n      5 \n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/__init__.py in &lt;module&gt;()\n      6 \n      7 from .thrift_structures import parquet_thrift\n----&gt; 8 from .core import read_thrift\n      9 from .writer import write\n     10 from . import core, schema, converted_types, api\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/core.py in &lt;module&gt;()\n     12 \n     13 from . import encoding\n---&gt; 14 from .compression import decompress_data\n     15 from .converted_types import convert, typemap\n     16 from .schema import _is_list_like, _is_map_like\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/compression.py in &lt;module&gt;()\n     43     def snappy_decompress(data, uncompressed_size):\n     44         return snappy.decompress(data)\n---&gt; 45     compressions['SNAPPY'] = snappy.compress\n     46     decompressions['SNAPPY'] = snappy_decompress\n     47 except ImportError:\n\nAttributeError: module 'snappy' has no attribute 'compress'\n<\/code><\/pre>\n\n<p>I noticed that the attribute is named snappy.compress. Shouldn't it be snappy_compress with underscore?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-20 00:05:20.667 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|jupyter-notebook|amazon-sagemaker|fastparquet",
        "Question_view_count":1378,
        "Owner_creation_date":"2016-02-23 01:29:53.537 UTC",
        "Owner_last_access_date":"2020-03-03 18:39:41.04 UTC",
        "Owner_reputation":111,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51433339",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60201321,
        "Question_title":"Unable to open Jupyter Notebook instance on AWS Sagemaker",
        "Question_body":"<p>I was working on the Notebook instance yesterday and was able to run some scripts successfully, but this morning I was unable to connect to the notebook instance. I keep on getting the loading icon before 504 error appears. Not sure if this is related to memory since I am currently using t2.medium and I can upgrade if that's the case. My only concern with this is that the data shouldn't be lost.<\/p>\n\n<p>Any help on how I can check what issue is will be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-13 05:58:25.327 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ec2|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1042,
        "Owner_creation_date":"2013-10-05 11:29:52.487 UTC",
        "Owner_last_access_date":"2022-09-22 10:42:27.737 UTC",
        "Owner_reputation":180,
        "Owner_up_votes":347,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pune, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60201321",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68502272,
        "Question_title":"Deploying custom preprocessing and postprocessing scripts in SageMaker",
        "Question_body":"<p>I am trying to convert some python scripts into a callable endpoint in SageMaker. My preprocessing(feature engineering) and postprocessing scripts are written in python and have a few interdependent scripts and methods in them. The preprocessing steps are also not necessarily from SKLearn, they are customized functions and need to be called from the preprocessing endpoint every time on the raw data that will then be used for prediction using a model saved as a second endpoint. The third endpoint will be for the postprocessing steps and connecting these 3 endpoints we want to get our data from the raw format to the output format every time.<\/p>\n<p>We currently have normal python scripts that preprocesses the data using some highly customized functions( all features are ultimately derived features) and then performs some inference and then again using some highly customized postprocessing generates the final results. While the input is a CSV file, after each stage of preprocessing and post processing the dimensions of the data and also the format of the output(dataframe, list, list of lists) are likely to change.<\/p>\n<p>For reference, we are using, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/scikit_learn_estimator_example_with_batch_transform.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/scikit_learn_estimator_example_with_batch_transform.ipynb<\/a> and <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a>.<\/p>\n<p>Please let me know if there is any better reference that caters to our specific requirements.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-23 16:21:53.75 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":543,
        "Owner_creation_date":"2021-07-13 16:28:54.377 UTC",
        "Owner_last_access_date":"2022-03-21 06:24:34.5 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68502272",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54931270,
        "Question_title":"Download an entire folder from AWS sagemaker to laptop",
        "Question_body":"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. \nThis might sound so simple and easy, but I could not find a way to do it. Appreciate any help.<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-28 17:36:21.97 UTC",
        "Question_favorite_count":8.0,
        "Question_last_edit_date":null,
        "Question_score":17,
        "Question_tags":"amazon-web-services|download|amazon-sagemaker",
        "Question_view_count":13015,
        "Owner_creation_date":"2013-04-21 07:52:05.213 UTC",
        "Owner_last_access_date":"2022-09-22 19:54:43.253 UTC",
        "Owner_reputation":440,
        "Owner_up_votes":149,
        "Owner_down_votes":1,
        "Owner_views":24,
        "Answer_body":"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it<\/p>\n\n<pre><code>zip -r -X archive_name.zip folder_to_compress\n<\/code><\/pre>\n\n<p>You will find the zipped folder. You can then select it and download it.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-02-28 17:45:26.143 UTC",
        "Answer_score":35.0,
        "Owner_location":"California, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54931270",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70513398,
        "Question_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Question_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-28 22:56:44.667 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"git|amazon-sagemaker|apt",
        "Question_view_count":489,
        "Owner_creation_date":"2020-08-06 15:31:21.047 UTC",
        "Owner_last_access_date":"2022-08-22 18:00:08.567 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-28 22:59:15.18 UTC",
        "Answer_score":6.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53125108,
        "Question_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Question_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2018-11-02 19:59:19.393 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|aws-cli|amazon-sagemaker",
        "Question_view_count":8552,
        "Owner_creation_date":"2011-03-10 10:26:00.99 UTC",
        "Owner_last_access_date":"2022-09-24 23:25:14.187 UTC",
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Answer_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-12 17:02:25.47 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2018-11-13 16:12:30.813 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69728847,
        "Question_title":"Error when trying to read in data to AWS SageMaker from S3 within VPC",
        "Question_body":"<p>I am in a VPC trying to read in data (using Python) from an S3 bucket into a Jupyter notebook hosted on a SageMaker notebook instance and I get the following error:<\/p>\n<p>ClientConnectorError: Cannot connect to host my-bucket.s3.amazonaws.com:443 ssl:default [Temporary failure in name resolution]<\/p>\n<p>Any thoughts on remedies?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-26 19:22:41.517 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":151,
        "Owner_creation_date":"2017-01-08 18:13:15.363 UTC",
        "Owner_last_access_date":"2022-06-07 19:08:27.657 UTC",
        "Owner_reputation":311,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69728847",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72871765,
        "Question_title":"Unable to attach custom image to AWS Sagemaker Studio",
        "Question_body":"<p>I am trying to attach custom image in SageMaker, it was working fine until I deleted couple of previous version, it started giving me errors as bellow and now I am unable to attach either new image or a new version for existing image.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xc1QA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xc1QA.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-05 15:13:19.37 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ecr",
        "Question_view_count":172,
        "Owner_creation_date":"2019-01-28 05:40:28.333 UTC",
        "Owner_last_access_date":"2022-09-15 15:39:08.883 UTC",
        "Owner_reputation":115,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72871765",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828700,
        "Question_title":"AWS SageMaker Canvas Model usage on Edge device in Python",
        "Question_body":"<p>This way I wanted to ask a question about AWS Sagemaker. I must confess that I'm quite a newbee to the subject and therefor I was very happy with the SageMaker Canvas app. It works really easy and gives me some nice results.<\/p>\n<p>First of all my model. I try to predict solar power production based on the time (dt), the AWS IoT Thingname (thingname), clouds percentage (clouds) and temperature (temp). I have a csv filled with data measured by IoT things<\/p>\n<p><code>clouds<\/code> + <code>temp<\/code> + <code>dt<\/code> + <code>thingname<\/code> =&gt; <code>import<\/code><\/p>\n<pre><code>dt,clouds,temp,import,thingname\n2022-08-30 07:45:00+02:00,1.0,0.1577,0.03,***\n2022-08-30 08:00:00+02:00,1.0,0.159,0.05,***\n2022-08-30 08:15:00+02:00,1.0,0.1603,0.06,***\n2022-08-30 08:30:00+02:00,1.0,0.16440000000000002,0.08,***\n2022-08-30 08:45:00+02:00,,,0.09,***\n2022-08-30 09:00:00+02:00,1.0,0.17,0.12,***\n2022-08-30 09:15:00+02:00,1.0,0.1747,0.13,***\n2022-08-30 09:30:00+02:00,1.0,0.1766,0.15,***\n2022-08-30 09:45:00+02:00,0.75,0.1809,0.18,***\n2022-08-30 10:00:00+02:00,1.0,0.1858,0.2,***\n2022-08-30 10:15:00+02:00,1.0,0.1888,0.21,***\n2022-08-30 10:30:00+02:00,0.75,0.1955,0.24,***\n<\/code><\/pre>\n<p>In AWS SageMaker canvas I upload the csv and build the model. All is very easy and when I use the predict tab I upload a CSV where the import column is missing and containing API weather data for some future moment:<\/p>\n<pre><code>dt,thingname,temp,clouds\n2022-09-21 10:15:00+02:00,***,0.1235,1.0\n2022-09-21 10:30:00+02:00,***,0.1235,1.0\n2022-09-21 10:45:00+02:00,***,0.1235,1.0\n2022-09-21 11:00:00+02:00,***,0.1235,1.0\n2022-09-21 11:15:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:30:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:45:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:00:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:15:00+02:00,***,0.1351,0.69\n2022-09-21 12:30:00+02:00,***,0.1351,0.69\n2022-09-21 12:45:00+02:00,***,0.1351,0.69\n<\/code><\/pre>\n<p>From this data SageMaker Canvas predicts some real realistic numbers, from which I assume the model is nicely build. So I want to move this model to my Greengrass Core Device to do predictions on site. I found the best model location using the sharing link to the Junyper notebook.<\/p>\n<p>From reading in the AWS docs I seem to have a few options to run the model on an edge device:<\/p>\n<ul>\n<li>Run the Greengrass SageMaker Edge component and run the model as a component and write an inference component<\/li>\n<li>Run the SageMaker Edge Agent yourself<\/li>\n<li>Just download the model yourself and do your thing with it on the device<\/li>\n<\/ul>\n<p>Now it seems that SageMaker used XGBoost to create the model and I found the <code>xgboost-model<\/code> file and downloaded it to the device.<\/p>\n<p>But here is where the trouble started:\nSageMaker Canvas never gives any info on what it does with the CSV to format it, so I have really no clue on how to make a prediction using the model.\nI get some results when I try to open the same csv file I used for the Canvas prediction, but the data is completely different and not realistic at all<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\n\nfilename = f'solar-prediction-data.csv'\ndpredict = xgb.DMatrix(f'{filename}?format=csv')\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict)\nprint('Prediction result::')\nprint(result)\n<\/code><\/pre>\n<p>I read that the column order matters, the CSV may not contain a header.  But it does not get close to the SageMaker Canvas result.<\/p>\n<p>I also tried using <code>pandas<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\nimport pandas as pd\n\nfilename = f'solar-prediction-data.csv'\ndf = pd.read_csv(filename, index_col=None, header=None)\n\ndpredict = xgb.DMatrix(df, enable_categorical=True)\n\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict, pred_interactions=True)\nprint('Prediction result::')\nprint('===============')\nprint(result)\n<\/code><\/pre>\n<p>But this last one always gives me following error:<\/p>\n<pre><code>ValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\ncategorical type is supplied, DMatrix parameter `enable_categorical` must\nbe set to `True`. Invalid columns:dt, thingname\n<\/code><\/pre>\n<p>To be honest, I'm completely stuck and hope someone around here can give me some advice or clue on how I can proceed.<\/p>\n<p>Thanks!\nKind regards<\/p>\n<p>Hacor<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 13:45:00.76 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|aws-iot|aws-iot-core|aws-iot-greengrass",
        "Question_view_count":18,
        "Owner_creation_date":"2016-02-10 07:57:24.887 UTC",
        "Owner_last_access_date":"2022-09-23 17:55:05.747 UTC",
        "Owner_reputation":43,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828700",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73645084,
        "Question_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Question_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 07:17:04.647 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-08 09:02:38.727 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":27,
        "Owner_creation_date":"2022-09-08 07:14:26.503 UTC",
        "Owner_last_access_date":"2022-09-23 21:03:19.637 UTC",
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-08 16:08:29.533 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53770876,
        "Question_title":"AWS Sagemaker, InvokeEndpoint operation, Model error: \"setting an array element with a sequence.\"",
        "Question_body":"<p>I am trying to Invoke Endpoint, previously deployed on Amazon SageMaker.\nHere is my code:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = np.array([3.60606061e+00, \n                        3.91395664e+00, \n                        1.34200000e+03, \n                        4.56100000e+03,\n                        2.00000000e+02, \n                        2.00000000e+02]) \ncsv_test_vector = np2csv(test_vector)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=csv_test_vector)\n<\/code><\/pre>\n\n<p>And here is the error I get:<\/p>\n\n<blockquote>\n  <p>ModelErrorTraceback (most recent call last)\n   in ()\n        1 response = client.invoke_endpoint(EndpointName=endpoint_name,\n        2                                    ContentType='text\/csv',\n  ----> 3                                    Body=csv_test_vector)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _api_call(self, *args, **kwargs)\n      318                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      319             # The \"self\" in this scope is referring to the BaseClient.\n  --> 320             return self._make_api_call(operation_name, kwargs)\n      321 \n      322         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _make_api_call(self, operation_name, api_params)\n      621             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      622             error_class = self.exceptions.from_code(error_code)\n  --> 623             raise error_class(parsed_response, operation_name)\n      624         else:\n      625             return parsed_response<\/p>\n  \n  <p>ModelError: An error occurred (ModelError) when calling the\n  InvokeEndpoint operation: Received client error (415) from model with\n  message \"setting an array element with a sequence.\". See\n  <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28\" rel=\"nofollow noreferrer\">https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28<\/a>\n  in account 249707424405 for more information.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-13 22:18:29.967 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2561,
        "Owner_creation_date":"2011-12-27 12:21:49.37 UTC",
        "Owner_last_access_date":"2022-09-22 11:58:08.16 UTC",
        "Owner_reputation":1593,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":93,
        "Answer_body":"<p>This works:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = [3.60606061e+00, \n               3.91395664e+00, \n               1.34200000e+03, \n               4.56100000e+03,\n               2.00000000e+02, \n               2.00000000e+02]) \n\nbody = ',',join([str(item) for item in test_vector])\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-16 18:59:52.37 UTC",
        "Answer_score":3.0,
        "Owner_location":"Moscow, Russia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53770876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72597884,
        "Question_title":"I am trying to create AWS Sagemaker Pipeline. ClientError: An error occurred (ValidationException) when calling the CreatePipeline operation:",
        "Question_body":"<p>I am trying to create AWS Sagemaker Pipeline. I created this pipeline 2 months back and it was running then. But now I am getting following error on running<\/p>\n<pre><code>pipeline.upsert(role_arn=role)\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Unknown property reference [Parameters.DataSplitPercent].\n<\/code><\/pre>\n<p>Following is the some code written to create pipeline:<\/p>\n<pre><code>Input_data =ParameterString(name=&quot;InputDataUrl&quot;,default_value=f&quot;s3:\/\/akgargbucket\/iris.data&quot;)&lt;br\/&gt;\n<\/code><\/pre>\n<p><code>data_split_percent = ParameterFloat(name=&quot;DataSplitPercent&quot;, default_value=0.7)&lt;br\/&gt;<\/code><\/p>\n<pre><code>    framework_version=&quot;0.23-1&quot;,\n        instance_type=processing_instance_type,\n        instance_count=processing_instance_count,\n        base_job_name=f&quot;{base_job_prefix}\/sklearn-abalone-preprocess&quot;,\n        sagemaker_session=sagemaker_session,\n        role=role)\n    step_process = ProcessingStep(\n        name=&quot;ReadAndEncodeData&quot;,\n        processor=sklearn_processor,\n        outputs=[\n            ProcessingOutput(output_name=&quot;train&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n            ProcessingOutput(output_name=&quot;test&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n             ],\n        code=os.path.join(BASE_DIR, &quot;preprocess.py&quot;),\n        job_arguments=[&quot;--input-data&quot;, input_data,\n        &quot;--data_split_percent&quot;,data_split_percent])\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 04:50:16.74 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker|aws-pipeline",
        "Question_view_count":263,
        "Owner_creation_date":"2019-07-05 09:43:42.373 UTC",
        "Owner_last_access_date":"2022-09-23 15:01:08.727 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72597884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69685819,
        "Question_title":"Taking Json type as parameter for cloudformation template",
        "Question_body":"<p>I am trying to take the environment variables as parameters for the template:\n<a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html<\/a><\/p>\n<p>The type seems to be Json in the template and I dont understand how to populate it.<\/p>\n<p>It seems like I can define this if i hardcode environment variables as below:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          REQUEST_KEEP_ALIVE_TIME_SEC: '90'\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>However, there doesnt seem to be a way pass this in ? Anyone figured this out or any recommended way to do this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-23 06:50:49.693 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":415,
        "Owner_creation_date":"2013-05-22 07:21:58.273 UTC",
        "Owner_last_access_date":"2021-12-29 01:42:56.513 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>I was able to get this to work by using AWS:Include and Fn:Transform and storing my environment variables as json in passed s3 file.<\/p>\n<p>My cfn template looks like:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          Fn::Transform:\n            Name: AWS::Include\n            Parameters:\n              Location: &lt;your S3 file&gt;\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>My s3 file looks like:<\/p>\n<pre><code>{\n  &quot;REQUEST_KEEP_ALIVE_TIME_SEC&quot;: &quot;90&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-26 19:48:29.907 UTC",
        "Answer_score":0.0,
        "Owner_location":"Seattle, WA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69685819",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73595234,
        "Question_title":"Deploying tensorflow model on sagemaker async endpoint and including an inference.py script",
        "Question_body":"<p>I am trying to deploy a tensorflow model to async endpoint on sagemaker.<\/p>\n<p>I've previously deployed the same model to a real time endpoint using the following code:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import Model\n\ntensorflow_serving_model = Model(model_data=model_artifact,\n                                 entry_point = 'inference.py',\n                                 source_dir = 'code',\n                                 role=role,\n                                 framework_version='2.3',\n                                 sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<pre><code>predictor = tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>Using the source_dir argument; I was able to include inference.py and requirements.txt files with my model...<\/p>\n<p><strong>What iam  trying to do now:<\/strong>\niam trying to deploy the same model to an async endpoint, following<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference-create-endpoint.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">this<\/a> blog example...\nI used the following snippits:<\/p>\n<pre><code>from sagemaker.image_uris import retrieve\n\ndeploy_instance_type = 'ml.m5.xlarge'\ntensorflow_inference_image_uri = retrieve('tensorflow',\n                                       region,\n                                       version='2.8',\n                                       py_version='py3',\n                                       instance_type = deploy_instance_type,\n                                       accelerator_type=None,\n                                       image_scope='inference')\n\ncontainer = tensorflow_inference_image_uri\nmodel_name = 'sagemaker-{0}'.format(str(int(time.time())))\n\n# Create model\ncreate_model_response = sm_client.create_model(\n    ModelName = model_name,\n    ExecutionRoleArn = role,\n    PrimaryContainer = {\n        'Image': container,\n        'ModelDataUrl': model_artifact,\n        'Environment': {\n            'TS_MAX_REQUEST_SIZE': '100000000', #default max request size is 6 Mb for torchserve, need to update it to support the 70 mb input payload\n            'TS_MAX_RESPONSE_SIZE': '100000000',\n            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n        }\n    },    \n)\n<\/code><\/pre>\n<pre><code># Create endpoint config\nendpoint_config_name = f&quot;AsyncEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}&quot;\ncreate_endpoint_config_response = sm_client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;VariantName&quot;: &quot;variant1&quot;,\n            &quot;ModelName&quot;: model_name,\n            &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n            &quot;InitialInstanceCount&quot;: 1\n        }\n    ],\n    AsyncInferenceConfig={\n        &quot;OutputConfig&quot;: {\n            &quot;S3OutputPath&quot;: f&quot;s3:\/\/{bucket}\/{bucket_prefix}\/output&quot;,\n            #  Optionally specify Amazon SNS topics\n            &quot;NotificationConfig&quot;: {\n              &quot;SuccessTopic&quot;: success_topic,\n              &quot;ErrorTopic&quot;: error_topic,\n            }\n        },\n        &quot;ClientConfig&quot;: {\n            &quot;MaxConcurrentInvocationsPerInstance&quot;: 2\n        }\n    }\n)\n<\/code><\/pre>\n<pre><code># Create endpoint\nendpoint_name = f&quot;sm-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}&quot;\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>\n<p><strong>The problem I am having:<\/strong>\nI can not specify a source directory containing my inference.py and my requirements.txt when trying to deploy the model to an async endpoint.<\/p>\n<p>I am sure i can't include the code\/ directory in the .tar model file according to the docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#deploy-tensorflow-serving-models\" rel=\"nofollow noreferrer\">here<\/a> the only way is through the source_dir argument in the SDK Model class initialization.<\/p>\n<p><strong>my question:<\/strong>\nhow can i use my code\/ directory containing my inference.py with my tensorflow model on async endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-03 20:27:27.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-03 22:48:49.827 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_date":"2021-09-26 02:19:45.68 UTC",
        "Owner_last_access_date":"2022-09-22 17:23:33.553 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Cairo, Egypt",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73595234",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65664382,
        "Question_title":"SageMaker cannot create first user",
        "Question_body":"<p>I need to create first user to getting start, but Amazon won't let me.<\/p>\n<blockquote>\n<p>ResourceLimitExceeded The account-level service limit 'Maximum number\nof user profiles per domain' is 0 UserProfiles, with current\nutilization of 0 UserProfiles and a request delta of 1 UserProfiles.\nPlease contact AWS support to request an increase for this limit.<\/p>\n<\/blockquote>\n<p>I cannot get pass the quickstart guide.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Twa8Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Twa8Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How should I fix ? What did I miss ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-11 09:39:48.797 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":378,
        "Owner_creation_date":"2015-08-21 06:48:55.517 UTC",
        "Owner_last_access_date":"2022-09-25 03:29:15.82 UTC",
        "Owner_reputation":4748,
        "Owner_up_votes":4859,
        "Owner_down_votes":15,
        "Owner_views":596,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Ho Chi Minh City, Vietnam",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65664382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63827706,
        "Question_title":"Is there a better way to handle warnings for built-in algorithms when using TrainingJobAnalytics function in sagemaker sdk?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/7BWoE.png\" rel=\"nofollow noreferrer\"><\/a>\nWhen using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/analytics.py\" rel=\"nofollow noreferrer\">TrainingJobAnalytics<\/a> function in sagemaker sdk to view training job results, I get a bunch of scary looking warnings which I understand are harmless.<\/p>\n<p>The function checks for metric statistics from cloud watch for each metric definition defined in training job configuration. From what I understand the implementation allows to keep the logic generic enough to handle custom(byoc) algorithms as well as inbuilt and avoid hard-coding.<\/p>\n<p>I want to know if there is a better way to do this and avoid the warnings instead of suppressing them.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7BWoE.png\" rel=\"nofollow noreferrer\">warnings image<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-10 10:26:42.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-10 10:32:20.49 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":33,
        "Owner_creation_date":"2020-09-10 08:10:06.837 UTC",
        "Owner_last_access_date":"2021-02-26 11:25:47.89 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63827706",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72419908,
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-28 23:26:47.103 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-06-02 00:17:48.707 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56121070,
        "Question_title":"How to run a jupyter notebook programmatically (inside a Sagemaker notebook) from a local environment",
        "Question_body":"<p>I can start\/stop Sagemaker notebooks with boto3, but how do run the jupyter notebooks or <code>.py<\/code> scripts inside?<\/p>\n\n<p>This is something I'll run from a local environment or lambda (but that's no problem).<\/p>\n\n<p><strong>Start Sagemaker notebook instance:<\/strong><\/p>\n\n<pre><code>import boto3\n\nclient = boto3.client('sagemaker')\n\nclient.start_notebook_instance(\n    NotebookInstanceName='sagemaker-notebook-name'\n)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_notebook_instance\" rel=\"noreferrer\">docs<\/a><\/p>\n\n<p>In the UI I would just click \"Open Jupyter\", then run a notebook or a <code>.py<\/code> script inside it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tGVGx.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tGVGx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>But I want to do it programmatically with boto3 or other.<\/p>\n\n<p>My file inside is called <code>lemmatize-input-data.ipynb<\/code>.<\/p>\n\n<p>This must be possible but I'm unsure how?<\/p>\n\n<p><strong>I also tried:<\/strong><\/p>\n\n<p>In a \"start notebook\" lifecycle configuration script, after creating a simpler test file called <code>test_script.ipynb<\/code> to be certain it wasn't something advanced in my jupyter notebook that caused the error.<\/p>\n\n<pre><code>set -e\n\njupyter nbconvert --execute test_script.ipynb\n<\/code><\/pre>\n\n<p>But got the error:<\/p>\n\n<blockquote>\n  <p>[NbConvertApp] WARNING | pattern 'test_script.ipynb' matched no files<\/p>\n<\/blockquote>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-13 23:26:15.813 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2019-05-14 22:08:59.82 UTC",
        "Question_score":5,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|boto3|amazon-sagemaker",
        "Question_view_count":6693,
        "Owner_creation_date":"2015-05-17 14:29:15.547 UTC",
        "Owner_last_access_date":"2022-09-13 18:51:39.033 UTC",
        "Owner_reputation":9923,
        "Owner_up_votes":288,
        "Owner_down_votes":0,
        "Owner_views":530,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Unknown",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56121070",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57243583,
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-28 17:55:14.917 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-28 19:36:44.717 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":522,
        "Owner_creation_date":"2016-05-27 23:31:32.937 UTC",
        "Owner_last_access_date":"2022-07-18 14:07:45.653 UTC",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-28 18:55:23.623 UTC",
        "Answer_score":2.0,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70681074,
        "Question_title":"Amazon SageMaker could not get a response from the endpoint",
        "Question_body":"<p>I have built an anomaly detection model using AWS SageMaker inbuilt model: random cut forest.<\/p>\n<pre><code>    rcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    num_samples_per_tree=1000,\n    num_trees=100,\n    encrypt_inter_container_traffic=True,\n    enable_network_isolation=True,\n    enable_sagemaker_metrics=True)\n<\/code><\/pre>\n<p>and created the endpoint:-<\/p>\n<pre><code>    rcf_inference = rcf.deploy(\n              initial_instance_count=4, instance_type=&quot;ml.m5.xlarge&quot;,\n              endpoint_name='RCF-container2',\n              enable_network_isolation=True)\n<\/code><\/pre>\n<p>But when I tried to get the prediction using the endpoint I am running into the following error:-<\/p>\n<pre><code>    results = rcf_inference.predict(df.values)\n\n    ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Amazon SageMaker could not get a response from the RCF-container2 endpoint. This can occur when CPU or memory utilization is high. To check your utilization, see Amazon CloudWatch. To fix this problem, use an instance type with more CPU capacity or memory.&quot;\n<\/code><\/pre>\n<p>I have tried with larger cpu instance but still I am getting the same issue. I guess the issue is functional.<\/p>\n<p>Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-12 11:50:25.017 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_date":"2018-07-15 07:40:34.317 UTC",
        "Owner_last_access_date":"2022-06-20 05:07:35.513 UTC",
        "Owner_reputation":205,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70681074",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61389632,
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-23 14:27:38.893 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":398,
        "Owner_creation_date":"2016-09-13 12:35:38.817 UTC",
        "Owner_last_access_date":"2022-09-12 09:54:06.233 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-28 08:50:49.633 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59773503,
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 16:00:15.88 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2016-03-21 08:49:39.92 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.84 UTC",
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-01-16 20:30:04.95 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-01-20 07:54:05.7 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72269991,
        "Question_title":"Getting Provisioning failed for CloudFormation when creating a project in SageMaker studio",
        "Question_body":"<p>I am testing out MLOps using SageMaker studio and am creating a project using a template for MLOps provided by SageMaker: <em>MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline<\/em><\/p>\n<p>I am getting this error when creating the project<\/p>\n<pre><code>Your project couldn't be created\nStudio encountered an error when creating your project. Try recreating the project again.\n\nClient error: Provisioning failed with error: Errors from CloudFormation: [{LogicalResourceId : SC-493356053890-pp-khphda3aqa5qq, ResourceType : AWS::CloudFormation::Stack,\n\nStatusReason : The following resource(s) failed to create: [SageMakerModelDeploySeedCodeCheckinProjectTriggerLambdaInvoker, SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker]. Rollback requested by user.},\n\n{LogicalResourceId : SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker, ResourceType : AWS::CloudFormation::CustomResource, StatusReason : Resource creation cancelled}, {LogicalResourceId : SageMakerModelDeploySeedCodeCheckinProjectTriggerLambdaInvoker, ResourceType : AWS::CloudFormation::CustomResource,\n\nStatusReason : Received response status [FAILED] from custom resource. Message returned: Codebuild to checkin seedcode has status FAILED (RequestId: ab735779-d179-4714-af64-f0e17b5b671b)},\n\n{LogicalResourceId : SageMakerModelDeploySeedCodeCheckinProjectTriggerLambdaInvoker, ResourceType : AWS::CloudFormation::CustomResource, StatusReason : Resource creation Initiated}, {LogicalResourceId : SageMakerModelPipelineBuildProject, ResourceType : AWS::CodeBuild::Project, StatusReason : Resource creation Initiated}, {LogicalResourceId : ModelDeployBuildProject, ResourceType : AWS::CodeBuild::Project, StatusReason : Resource creation Initiated}, {LogicalResourceId : GitSeedCodeCheckinProjectTriggerLambda, ResourceType : AWS::Lambda::Function, StatusReason : Resource creation Initiated}, {LogicalResourceId : GitSeedCodeCheckinProject, ResourceType : AWS::CodeBuild::Project, StatusReason : Resource creation Initiated}, {LogicalResourceId : ModelBuildSagemakerCodeRepository, ResourceType : AWS::SageMaker::CodeRepository, StatusReason : Resource creation Initiated}, {LogicalResourceId : ModelDeploySagemakerCodeRepository, ResourceType : AWS::SageMaker::CodeRepository, StatusReason : Resource creation Initiated}, {LogicalResourceId : MlOpsArtifactsBucket, ResourceType : AWS::S3::Bucket, StatusReason : Resource creation Initiated}, {LogicalResourceId : ModelDeployTestProject, ResourceType : AWS::CodeBuild::Project, StatusReason : Resource creation Initiated}, {LogicalResourceId : WaitHandle, ResourceType : AWS::CloudFormation::WaitConditionHandle, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-493356053890-pp-khphda3aqa5qq, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}].\n<\/code><\/pre>\n<p>The IAM role attached to the SageMaker studio has the following policies attached<\/p>\n<pre><code>{\n    &quot;AttachedPolicies&quot;: [\n        {\n            &quot;PolicyName&quot;: &quot;AWSCodePipelineFullAccess&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::aws:policy\/AWSCodePipelineFullAccess&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;AmazonSageMakerFullAccess&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;AmazonSageMaker-ExecutionPolicy-20211020T164534&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::493356053890:policy\/service-role\/AmazonSageMaker-ExecutionPolicy-20211020T164534&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;ECR_FullAccess&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::493356053890:policy\/ECR_FullAccess&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;AmazonSageMakerServiceCatalogProductsUseRole-20211029T121670&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::493356053890:policy\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole-20211029T121670&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;SagemakerDomainKeyAccess-asp&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::493356053890:policy\/SagemakerDomainKeyAccess-asp&quot;\n        },\n        {\n            &quot;PolicyName&quot;: &quot;AWSLambda_FullAccess&quot;,\n            &quot;PolicyArn&quot;: &quot;arn:aws:iam::aws:policy\/AWSLambda_FullAccess&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>When creating a new project I pass,<\/p>\n<ul>\n<li>The URLs to the training and deployment git repositories<\/li>\n<li>Respective username\/repositoryname<\/li>\n<li>ARN of the CodeStar connection to GitHub with a tag <code>sagemaker=true<\/code> (access to all the repos in my account)<\/li>\n<\/ul>\n<p>What am I missing here?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-17 07:43:13.2 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|mlops",
        "Question_view_count":176,
        "Owner_creation_date":"2021-10-05 11:38:25.427 UTC",
        "Owner_last_access_date":"2022-09-16 13:13:37.81 UTC",
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72269991",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58815367,
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-12 09:11:44.24 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-11-12 09:16:44.733 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":4605,
        "Owner_creation_date":"2019-02-21 13:41:11.933 UTC",
        "Owner_last_access_date":"2022-09-23 13:21:28.837 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-13 07:40:07.523 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71408963,
        "Question_title":"Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker",
        "Question_body":"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()<\/code>.<\/p>\n<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))<\/code> to infer input and output signatures.<\/p>\n<p>Model is deployed successfully. When trying to query the model I get <code>ModelError<\/code> (full traceback below).<\/p>\n<p>To query the model, I am using precisely the same <code>test_example<\/code> that I used for <code>infer_signature()<\/code>:<\/p>\n<p><code>test_example = [['This is the subject', 'This is the body']]<\/code><\/p>\n<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()<\/code> as that is not <code>json<\/code>-serializeable.<\/p>\n<p>To query the model I tried two different approaches:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nSAGEMAKER_REGION = 'us-west-2'\nMODEL_NAME = '...'\n\nclient = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)\n\n# Approach 1\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=json.dumps(test_example),\n                ContentType=&quot;application\/json&quot;,\n            )\n\n# Approach 2\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),\n                ContentType=&quot;application\/json; format=pandas-split&quot;,\n            )\n<\/code><\/pre>\n<p>but they result in the same error.<\/p>\n<p>Will be grateful for your suggestions.<\/p>\n<p>Thank you!<\/p>\n<p>Note: I am using Python 3 and all <strong>strings are unicode<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;\n      2                 EndpointName=MODEL_NAME,\n      3                 Body=test_example,\n----&gt; 4                 ContentType=&quot;application\/json; format=pandas-split&quot;,\n      5             )\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    394             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 395             return self._make_api_call(operation_name, kwargs)\n    396 \n    397         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    724             error_class = self.exceptions.from_code(error_code)\n--&gt; 725             raise error_class(parsed_response, operation_name)\n    726         else:\n    727             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\n<\/code><\/pre>\n<p>Environment info:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],\n 'dependencies': ['python=3.6.10',\n  'pip==21.3.1',\n  'pytorch=1.10.2',\n  'cudatoolkit=10.2',\n  {'pip': ['mlflow==1.22.0',\n    'transformers==4.17.0',\n    'datasets==1.18.4',\n    'cloudpickle==1.3.0']}],\n 'name': 'bert_bec_test_env'}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-09 11:56:29.87 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-09 14:44:47.963 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|nlp|amazon-sagemaker|mlflow",
        "Question_view_count":61,
        "Owner_creation_date":"2017-03-23 13:26:01.927 UTC",
        "Owner_last_access_date":"2022-09-22 20:27:37.72 UTC",
        "Owner_reputation":83,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71408963",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72829407,
        "Question_title":"How to deploy the best tuned model using sagemaker pipelines?",
        "Question_body":"<p>I have trained an XGBoost model, finetuned it, evaluated it and registered it using aws sagemaker pipeline. Now I want to deploy the model. However, the location of the model artefact is saved as <code>Join<\/code> object because of which deployment is not working by <code>best_model.deploy(...)<\/code>. Any suggestions on how to deploy the best trained model.<\/p>\n<pre><code>tuning_step = TuningStep(name=&quot;HPTuning&quot;,\n                        tuner=tuner_log,\n                        inputs={\n                            &quot;train&quot;:...,\n                            &quot;validation&quot;:...\n                        },\n                        cache_config=cache_config)\n\n\nbest_model = Model(image_uri=image_uri, \n                  model_data=tuning_step.get_top_model_s3_uri(top_k=0,s3_bucket=model_bucket_key),\n                  sagemaker_session=sm_session,\n                  role=role,\n                  predictor_cls=XGBoostPredictor)\n\n\nregister_step = RegisterModel(name=&quot;RegisterBestChurnModel&quot;,\n                             estimator=xgb_estimator,\n                             model_data=tuning_step.get_top_model_s3_uri(top_k=0, s3_bucket=model_bucket_key),\n                             content_types=[&quot;text\/csv&quot;],\n                             response_types=[&quot;test\/csv&quot;],\n                             inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n                             transform_instances=[&quot;ml.m5.large&quot;],\n                             approval_status=&quot;Approved&quot;,\n                             model_metrics=model_metrics)\n\n\n<\/code><\/pre>\n<p>The problem with <code>best_model.deploy(...)<\/code> is that <code>tuning_step.get_top_model_s3_uri(top_k=0,s3_bucket=model_bucket_key)<\/code> is a Join object. And deploy needs the s3 bucket location as a string. So that doesn't work.<\/p>\n<p>I was also trying to deploy the registered model using<\/p>\n<pre><code>model_package_arn = register_step.properties.ModelPackageArn,\nmodel = ModelPackage(role=role, \n                     model_package_arn=create_top_step.properties.ModelArn, \n                     sagemaker_session=session)\nmodel.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>which is giving me the error<\/p>\n<pre><code>...\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, *args, **kwargs)\n   1532             container_def[&quot;Environment&quot;] = self.env\n   1533 \n-&gt; 1534         self._ensure_base_name_if_needed(model_package_name.split(&quot;\/&quot;)[-1])\n   1535         self._set_model_name_if_needed()\n   1536 \n\nAttributeError: 'tuple' object has no attribute 'split'\n<\/code><\/pre>\n<p>Which I also suspect arise for the same reason.<\/p>\n<p>I followed this tutorial pretty much<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tuning-step\/sagemaker-pipelines-tuning-step.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tuning-step\/sagemaker-pipelines-tuning-step.ipynb<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-01 12:39:53.943 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|deployment|pipeline|amazon-sagemaker",
        "Question_view_count":79,
        "Owner_creation_date":"2022-06-23 11:22:18.203 UTC",
        "Owner_last_access_date":"2022-08-17 05:32:40.397 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72829407",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72653823,
        "Question_title":"Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
        "Question_body":"<p>I have the following model in a jupyter notebook:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n<\/code><\/pre>\n<p>Sagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.<\/p>\n<p>nvidia-smi output on SageMaker:<\/p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   37C    P0    24W \/ 300W |      0MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n<\/code><\/pre>\n<p>nvidia-smi output on EC2:<\/p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   42C    P0    51W \/ 300W |   2460MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N\/A  N\/A     11802      C   \/bin\/python3.8                    537MiB |\n|    0   N\/A  N\/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n<\/code><\/pre>\n<p>The model.summary() output on SageMaker is (this is what I want it to be):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n<\/code><\/pre>\n<p>The model.summary() output on EC2 is (notice the shape change):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nModel: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n<\/code><\/pre>\n<p>One other thing that is interesting, if I change my model on the EC2 instance to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>inp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n<\/code><\/pre>\n<p>My model.summary() output becomes:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model_2&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n<\/code><\/pre>\n<p>In the last model the shape is correct but the trainable parameters is very low.<\/p>\n<p>Any ideas as to why the output shape is different and why this is happening with the filters?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-17 02:45:17.067 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-17 04:19:15.147 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-ec2|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_date":"2020-10-02 00:29:37.08 UTC",
        "Owner_last_access_date":"2022-09-02 11:23:54.887 UTC",
        "Owner_reputation":131,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72653823",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57210422,
        "Question_title":"How does AWS-Sagemaker XGBoost perform compared to XGBoost installed locally?",
        "Question_body":"<p>I did hyperparameter tuning on two XGBoost model -- one is the XGBoost in AWS-Sagemaker, the other is XGBoost installed locally using the same parameter range. It seems the optimized model via the former performs worse than the latter (18% less in prediction accuracy for a binary classification problem). I wonder has anyone encounter similar problem and if so, what would be the possible reasons? Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2019-07-25 21:36:39.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":134,
        "Owner_creation_date":"2014-05-25 15:30:58.507 UTC",
        "Owner_last_access_date":"2022-07-24 23:13:53.413 UTC",
        "Owner_reputation":1021,
        "Owner_up_votes":92,
        "Owner_down_votes":0,
        "Owner_views":120,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57210422",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71944226,
        "Question_title":"Random cut forest score distribution",
        "Question_body":"<p>I am using SageMaker Random Cut Forest. It seems that setting too high values for hyperparameters &lt;num_samples_per_tree, num_trees&gt; (if num_samples_per_tree*num_trees is higher than number of records) leads to very strange score distribution.<\/p>\n<p>Example 1 - Number of records in training set=cca 200000, num_samples_per_tree=2048, num_trees=256\n<a href=\"https:\/\/i.stack.imgur.com\/s0oLj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0oLj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Example 2 - Number of records in training set=cca 200000, num_samples_per_tree=1028, num_trees=100\n<a href=\"https:\/\/i.stack.imgur.com\/p93lp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p93lp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Samples should be created using Reservoir Sampling - so I do not see any reason why this happens. Is there any relationship between number of records and these hyperparameters?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-20 18:03:10.537 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-20 18:06:08.023 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":29,
        "Owner_creation_date":"2016-01-09 17:59:05.553 UTC",
        "Owner_last_access_date":"2022-09-14 09:56:46.717 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71944226",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59202898,
        "Question_title":"Amazon SageMaker: TrainingJobAnalytics returns only one timestamp for inbuilt xgboost",
        "Question_body":"<p>I am trying to use <code>TrainingJobAnalytics<\/code> to plot the training and validation loss curves for a training job using XGBoost on SageMaker. The training job completes successfully and I can see the training and validation rmse values in the CloudWatch logs. <\/p>\n\n<p>However when I try to get them in my notebook using <code>TrainingJobAnalytics<\/code>, I only get the metrics for a single timestamp and not all of them.<\/p>\n\n<p>My code is as below:<\/p>\n\n<pre><code>metrics_dataframe = TrainingJobAnalytics(training_job_name=job_name).dataframe()\n<\/code><\/pre>\n\n<p>What's going wrong and how can I fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-12-05 20:34:59.463 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-cloudwatch|xgboost|amazon-sagemaker",
        "Question_view_count":346,
        "Owner_creation_date":"2013-05-17 17:42:28.267 UTC",
        "Owner_last_access_date":"2022-09-21 14:33:23.777 UTC",
        "Owner_reputation":1499,
        "Owner_up_votes":599,
        "Owner_down_votes":22,
        "Owner_views":210,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"New York, NY, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59202898",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72887909,
        "Question_title":"AWS SageMaker Jupyter Notebook instance: Forbidden error unable to save the notebook",
        "Question_body":"<p>I am working on a Jupyter Notebook instance created in AWS SageMaker. I am using the awswrangler library to connect to the AWS Athena database and import the data. While trying to save the notebook after running few results, I am seeing a forbidden error and also unable to save my progress once I started seeing this error. I could not find any documentation related this. Can anyone help me with resolving this issue?<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-06 17:49:23.553 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":71,
        "Owner_creation_date":"2022-07-06 17:33:16.273 UTC",
        "Owner_last_access_date":"2022-09-12 21:49:39.137 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72887909",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59462107,
        "Question_title":"AWS SageMaker TensorFlow Serving - Endpoint failure - CloudWatch log ref: \"NET_LOG: Entering the event loop ...\"",
        "Question_body":"<p>It's my first time using sagemaker to serve my own custom tensorflow model so I have been using the medium articles to get me started:<\/p>\n\n<p><a href=\"https:\/\/medium.com\/ml-bytes\/how-to-create-a-tensorflow-serving-container-for-aws-sagemaker-4853842c9751\" rel=\"noreferrer\">How to Create a TensorFlow Serving Container for AWS SageMaker<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-push-a-docker-image-to-aws-ecs-repository-fba579a9f419?\" rel=\"noreferrer\">How to Push a Docker Image to AWS ECS Repository<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-deploy-an-aws-sagemaker-container-using-tensorflow-serving-4587dad76169?\" rel=\"noreferrer\">How to Deploy an AWS SageMaker Container Using TensorFlow Serving<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-make-predictions-against-a-sagemaker-endpoint-using-tensorflow-serving-8b423b9b316a\" rel=\"noreferrer\">How to Make Predictions Against a SageMaker Endpoint Using TensorFlow Serving<\/a><\/p>\n\n<p>I managed to create my serving container, push it successfully to ECR, and create the sagemaker model from my docker image. However, when i tried to create the endpoints it started creating but after 3-5 minutes ended with the failure message:<\/p>\n\n<blockquote>\n  <p>\"The primary container for production variant Default did not pass the\n  ping health check. Please check CloudWatch logs for this endpoint.\"<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/iXCnK.png\" rel=\"noreferrer\">Failure Image<\/a><\/p>\n\n<p>I then checked my cloud watch logs which looked like this...<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pdp4S.png\" rel=\"noreferrer\">CloudWatch Logs<\/a><\/p>\n\n<p>...ending with \"NET_LOG: Entering the event loop ...\"<\/p>\n\n<p>I tried to google more about this log message in relation to deploying sagemaker models with tf-serving, but could not find any helpful solutions.<\/p>\n\n<p>To give more context, before running into this problem I encountered 2 other issues:<\/p>\n\n<blockquote>\n  <ol>\n  <li>\"FileSystemStoragePathSource encountered a file-system access error:\n  Could not find base path <p>&lsaquo;MODEL_PATH&rsaquo;\/&lsaquo;MODEL_NAME&rsaquo;\/ for &lsaquo;MODEL_NAME&rsaquo;\"<\/li>\n  <li>\"No versions of servable  found under base path\"<\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>Both of which I managed to solve using the following links:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\" rel=\"noreferrer\">[Documentation] TensorFlowModel endpoints need the <code>export\/Servo<\/code> folder structure, but this is not documented<\/a><br><br>\n<a href=\"https:\/\/github.com\/tensorflow\/serving\/issues\/697\" rel=\"noreferrer\">Failed Reason: The primary container for production variant AllTraffic did not pass the ping health check.<\/a><br\/><\/p>\n\n<p>It's also worth noting that my Tensorflow model was created using TF version 2.0 (hence why I needed the docker container). I solely used AWS CLI to carry out my tensorflow serving instead of the sagemaker SDK.<\/p>\n\n<p>Here are snippets of my shell scripts:<\/p>\n\n<p><strong>nginx.config<\/strong><\/p>\n\n<pre><code>events {\n    # determines how many requests can simultaneously be served\n    # https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-optimize-nginx-configuration\n    # for more information\n    worker_connections 2048;\n}\n\nhttp {\n  server {\n    # configures the server to listen to the port 8080\n    # Amazon SageMaker sends inference requests to port 8080.\n    # For more information: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response\n    listen 8080 deferred;\n\n    # redirects requests from SageMaker to TF Serving\n    location \/invocations {\n      proxy_pass http:\/\/localhost:8501\/v1\/models\/pornilarity_model:predict;\n    }\n\n    # Used by SageMaker to confirm if server is alive.\n    # https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests\n    location \/ping {\n      return 200 \"OK\";\n    }\n  }\n}\n<\/code><\/pre>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>\n# RUN pip install sagemaker-containers\n\n# Installing NGINX, used to reverse proxy the predictions from SageMaker to TF Serving\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx git\n\n# Copy our model folder to the container \n# NB: Tensorflow serving requires you manually assign version numbering to models e.g. model_path\/1\/\n# see below links: \n\n# https:\/\/stackoverflow.com\/questions\/45544928\/tensorflow-serving-no-versions-of-servable-model-found-under-base-path\n# https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\nCOPY pornilarity_model \/opt\/ml\/model\/export\/Servo\/1\/\n\n# Copy NGINX configuration to the container\nCOPY nginx.conf \/opt\/ml\/code\/nginx.conf\n\n# Copies the hosting code inside the container\n# COPY serve.py \/opt\/ml\/code\/serve.py\n\n# Defines serve.py as script entrypoint\n# ENV SAGEMAKER_PROGRAM serve.py\n\n# starts NGINX and TF serving pointing to our model\nENTRYPOINT service nginx start | tensorflow_model_server --rest_api_port=8501 \\\n --model_name=pornilarity_model \\\n --model_base_path=\/opt\/ml\/model\/export\/Servo\/\n<\/code><\/pre>\n\n<p><strong>Build and push<\/strong><\/p>\n\n<pre><code>%%sh\n\n# The name of our algorithm\necr_repo=sagemaker-tf-serving\ndocker_image=sagemaker-tf-serving\n\ncd container\n\n# chmod a+x container\/serve.py\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\nregion=$(aws configure get region)\nregion=${region:-eu-west-2}\n\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com\/${ecr_repo}:latest\"\n\n# If the repository doesn't exist in ECR, create it.\n\naws ecr describe-repositories --repository-names \"${ecr_repo}\" &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name \"${ecr_repo}\" &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build -t ${docker_image} .\n# docker tag ${docker_image} ${fullname}\ndocker tag ${docker_image}:latest ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n\n<p><strong>Create SageMaker Model<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nCONTAINER_NAME=\"Pornilarity-Container\"\nMODEL_NAME=pornilarity-model-v1\n\n# the role named created with\n# https:\/\/gist.github.com\/mvsusp\/599311cb9f4ee1091065f8206c026962\nROLE_NAME=AmazonSageMaker-ExecutionRole-20191202T133391\n\n# the name of the image created with\n# https:\/\/gist.github.com\/mvsusp\/07610f9cfecbec13fb2b7c77a2e843c4\nECS_IMAGE_NAME=sagemaker-tf-serving\n# the role arn of the role\nEXECUTION_ROLE_ARN=$(aws iam get-role --role-name ${ROLE_NAME} | jq -r .Role.Arn)\n\n# the ECS image URI\nECS_IMAGE_URI=$(aws ecr describe-repositories --repository-name ${ECS_IMAGE_NAME} |\\\njq -r .repositories[0].repositoryUri)\n\n# defines the SageMaker model primary container image as the ECS image\nPRIMARY_CONTAINER=\"ContainerHostname=${CONTAINER_NAME},Image=${ECS_IMAGE_URI}\"\n\n# Createing the model\naws sagemaker create-model --model-name ${MODEL_NAME} \\\n--primary-container=${PRIMARY_CONTAINER}  --execution-role-arn ${EXECUTION_ROLE_ARN}\n<\/code><\/pre>\n\n<p><strong>Endpoint config<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nMODEL_NAME=pornilarity-model-v1\n\nENDPOINT_CONFIG_NAME=pornilarity-model-v1-config\n\nENDPOINT_NAME=pornilarity-v1-endpoint\n\nPRODUCTION_VARIANTS=\"VariantName=Default,ModelName=${MODEL_NAME},\"\\\n\"InitialInstanceCount=1,InstanceType=ml.c5.large\"\n\naws sagemaker create-endpoint-config --endpoint-config-name ${ENDPOINT_CONFIG_NAME} \\\n--production-variants ${PRODUCTION_VARIANTS}\n\naws sagemaker create-endpoint --endpoint-name ${ENDPOINT_NAME} \\\n--endpoint-config-name ${ENDPOINT_CONFIG_NAME}\n<\/code><\/pre>\n\n<p><strong>Docker Container Folder Structure<\/strong><\/p>\n\n<pre><code>\u251c\u2500\u2500 container\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 nginx.conf\n\u2502   \u251c\u2500\u2500 pornilarity_model\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u251c\u2500\u2500 saved_model.pb\n\u2502   \u2502   \u2514\u2500\u2500 variables\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00000-of-00002\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00001-of-00002\n\u2502   \u2502       \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>Any guidance would be much appreciated!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-23 22:58:04.527 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"docker|tensorflow|nginx|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":1220,
        "Owner_creation_date":"2016-03-28 13:58:51.21 UTC",
        "Owner_last_access_date":"2022-09-24 17:25:49.47 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59462107",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69366315,
        "Question_title":"Can I use AWS Sagemaker like Google Colab?",
        "Question_body":"<p>I don't want to make use of any advanced features or automation. I just want a Python Notebook that can access cloud GPUs, and is persistent, so that I can close my laptop, and it will keep running in the cloud.\nI understand Sagemaker is a paid service, but Colab just doesn't cut it for me sometimes: I need to keep my laptop running, there is only a limited quota and a chance that the session will timeout.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-09-28 17:47:10.523 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|google-colaboratory|amazon-sagemaker",
        "Question_view_count":765,
        "Owner_creation_date":"2020-08-07 07:55:34.54 UTC",
        "Owner_last_access_date":"2021-12-27 04:56:36.123 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pune, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69366315",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73467338,
        "Question_title":"How to extract S3uri as string in sagemaker",
        "Question_body":"<p>am trying to execute the below code in sagemaker pipeline<\/p>\n<pre><code>train_data = TrainingInput(\n    s3_data = step_process.properties.ProcessingOutputConfig.Outputs[&quot;train_data&quot;].S3Output.S3Uri+'\/training.txt',\n    distribution=&quot;FullyReplicated&quot;,\n    content_type=&quot;text\/plain&quot;,\n    s3_data_type=&quot;S3Prefix&quot;,\n)\n<\/code><\/pre>\n<p>I keep getting this error <code>AttributeError: 'Properties' object has no attribute 'path'<\/code> but am not sure how to extract the URI as string<\/p>\n<p>I can put that in a general question, how to access\/print <code>properties<\/code> data like <code>step_train.properties.ModelArtifacts.S3ModelArtifacts<\/code> will print  <code>&lt;sagemaker.workflow.properties.Properties at 0x7fa7d68acbd0&gt;<\/code> instead of the models path<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-24 04:11:09.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-24 15:32:58.537 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":31,
        "Owner_creation_date":"2012-04-26 13:33:06.71 UTC",
        "Owner_last_access_date":"2022-09-25 00:14:52.287 UTC",
        "Owner_reputation":1972,
        "Owner_up_votes":701,
        "Owner_down_votes":11,
        "Owner_views":547,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Egypt",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73467338",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70955992,
        "Question_title":"AWS SageMaker - Upload our own docker image",
        "Question_body":"<p>I am new to AWS SageMaker and i am using this technology for building and training the machine learning models. I have now developed a docker image which contains our custom code for tensorflow. I would like to upload this custom docker image to AWS SageMaker and make use of it.<\/p>\n<p>I have searched various links but could not find proper information on how to upload our own custom docker image.<\/p>\n<p>Can you please suggest me the recommended links regarding the process of uploading our own docker image to AWS SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-02 12:54:39.573 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|tensorflow|amazon-sagemaker|docker-image",
        "Question_view_count":279,
        "Owner_creation_date":"2013-11-04 05:16:12.667 UTC",
        "Owner_last_access_date":"2022-09-23 02:49:55.433 UTC",
        "Owner_reputation":397,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":88,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70955992",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72120382,
        "Question_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Question_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-04 23:13:17.73 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":809,
        "Owner_creation_date":"2011-12-23 23:18:48.743 UTC",
        "Owner_last_access_date":"2022-09-18 02:19:17.403 UTC",
        "Owner_reputation":969,
        "Owner_up_votes":768,
        "Owner_down_votes":10,
        "Owner_views":130,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_date":"2022-05-05 07:15:00.863 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57646477,
        "Question_title":"Can I use open Sourced Sagemaker-Neo (neo-ai-dlr) Externally on platform other than AWS?",
        "Question_body":"<p>My question is about sagemaker neo which was open sourced as neo-ai-dlr.\nCan i use it externally on my trained model on platform other than AWS like my local machine?<\/p>\n\n<p>Any Help would be appreciated\nThanks!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-25 13:34:10.957 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|optimization|amazon-sagemaker|inference",
        "Question_view_count":99,
        "Owner_creation_date":"2017-09-21 06:43:55.347 UTC",
        "Owner_last_access_date":"2019-11-18 07:57:43.547 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57646477",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68503580,
        "Question_title":"Athena query in Sage maker notebook. How to get the location of output file for reusability?",
        "Question_body":"<p>My query worked:<\/p>\n<pre><code>from pyathena import connect\nimport pandas as pd\nconn = connect(s3_staging_dir='s3:\/\/alphabucket\/query-results\/myfolder\/',region_name='us-east-1')\n\ndf = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)\n<\/code><\/pre>\n<p>How ever.. next time when I run the notebook, I would like to avoid running the query again.<\/p>\n<p>I am looking for API that would return me the result file.<\/p>\n<p>eg:<\/p>\n<pre><code>df = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)\nfile = conn.last_query_output_location() # Hypothetical function Doesnt Work\nprint(file) # --&gt; s3:\/\/alphabucket\/query-results\/myfolder\/2021\/07\/23\/dfjj00772hh.csv\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-23 18:20:12.717 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-24 05:38:39.44 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-athena|amazon-sagemaker",
        "Question_view_count":481,
        "Owner_creation_date":"2021-01-22 15:51:57.83 UTC",
        "Owner_last_access_date":"2021-07-27 15:29:36.38 UTC",
        "Owner_reputation":43,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68503580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57772626,
        "Question_title":"When trying to define transformer for a batch transform process for a sklearn estimator, i am getting an error",
        "Question_body":"<p>When trying to define transformer for a batch transform process for a sklearn estimator, i am getting the following error : TypeError: <strong>init<\/strong>() got multiple values for argument 'entry_point'\"<\/p>\n\n<p>These are the steps i followed:<\/p>\n\n<p><strong>STEP 1:<\/strong><\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'transformer.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p><strong>STEP 2:<\/strong><\/p>\n\n<pre><code>sklearn_preprocessor.fit({'train': \"s3:\/\/training-data\/train.csv\"})\n<\/code><\/pre>\n\n<p>training was successful.<\/p>\n\n<p><strong>STEP 3:<\/strong><\/p>\n\n<pre><code>transformer = sklearn_preprocessor.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    assemble_with = 'Line',\n    output_path='s3:\/\/training-data\/transformed.csv',\n    accept = 'text\/csv')\n<\/code><\/pre>\n\n<p>Error at Step3:<\/p>\n\n<pre><code>TypeError: __init__() got multiple values for argument 'entry_point'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-03 13:20:17.863 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":377,
        "Owner_creation_date":"2018-04-02 06:38:43.863 UTC",
        "Owner_last_access_date":"2022-09-18 12:02:20.327 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57772626",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64302986,
        "Question_title":"How to highlight custom extractions using a2i's crowd-textract-analyze-document?",
        "Question_body":"<p>I would like to create a human review loop for images that undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend.<\/p>\n<p>My process is:<\/p>\n<ol>\n<li>send image to Textract to extract the text<\/li>\n<li>send text to Comprehend to extract entities<\/li>\n<li>find the Block IDs in Textract's output of the entities extracted by Comprehend<\/li>\n<li>add new Blocks of type <code>KEY_VALUE_SET<\/code> to textract's JSON output <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">per the docs<\/a><\/li>\n<li>create a Human Task with <code>crowd-textract-analyze-document<\/code> element in the template and feed it the modified textract output<\/li>\n<\/ol>\n<p>What fails to work in this process is step 5. My custom entities are not rendered properly. By &quot;fails to work&quot; I mean that the entities are not highlighted on the image when I click them on the sidebar. There is no error in the browser's console.<\/p>\n<p>Has anyone tried such a thing?<\/p>\n<p><em>Sorry for not including examples. I will remove secrets\/PII from my files and attach them to the question<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-11 10:29:04.293 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-19 08:28:21.367 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-textract|amazon-comprehend",
        "Question_view_count":252,
        "Owner_creation_date":"2012-11-28 14:00:42.423 UTC",
        "Owner_last_access_date":"2022-09-24 22:55:26.03 UTC",
        "Owner_reputation":2162,
        "Owner_up_votes":390,
        "Owner_down_votes":16,
        "Owner_views":307,
        "Answer_body":"<p>I used the AWS documentation of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">a2i-crowd-textract-detection human task element<\/a> to generate the value of the <code>initialValue<\/code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:<\/p>\n<pre><code>[\n        {\n            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;Confidence&quot;: 38.43309020996094,\n            &quot;Geometry&quot;: { ... }\n            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;Relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}\n            ],\n            &quot;EntityTypes&quot;: [&quot;KEY&quot;],\n            &quot;Text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>the <code>a2i-crowd-textract-detection<\/code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:<\/p>\n<pre><code>[\n        {\n            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;confidence&quot;: 38.43309020996094,\n            &quot;geometry&quot;: { ... }\n            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}\n            ],\n            &quot;entityTypes&quot;: [&quot;KEY&quot;],\n            &quot;text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>I opened a support case about this documentation error to AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-18 09:15:33.233 UTC",
        "Answer_score":1.0,
        "Owner_location":"Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64302986",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54660994,
        "Question_title":"AWS SageMaker GroundTruth permissions issue (can't read manifest)",
        "Question_body":"<p>I'm trying to run a simple GroundTruth labeling job with a public workforce. I upload my images to S3, start creating the labeling job, generate the manifest using their tool automatically, and explicitly specify a role that most certainly has permissions on both S3 bucket (input and output) as well as full access to SageMaker. Then I create the job (standard rest of stuff -- I just wanted to be clear that I'm doing all of that).<\/p>\n\n<p>At first, everything looks fine. All green lights, it says it's in progress, and the images are properly showing up in the bottom where the dataset is. However, after a few minutes, the status changes to Failure and I get this: <code>ClientError: Access Denied. Cannot access manifest file: arn:aws:sagemaker:us-east-1:&lt;account number&gt;:labeling-job\/&lt;job name&gt; using roleArn: null<\/code> in the reason for failure.<\/p>\n\n<p>I also get the error underneath (where there used to be images but now there are none):<\/p>\n\n<p><code>The specified key &lt;job name&gt;\/manifests\/output\/output.manifest isn't present in the S3 bucket &lt;output bucket&gt;<\/code>.<\/p>\n\n<p>I'm very confused for a couple of reasons. First of all, this is a super simple job. I'm just trying to do the most basic bounding box example I can think of. So this should be a very well-tested path. Second, I'm explicitly specifying a role arn, so I have no idea why it's saying it's null in the error message. Is this an Amazon glitch or could I be doing something wrong?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-13 01:01:54.55 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2019-02-13 01:31:24.527 UTC",
        "Question_score":5,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":3282,
        "Owner_creation_date":"2012-01-04 07:55:26.97 UTC",
        "Owner_last_access_date":"2022-09-24 23:53:29.847 UTC",
        "Owner_reputation":2906,
        "Owner_up_votes":467,
        "Owner_down_votes":16,
        "Owner_views":335,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54660994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59380426,
        "Question_title":"How to use Ruby to send image to a deployed Sagemaker endpoint running a TensorFlow\/Keras CNN?",
        "Question_body":"<p>I have trained a CNN using Tensorflow\/Keras and successfully deployed it to Sagemaker using the saved_model format.  It answers pings and the dashboard shows it is running.  <\/p>\n\n<p>I now need to be able to send it images and get back inferences.  I have already successfully deployed an ANN to Sagemaker and gotten predictions back, so most of the \"plumbing\" is already working.<\/p>\n\n<p>The Ruby performing the request is as follows:<\/p>\n\n<pre><code>def predict\nsagemaker = Aws::SageMakerRuntime::Client.new(\n  access_key_id: Settings.sagemaker_key_id,\n  secret_access_key: Settings.sagemaker_secret,\n  region: Settings.sagemaker_aws_region\n)\n\nresponse = sagemaker.invoke_endpoint(endpoint_name: Settings.sagemaker_endpoint_name,\n                                     content_type: 'application\/x-image',\n                                     body: File.open('developer\/ai\/caox_test_128.jpg', 'rb'))\n\nreturn response[:body].string\n\nend\n<\/code><\/pre>\n\n<p>(For now, I simply hardcoded a known file for testing.)<\/p>\n\n<p>When I fire this, I get back this error:  <strong>Aws::SageMakerRuntime::Errors::ModelError: Received client error (400) from model with message \"{ \"error\": \"JSON Parse error: Invalid value. at offset: 0\" }\"<\/strong><\/p>\n\n<p>It's almost as if the model is expecting more in the body than just the image, but I can't tell what.  AWS's documentation has an example for Python using boto: <\/p>\n\n<pre><code>import boto3\nimport json\n\nendpoint = '&lt;insert name of your endpoint here&gt;'\n\nruntime = boto3.Session().client('sagemaker-runtime')\n\n# Read image into memory\nwith open(image, 'rb') as f:\n    payload = f.read()\n# Send image via InvokeEndpoint API\nresponse = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='application\/x-image', Body=payload)\n# Unpack response\nresult = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n\n<p>As far as I can tell, they are simply opening a file and sending it directly to sagemaker with no additional pre-processing.  And, insofar as I can tell, I'm doing exactly what they are doing in Ruby, just using 'aws-sdk'.<\/p>\n\n<p>I've looked through Amazon's documentation, and for examples on Google, but there is scant mention of doing anything special before sending the file, so I'm scratching my head.<\/p>\n\n<p>What else do I need to consider when sending a file to a Sagemaker endpoint running a TensorFlow\/Keras CNN to get it to respond with a prediction?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-17 18:53:52.527 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"ruby|keras|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":303,
        "Owner_creation_date":"2011-08-26 21:39:16.117 UTC",
        "Owner_last_access_date":"2019-12-30 06:50:59.91 UTC",
        "Owner_reputation":213,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59380426",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54432761,
        "Question_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Question_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-30 02:58:11.42 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":595,
        "Owner_creation_date":"2008-10-23 03:43:42.317 UTC",
        "Owner_last_access_date":"2022-08-22 05:29:27.157 UTC",
        "Owner_reputation":7707,
        "Owner_up_votes":162,
        "Owner_down_votes":6,
        "Owner_views":583,
        "Answer_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-30 02:58:11.42 UTC",
        "Answer_score":2.0,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59762829,
        "Question_title":"AWS Sagemaker scikit_bring_your_own example",
        "Question_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 03:59:10.087 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"scikit-learn|amazon-sagemaker|svd",
        "Question_view_count":341,
        "Owner_creation_date":"2009-04-29 11:42:36.853 UTC",
        "Owner_last_access_date":"2022-09-18 14:23:33.13 UTC",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Answer_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-17 15:03:32.89 UTC",
        "Answer_score":1.0,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72543979,
        "Question_title":"Retrieve Sagemaker Model from Model Registry in Sagemaker Pipelines",
        "Question_body":"<p>I am implementing inference pipeline via AWS Sagemaker Pipelines with Python SDK. I have a Model Package Group in Model Registry and I want to use the latest approved model version from the package group for inference (I am going to use batch-transform inference). However, I don't know which Pipeline step to use to retrieve the latest approved model version. As a workaround, I tried to use from <code>sagemaker.workflow.lambda_step.LambdaStep<\/code> to retrieve model version ARN and then <code>sagemaker.ModelPackage<\/code> to define <code>sagemaker.workflow.steps.CreateModelStep <\/code>. The minimal working code is the following<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.lambda_helper import Lambda\nfrom sagemaker.workflow.lambda_step import (\n    LambdaStep,\n    LambdaOutput,\n    LambdaOutputTypeEnum,\n)\nfrom sagemaker.workflow.pipeline import Pipeline\n\nfrom sagemaker import ModelPackage\nfrom sagemaker.workflow.steps import CreateModelStep\nfrom sagemaker.inputs import CreateModelInput\n\n\nrole = sagemaker.get_execution_role()\nsagemaker_sess = sagemaker.Session()\n\n# create lambda function that retrieves latest approved model version ARN\nfunction_name = f&quot;inference-pipeline-lambda-step&quot;\nfunc = Lambda(\n    function_name=function_name,\n    execution_role_arn=role,\n    script=&quot;get_model_arn.py&quot;,\n    handler=&quot;get_model_arn.lambda_handler&quot;,\n    timeout=600,\n    memory_size=10240,\n)\noutput_metric_value = LambdaOutput(output_name=&quot;model_package_arn&quot;, output_type=LambdaOutputTypeEnum.String)\n\n# define Lambda step that retrieves latest approved model version ARN\nstep_get_model_arn = LambdaStep(\n    name=&quot;GetModelARN&quot;,\n    lambda_func=func,\n    inputs={\n    },\n    outputs=[output_metric_value] \n)\n\n# use output of the previous Lambda step to define a sagemaker Model\nmodel = ModelPackage(\n    role=role, \n    model_package_arn=step_get_model_arn.properties.Outputs['model_package_arn'], \n    sagemaker_session=sagemaker_sess\n)\n\n# define CreateModelStep so that the model can be later used in Transform step for batch-transform inference\ninputs = CreateModelInput(\n        instance_type='ml.m5.large',\n    )\n\nstep_create_model = CreateModelStep(\n    name=&quot;create-inference-model&quot;,\n    model=model,\n    inputs=inputs,\n)\n\n# Pipeline definition and creation\/update\npipeline = Pipeline(\n    name='well-logs-inference-pipeline',\n    parameters=[],\n    steps=[\n        step_get_model_arn,\n        step_create_model\n    ],\n)\n\npipeline.upsert(role_arn=role)\n<\/code><\/pre>\n<p>This gives an error<\/p>\n<pre><code>TypeError: expected string or bytes-like object\n<\/code><\/pre>\n<p>As I understand it, the error happens in <code>model = ModelPackage(...)<\/code> expression. ModelPackage requires 'model_package_arn' to be a string, however, it is <code>sagemaker.workflow.properties.Properties<\/code> instead.<\/p>\n<p>Is there a chance to retrieve model version from Model Package Group so that it can be later used in TransformStep?<\/p>\n<p>The full traceback is here<\/p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-6-63bdf0b9bf74&gt; in &lt;module&gt;\n     65 )\n     66 \n---&gt; 67 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    114         &quot;&quot;&quot;\n    115         tags = _append_project_tags(tags)\n--&gt; 116         kwargs = self._create_args(role_arn, description, parallelism_config)\n    117         update_args(\n    118             kwargs,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in _create_args(self, role_arn, description, parallelism_config)\n    136             A keyword argument dict for calling create_pipeline.\n    137         &quot;&quot;&quot;\n--&gt; 138         pipeline_definition = self.definition()\n    139         kwargs = dict(\n    140             PipelineName=self.name,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in definition(self)\n    299     def definition(self) -&gt; str:\n    300         &quot;&quot;&quot;Converts a request structure to string representation for workflow service calls.&quot;&quot;&quot;\n--&gt; 301         request_dict = self.to_request()\n    302         request_dict[&quot;PipelineExperimentConfig&quot;] = interpolate(\n    303             request_dict[&quot;PipelineExperimentConfig&quot;], {}, {}\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in to_request(self)\n     89             if self.pipeline_experiment_config is not None\n     90             else None,\n---&gt; 91             &quot;Steps&quot;: list_to_request(self.steps),\n     92         }\n     93 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in list_to_request(entities)\n     40     for entity in entities:\n     41         if isinstance(entity, Entity):\n---&gt; 42             request_dicts.append(entity.to_request())\n     43         elif isinstance(entity, StepCollection):\n     44             request_dicts.extend(entity.request_dicts())\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/steps.py in to_request(self)\n    212     def to_request(self) -&gt; RequestType:\n    213         &quot;&quot;&quot;Gets the request structure for `ConfigurableRetryStep`.&quot;&quot;&quot;\n--&gt; 214         step_dict = super().to_request()\n    215         if self.retry_policies:\n    216             step_dict[&quot;RetryPolicies&quot;] = self._resolve_retry_policy(self.retry_policies)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/steps.py in to_request(self)\n    101             &quot;Name&quot;: self.name,\n    102             &quot;Type&quot;: self.step_type.value,\n--&gt; 103             &quot;Arguments&quot;: self.arguments,\n    104         }\n    105         if self.depends_on:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/steps.py in arguments(self)\n    411                 container_defs=self.model.prepare_container_def(\n    412                     instance_type=self.inputs.instance_type,\n--&gt; 413                     accelerator_type=self.inputs.accelerator_type,\n    414                 ),\n    415                 vpc_config=self.model.vpc_config,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in prepare_container_def(self, instance_type, accelerator_type, serverless_inference_config)\n    411         &quot;&quot;&quot;\n    412         deploy_key_prefix = fw_utils.model_code_key_prefix(\n--&gt; 413             self.key_prefix, self.name, self.image_uri\n    414         )\n    415         deploy_env = copy.deepcopy(self.env)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/fw_utils.py in model_code_key_prefix(code_location_key_prefix, model_name, image)\n    393         str: the key prefix to be used in uploading code\n    394     &quot;&quot;&quot;\n--&gt; 395     training_job_name = sagemaker.utils.name_from_image(image)\n    396     return &quot;\/&quot;.join(filter(None, [code_location_key_prefix, model_name or training_job_name]))\n    397 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/utils.py in name_from_image(image, max_length)\n     58         max_length (int): Maximum length for the resulting string (default: 63).\n     59     &quot;&quot;&quot;\n---&gt; 60     return name_from_base(base_name_from_image(image), max_length=max_length)\n     61 \n     62 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n    100         str: Algorithm name, as extracted from the image name.\n    101     &quot;&quot;&quot;\n--&gt; 102     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n    103     algo_name = m.group(2) if m else image\n    104     return algo_name\n\n\/opt\/conda\/lib\/python3.7\/re.py in match(pattern, string, flags)\n    173     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n    174     a Match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 175     return _compile(pattern, flags).match(string)\n    176 \n    177 def fullmatch(pattern, string, flags=0):\n\nTypeError: expected string or bytes-like object\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-08 10:18:16.127 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":48,
        "Owner_creation_date":"2013-10-31 09:22:47.57 UTC",
        "Owner_last_access_date":"2022-09-23 16:02:10.247 UTC",
        "Owner_reputation":486,
        "Owner_up_votes":100,
        "Owner_down_votes":1,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72543979",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67396101,
        "Question_title":"read data from rds mysql in sagemaker",
        "Question_body":"<p>Im new in AWS world. I have a rds mysql database. And i wanna read data from this db in sagemaker, but there is a problem.\nActually on local I can read this data like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def readFromAWS():\n    conn = pymysql.connect(host='MY-ENDPOINT', port=3306, user='MY-USERNAME', password='MY-PASSWORD', db='DB-NAME')\n\n    sqlText = &quot;&quot;&quot;SELECT * FROM DB-NAME.XXX order by ID&quot;&quot;&quot;\n\n    df = pd.read_sql(sqlText, conn)\n    conn.close()\n    return df\ndf=readFromAWS()\n<\/code><\/pre>\n<p>And on local it works well. But in sagemaker this code is useless.\nI need some advice. Are there any other way to read this? Should i use other services to read this data in sagemaker? Can you give me some advice??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-05-05 06:23:12.65 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"mysql|amazon-web-services|amazon-s3|amazon-rds|amazon-sagemaker",
        "Question_view_count":426,
        "Owner_creation_date":"2020-04-24 00:32:02.72 UTC",
        "Owner_last_access_date":"2022-08-31 20:00:02.297 UTC",
        "Owner_reputation":63,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"\u0130zmir, T\u00fcrkiye",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67396101",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63783682,
        "Question_title":"Can't pip install private Github repo with bash script",
        "Question_body":"<p>It's been a while since I asked this question. To simplify, I just want a lifecycle configuration in AWS SageMaker which can successfully install a private GitHub repo.<\/p>\n<hr \/>\n<p>I'm trying to install a private github repo with a bash script. The script does the following:<\/p>\n<ul>\n<li>makes sure there's an ssh agent active<\/li>\n<li>adds the ssh key from a persistent portion of memory<\/li>\n<li>attempts to install the github repo<\/li>\n<\/ul>\n<p>This is all happening in a SageMaker AWS EC2 instance via a lifecycle configuration. The implementation looks something like this:<\/p>\n<pre><code>HOME=\/home\/ec2-user\/\nENVPIP=$HOME\/anaconda3\/envs\/tensorflow2_p36\/bin\/pip\n\neval &quot;$(ssh-agent -s)&quot;\nssh-add ${HOME}SageMaker\/Setup\/id_rsa\n\nyes | $ENVPIP install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>Running this, I get the following error:<\/p>\n<pre><code>ERROR: Command errored out with exit status 128: git clone -q 'ssh:\/\/****@github.com\/...' \/tmp\/pip-req-build-ysacff_l Check the logs for full command output.\n<\/code><\/pre>\n<p>Here's all the pertinent output from cloudwatch:<\/p>\n<pre><code>Agent pid 5146\n\nIdentity added: \/home\/ec2-user\/SageMaker\/Setup\/id_rsa (\/home\/ec2user\/SageMaker\/Setup\/id_rsa)\n\n2020-09-07T17:11:00.605-04:00\n\nCollecting git+ssh:\/\/****@github.com\/********1\/*****-*****Library\n  Cloning ssh:\/\/****@github.com\/********1\/*****-*****Library to \/tmp\/pip-req-build-ysacff_l\n\n2020-09-07T17:11:00.605-04:00\n\nCopy\nERROR: Command errored out with exit status 128: git clone -q 'ssh:\/\/****@github.com\/********1\/*****-*****Library' \/tmp\/pip-req-build-ysacff_l Check the logs for full command output.\n<\/code><\/pre>\n<p>looking into it, <a href=\"https:\/\/stackoverflow.com\/questions\/58424348\/pip-cannot-clone-from-https-anymore-error-128\">this seems like an issue with the cloning protocol<\/a>, but I couldn't find anything pertinent to ssh.<\/p>\n<hr \/>\n<h4>P.s.<\/h4>\n<ul>\n<li>running the same few lines in the terminal works<\/li>\n<li>I sanity checked the url to the repo, went right to it, so I don't think its a problem with anything after the <code>...<\/code><\/li>\n<\/ul>\n<hr \/>\n<h4>Updates:<\/h4>\n<ul>\n<li>tried updating git with <code>yum install git<\/code>. Apparently my version is up to date, so doing this resulted in the same error.<\/li>\n<li>I commented out the pip install so that the EC2 Instance would start up successfully, then ran <code>curl http:\/\/www.google.com<\/code>, which resulted in a bunch of html. So it appears, at least after the EC2 instance boots, outbound traffic is allowed.<\/li>\n<li>running <code>curl http:\/\/www.google.com<\/code> within the bash script (lifecycle configuration, with the problematic code commented out) results in the same html output, and the instance started up perfectly. this leads me to believe that there is, indeed, outbound traffic allowed on instance startup<\/li>\n<li>a lot of people have viewed this question, and no one has answered it. I'm not married to the specific way I'm trying to install the repo, so if there are any working alternatives I'll gladly take them.<\/li>\n<li>Is it possible that I'm encountering a race condition with some other system? this is happening close to when the instance starts. Are their any way to check that all dependent systems are running?<\/li>\n<li>while doing some other stuff, in console I got the same error. I reinitialized the ssh agent, added the key, and it worked. I wonder if it's a race condition between <code>eval &quot;$(ssh-agent -s)&quot;<\/code> and <code>yes | $ENVPIP install git+ssh:\/\/git@github.com\/...<\/code>?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_date":"2020-09-07 20:13:23.973 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-06-23 16:52:41.4 UTC",
        "Question_score":4,
        "Question_tags":"bash|git|github|amazon-ec2|amazon-sagemaker",
        "Question_view_count":539,
        "Owner_creation_date":"2018-12-21 02:51:36.8 UTC",
        "Owner_last_access_date":"2022-09-25 01:54:35.743 UTC",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Earth",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63783682",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70217529,
        "Question_title":"Mount NVME drive of C5d instance in Amazon Sagemaker Studio",
        "Question_body":"<p>Is it possible to mount the NVME drive of an C5d instance in an Amazon Sagemaker Studio notebook? I can see the drive under 'lsblk' but am not allowed to format or mount it. The drive is also not visible in the '\/dev' folder. I tried this with a regular C5d EC2 instance and there it works without any issue.<\/p>\n<p>Edit: I tested it with Sagemaker Notebooks, not Studio, and it also works. It seems Studio doesn't have real access to the underlying infrastructure.<\/p>\n<p>Kind regards<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-03 16:21:08.933 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-03 21:03:38.823 UTC",
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_date":"2017-05-17 21:41:06.93 UTC",
        "Owner_last_access_date":"2022-06-06 11:58:49.713 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70217529",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69738478,
        "Question_title":"No filesystem for scheme s3 when using PySpark in AWS SageMaker",
        "Question_body":"<p>I am trying to read a parquet file from an S3 bucket on an AWS SageMaker notebook, like this:<\/p>\n<pre><code>spark = SparkSession.builder.master(&quot;local&quot;).appName(&quot;Order model&quot;).getOrCreate()\n\npartitionstring = f'year={today.year}\/month={today.month:02}\/day={today.day:02}\/hour={today.hour:02}'\n\ndf = spark.read.parquet(f'{path}{partitionstring}\/')\n\n<\/code><\/pre>\n<p>but this keeps returning the error:<\/p>\n<pre><code>\nPy4JJavaError: An error occurred while calling o1061.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n    at scala.collection.immutable.List.flatMap(List.scala:355)\n    at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n    at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:643)\n    at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>\n<p>It is possible to read the file using awswrangler, but I need to use PySpark for other functionalities.<\/p>\n<p>Any help is appreciated. Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-27 12:12:52.943 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|amazon-sagemaker",
        "Question_view_count":112,
        "Owner_creation_date":"2017-12-05 17:56:23.51 UTC",
        "Owner_last_access_date":"2022-09-15 15:36:53.283 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Tilburg",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69738478",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69775255,
        "Question_title":"Recover deleted AWS Sagemaker Jupyter Notebook",
        "Question_body":"<p>I think I may have accidentally deleted a notebook in my Sagemaker instance, is there a way to recover it? Any help is greatly appreciated, thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-29 22:08:21.633 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":238,
        "Owner_creation_date":"2021-09-13 21:20:26.793 UTC",
        "Owner_last_access_date":"2022-05-29 18:34:09.77 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69775255",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54465049,
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 16:27:28.437 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-02-07 04:20:17.233 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_date":"2013-02-25 19:50:19.38 UTC",
        "Owner_last_access_date":"2022-09-23 22:29:29.277 UTC",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-31 18:44:45.577 UTC",
        "Answer_score":2.0,
        "Owner_location":"Puebla",
        "Answer_last_edit_date":"2019-01-31 19:17:47.733 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58410362,
        "Question_title":"Sagemaker ML - Load Tensorflow model endpoint directly inside a lambda function",
        "Question_body":"<p>I've already managed to create a lambda function that loads a model.pb from S3 and apply object detection to an input image (installed tensorflow 1.12)<\/p>\n\n<p>Is it possible to load a Sagemaker model\/endpoint-configuration inside a lambda function ? I mean install all packages needed inside the lambda, without deploying an endpoint\/ec2-like instance.<\/p>\n\n<p>I guess inference performance would drop, but the solution seems to be more cost effective and scalable ready.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-16 09:44:14.073 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":117,
        "Owner_creation_date":"2016-12-30 18:38:46.907 UTC",
        "Owner_last_access_date":"2022-08-22 16:37:54.897 UTC",
        "Owner_reputation":497,
        "Owner_up_votes":45,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58410362",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72559724,
        "Question_title":"I wonder if model.tar.gz from Sagemaker can be used for inference in another notebook?",
        "Question_body":"<p>Little background: I successfully ran a regression experiment on AWS and saved the best model from that experiment. I have downloaded my best model as <code>model.tar.gz.<\/code> to use it for inference on my dataset elsewhere. I extracted it and uploaded the 'xgboost-model' file into my Jupyter Lab workspace, where my dataset is.<\/p>\n<p><code>regression_model = 'xgboost-model'<\/code><\/p>\n<p><code>predictions = regression_model.predict(X_test)<\/code><\/p>\n<p>The error I'm getting is:<\/p>\n<pre><code>----&gt; 1 predictions = regression_model.predict(X_test)\n\nAttributeError: 'str' object has no attribute 'predict'\n<\/code><\/pre>\n<p>I know that <code>XGBRegressor<\/code> has <code>predict<\/code> attribute, but my model doesn't seem to have it though it's exported as an <code>xgboost<\/code> model. Any suggesstions on what I'm supposed to be doing instead?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-09 11:47:46.397 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|jupyter-notebook|xgboost|jupyter-lab|amazon-sagemaker",
        "Question_view_count":139,
        "Owner_creation_date":"2022-05-31 18:49:04.143 UTC",
        "Owner_last_access_date":"2022-06-14 10:54:10.363 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72559724",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66389708,
        "Question_title":"AWS SageMaker Notebook Extension Installation",
        "Question_body":"<p>I am Using SageMaker Notebook for some deep learning tasks. However, raw SageMaker does not provide much of nbextension options such as auto complete for Hinter, not even provide a <code>configurator<\/code> as traditional Notebook. So, I installed <code>jupyter_contrib_nbextensions<\/code> and try to enable <code>Hinter<\/code> by <code>jupyter nbextension enable Hinter\/main<\/code>.\nHowever, the functionality still does not show up.\nWhen I run <code>Jupyter nbextension list<\/code>, it shows like:\n<a href=\"https:\/\/i.stack.imgur.com\/WzwSA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WzwSA.png\" alt=\"enter image description here\" \/><\/a>\nCan someone tell me how to successfully add and use nbextension on SageMaker Notebook?\nThanks in advance<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-02-26 16:45:14.783 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-01 16:56:17.117 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":471,
        "Owner_creation_date":"2020-01-23 20:42:38.29 UTC",
        "Owner_last_access_date":"2022-08-30 20:57:44.27 UTC",
        "Owner_reputation":25,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66389708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59861165,
        "Question_title":"installing python package in sagemaker sparkmagic pyspark notebook",
        "Question_body":"<p>I want to install new libraries in a running kernel (not bootstrapping). I'm able to create a sagemaker notebook, which is connected to a EMR cluster, but installing package is a headache.\nUnable to install packages on notebook. I've tried several methods like installing packages via terminal in jupyterLab. <\/p>\n\n<pre><code>$ conda install numba\n<\/code><\/pre>\n\n<p>The installation seems to be working fine on conda_pytorch_p36 notebook, but the packages are not installed on SparkMagic (pyspark) notebook.<\/p>\n\n<p>Error code:<\/p>\n\n<pre><code>An error was encountered:\nNo module named numba\nTraceback (most recent call last):\nImportError: No module named numba\n<\/code><\/pre>\n\n<p>The jupyter magic command also doesn't work only in pyspark notebook<\/p>\n\n<pre><code>!pip install keras\n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>An error was encountered:\ninvalid syntax (&lt;stdin&gt;, line 1)\n  File \"&lt;stdin&gt;\", line 1\n    !pip install keras\n    ^\nSyntaxError: invalid syntax\n<\/code><\/pre>\n\n<p>Based on answer in a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/52\" rel=\"nofollow noreferrer\">github<\/a> post, neither did this work<\/p>\n\n<pre><code>from subprocess import call\ncall(\"pip install dm-sonnet\".split(\" \"))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-01-22 13:46:44.397 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"pyspark|amazon-emr|amazon-sagemaker",
        "Question_view_count":3390,
        "Owner_creation_date":"2012-11-17 11:31:07.41 UTC",
        "Owner_last_access_date":"2022-09-22 08:40:30.397 UTC",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59861165",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59289258,
        "Question_title":"AWS SageMaker Notebook instance cannot connect to internet",
        "Question_body":"<p>Trying to understand why my SageMaker notebook instance cannot connect to the internet.<\/p>\n\n<pre><code># jupyter notebook running conda_python3 kernel\n\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\nprint(role)\n\n'Could not connect to the endpoint URL: \"https:\/\/api.sagemaker.us-east-1.amazonaws.com\/\"'\n<\/code><\/pre>\n\n<p><br><\/p>\n\n<pre><code># terminal\n\nwget tools.geekflare.com\n\n'unable to resolve host address \"tools.geekflare.com\"'\n<\/code><\/pre>\n\n<p>Any tips on how to debug the issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-11 15:38:27.953 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2613,
        "Owner_creation_date":"2015-03-13 19:05:58.767 UTC",
        "Owner_last_access_date":"2022-09-23 15:20:14.637 UTC",
        "Owner_reputation":82,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59289258",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-26 10:59:50.023 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-26 11:36:18.47 UTC",
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_date":"2015-01-03 12:04:10.807 UTC",
        "Owner_last_access_date":"2022-09-22 10:10:59.487 UTC",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2022-06-28 09:54:48.38 UTC",
        "Answer_score":2.0,
        "Owner_location":"Trondheim, Norway",
        "Answer_last_edit_date":"2022-07-07 08:05:21.617 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72096297,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-05-03 07:52:01.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|cross-validation|amazon-sagemaker|hyperparameters",
        "Question_view_count":70,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-04 02:05:55.79 UTC",
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72093316,
        "Question_title":"How to use packaged model tar.gz inside SageMaker Processing Job?",
        "Question_body":"<p>I am working on deploying a full ML pipeline for SageMaker and Airflow. I would like to separate training and processing part of the pipeline.<\/p>\n<p>I have a question concerning the <code>SageMakerProcessingOperator<\/code>(<a href=\"https:\/\/airflow.apache.org\/docs\/apache-airflow-providers-amazon\/1.1.0\/_modules\/airflow\/providers\/amazon\/aws\/operators\/sagemaker_processing.html\" rel=\"nofollow noreferrer\">source_code<\/a>). This operator relies on <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_processing_job\" rel=\"nofollow noreferrer\">create_processing_job()<\/a> function. When using this operator, I would like to extend the base docker image used for processing in order to use an home-made script. Currently, the processing works fine when I push my container to aws ECR. However, I would prefer to use a part of the script stored inside my packaged model (with tar.gz format).<\/p>\n<p>For training and registering the model, we can specify the image used to extend with <code>sagemaker_submit_directory<\/code> and <code>SAGEMAKER_PROGRAM<\/code> env variable (cf <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">aws_doc<\/a>). However it looks like it is not possible using the SageMakerProcessingOperator.\nBelow is a extract of the config used in the operator, with no success yet.<\/p>\n<pre><code>&quot;Environment&quot;: {\n    &quot;sagemaker_enable_cloudwatch_metrics&quot;: &quot;false&quot;,\n    &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;,\n    &quot;SAGEMAKER_REGION&quot;: f&quot;{self.region_name}&quot;,\n    &quot;SAGEMAKER_SUBMIT_DIRECTORY&quot;: f&quot;{self.train_code_path}&quot;,\n    &quot;SAGEMAKER_PROGRAM&quot;: f&quot;{self.processing_entry_point}&quot;,\n    &quot;sagemaker_job_name&quot;: f&quot;{self.process_job_name}&quot;,\n},\n<\/code><\/pre>\n<p>Did anyone manage to use these parameters for Sagemaker create_processing_job() ? Or is it only limited to AWS ECR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-02 23:18:16.5 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|tensorflow|airflow|amazon-sagemaker|mlops",
        "Question_view_count":211,
        "Owner_creation_date":"2020-06-10 07:58:06.757 UTC",
        "Owner_last_access_date":"2022-06-21 09:13:33.713 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Paris, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72093316",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59429202,
        "Question_title":"Client Error: ValidationException error when trying to call the CreateModel Operation - Build my own container",
        "Question_body":"<p>I am trying to build my container and tag it to the docker Image created in the ECR Repository.\nI am receiving a Validation Exception error and it is not able to access the Model output placed the S3 bucket. I have checked the IAM policy associated with it and it had the sts: AssumeRole and s3:getObject permissions with the associated role. Can anyone please help in identifying the cause of the error.<\/p>\n\n<p>The container looks something like below:\n_container = {\n    'Image':        MULTI_MODEL_SKLEARN_IMAGE, # Docker Image inside the ECR Repository\n    'ModelDataUrl': _model_url, # Model URL Location \n    'Mode':         'MultiModel'\n    }<\/p>\n\n<p>and the error image is <a href=\"https:\/\/i.stack.imgur.com\/nrjaF.png\" rel=\"nofollow noreferrer\">Error Message being generated <\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-20 17:26:16.523 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|containers|roles|amazon-sagemaker",
        "Question_view_count":209,
        "Owner_creation_date":"2019-11-28 15:14:17.27 UTC",
        "Owner_last_access_date":"2022-02-21 15:48:49.123 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Carlow, Ireland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59429202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69119171,
        "Question_title":"estimator.fit hangs on sagemaker on local mode",
        "Question_body":"<p>I am trying to train a <code>pytorch<\/code> model using <code>Sagemaker<\/code> on local mode, but whenever I call <code>estimator.fit<\/code> the code hangs indefinitely and I have to interrupt the notebook kernel. This happens both in my local machine and in <code>Sagemaker Studio<\/code>. But when I use EC2, the training runs normally.<\/p>\n<p>Here the call to the estimator, and the stack trace once I interrupt the kernel:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nbucket = &quot;bucket-name&quot;\nrole = sagemaker.get_execution_role()\ntraining_input_path = f&quot;s3:\/\/{bucket}\/dataset\/path&quot;\n\nsagemaker_session = sagemaker.LocalSession()\nsagemaker_session.config = {&quot;local&quot;: {&quot;local_code&quot;: True}}\n\noutput_path = &quot;file:\/\/.&quot;\n\nestimator = PyTorch(\n    entry_point=&quot;train.py&quot;,\n    source_dir=&quot;src&quot;,\n    hyperparameters={&quot;max-epochs&quot;: 1},\n    framework_version=&quot;1.8&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,\n    instance_type=&quot;local&quot;,\n    role=role,\n    output_path=output_path,\n    sagemaker_session=sagemaker_session,\n)\n\n\nestimator.fit({&quot;training&quot;: training_input_path})\n<\/code><\/pre>\n<p>Stack trace:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-9-35cdd6021288&gt; in &lt;module&gt;\n----&gt; 1 estimator.fit({&quot;training&quot;: training_input_path})\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    678         self._prepare_for_training(job_name=job_name)\n    679 \n--&gt; 680         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)\n    681         self.jobs.append(self.latest_training_job)\n    682         if wait:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in start_new(cls, estimator, inputs, experiment_config)\n   1450         &quot;&quot;&quot;\n   1451         train_args = cls._get_train_args(estimator, inputs, experiment_config)\n-&gt; 1452         estimator.sagemaker_session.train(**train_args)\n   1453 \n   1454         return cls(estimator.sagemaker_session, estimator._current_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in train(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\n    572         LOGGER.info(&quot;Creating training-job with name: %s&quot;, job_name)\n    573         LOGGER.debug(&quot;train request: %s&quot;, json.dumps(train_request, indent=4))\n--&gt; 574         self.sagemaker_client.create_training_job(**train_request)\n    575 \n    576     def _get_train_request(  # noqa: C901\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/local_session.py in create_training_job(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\n    184         hyperparameters = kwargs[&quot;HyperParameters&quot;] if &quot;HyperParameters&quot; in kwargs else {}\n    185         logger.info(&quot;Starting training job&quot;)\n--&gt; 186         training_job.start(InputDataConfig, OutputDataConfig, hyperparameters, TrainingJobName)\n    187 \n    188         LocalSagemakerClient._training_jobs[TrainingJobName] = training_job\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/entities.py in start(self, input_data_config, output_data_config, hyperparameters, job_name)\n    219 \n    220         self.model_artifacts = self.container.train(\n--&gt; 221             input_data_config, output_data_config, hyperparameters, job_name\n    222         )\n    223         self.end_time = datetime.datetime.now()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/image.py in train(self, input_data_config, output_data_config, hyperparameters, job_name)\n    200         data_dir = self._create_tmp_folder()\n    201         volumes = self._prepare_training_volumes(\n--&gt; 202             data_dir, input_data_config, output_data_config, hyperparameters\n    203         )\n    204         # If local, source directory needs to be updated to mounted \/opt\/ml\/code path\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/image.py in _prepare_training_volumes(self, data_dir, input_data_config, output_data_config, hyperparameters)\n    487             os.mkdir(channel_dir)\n    488 \n--&gt; 489             data_source = sagemaker.local.data.get_data_source_instance(uri, self.sagemaker_session)\n    490             volumes.append(_Volume(data_source.get_root_dir(), channel=channel_name))\n    491 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/data.py in get_data_source_instance(data_source, sagemaker_session)\n     52         return LocalFileDataSource(parsed_uri.netloc + parsed_uri.path)\n     53     if parsed_uri.scheme == &quot;s3&quot;:\n---&gt; 54         return S3DataSource(parsed_uri.netloc, parsed_uri.path, sagemaker_session)\n     55     raise ValueError(\n     56         &quot;data_source must be either file or s3. parsed_uri.scheme: {}&quot;.format(parsed_uri.scheme)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/local\/data.py in __init__(self, bucket, prefix, sagemaker_session)\n    183             working_dir = &quot;\/private{}&quot;.format(working_dir)\n    184 \n--&gt; 185         sagemaker.utils.download_folder(bucket, prefix, working_dir, sagemaker_session)\n    186         self.files = LocalFileDataSource(working_dir)\n    187 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/utils.py in download_folder(bucket_name, prefix, target, sagemaker_session)\n    286                 raise\n    287 \n--&gt; 288     _download_files_under_prefix(bucket_name, prefix, target, s3)\n    289 \n    290 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/utils.py in _download_files_under_prefix(bucket_name, prefix, target, s3)\n    314             if exc.errno != errno.EEXIST:\n    315                 raise\n--&gt; 316         obj.download_file(file_path)\n    317 \n    318 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/s3\/inject.py in object_download_file(self, Filename, ExtraArgs, Callback, Config)\n    313     return self.meta.client.download_file(\n    314         Bucket=self.bucket_name, Key=self.key, Filename=Filename,\n--&gt; 315         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n    316 \n    317 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/s3\/inject.py in download_file(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\n    171         return transfer.download_file(\n    172             bucket=Bucket, key=Key, filename=Filename,\n--&gt; 173             extra_args=ExtraArgs, callback=Callback)\n    174 \n    175 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/s3\/transfer.py in download_file(self, bucket, key, filename, extra_args, callback)\n    305             bucket, key, filename, extra_args, subscribers)\n    306         try:\n--&gt; 307             future.result()\n    308         # This is for backwards compatibility where when retries are\n    309         # exceeded we need to throw the same error from boto3 instead of\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/s3transfer\/futures.py in result(self)\n    107         except KeyboardInterrupt as e:\n    108             self.cancel()\n--&gt; 109             raise e\n    110 \n    111     def cancel(self):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/s3transfer\/futures.py in result(self)\n    104             # however if a KeyboardInterrupt is raised we want want to exit\n    105             # out of this and propogate the exception.\n--&gt; 106             return self._coordinator.result()\n    107         except KeyboardInterrupt as e:\n    108             self.cancel()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/s3transfer\/futures.py in result(self)\n    258         # possible value integer value, which is on the scale of billions of\n    259         # years...\n--&gt; 260         self._done_event.wait(MAXINT)\n    261 \n    262         # Once done waiting, raise an exception if present or return the\n\n\/opt\/conda\/lib\/python3.7\/threading.py in wait(self, timeout)\n    550             signaled = self._flag\n    551             if not signaled:\n--&gt; 552                 signaled = self._cond.wait(timeout)\n    553             return signaled\n    554 \n\n\/opt\/conda\/lib\/python3.7\/threading.py in wait(self, timeout)\n    294         try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    295             if timeout is None:\n--&gt; 296                 waiter.acquire()\n    297                 gotit = True\n    298             else:\n\nKeyboardInterrupt: \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-09-09 13:33:36.817 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":401,
        "Owner_creation_date":"2015-10-08 18:35:03.803 UTC",
        "Owner_last_access_date":"2022-04-22 19:22:28.98 UTC",
        "Owner_reputation":648,
        "Owner_up_votes":3,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Salvador - State of Bahia, Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69119171",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60086938,
        "Question_title":"How to configure dynamically a AWS Sage Maker task with AWS Step Function",
        "Question_body":"<p>I'm trying to build an ML pipeline using AWS Step Function.<\/p>\n\n<p>I would like to configure the 'CreateHyperParameterTuningJob' dynamically depending on the input of the task.\nHere is a screenshot of the State Machine that I'm trying to build:\n<a href=\"https:\/\/i.stack.imgur.com\/06Ctf.png\" rel=\"nofollow noreferrer\">ML State Machine <\/a><\/p>\n\n<p>When I try to create this State Machine, I got the following error: <\/p>\n\n<ul>\n<li><strong>The value for the field 'MaxParallelTrainingJobs' must be an INTEGER<\/strong><\/li>\n<\/ul>\n\n<p>I'm struggling to figure out what is the issue here. \n<strong>Do you have any suggestion to make the SM configuration dynamic with Step Function?<\/strong> Is it even possible?<\/p>\n\n<p>Here is the input data passed to the <strong>'Run training job'<\/strong> task:<\/p>\n\n<pre><code>{\n  \"client_id\": \"test\",\n  \"training_job_definition\": {\n    \"AlgorithmSpecification\": {\n      \"TrainingImage\": \"433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest\",\n      \"TrainingInputMode\": \"File\"\n    },\n    \"ResourceConfig\": {\n      \"InstanceCount\": 1,\n      \"InstanceType\": \"ml.m5.large\",\n      \"VolumeSizeInGB\": 5\n    },\n    \"StaticHyperParameters\": {\n      \"num_round\": 750\n    },\n    \"StoppingCondition\": {\n      \"MaxRuntimeInSeconds\": 900\n    },\n    \"InputDataConfig\": [\n      {\n        \"ChannelName\": \"train\",\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"csv\",\n        \"DataSource\": {\n          \"S3DataSource\": {\n            \"S3DataDistributionType\": \"FullyReplicated\",\n            \"S3DataType\": \"S3Prefix\",\n            \"S3Uri\": \"...\"\n          }\n        }\n      },\n      {\n        \"ChannelName\": \"validation\",\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"csv\",\n        \"DataSource\": {\n          \"S3DataSource\": {\n            \"S3DataDistributionType\": \"FullyReplicated\",\n            \"S3DataType\": \"S3Prefix\",\n            \"S3Uri\": \"...\"\n          }\n        }\n      }\n    ],\n    \"OutputDataConfig\": {\n      \"S3OutputPath\": \"...\"\n    },\n    \"RoleArn\": \"arn:aws:iam::679298748479:role\/landingzone_sagemaker_role\"\n  },\n  \"hyper_parameter_tuning_job_config\": {\n    \"HyperParameterTuningJobObjective\": {\n      \"MetricName\": \"validation:rmse\",\n      \"Type\": \"Minimize\"\n    },\n    \"Strategy\": \"Bayesian\",\n    \"ResourceLimits\": {\n      \"MaxParallelTrainingJobs\": 2,\n      \"MaxNumberOfTrainingJobs\": 10\n    },\n    \"ParameterRanges\": {\n      \"ContinuousParameterRanges\": [\n        {\n          \"Name\": \"eta\",\n          \"MinValue\": 0.01,\n          \"MaxValue\": 0.04\n        },\n        {\n          \"Name\": \"gamma\",\n          \"MinValue\": 0,\n          \"MaxValue\": 100\n        },\n        {\n          \"Name\": \"subsample\",\n          \"MinValue\": 0.6,\n          \"MaxValue\": 1\n        },\n        {\n          \"Name\": \"lambda\",\n          \"MinValue\": 0,\n          \"MaxValue\": 5\n        },\n        {\n          \"Name\": \"alpha\",\n          \"MinValue\": 0,\n          \"MaxValue\": 2\n        }\n      ],\n      \"IntegerParameterRanges\": [\n        {\n          \"Name\": \"max_depth\",\n          \"MinValue\": 5,\n          \"MaxValue\": 10\n        }\n      ]\n}\n<\/code><\/pre>\n\n<p>}\n}<\/p>\n\n<p>Here is JSON file that describes the State Machine: <\/p>\n\n<pre><code>{\n  \"StartAt\": \"Generate Training Dataset\",\n  \"States\": {\n    \"Generate Training Dataset\": {\n      \"Resource\": \"arn:aws:lambda:uswest-2:012345678912:function:StepFunctionsSample-SageMaLambdaForDataGeneration-1TF67BUE5A12U\",\n      \"Type\": \"Task\",\n      \"Next\": \"Run training job\"\n    },\n    \"Run training job\": {\n      \"Resource\": \"arn:aws:states:::sagemaker:createHyperParameterTuningJob.sync\",\n      \"Parameters\": {\n        \"HyperParameterTuningJobName.$\": \"$.execution_date\",\n        \"HyperParameterTuningJobConfig\": {\n          \"HyperParameterTuningJobObjective\": {\n            \"MetricName\": \"$.hyper_parameter_tuning_job_config.HyperParameterTuningJobObjective.MetricName\",\n            \"Type\": \"Minimize\"\n          },\n          \"Strategy\": \"$.hyper_parameter_tuning_job_config.Strategy\",\n          \"ResourceLimits\": {\n            \"MaxParallelTrainingJobs\": \"$.hyper_parameter_tuning_job_config.ResourceLimits.MaxParallelTrainingJobs\",\n            \"MaxNumberOfTrainingJobs\": \"$.hyper_parameter_tuning_job_config.ResourceLimits.MaxNumberOfTrainingJobs\"\n          },\n          \"ParameterRanges\": \"$.hyper_parameter_tuning_job_config.ParameterRanges\"\n        },\n        \"TrainingJobDefinition\": {\n          \"AlgorithmSpecification\": \"$.training_job_definition.AlgorithmSpecification\",\n          \"StoppingCondition\": \"$.training_job_definition.StoppingCondition\",\n          \"ResourceConfig\": \"$.training_job_definition.ResourceConfig\",\n          \"RoleArn\": \"$.training_job_definition.RoleArn\",\n          \"InputDataConfig\": \"$.training_job_definition.InputDataConfig\",\n          \"OutputDataConfig\": \"$.training_job_definition.OutputDataConfig\",\n          \"StaticHyperParameters\": \"$.training_job_definition.StaticHyperParameters\"\n        },\n        \"HyperParameterTuningJobConfig.ResourceLimits\": \"\"\n      },\n      \"Type\": \"Task\",\n      \"End\": true\n    }\n  }\n}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-06 02:34:24.05 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-02-06 02:43:18.1 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|state-machine|amazon-sagemaker|aws-step-functions",
        "Question_view_count":185,
        "Owner_creation_date":"2020-02-06 02:10:38.237 UTC",
        "Owner_last_access_date":"2021-11-11 07:07:20.72 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60086938",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70503281,
        "Question_title":"Model Management in SageMaker : Use of Model Package Group",
        "Question_body":"<p>We have a huge number of models which we plan to train\/test\/deploy\/use. We have several levels of products - starting from Country -&gt; Region -&gt; Warehouse -&gt; Department -&gt; Class A -&gt; Class B -&gt; Product ID etc., Some of the models will be for overall country level, some Country+Region level, and of course, similarly it will go down to the Class B and Product level. Which means if there are total 10,000 products - we will have 10,000 models in the product level.\nIn this kind of scenario, where we can have 10s of thousands of models, we were wondering how to manage these models. Naming convention as well as grouping them together for easy look up etc. are part of the model management also.\nOur initial thought was to use Sagemaker Model Package Group to group these models - so we can easily organize and find them as needed. However, I just learned that one SageMaker Model Package Group is a flat structure - which means one Model Package Group cannot contain another Model Package Group - so this hierarchical organization of models could not be replicated using Model Package Group.<\/p>\n<p>Question is: How do we achieve this kind model organization in production? This is not an unusual situation or anything, I believe there are many retailers who are dealing with model management scenarios like this. How do they do it? If model package is not used, what else can be used to organize them and how?<\/p>\n<p>Any suggestions\/idea will be highly appreciated.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-28 05:58:08.727 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-28 23:04:21.227 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":63,
        "Owner_creation_date":"2019-05-08 16:39:57.23 UTC",
        "Owner_last_access_date":"2022-09-23 17:21:26.617 UTC",
        "Owner_reputation":99,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70503281",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65110736,
        "Question_title":"SageMaker Estimator fit job never ends",
        "Question_body":"<p>I have the following code<\/p>\n<pre><code>estimator = Estimator(                                                     \n    image_uri=ecr_image,                                                   \n    role=role,                                                             \n    instance_count=1,                                                      \n    instance_type=instance_type,                                           \n    hyperparameters=hyperparameters                                        \n)                                                                          \n\nestimator.fit({&quot;training&quot;: &quot;s3:\/\/&quot; + sess.default_bucket() + &quot;\/&quot; + prefix})\n<\/code><\/pre>\n<p>which seems to run smoothly until it is stuck at:<\/p>\n<pre><code>Finished Training\n2020-12-02 15:00:45,352 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n<\/code><\/pre>\n<p>and I see InProgress job in AWS SageMaker console. How can I fix this?<\/p>\n<p>I use <code>763104351884.dkr.ecr.us-west-2.amazonaws.com\/pytorch-inference-eia:1.3.1-cpu-py36-ubuntu16.04<\/code> Docker image with <code>pip install sagemaker-training<\/code> added.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2020-12-02 15:08:44.283 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-02 15:18:43.817 UTC",
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":252,
        "Owner_creation_date":"2013-10-28 16:49:44.19 UTC",
        "Owner_last_access_date":"2022-09-24 09:44:05.48 UTC",
        "Owner_reputation":1311,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65110736",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54134805,
        "Question_title":"VIM (or other Plugin) installation in SageMaker Jupyter(Lab)",
        "Question_body":"<p>Can one install Jupyter\/JupyterLab plugins on SageMaker? I don't see any options to add plugins either in JupyterLab or the SageMaker interface. Would love to have at least the VIM plugin installed.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-10 18:24:17.967 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2019-01-11 09:14:19.713 UTC",
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1635,
        "Owner_creation_date":"2014-07-14 03:33:39.73 UTC",
        "Owner_last_access_date":"2022-09-23 14:53:07.687 UTC",
        "Owner_reputation":724,
        "Owner_up_votes":362,
        "Owner_down_votes":1,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Modena, Province of Modena, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54134805",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70137011,
        "Question_title":"How to create debug log in Amazon S3 bucket while running a code in Amazon Sagemaker",
        "Question_body":"<p>I am having a very big loop. I wanted to run it over the Amazon Sage maker. For this I need to create debug log in Amazon S3 bucket. How to do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-27 17:09:59.45 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"debugging|amazon-s3|amazon-sagemaker",
        "Question_view_count":144,
        "Owner_creation_date":"2020-04-24 10:34:21.987 UTC",
        "Owner_last_access_date":"2022-08-13 06:30:57.293 UTC",
        "Owner_reputation":95,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70137011",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58918995,
        "Question_title":"Invoking sagemaker endpoint using AWS-Lamda throwing parameter validation error",
        "Question_body":"<p>I trained an ML model in AWS Sagemaker and created an endpoint. I want to invoke it using AWS-Lambda. My model has 30 predictor variables. So I passed them into test event of Lambda as dict type as mentioned below<\/p>\n\n<pre><code>{\n  \"Time\": \"10 \",\n  \"V1\": \"1.449043781 \",\n  \"V2\": \"-1.176338825 \",\n  \"V3\": \"0.913859833 \",\n  \"V4\": \"-1.375666655 \",\n  \"V5\": \"-1.971383165 \",\n  \"V6\": \"-0.629152139 \",\n  \"V7\": \"-1.423235601 \",\n  \"V8\": \"0.048455888 \",\n  \"V9\": \"-1.720408393 \",\n  \"V10\": \"1.626659058 \",\n  \"V11\": \"1.19964395 \",\n  \"V12\": \"-0.671439778 \",\n  \"V13\": \"-0.513947153 \",\n  \"V14\": \"-0.095045045 \",\n  \"V15\": \"0.230930409 \",\n  \"V16\": \"0.031967467 \",\n  \"V17\": \"0.253414716 \",\n  \"V18\": \"0.854343814 \",\n  \"V19\": \"-0.221365414 \",\n  \"V20\": \"-0.387226474 \",\n  \"V21\": \"-0.009301897 \",\n  \"V22\": \"0.313894411 \",\n  \"V23\": \"0.027740158 \",\n  \"V24\": \"0.500512287 \",\n  \"V25\": \"0.251367359 \",\n  \"V26\": \"-0.129477954 \",\n  \"V27\": \"0.042849871 \",\n  \"V28\": \"0.016253262 \",\n  \"Amount\": \"7.8\"\n}\n<\/code><\/pre>\n\n<p>Now I ran below mentioned code in AWS Lambda <\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport boto3\nimport io\nimport codecs\n\nendpoint_name = os.environ['ENDPOINT_NAME']\nruntime = boto3.client('runtime.sagemaker')\n\n\ndef lambda_handler(event, context):\n    print(\"received event: \"+json.dumps(event,indent=2))\n    data = json.loads(json.dumps(event))\n    payload = data[\"Time\"]+data[\"V1\"]+data[\"V2\"]+data[\"V3\"]+data[\"V4\"]+data[\"V5\"]+data[\"V6\"]+data[\"V7\"]+data[\"V8\"]+data[\"V9\"]+data[\"V10\"]+data[\"V11\"]+data[\"V12\"]+data[\"V13\"]+data[\"V14\"]+data[\"V15\"]+data[\"V16\"]+data[\"V17\"]+data[\"V18\"]+data[\"V19\"]+data[\"V20\"]+data[\"V21\"]+data[\"V22\"]+data[\"V23\"]+data[\"V24\"]+data[\"V25\"]+data[\"V26\"]+data[\"V27\"]+data[\"V28\"]+data[\"Amount\"]\n    payload = payload.split(\" \")\n    payload = [codecs.encode(i,'utf-8') for i in payload]\n    payload=[bytearray(i) for i in payload]\n    print(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='text\/csv',Body=payload)\n    print(response)\n    result=json.loads(response['Body'].decode())\n    pred = int(float(response))\n    predicted_label = 'fraud' if pred==1 else 'not fraud'\n    return predicted_label\n<\/code><\/pre>\n\n<p>This code is throwing below this error<\/p>\n\n<pre><code>[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: [bytearray(b'10'), bytearray(b'1.449043781'), bytearray(b'-1.176338825'), bytearray(b'0.913859833'), bytearray(b'-1.375666655'), bytearray(b'-1.971383165'), bytearray(b'-0.629152139'), bytearray(b'-1.423235601'), bytearray(b'0.048455888'), bytearray(b'-1.720408393'), bytearray(b'1.626659058'), bytearray(b'1.19964395'), bytearray(b'-0.671439778'), bytearray(b'-0.513947153'), bytearray(b'-0.095045045'), bytearray(b'0.230930409'), bytearray(b'0.031967467'), bytearray(b'0.253414716'), bytearray(b'0.854343814'), bytearray(b'-0.221365414'), bytearray(b'-0.387226474'), bytearray(b'-0.009301897'), bytearray(b'0.313894411'), bytearray(b'0.027740158'), bytearray(b'0.500512287'), bytearray(b'0.251367359'), bytearray(b'-0.129477954'), bytearray(b'0.042849871'), bytearray(b'0.016253262'), bytearray(b'7.8')], type: &lt;class 'list'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I understand that somehow I need to pass my 30 features into Lambda function such that data type of <code>payload<\/code> is compatible with <code>ContentType<\/code> for <code>respnse<\/code> to work. Can someone please explain how to do it? \nedit: I'm trying this problem by looking at <a href=\"http:\/\/%20https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws blog. I don't quite understand how the author of above mentioned blog did it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-18 16:28:11.987 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":895,
        "Owner_creation_date":"2019-09-12 20:07:41.627 UTC",
        "Owner_last_access_date":"2022-09-21 14:35:39.23 UTC",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Hyderabad, Telangana, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58918995",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72564149,
        "Question_title":"AWS Sagemaker Unable to Parse Augmented Manifest File",
        "Question_body":"<p>I've read all of the existing documentation on Augmented Manifest files. I see no difference from my file, but I keep experiencing this error when training:<\/p>\n<pre><code>ClientError: Data download failed:Failed to download data. Unable to parse augmented manifest, error in line: 1\n<\/code><\/pre>\n<p>My example first line:<\/p>\n<pre><code>{&quot;source-ref&quot;: &quot;s3:\/\/test-bucket\/test-data\/test\/bucket\/10done.png&quot;, &quot;video-frame-object-detection&quot;: {&quot;annotations&quot;: [{&quot;class_id&quot;: 1, &quot;top&quot;: 880, &quot;left&quot;: 43, &quot;width&quot;: 2499, &quot;height&quot;: 324}], &quot;image_size&quot;: [{&quot;width&quot;: 2543, &quot;height&quot;: 2543, &quot;depth&quot;: 3}]}, &quot;video-frame-object-detection-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;Good&quot;, &quot;1&quot;: &quot;Bad&quot;}, &quot;human-annotated&quot;: &quot;no&quot;, &quot;creation-date&quot;: &quot;2022-06-09T11:01:27.440682&quot;, &quot;type&quot;: &quot;programmatically-created-labels&quot;}}\n<\/code><\/pre>\n<p>These are how my breaks look (end of file):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PkRW3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PkRW3.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>How I manually created the manifest file:\n<a href=\"https:\/\/i.stack.imgur.com\/uAQmR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAQmR.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Sagemaker is recognizing my attributes as well.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-09 17:06:09.303 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-09 18:32:27.79 UTC",
        "Question_score":0,
        "Question_tags":"python|json|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":129,
        "Owner_creation_date":"2020-08-06 11:57:10.037 UTC",
        "Owner_last_access_date":"2022-06-27 20:22:44.427 UTC",
        "Owner_reputation":63,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72564149",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60423734,
        "Question_title":"SageMaker XGBoost hyperparameter tuning versus XGBoost python package",
        "Question_body":"<p>I am trying to do hyperparameter tuning of xgboost model. I started with AWS Sagemaker Hyperparameter Tuning, with the following parameter range:<\/p>\n\n<pre><code>xgb.set_hyperparameters(eval_metric='auc',\n                        objective='binary:logistic',\n                        early_stopping_rounds=500,\n                        rate_drop=0.1,\n                        colsample_bytree=0.8,\n                        subsample=0.75,\n                        min_child_weight=0)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.3),\n                         'lambda': ContinuousParameter(0.1, 2),\n                         'alpha': ContinuousParameter(0.5, 2),\n                         'max_depth': IntegerParameter(5, 10),\n                         'num_round': IntegerParameter(500, 2000)}\n\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=10,  \n                            max_parallel_jobs=3,\n                            tags=[{'Key': 'Application', 'Value': 'cxxx'}])\n<\/code><\/pre>\n\n<p>And get a best model with the following set of hyperparameters:<\/p>\n\n<pre><code>{\n  \"alpha\": \"1.4009334471163981\",\n  \"eta\": \"0.05726016655019904\",\n  \"lambda\": \"1.2070623852474922\",\n  \"max_depth\": \"7\",\n  \"num_round\": \"1052\"\n}\n<\/code><\/pre>\n\n<p>Out of curiosity, I hooked up these hyperparameters into xgboost python package, as such:<\/p>\n\n<pre><code>xgb_model = xgb.XGBClassifier(max_depth = 7,\n                          silent = False,\n                          random_state = 42,\n                          n_estimators = 1052,\n                          learning_rate = 0.05726016655019904,\n                          objective = 'binary:logistic',\n                          verbosity = 1,\n                          reg_alpha = 1.4009334471163981,\n                          reg_lambda = 1.2070623852474922,\n                          rate_drop=0.1,\n                          colsample_bytree=0.8,\n                          subsample=0.75,\n                          min_child_weight=0\n                        )\n\n<\/code><\/pre>\n\n<p>I retrained the model and realized the results I got from the latter is better than that from SageMaker.\nxgboost (auc of validation set): 0.766\nSageMaker best model (auc of validation set):0.751<\/p>\n\n<p>I wonder why SageMaker perform so poorly? If SageMaker usually perform worse than xgboost python package, how do people usually do xgboost hyperparameter tuning? Thanks for any hints!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-02-26 22:44:18.69 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"xgboost|amazon-sagemaker|hyperparameters",
        "Question_view_count":1300,
        "Owner_creation_date":"2014-05-25 15:30:58.507 UTC",
        "Owner_last_access_date":"2022-07-24 23:13:53.413 UTC",
        "Owner_reputation":1021,
        "Owner_up_votes":92,
        "Owner_down_votes":0,
        "Owner_views":120,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60423734",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60929678,
        "Question_title":"How to pass image to AWS SageMaker endpoint",
        "Question_body":"<p>I need to use the <a href=\"https:\/\/aws.amazon.com\/marketplace\/pp\/prodview-7y6xdiukxucr2\" rel=\"nofollow noreferrer\">WireframeToCode<\/a> model from the AWS Marketplace, I used Nodejs to read and send the file data to the model like this:<\/p>\n\n<pre><code>var sageMakerRuntime = new AWS.SageMakerRuntime();\n\nvar bitmap = fs.readFileSync(\"sample.jpeg\", \"utf8\");\nvar buffer = new Buffer.from(bitmap, \"base64\");\n\nvar params = {\n  Body: buffer.toJSON(),\n  EndpointName: \"wireframe-to-code\",\n  Accept: \"image\/jpeg\",\n  ContentType: \"application\/json\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else console.log(data);\n});\n<\/code><\/pre>\n\n<p>but i get this error:<\/p>\n\n<blockquote>\n  <p>message: 'Expected params.Body to be a string, Buffer, Stream, Blob,\n  or typed array object',   code: 'InvalidParameterType',   time:\n  2020-03-30T11:06:27.535Z<\/p>\n<\/blockquote>\n\n<p>From the documentation, the supported content type for input is  <code>image\/jpeg<\/code> output is <code>application\/json<\/code>.<\/p>\n\n<p>when I try to convert the Body to a string like this: <code>JSON.stringify(buffer.toJSON())<\/code> I get this error:<\/p>\n\n<blockquote>\n  <p>Received client error (415) from model with message \"This predictor\n  only supports JSON formatted data\"<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-30 11:35:25.227 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"node.js|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2170,
        "Owner_creation_date":"2015-06-14 15:17:07.12 UTC",
        "Owner_last_access_date":"2022-08-14 17:05:46.267 UTC",
        "Owner_reputation":951,
        "Owner_up_votes":142,
        "Owner_down_votes":3,
        "Owner_views":157,
        "Answer_body":"<p>I had to pass in bitmap and change <code>ContentType<\/code> to <code>\"image\/jpeg\"<\/code><\/p>\n\n<pre><code>const AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\n\nconst sageMakerRuntime = new AWS.SageMakerRuntime({\n  region: \"us-east-1\",\n  accessKeyId: \"XXXXXXXXXXXX\",\n  secretAccessKey: \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n});\n\nconst bitmap = fs.readFileSync(\"sample.jpeg\");\n\nvar params = {\n  Body: bitmap,\n  EndpointName: \"wireframe-to-code\",\n  ContentType: \"image\/jpeg\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    responseData = JSON.parse(Buffer.from(data.Body).toString());\n    console.log(responseData);\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-06 07:40:24.067 UTC",
        "Answer_score":3.0,
        "Owner_location":"Lusaka, Zambia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60929678",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54446584,
        "Question_title":"Object Detection - can you get mAP per class?",
        "Question_body":"<p>In the Sagemaker UI, when you run the object detection training jobs you can see the progress of mAP over time. Is there a way to segment this value so you can track it per class?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-30 17:48:33.26 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":149,
        "Owner_creation_date":"2012-09-23 23:56:52.63 UTC",
        "Owner_last_access_date":"2022-09-22 19:11:36.75 UTC",
        "Owner_reputation":346,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54446584",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73077863,
        "Question_title":"why Sagemaker Processing have several ouputs",
        "Question_body":"<p>I have been using AWS Sagemaker for years.<\/p>\n<p>I don't understand why processing jobs can have several outputs ? In what kind of scenario, can you use more than one output destination ?<\/p>\n<p>When I say several outputs I refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingOutputConfig.html\" rel=\"nofollow noreferrer\">ProcessingOutputConfig<\/a> containing an array<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-22 09:10:00.477 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":12,
        "Owner_creation_date":"2014-07-12 09:14:39.23 UTC",
        "Owner_last_access_date":"2022-09-23 13:24:43.56 UTC",
        "Owner_reputation":1075,
        "Owner_up_votes":67,
        "Owner_down_votes":7,
        "Owner_views":199,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Paris, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73077863",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62142825,
        "Question_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Question_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-02 00:08:19.55 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":215,
        "Owner_creation_date":"2014-02-17 03:18:20.777 UTC",
        "Owner_last_access_date":"2022-09-25 01:28:19.613 UTC",
        "Owner_reputation":133,
        "Owner_up_votes":304,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-04 21:33:00.71 UTC",
        "Answer_score":0.0,
        "Owner_location":"Sydney NSW, Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69648442,
        "Question_title":"Sagemaker AWSWrangler>2.3.0",
        "Question_body":"<p>As I am trying to use the function read_excel as part of AWS Wrangler, available as of version 2.3.0 in Sagemaker Jupyter Lab on Amazon Web Services, it does not install properly. The Python version of the Conda Instance is 3.6.<\/p>\n<p>When running !pip install awswrangler &gt;=2.3.0 directly in the notebook, the error provided is<\/p>\n<blockquote>\n<p>ERROR: Could not install packages due to an OSError: [Errno 2] No such\nfile or directory:\n'\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/s3transfer-0.5.0.dist-info\/METADATA'<\/p>\n<\/blockquote>\n<p>Does anyone know how to solve this issue? Thanks in advance!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PUwJc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PUwJc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-20 15:01:10.593 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-30 23:19:16.767 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":154,
        "Owner_creation_date":"2021-10-20 14:57:56.677 UTC",
        "Owner_last_access_date":"2022-06-09 11:53:15.51 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69648442",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65807148,
        "Question_title":"Download data from S3 bucket onto SageMaker inference container",
        "Question_body":"<p>I deployed a model to a SageMaker endpoint for inference. My input data is quite large and I would like to send its S3 URI to the endpoint instead, so that I can download it onto the deployed Docker container. Unfortunately, when I try using the SageMaker SDK to download the data, I get this error:<\/p>\n<pre><code>Read-only file system: '\/opt\/ml\/model\/'\n<\/code><\/pre>\n<p>I would really appreciate if someone could help me solve this issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-20 09:57:20.387 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-20 10:04:46.597 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|docker|amazon-s3|amazon-sagemaker",
        "Question_view_count":962,
        "Owner_creation_date":"2016-11-02 11:35:55.173 UTC",
        "Owner_last_access_date":"2021-02-01 04:41:36.563 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>The SageMaker Inference instance has a directory called <code>\/temp\/<\/code> which is writeable, and can be used for non-persistent storage. I used the S3Downloader utility in the SageMaker SDK to download data to this directory. For instance:<\/p>\n<pre><code>from sagemaker.s3 import S3Downloader\nfrom sagemaker.session import Session\n\nsagemaker_session = Session()\n\ndef download_data_from_s3(s3_uri):\n    S3Downloader.download(s3_uri=s3_uri,\n                          local_path='\/tmp\/',\n                          sagemaker_session=sagemaker_session)\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-23 12:06:26.437 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65807148",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64449982,
        "Question_title":"Use SageMaker Lifecycle configuration to execute a jupyter notebook on start",
        "Question_body":"<p>I want to set up some automatic schedule for running my SageMaker Notebook.<br \/>\nCurrently I found link like this:<br \/>\n<a href=\"https:\/\/towardsdatascience.com\/automating-aws-sagemaker-notebooks-2dec62bc2c84\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/automating-aws-sagemaker-notebooks-2dec62bc2c84<\/a><\/p>\n<p>I followed the steps to set up the lamda, cloudwatch, and the Lifecycle configuration.<br \/>\nDuring different experiment, some times the on_start lifecycle configuration can execute the jupyter notebook (In the notebook i just install some package and load the package and save the loading status to S3 bucket). However, it failed due to it can't stop the notebook.<\/p>\n<p>Then I added permission to my IAM role for SageMaker autostop. Now the notebook instance can be turn on and turn off. But I don't see anything uploaded to my S3 any more. I am wondering if the on_start started the auto-stop too early before it finish the steps?<\/p>\n<p>Below is my script for the current lifecycle configuration<\/p>\n<pre><code>set -e\n\nENVIRONMENT=python3\nNOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/Test Notebook.ipynb&quot;\nAUTO_STOP_FILE=&quot;\/home\/ec2-user\/SageMaker\/auto-stop.py&quot;\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\n\nnohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &amp;\n\necho &quot;Finishing running the jupyter notebook&quot;\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n# PARAMETERS\nIDLE_TIME=60  # 1 minute\n\necho &quot;Fetching the autostop script&quot;\nwget -O autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/auto-stop-idle\/autostop.py\n\necho &quot;Starting the SageMaker autostop script in cron&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/1 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $DIR\/autostop.py --time ${IDLE_TIME} | tee -a \/home\/ec2-user\/SageMaker\/auto-stop-idle.log'&quot;) | crontab -\n<\/code><\/pre>\n<p>Note that, I do see the echo &quot;Finishing running the jupyter notebook&quot; from the cloudwatch log. But that's usually the first thing i saw from the log and it shows up immediately - faster than I expect how long it should take.<\/p>\n<p>Also, currently the notebook is only running some fake task. The real task may take more than an hour.<\/p>\n<p>Any suggestions help! Thank you for taking the time to read my questions.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-10-20 17:02:30.937 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|lifecycle|amazon-sagemaker",
        "Question_view_count":3101,
        "Owner_creation_date":"2020-09-22 03:10:31.77 UTC",
        "Owner_last_access_date":"2021-09-28 00:21:34.097 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64449982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71385524,
        "Question_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Question_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-07 18:13:04.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-08 06:26:38.767 UTC",
        "Question_score":1,
        "Question_tags":"r|amazon-sagemaker|serverless.com",
        "Question_view_count":369,
        "Owner_creation_date":"2010-09-29 12:36:11.347 UTC",
        "Owner_last_access_date":"2022-07-20 12:26:21.99 UTC",
        "Owner_reputation":6958,
        "Owner_up_votes":123,
        "Owner_down_votes":1,
        "Owner_views":787,
        "Answer_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-07 22:06:30.177 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bangalore, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71385524",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63204995,
        "Question_title":"How to get reproducible result in Amazon SageMaker with TensorFlow Estimator?",
        "Question_body":"<p>I am currently using AWS SageMaker Python SDK to train EfficientNet model (<a href=\"https:\/\/github.com\/qubvel\/efficientnet\" rel=\"nofollow noreferrer\">https:\/\/github.com\/qubvel\/efficientnet<\/a>) to my data. Specifically, I use TensorFlow estimator as below. This code is in SageMaker notebook instance<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow.estimator import TensorFlow\n### sagemaker version = 1.50.17, python version = 3.6\n\nestimator = TensorFlow(&quot;train.py&quot;, py_version = &quot;py3&quot;, framework_version = &quot;2.1.0&quot;,\n                       role = sagemaker.get_execution_role(), \n                       train_instance_type = &quot;ml.m5.xlarge&quot;, \n                       train_instance_count = 1,\n                       image_name = 'xxx.dkr.ecr.xxx.amazonaws.com\/xxx',\n                       hyperparameters = {list of hyperparameters here: epochs, batch size},\n                       subnets = [xxx], \n                       security_group_ids = [xxx]\nestimator.fit({\n   'class_1': 's3_path_class_1',\n   'class_2': 's3_path_class_2'\n})\n<\/code><\/pre>\n<p>The code for train.py contains the usual training procedure, getting the image and labels from S3, transform them into the right array shape for EfficientNet input, and split into train, validation, and test set. In order to get reproducible result, I use the following reset_random_seeds function (<a href=\"https:\/\/stackoverflow.com\/questions\/59075244\/\">If Keras results are not reproducible, what&#39;s the best practice for comparing models and choosing hyper parameters?<\/a>) before calling EfficientNet model itself.<\/p>\n<pre><code>### code of train.py\n\nimport os\nos.environ['PYTHONHASHSEED']=str(1)\nimport numpy as np\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nimport random\n\n### tensorflow version = 2.1.0\n### tf.keras version = 2.2.4-tf\n### efficientnet version = 1.1.0\n\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(1)\n   tf.random.set_seed(1)\n   np.random.seed(1)\n   random.seed(1)\n\nif __name__ == &quot;__main__&quot;:\n\n   ### code for getting training data\n   ### ... (I have made sure that the training input is the same every time i re-run the code)\n   ### end of code\n\n   reset_random_seeds()\n   model = efn.EfficientNetB5(include_top = False, \n      weights = 'imagenet', \n      input_shape = (80, 80, 3),\n      pooling = 'avg',\n      classes = 3)\n   model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy')\n   model.fit(X_train, Y_train, batch_size = 64, epochs = 30, shuffle = True, verbose = 2)\n\n   ### Prediction section here\n<\/code><\/pre>\n<p>However, each time i run the notebook instance, i always get a different result from the previous run. When I switched train_instance_type to &quot;local&quot; i always get the same result each time i run the notebook. Therefore, is the non-reproducible result caused by the training instance type that I have chosen? this instance (ml.m5.xlarge) has 4 vCPU, 16 Mem (GiB), and no GPU. If so, how to obtain reproducible results under this training instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-01 12:04:56.037 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|keras|deep-learning|amazon-sagemaker|efficientnet",
        "Question_view_count":509,
        "Owner_creation_date":"2018-02-05 11:21:13.647 UTC",
        "Owner_last_access_date":"2021-02-05 03:32:27.627 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63204995",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63116338,
        "Question_title":"Serving models from mlflow registry to sagemaker",
        "Question_body":"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local<\/code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:<\/p>\n<pre><code>export MODEL_PATH=models:\/churn-lgb-test\/2\nexport LOCAL_PORT=8000\nmlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine\/mlflow-pyfunc:1.6.0\n<\/code><\/pre>\n<p>it starts the container and I immediately get this error:<\/p>\n<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)\n[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)\n[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)\n[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 209, in run\n    self.sleep()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 357, in sleep\n    ready = select.select([self.PIPE[0]], [], [], 1.0)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n    sys.exit(run())\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n    super().run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n    Arbiter(self).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n    self.stop()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n    time.sleep(0.1)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\ncreating and activating custom environment\nGot sigterm signal, exiting.\n[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)\n[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)\n[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-27 13:26:19.06 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":429,
        "Owner_creation_date":"2020-06-05 19:27:43.857 UTC",
        "Owner_last_access_date":"2020-11-19 23:45:52.31 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63116338",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60554471,
        "Question_title":"ML for object search",
        "Question_body":"<p>I'm trying to find a way to build ML using AWS, preferably using their services such as SageMaker and not just EC2, for object detection in images using an image as input.<\/p>\n\n<p>AWS Rekognition offers Image Comparison and Object detection APIs, but they are not exactly what I'm looking for, the comparison works only with faces and not objects and object detection is too basic.<\/p>\n\n<p>AlibabCloud has that functionality as a service (<a href=\"https:\/\/www.alibabacloud.com\/product\/imagesearch\" rel=\"nofollow noreferrer\">https:\/\/www.alibabacloud.com\/product\/imagesearch<\/a>) but I would like to use something similar on AWS, rather than Alibaba.<\/p>\n\n<p>How would I go about and build something like this?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-05 21:51:00.257 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-04-14 21:08:40.163 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|computer-vision|amazon-sagemaker",
        "Question_view_count":86,
        "Owner_creation_date":"2013-04-10 11:03:40.927 UTC",
        "Owner_last_access_date":"2021-03-22 14:28:39.59 UTC",
        "Owner_reputation":549,
        "Owner_up_votes":59,
        "Owner_down_votes":2,
        "Owner_views":89,
        "Answer_body":"<p><em>edited 03\/08\/2020 to add pointers for visual search<\/em><\/p>\n\n<p>Since you seem interested both in the tasks of <strong>object detection<\/strong> (input an image, and return bounding boxes with object classes) and <strong>visual search<\/strong> (input an image and return relevant images) let me give you pointers for both :)<\/p>\n\n<p>For <strong>object detection<\/strong> you have 3 options:<\/p>\n\n<ul>\n<li><strong>Using the managed service <a href=\"https:\/\/aws.amazon.com\/rekognition\/custom-labels-features\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Amazon Rekognition Custom Labels<\/a><\/strong>. The key benefits of this service is that (1) it doesn't require writing ML code, as the service runs autoML internally to find the best model, (2) it is very flexible in terms of interaction (SDKs, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less).<\/li>\n<li><strong>Using SageMaker Object Detection model<\/strong> (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb\" rel=\"nofollow noreferrer\">demo<\/a>). In this option, the model is also already written (SSD architecture with Resnet or VGG backbone) and you just need to choose  or tune <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-api-config.html\" rel=\"nofollow noreferrer\">hyperparameters<\/a><\/li>\n<li><strong>Using your own model on Amazon SageMaker<\/strong>. This could be your own code in docker, or code from an ML framework in a SageMaker ML Framework container. There are such containers for <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html\" rel=\"nofollow noreferrer\">Pytorch<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_mxnet.html\" rel=\"nofollow noreferrer\">MXNet<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_chainer.html\" rel=\"nofollow noreferrer\">Chainer<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">Sklearn<\/a>. In terms of model code, I recommend considering <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\"><strong><code>gluoncv<\/code><\/strong><\/a>, a compact python computer vision toolkit (based on mxnet backend) that comes with <a href=\"https:\/\/gluon-cv.mxnet.io\/model_zoo\/detection.html\" rel=\"nofollow noreferrer\">many state-of-the-art models<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/build\/examples_detection\/index.html\" rel=\"nofollow noreferrer\">tutorials<\/a> for object detection<\/li>\n<\/ul>\n\n<p>The task of <strong>visual search<\/strong> requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. If all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run kNN search between the query image and the collection of transformed images. There are 2 tutos showing how to build such systems on AWS here:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/visual-search-on-aws-part-1-engine-implementation-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Blog Post Visual Search on AWS<\/a> (MXNet resnet embeddings +\nSageMaker kNN)<\/li>\n<li><a href=\"https:\/\/thomasdelteil.github.io\/VisualSearch_MXNet\/\" rel=\"nofollow noreferrer\">Visual Search on MMS demo<\/a> (MXNet resnet\nembeddings + HNSW kNN on AWS Fargate)<\/li>\n<\/ul>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2020-03-06 13:11:44.257 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-03-08 21:32:19.203 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60554471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58469273,
        "Question_title":"Classification using H2O.ai H2O-3 Automl Algorithm on AWS SageMaker: Categorical Columns",
        "Question_body":"<p>I'm trying to train a model using H2O.ai's H2O-3 Automl Algorithm on AWS SageMaker using the console.<\/p>\n\n<p>My model's goal is to predict if an arrest will be made based upon the year, type of crime, and location.<\/p>\n\n<p>My data has 8 columns:<\/p>\n\n<ul>\n<li><code>primary_type<\/code>: enum<\/li>\n<li><code>description<\/code>: enum<\/li>\n<li><code>location_description<\/code>: enum<\/li>\n<li><code>arrest<\/code>: enum (true\/false), this is the target column<\/li>\n<li><code>domestic<\/code>: enum (true\/false)<\/li>\n<li><code>year<\/code>: number<\/li>\n<li><code>latitude<\/code>: number<\/li>\n<li><code>longitude<\/code>: number<\/li>\n<\/ul>\n\n<p>When I use the SageMaker console on AWS and create a new training job using the H2O-3 Automl Algorithm, I specify the <code>primary_type<\/code>, <code>description<\/code>, <code>location_description<\/code>, and <code>domestic<\/code> columns as categorical.<\/p>\n\n<p>However in the logs of the training job I always see the following two lines:<\/p>\n\n<pre><code>Converting specified columns to categorical values:\n[]\n<\/code><\/pre>\n\n<p>This leads me to believe the <code>categorical_columns<\/code> attribute in the <code>training<\/code> hyperparameter is not being taken into account.<\/p>\n\n<p>I have tried the following hyperparameters with the same output in the logs each time:<\/p>\n\n<pre><code>{'classification': 'true', 'categorical_columns':'primary_type,description,location_description,domestic', 'target': 'arrest'}\n<\/code><\/pre>\n\n<pre><code>{'classification': 'true', 'categorical_columns':['primary_type','description','location_description','domestic'], 'target': 'arrest'}\n<\/code><\/pre>\n\n<p>I thought the list of categorical columns was supposed to be delimited by comma, which would then be split into a list.<\/p>\n\n<p>I expected the list of categorical column names to be output in the logs instead of an empty list, like so:<\/p>\n\n<pre><code>Converting specified columns to categorical values:\n['primary_type','description','location_description','domestic']\n<\/code><\/pre>\n\n<p>Can anyone help me figure out how to get these categorical columns to apply to the training of my model?<\/p>\n\n<p>Also-\nI <em>think<\/em> this is the code that's running when I train my model but I have yet to confirm that: <a href=\"https:\/\/github.com\/h2oai\/h2o3-sagemaker\/blob\/master\/automl\/automl_scripts\/train#L93-L151\" rel=\"nofollow noreferrer\">https:\/\/github.com\/h2oai\/h2o3-sagemaker\/blob\/master\/automl\/automl_scripts\/train#L93-L151<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-20 01:08:30.71 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|h2o|amazon-sagemaker|automl",
        "Question_view_count":354,
        "Owner_creation_date":"2016-06-16 05:21:30.267 UTC",
        "Owner_last_access_date":"2022-09-25 05:33:21.687 UTC",
        "Owner_reputation":1591,
        "Owner_up_votes":80,
        "Owner_down_votes":2,
        "Owner_views":70,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58469273",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62734994,
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-04 22:17:16.277 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|conda|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_date":"2018-10-30 17:35:56.27 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883 UTC",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-10 05:34:20.763 UTC",
        "Answer_score":1.0,
        "Owner_location":"United Kingdom",
        "Answer_last_edit_date":"2020-11-16 21:55:49.573 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70486162,
        "Question_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Question_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-12-26 11:42:01.197 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python-3.x|amazon-web-services|jupyter-notebook|amazon-sagemaker|anaconda3",
        "Question_view_count":768,
        "Owner_creation_date":"2012-03-14 12:18:03.733 UTC",
        "Owner_last_access_date":"2022-09-23 12:09:26.313 UTC",
        "Owner_reputation":1060,
        "Owner_up_votes":102,
        "Owner_down_votes":0,
        "Owner_views":139,
        "Answer_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-25 13:00:46.503 UTC",
        "Answer_score":1.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70486162",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52317237,
        "Question_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Question_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2018-09-13 15:39:54.713 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-22 18:40:03.017 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|nlp|amazon-sagemaker|aws-cloud9",
        "Question_view_count":1442,
        "Owner_creation_date":"2016-09-07 15:36:35.243 UTC",
        "Owner_last_access_date":"2019-01-07 14:42:06.263 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-09-14 13:56:57.933 UTC",
        "Answer_score":2.0,
        "Owner_location":"Amsterdam, Netherlands",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69564996,
        "Question_title":"Ensure Java is installed and PATH is set for `java` in Amazon SageMaker Jupyter Notebook",
        "Question_body":"<p>I am importing the tabula library and when I call this library's methods, I receive the following error in SageMaker.<\/p>\n<pre><code>JavaNotFoundError: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n<\/code><\/pre>\n<p>How can I install Java to SageMaker?<\/p>\n<p>Below is my code:<\/p>\n<pre><code>import tabula\npdf_path = &quot;https:\/\/github.com\/chezou\/tabula-py\/raw\/master\/tests\/resources\/data.pdf&quot;\ndfs = tabula.read_pdf(pdf_path, stream=True)\n<\/code><\/pre>\n<p>Once I enter in this code, I receive the error message that SageMaker does not have Java.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-14 04:13:34.133 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"java|amazon-web-services|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_date":"2021-03-12 13:24:19.547 UTC",
        "Owner_last_access_date":"2022-05-25 02:11:47.037 UTC",
        "Owner_reputation":141,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69564996",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67310256,
        "Question_title":"List of Python library in Sagemaker SKlearnProcessor",
        "Question_body":"<p>Is there a way to list the Python library in Sagemaker SKlearnProcessor?\nCould not find the list from official doc. ,\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#data-pre-processing-and-model-evaluation-with-scikit-learn\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#data-pre-processing-and-model-evaluation-with-scikit-learn<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-29 02:30:40.23 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":533,
        "Owner_creation_date":"2015-03-19 07:58:52.583 UTC",
        "Owner_last_access_date":"2022-09-21 17:26:20.04 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangkok Thailand",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67310256",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51538071,
        "Question_title":"Create recordio file for linear regression",
        "Question_body":"<p>I'm using AWS Sagemaker to run linear regression on a CSV dataset. I have made some tests, and with my sample dataset that is 10% of the full dataset, the csv file ends up at 1.5 GB in size.<\/p>\n\n<p>Now I want to run the full dataset, but I'm facing issues with the 15 GB file. When I compress the file with Gzip, it ends up only 20 MB. However, Sagemaker only supports Gzip on \"Protobuf-Recordio\" files. I know I can make Recordio files with im2rec, but it seems to be intended for image files for image classication. I'm also not sure how to generate the protobuf file.<\/p>\n\n<p>To make things even worse(?) :) I'm generating the dataset in Node.<\/p>\n\n<p>I would be very grateful to get some pointers in the right direction how to do this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2018-07-26 11:45:26.123 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"node.js|machine-learning|amazon-sagemaker",
        "Question_view_count":1226,
        "Owner_creation_date":"2011-12-31 09:32:48.517 UTC",
        "Owner_last_access_date":"2018-08-28 16:37:34.49 UTC",
        "Owner_reputation":514,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51538071",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53609409,
        "Question_title":"Automatically \"stop\" Sagemaker notebook instance after inactivity?",
        "Question_body":"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... <\/p>\n\n<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-04 09:18:11.383 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":19,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":12683,
        "Owner_creation_date":"2011-03-10 10:26:00.99 UTC",
        "Owner_last_access_date":"2022-09-24 23:25:14.187 UTC",
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Answer_body":"<p>You can use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">Lifecycle configurations<\/a> to set up an automatic job that will stop your instance after inactivity.<\/p>\n\n<p>There's <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\" rel=\"noreferrer\">a GitHub repository<\/a> which has samples that you can use. In the repository, there's a <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"noreferrer\">auto-stop-idle<\/a> script which will shutdown your instance once it's idle for more than 1 hour.<\/p>\n\n<p>What you need to do is<\/p>\n\n<ol>\n<li>to create a Lifecycle configuration using the script and<\/li>\n<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.<\/li>\n<\/ol>\n\n<p>If you think 1 hour is too long you can tweak the script. <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh#L17\" rel=\"noreferrer\">This line<\/a> has the value.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-10-15 09:46:13.2 UTC",
        "Answer_score":26.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53609409",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54485769,
        "Question_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Question_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-02-01 19:08:33.447 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-02-01 19:23:02.467 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|tensorflow|amazon-ec2|amazon-sagemaker|docker-pull",
        "Question_view_count":378,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-02-11 20:10:25.3 UTC",
        "Answer_score":2.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":"2020-04-01 23:38:32.46 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52923569,
        "Question_title":"Viewing directory and folders in sagemaker",
        "Question_body":"<p>I have created a directory and folder for keeping the model and checkpoints in amazon sagemaker. How can i access this directory and folder to see model and checkpoints ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-22 06:39:13.123 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":3088,
        "Owner_creation_date":"2018-07-24 04:50:21.357 UTC",
        "Owner_last_access_date":"2020-06-28 14:13:29.797 UTC",
        "Owner_reputation":141,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52923569",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65699980,
        "Question_title":"Change model file save location on AWS SageMaker Training Job",
        "Question_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-13 10:24:53.827 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|scikit-learn|amazon-sagemaker",
        "Question_view_count":1244,
        "Owner_creation_date":"2019-11-05 12:37:54.857 UTC",
        "Owner_last_access_date":"2021-07-21 11:27:56.557 UTC",
        "Owner_reputation":123,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-13 13:47:25.387 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73072582,
        "Question_title":"Extract model saved in S3 bucket as tar.gz format to sagemaker notebook instance",
        "Question_body":"<p>I have a tar.gz file inside a S3 bucket, this is a file containing 6 different 'pickled' model zipped together. This was created after training a model with SageMaker docker container in single run.<\/p>\n<p>In order to make an inference, I would like untar these models into separate models to run model.predict() on the test data.<\/p>\n<p>My S3 bucket structure: 's3:\/\/\/output\/train_best_params\/model.tar.gz'<\/p>\n<p>How can I download these into SageMaker notebook instance and extract the 6 different models from it: as model1, model2, ....<\/p>\n<p>If I simply use sagemaker.model.Model() method, I couldn't make any inference, because this model object will have multiple models inside it.<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-21 21:02:27.837 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_date":"2017-07-27 22:34:49.14 UTC",
        "Owner_last_access_date":"2022-09-23 22:06:42.447 UTC",
        "Owner_reputation":119,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73072582",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68736614,
        "Question_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Question_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-11 05:50:17.973 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_date":"2012-12-05 11:02:16.8 UTC",
        "Owner_last_access_date":"2022-09-18 04:19:36.33 UTC",
        "Owner_reputation":2517,
        "Owner_up_votes":90,
        "Owner_down_votes":13,
        "Owner_views":78,
        "Answer_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-29 06:06:10.813 UTC",
        "Answer_score":0.0,
        "Owner_location":"Pittsburgh",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52070950,
        "Question_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Question_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-29 06:35:38.62 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-08-29 11:06:57.293 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":1519,
        "Owner_creation_date":"2017-07-30 08:26:08.107 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.87 UTC",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-08-29 11:06:41.19 UTC",
        "Answer_score":1.0,
        "Owner_location":"Delhi, India",
        "Answer_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52070950",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71711284,
        "Question_title":"Py4JJavaError while loading Jar file in SageMaker jupyter notebook",
        "Question_body":"<p>I am having issue when I try to load jar file in SageMaker Jupyter notebook.<\/p>\n<pre><code>import sagemaker_pyspark\nfrom pyspark.sql import SparkSession\n\nclasspath = &quot;\/home\/ec2-user\/SageMaker\/someJar.jar&quot;\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath).getOrCreate()\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DRZBS.png\" rel=\"nofollow noreferrer\">This is image of the error I am getting<\/a><\/p>\n<pre><code>Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NoClassDefFoundError: Could not initialize class scala.xml.Null$\n    at org.apache.spark.ui.jobs.AllJobsPage.&lt;init&gt;(AllJobsPage.scala:43)\n    at org.apache.spark.ui.jobs.JobsTab.&lt;init&gt;(JobsTab.scala:45)\n    at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:61)\n    at org.apache.spark.ui.SparkUI.&lt;init&gt;(SparkUI.scala:80)\n    at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:175)\n    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:444)\n    at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:58)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:238)\n    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>\n<p>I was able to load different jar file without any issue.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/3jjUm.png\" alt=\"enter image description here\" \/><\/p>\n<p>My assumption is that it could be because of Scala version mismatch between jar and SageMaker spark. I don't know how to resolve this issue. If anyone have any insight into this issue. Please let me know.<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-01 18:21:43.433 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-01 18:32:39.45 UTC",
        "Question_score":0,
        "Question_tags":"scala|jupyter-notebook|jvm|amazon-sagemaker|py4j",
        "Question_view_count":184,
        "Owner_creation_date":"2022-03-30 14:13:10.15 UTC",
        "Owner_last_access_date":"2022-09-19 23:04:55.96 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71711284",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69924340,
        "Question_title":"Sagemaker Distributed Data Parallelism not working as expected ( smdistributed.dataparallel.torch.distributed )",
        "Question_body":"<p>All,<\/p>\n<p>I was trying the AWS sagemaker data parallelism approach for the distributed training ( using the two lib ) from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\nimport smdistributed.dataparallel.torch.distributed as dist although the data is getting divided to all the GPUs(&quot;ml.p3.16xlarge&quot; , 8 Gpus) however the training time is still not getting reduced either with single instance or the double instance.<\/p>\n<p><strong>Earlier we tried with Sagemaker Inbuilt algo Resnet101 for the same data for 100 epoch training time was around 2080 sec  ( batch size - 64 ) which was the benchmark we wanted to improve with our distributed training<\/strong><\/p>\n<p>Now when we tried distributed training with the distributed data parallelism approach with the same instance we are training for 20 epoch - time is 1600 sec ( batch size - 64) we are training for 20 epoch - time is 1300 sec ( batch size - 128)\nwe are training for 20 epoch - time is 1063 sec ( batch size - 258).<\/p>\n<p>Even with different batch sizes training time is not improving much.<\/p>\n<p>Train Data - 6016 Images.\nTest Data - 745 Images.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-11 07:02:33.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pytorch|amazon-sagemaker|distributed-training",
        "Question_view_count":165,
        "Owner_creation_date":"2021-11-11 06:46:31.787 UTC",
        "Owner_last_access_date":"2022-04-19 13:38:48.017 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69924340",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68065670,
        "Question_title":"Cannot see the kernel Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized) in sagemaker",
        "Question_body":"<p>Amazon sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-kernels.html\" rel=\"nofollow noreferrer\">documentation<\/a>\nstates that <strong>TensorFlow 2.3 Python 3.7 GPU Optimized<\/strong> kernel should be available to use when a sagemaker notebook instance is used. But when I use a <em>ml.p2.xlarge<\/em> (us-west-2) <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">amazon sagemaker notebook instance<\/a> I cannot see the <em>TensorFlow 2.3 Python 3.7 GPU Optimized<\/em> kernel<\/p>\n<p>I can see other kernles such as<\/p>\n<ul>\n<li>Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized)<\/li>\n<li>Python 3 (MXNet 1.8 Python 3.7 GPU Optimized)<\/li>\n<\/ul>\n<p>Do I need to enable some particular setting to see <em>TensorFlow 2.3 Python 3.7 GPU Optimized<\/em> kernel<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-21 09:39:23.917 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":381,
        "Owner_creation_date":"2014-12-15 09:33:28.49 UTC",
        "Owner_last_access_date":"2022-09-22 13:59:19.453 UTC",
        "Owner_reputation":661,
        "Owner_up_votes":18,
        "Owner_down_votes":3,
        "Owner_views":63,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68065670",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63350039,
        "Question_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Question_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-11 00:59:05.867 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-11 01:05:25.233 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":395,
        "Owner_creation_date":"2018-12-21 02:51:36.8 UTC",
        "Owner_last_access_date":"2022-09-25 01:54:35.743 UTC",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Answer_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-11 01:07:54.44 UTC",
        "Answer_score":2.0,
        "Owner_location":"Earth",
        "Answer_last_edit_date":"2021-04-18 03:31:54.507 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63350039",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-02 04:01:47.17 UTC",
        "Question_favorite_count":14.0,
        "Question_last_edit_date":null,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-09-02 04:01:47.17 UTC",
        "Answer_score":65.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-07-04 05:43:30.063 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50271174,
        "Question_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Question_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-10 10:48:13.883 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":76,
        "Owner_creation_date":"2017-07-30 08:26:08.107 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.87 UTC",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2018-05-11 21:15:55.693 UTC",
        "Answer_score":1.0,
        "Owner_location":"Delhi, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73025706,
        "Question_title":"AccessDenied error when pushing docker image from SageMaker to ECR",
        "Question_body":"<p>I've created a docker image using AWS SageMaker and am now trying to push said image to ECR. When I do <code>docker push ${fullname}<\/code> it retries a couple of times and then errors.<\/p>\n<p>In CloudTrail I can see that I'm getting an access denied error with message:<\/p>\n<p>&quot;User: arn:aws:sts::xxxxxxxxxx:assumed-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx\/SageMaker is not authorized to perform: ecr:InitiateLayerUpload on resource: arn:aws:ecr:us-east-x:xxxxxxxxxx:repository\/image because no identity-based policy allows the ecr:InitiateLayerUpload action&quot;<\/p>\n<p>I have full permissions, but from the error message above it thinks the user is SageMaker and not me.<\/p>\n<p>How do I change the user? I'm guessing that's the problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-18 16:34:20.42 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-iam|amazon-sagemaker|amazon-ecr|docker-push",
        "Question_view_count":143,
        "Owner_creation_date":"2022-07-05 19:52:34.84 UTC",
        "Owner_last_access_date":"2022-09-21 21:49:37.727 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>When you're running commands from SageMaker, you're executing them as the SageMaker execution role, instead of your role. There are two options -<\/p>\n<ol>\n<li>[Straighforward solution] Add <em>ecr:InitiateLayerUpload<\/em> permissions to the <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> role<\/li>\n<li>Assume a different role using sts (in that case, <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> needs to have permissions to assume your Admin role) and then run <code>docker push<\/code> command.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-18 20:26:38.627 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73025706",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72904191,
        "Question_title":"How to upload a folder on SageMaker Notebooks",
        "Question_body":"<p>I am trying to upload a folder on SageMaker. However, I cannot select any folders, I need to go through the files and upload them one by one. Is there any way to upload the whole directory from local computer to SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-07 21:04:49.773 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-10 10:27:24.76 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|server|directory|upload|amazon-sagemaker",
        "Question_view_count":75,
        "Owner_creation_date":"2018-02-25 07:22:56.31 UTC",
        "Owner_last_access_date":"2022-08-29 16:07:10.467 UTC",
        "Owner_reputation":447,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Auburn, Al",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72904191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71790985,
        "Question_title":"Amazon SageMaker Unsupported Media Type: image\/jpeg",
        "Question_body":"<p>I trained my own model using Tensorflow and Keras for image classification and I'm trying to deploy and use it with Amazon's SageMaker. I went through the process of converting the <code>mymodel.h5<\/code> file into a <code>mymodel.tar.gz<\/code> file and moving it to the SageMaker S3 bucket. Then, following a tutorial I created the SageMaker model using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<p>And created the endpoint to access the model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Now, since the point of creating this model was for image classification I'm trying to pass the image to my endpoint but have been getting a response of <code>Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot;}&quot;<\/code>. After reading up I feel like I may need to do some more work to have access to the <code>image\/jpeg<\/code> content type as it seems like the defaults are <code>application\/json<\/code>, <code>text\/libsvm<\/code>, and <code>text\/csv<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-08 02:23:03.073 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2022-04-08 08:42:30.557 UTC",
        "Question_score":1,
        "Question_tags":"python|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_date":"2021-03-17 13:22:22.493 UTC",
        "Owner_last_access_date":"2022-09-25 05:49:30.783 UTC",
        "Owner_reputation":386,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71790985",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54045667,
        "Question_title":"How to input the correct shape for a Tensorflow Estimator?",
        "Question_body":"<p>I am trying to build a Tensorflow estimator to use on <code>SageMaker<\/code>. The main function trains and evaluates the estimator. Despite my best attempts, I keep getting the following error:<\/p>\n\n<blockquote>\n  <p>ValueError: Input 0 of layer inputs is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [50, 41]<\/p>\n<\/blockquote>\n\n<pre><code>def keras_model_fn(hyperparameters):\n    \"\"\"keras_model_fn receives hyperparameters from the training job and returns a compiled keras model.\n    The model will be transformed into a TensorFlow Estimator before training and it will be saved in a \n    TensorFlow Serving SavedModel at the end of training.\n\n    Args:\n        hyperparameters: The hyperparameters passed to the SageMaker TrainingJob that runs your TensorFlow \n                         training script.\n    Returns: A compiled Keras model\n    \"\"\"\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.LSTM(32, name='inputs', input_shape=( None, 41)))\n    model.add(tf.keras.layers.Dense(11, activation='softmax', name='dense'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n\n    return model\n\n\ndef train_input_fn(training_dir=None, hyperparameters=None):\n    # invokes _input_fn with training dataset\n    dataset = tf.data.Dataset.from_tensors(({INPUT_TENSOR_NAME: x_train}, y_train))\n    dataset = dataset.repeat()\n    return dataset.make_one_shot_iterator().get_next()\n\ndef eval_input_fn(training_dir=None, hyperparameters=None):\n    # invokes _input_fn with evaluation dataset\n\n    dataset =  tf.data.Dataset.from_tensors(({INPUT_TENSOR_NAME: x_test}, y_test))\n    return dataset.make_one_shot_iterator().get_next()\n\nif __name__ == '__main__':\n    print(x_train.shape, y_train.shape)\n    tf.logging.set_verbosity(tf.logging.INFO)\n    model = keras_model_fn(0)\n    estimator = tf.keras.estimator.model_to_estimator(keras_model=model)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n<\/code><\/pre>\n\n<p>My inputs and output shapes are:<\/p>\n\n<blockquote>\n  <p>(52388, 50, 41) (52388, 11)<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-01-04 20:34:49.44 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-01-05 09:58:44.26 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":753,
        "Owner_creation_date":"2012-12-24 14:35:59 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:00.417 UTC",
        "Owner_reputation":436,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Dubai - United Arab Emirates",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54045667",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49977679,
        "Question_title":"upload data to S3 with sagemaker",
        "Question_body":"<p>I have a problem with SageMaker when I try to upload Data into S3 bucket . I get this error : <\/p>\n\n<blockquote>\n  <hr>\n\n<pre><code>NameError                                 Traceback (most recent call last)\n&lt;ipython-input-26-d21b1cb0fcab&gt; in &lt;module&gt;()\n     19 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n     20 \n---&gt; 21 upload_to_s3('train', 'caltech-256-60-train.rec')\n\n&lt;ipython-input-26-d21b1cb0fcab&gt; in upload_to_s3(channel, file)\n     13     data = open(file, \"rb\")\n     14     key = channel + '\/' + file\n---&gt; 15     s3.Bucket(bucket).put_object(Key=key, Body=data)\n     16 \n     17 \n\nNameError: name 'bucket' is not defined\n<\/code><\/pre>\n<\/blockquote>\n\n<p>Here is the script:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import os\nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n\nupload_to_s3('train', 'caltech-256-60-train.rec')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-23 09:31:28.687 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-12-06 22:53:27.193 UTC",
        "Question_score":5,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":9575,
        "Owner_creation_date":"2013-03-27 10:20:38.763 UTC",
        "Owner_last_access_date":"2022-05-09 06:03:41.087 UTC",
        "Owner_reputation":187,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":108,
        "Answer_body":"<p>It is exactly as the error say, the variable <code>bucket<\/code> is not defined. \nyou might want to do something like <\/p>\n\n<pre><code>bucket = &lt;name of already created bucket in s3&gt;\n<\/code><\/pre>\n\n<p>before you call <\/p>\n\n<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-04-23 16:25:22.233 UTC",
        "Answer_score":8.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49977679",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61550297,
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-01 20:28:20.607 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"javascript|promise|async-await|amazon-sagemaker|mechanicalturk",
        "Question_view_count":63,
        "Owner_creation_date":"2017-01-03 11:49:04.907 UTC",
        "Owner_last_access_date":"2022-02-04 13:53:41.673 UTC",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-05-01 20:43:38.457 UTC",
        "Answer_score":1.0,
        "Owner_location":"Hoth",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63024900,
        "Question_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Question_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-22 00:19:39.513 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|tensorflow|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":758,
        "Owner_creation_date":"2013-09-04 04:27:22.847 UTC",
        "Owner_last_access_date":"2022-09-22 00:59:12.557 UTC",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Answer_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-24 21:06:01.857 UTC",
        "Answer_score":2.0,
        "Owner_location":"Pune, India",
        "Answer_last_edit_date":"2020-07-24 21:17:13.193 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 13:53:04.467 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-02-11 20:12:56.767 UTC",
        "Answer_score":3.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63304005,
        "Question_title":"sudo: not found on AWS Sagemaker Studio",
        "Question_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-07 14:39:07.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-07 17:59:24.337 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":980,
        "Owner_creation_date":"2017-01-02 15:26:06.803 UTC",
        "Owner_last_access_date":"2022-09-24 21:50:13.153 UTC",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Answer_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-08-07 14:45:24.96 UTC",
        "Answer_score":0.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":"2020-08-07 15:48:01.143 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57227757,
        "Question_title":"ConnectionClosedError: Connection was closed before we received a valid response from endpoint URL:",
        "Question_body":"<p>So I'm trying to figure out Sagemaker and can get an endpoint for a tensorflow model up and running. Strangely enough, when I send small images to it, up to 220x220 (40kb), it runs just fine! However I get a connection closed error when I try anything larger, say 225x225. <\/p>\n\n<p>I've tried from different networks and computers and get the same error. Also, I don't think I'm anywhere near the 5MB limit for sagemaker requests so I must admit I'm stumped on this one. Anyone happen to know what might be the cause of this? Thanks for your help. \nI'm calling my model via boto as follows:<\/p>\n\n<pre><code>config = Config(connect_timeout=999999, read_timeout=9999999)\nclient = boto3.client(\"runtime.sagemaker\", config=config)\ndata = {\"instances\": [{\"x\":x, \"x2\":x2}]}\nresponse = client.invoke_endpoint(EndpointName='dcscn', \n                                Body=json.dumps(data),\n                                ContentType=\"application\/json\")\n\nresponse_body = response['Body']\n<\/code><\/pre>\n\n<p>The error I get is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1239, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 125, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1285, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1234, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 152, in _send_output\n    self.send(msg)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 236, in send\n    return super(AWSConnection, self).send(str)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 986, in send\n    self.sock.sendall(data)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 975, in sendall\n    v = self.send(byte_view[count:])\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 944, in send\n    return self._sslobj.write(data)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 642, in write\n    return self._sslobj.write(data)\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/httpsession.py\", line 258, in send\n    decode_content=False,\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 638, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py\", line 343, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py\", line 685, in reraise\n    raise value.with_traceback(tb)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1239, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 125, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1285, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 1234, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 152, in _send_output\n    self.send(msg)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py\", line 236, in send\n    return super(AWSConnection, self).send(str)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/http\/client.py\", line 986, in send\n    self.sock.sendall(data)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 975, in sendall\n    v = self.send(byte_view[count:])\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 944, in send\n    return self._sslobj.write(data)\n  File \"\/Users\/bhhj\/anaconda\/lib\/python3.6\/ssl.py\", line 642, in write\n    return self._sslobj.write(data)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"ping_endpoint.py\", line 100, in &lt;module&gt;\n    ContentType=\"application\/json\")\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 357, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 648, in _make_api_call\n    operation_model, request_dict, request_context)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 667, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/endpoint.py\", line 102, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/endpoint.py\", line 137, in _send_request\n    success_response, exception):\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/endpoint.py\", line 231, in _needs_retry\n    caught_exception=caught_exception, request_dict=request_dict)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/hooks.py\", line 356, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/hooks.py\", line 228, in emit\n    return self._emit(event_name, kwargs)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/hooks.py\", line 211, in _emit\n    response = handler(**kwargs)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 183, in __call__\n    if self._checker(attempts, response, caught_exception):\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 251, in __call__\n    caught_exception)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 277, in _should_retry\n    return self._checker(attempt_number, response, caught_exception)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 317, in __call__\n    caught_exception)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 223, in __call__\n    attempt_number, caught_exception)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py\", line 359, in _check_caught_exception\n    raise caught_exception\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/endpoint.py\", line 200, in _do_get_response\n    http_response = self._send(request)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/endpoint.py\", line 244, in _send\n    return self.http_session.send(request)\n  File \"\/Users\/bhhj\/.local\/lib\/python3.6\/site-packages\/botocore\/httpsession.py\", line 289, in send\n    endpoint_url=request.url\nbotocore.exceptions.ConnectionClosedError: Connection was closed before we received a valid response from endpoint URL: \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_date":"2019-07-26 22:44:18.637 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-28 21:34:36.663 UTC",
        "Question_score":6,
        "Question_tags":"python|amazon-web-services|tensorflow|boto|amazon-sagemaker",
        "Question_view_count":11340,
        "Owner_creation_date":"2019-07-26 22:28:55.623 UTC",
        "Owner_last_access_date":"2019-12-02 23:42:04.163 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57227757",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64773819,
        "Question_title":"How to deploy naive Bayes model on AWS Sagemaker",
        "Question_body":"<p>I have a naive Bayes model which performs sentiment analysis of movie reviews. I want to deploy this naive Bayes model on AWS Sagemaker. Naive Bayes isn't one of the built in algorithms on Sagemaker.<\/p>\n<p>Can anyone guide me with the steps ? I already have the model built and I'm just left with deploying it on Sagemaker so that I can build a web app using its endpoint instance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-10 17:32:08.113 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|naivebayes",
        "Question_view_count":351,
        "Owner_creation_date":"2020-06-16 19:56:30.937 UTC",
        "Owner_last_access_date":"2020-12-18 16:35:40.767 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64773819",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52616037,
        "Question_title":"AWS Sagemaker: Is there a way to look at at the coefficients of the regressors of a 'linear learner - binary_classifier' model?",
        "Question_body":"<p>I'm interested in getting a little more information on the model that's being built. Namely, what are the coefficients of the regressors that it is using for the prediction?<\/p>\n\n<p>The loss function is automatically set to 'logistic'. I am assuming this means it is building a typical logistic regression model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-02 20:35:59.4 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":422,
        "Owner_creation_date":"2018-10-02 20:27:28.187 UTC",
        "Owner_last_access_date":"2021-06-24 15:20:06.333 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Boulder, CO, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52616037",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 11:34:26.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-25 12:39:54.637 UTC",
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-08 19:01:50.813 UTC",
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":"2022-06-09 17:44:18.207 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66782040,
        "Question_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Question_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-24 13:25:52.363 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-24 13:33:27.08 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":286,
        "Owner_creation_date":"2017-01-19 15:07:44.573 UTC",
        "Owner_last_access_date":"2022-09-22 14:55:11.743 UTC",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Answer_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-03-25 08:47:21.463 UTC",
        "Answer_score":0.0,
        "Owner_location":"Amsterdam, Nederland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62111580,
        "Question_title":"Trouble opening audio files stored on S3 in SageMaker",
        "Question_body":"<p>I stored like 300 GB of audio data (mp3\/wav mostly) on Amazon S3 and am trying to access it in a SageMaker notebook instance to do some data transformations. I'm trying to use either torchaudio or librosa to load a file as a waveform. torchaudio expects the file path as the input, librosa can either use a file path or file-like object. I tried using s3fs to get the url to the file but torchaudio doesn't recognize it as a file. And apparently SageMaker has problems installing librosa so I can't use that. What should I do?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-05-31 04:28:02.7 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":749,
        "Owner_creation_date":"2020-05-31 04:23:53.11 UTC",
        "Owner_last_access_date":"2022-06-17 22:34:42.813 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>I ended up not using SageMaker for this, but for anybody else having similar problems, I solved this by opening the file using s3fs and writing it to a <code>tempfile.NamedTemporaryFile<\/code>. This gave me a file path that I could pass into either <code>torchaudio.load<\/code> or <code>librosa.core.load<\/code>. This was also important because I wanted the extra resampling functionality of <code>librosa.core.load<\/code>, but it doesn't accept file-like objects for loading mp3s.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-15 21:46:02.683 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62111580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55410462,
        "Question_title":"Hyper parameter tuning for Random cut forest",
        "Question_body":"<p>I have used to below hyper parameters to train the model.<\/p>\n\n<pre><code>  rcf.set_hyperparameters(\n        num_samples_per_tree=200,\n        num_trees=250,\n        feature_dim=1,\n        eval_metrics =[\"accuracy\", \"precision_recall_fscore\"])\n<\/code><\/pre>\n\n<p>is there any best way to choose the num_samples_per_tree and  num_trees parameters.<\/p>\n\n<p>what are the best numbers for both num_samples_per_tree and num_trees.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-03-29 04:21:34.783 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":642,
        "Owner_creation_date":"2017-08-17 17:30:14.79 UTC",
        "Owner_last_access_date":"2021-01-08 07:02:03.013 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":"<p>There are natural interpretations for these two hyper-parameters that can help you determine good starting approximations for HPO:<\/p>\n\n<ul>\n<li><code>num_samples_per_tree<\/code> -- the reciprocal of this value approximates the density of anomalies in your data set\/stream. For example, if you set this to <code>200<\/code> then the assumption is that approximately 0.5% of the data is anomalous. Try exploring your dataset to make an educated estimate.<\/li>\n<li><code>num_trees<\/code> -- the more trees in your RCF model the less noise in scores. That is, if more trees are reporting that the input inference point is an anomaly then the point is much more likely to be an anomaly than if few trees suggest so.<\/li>\n<\/ul>\n\n<p>The total number of points sampled from the input dataset is equal to <code>num_samples_per_tree * num_trees<\/code>. You should make sure that the input training set is at least this size.<\/p>\n\n<p><em>(Disclosure - I <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection\/\" rel=\"nofollow noreferrer\">helped create<\/a> SageMaker Random Cut Forest)<\/em><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2019-04-16 17:39:40.77 UTC",
        "Answer_score":1.0,
        "Owner_location":"India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55410462",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70464579,
        "Question_title":"xgboost training fail in sagemaker studio lab",
        "Question_body":"<p>i was training a model in sagemaker studiolab but, the training job in aws sagemaker console was running but it throw me this error:<\/p>\n<p>UnexpectedStatusException: Error for Training job sagemaker-xgboost-2021-12-23-14-59-41-581: Failed. Reason: AlgorithmError: framework error:\nTraceback (most recent call last):\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nentrypoint()\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\ntrain(framework.training_env())\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nrun_algorithm_mode()\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\ncheckpoint_config=checkpoint_config\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 139, in sagemaker_train\ncsv_weights, is_pipe, combine_train_val)\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 82, in get_validated_dmatrices\nvalidate_data_file_path(train_path, content_type)\nFile &quot;\/miniconda3\/lib\/python3.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-12-23 16:01:32.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":217,
        "Owner_creation_date":"2021-12-23 15:42:37.72 UTC",
        "Owner_last_access_date":"2022-09-22 10:53:35.917 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Talcahuano, Chile",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70464579",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56240769,
        "Question_title":"SageMaker ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Could not assume role",
        "Question_body":"<p>When I get to Notebook Instance in SageMaker and create model in <strong>linear_learner_mnist<\/strong> example I get error even when I have the role with <strong>AmazonSageMakerFullAccess<\/strong>and <strong>AssumeRole<\/strong> attached policies attached. \nWhen I add  <strong>AdministratorAccess<\/strong>policy to the role - all works fine. What am I missing here?<\/p>\n\n<p>P.S role exists and attached :)<\/p>\n\n<pre><code>Resources:\n  SageMakerExecutionRole:\n    Condition: RoleArnEmpty\n    Type: \"AWS::IAM::Role\"\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: \"Allow\"\n            Principal:\n              Service:\n                - \"sagemaker.amazonaws.com\"\n            Action:\n              - sts:AssumeRole\n      ManagedPolicyArns:\n        - \"arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess\"\n      Path: \"\/service-role\/\"\n      Policies:\n        Fn::If:\n        - S3BucketNotEmpty\n        -\n          - PolicyName: SageMakerS3BucketAccess\n            PolicyDocument:\n              Version: '2012-10-17'\n              Statement:\n                - Effect: Allow\n                  Action:\n                    - s3:*\n                  Resource:\n                    - !Sub 'arn:aws:s3:::${SageMakerS3Bucket}'\n                    - !Sub 'arn:aws:s3:::${SageMakerS3Bucket}\/*'\n        - Ref: AWS::NoValue\n<\/code><\/pre>\n\n<blockquote>\n  <p>ClientError: An error occurred (ValidationException) when calling the\n  CreateTrainingJob operation: Could not assume role\n  arn:aws:iam::XXX:role\/sagemaker-stack-SageMakerExecutionRole-1JT7AT4OPUK9R.\n  Please ensure that the role exists and allows principal\n  'sagemaker.amazonaws.com' to assume the role.<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-21 14:41:26.76 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":8877,
        "Owner_creation_date":"2017-02-16 12:14:25.387 UTC",
        "Owner_last_access_date":"2022-09-21 12:35:23.003 UTC",
        "Owner_reputation":1810,
        "Owner_up_votes":1143,
        "Owner_down_votes":18,
        "Owner_views":219,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Odessa, Ukraine",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56240769",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65088542,
        "Question_title":"Error when createPresignedDomainUrl is called - AWS SageMaker",
        "Question_body":"<p>i'm working with nodeJs and i'm trying to get a presigned Url from SageMaker.\nHere's my code:<\/p>\n<pre><code>   const sagemaker = new AWS.SageMaker(); \n   module.exports.callSagemaker = async (req, res) =&gt; {\n\n   let params = {\n      DomainId: '...', \n      UserProfileName: '...', \n      SessionExpirationDurationInSeconds: 120\n   };\n   await sagemaker.createPresignedDomainUrl(params, function(err, data) {\n      if (err) console.log(err, err.stack); \/\/ an error occurred\n      else     console.log(data);           \/\/ successful response\n   });\n}\n<\/code><\/pre>\n<p>But i got this error:<\/p>\n<blockquote>\n<p>TypeError: Cannot set property 'Timestamp' of undefined\nat features.constructor.addAuthorization (\/home\/loredana\/Documents\/projects\/edison_zeus\/backend-zeus\/node_modules\/aws-sdk\/lib\/signers\/v2.js:14:24)<\/p>\n<\/blockquote>\n<p>My sdk version is:<\/p>\n<blockquote>\n<p>&quot;aws-sdk&quot;: &quot;^2.800.0&quot;<\/p>\n<\/blockquote>\n<p>Is anyone facing the same issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-01 10:14:45.713 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-01 10:31:46.057 UTC",
        "Question_score":1,
        "Question_tags":"node.js|aws-sdk|amazon-sagemaker|aws-sdk-js|aws-sdk-nodejs",
        "Question_view_count":93,
        "Owner_creation_date":"2020-11-25 10:51:00.04 UTC",
        "Owner_last_access_date":"2021-10-24 15:12:31.47 UTC",
        "Owner_reputation":75,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65088542",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64752554,
        "Question_title":"GPU utilization is zero when running batch transform in Amazon SageMaker",
        "Question_body":"<p>I want to run a batch transform job on AWS SageMaker. I have an image classification model which I have trained on a local GPU. Now I want to deploy it on AWS SageMaker and make predictions using Batch Transform. While the batch transform job runs successfully, the GPU utilization during the job is always zero (GPU Memory utilization, however, is at 97%). That's what CloudWatch is telling me. Also, the job takes approx. 7 minutes to process 500 images, I would expect it to run much faster than this, at least when comparing it to the time it takes to process the images on a local GPU.<\/p>\n<p><strong>My question:<\/strong> Why doesn't the GPU get used during batch transform, even though I am using a GPU instance (I am using an ml.p3.2xlarge instance)? I was able to deploy the very same model to an endpoint and send requests. When deploying to an endpoint instead of using batch transform, the GPU actually gets used.<\/p>\n<p><strong>Model preparation<\/strong><\/p>\n<p>I am using a Keras Model with TensorFlow backend. I converted this model to a sagemaker model using this guide <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a> :<\/p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\nimport tarfile\nimport sagemaker\n\n# deactivate eager mode\nif tf.executing_eagerly():\n   tf.compat.v1.disable_eager_execution()\n\nbuilder = builder.SavedModelBuilder(export_dir)\n\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;image_bytes&quot;: model.input}, outputs={&quot;score_bytes&quot;: model.output})\n\nwith tf.compat.v1.keras.backend.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\n\nwith tarfile.open(tar_model_file, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nsagemaker_session = sagemaker.Session()\ns3_uri = sagemaker_session.upload_data(path=tar_model_file, bucket=bucket, key_prefix=sagemaker_model_dir)\n<\/code><\/pre>\n<p><strong>Batch Transform<\/strong><\/p>\n<p>Container image used for batch transform: 763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:2.0.0-gpu<\/p>\n<pre><code>framework = 'tensorflow'\ninstance_type='ml.p3.2xlarge'\nimage_scope = 'inference'\ntf_version = '2.0.0'\npy_version = '3.6'\n\nsagemaker_model = TensorFlowModel(model_data=MODEL_TAR_ON_S3, role=role, image_uri=tensorflow_image)\n\ntransformer = sagemaker_model.transformer(\n    instance_count = 1,\n    instance_type = instance_type,\n    strategy='MultiRecord',\n    max_concurrent_transforms=8,\n    max_payload=10, # in MB\n    output_path = output_data_path,\n)\n\ntransformer.transform(data = input_data_path,\n                      job_name = job_name,\n                      content_type = 'application\/json', \n                      logs=False,\n                      wait=True\n)\n<\/code><\/pre>\n<p><strong>Log file excerpts<\/strong><\/p>\n<p>Loading the model takes quite long (several minutes). During this time, the following error message is getting logged:<\/p>\n<blockquote>\n<p>2020-11-08T15:14:12.433+01:00            2020\/11\/08 14:14:12 [error]\n14#14: *3066 no live upstreams while connecting to upstream, client:\n169.254.255.130, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, subrequest: &quot;\/v1\/models\/my_model:predict&quot;, upstream:\n&quot;http:\/\/tfs_upstream\/v1\/models\/my_model:predict&quot;, host:\n&quot;169.254.255.131:8080&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n169.254.255.130 - - [08\/Nov\/2020:14:14:12 +0000] &quot;GET \/ping HTTP\/1.1&quot; 502 157 &quot;-&quot; &quot;Go-http-client\/1.1&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n2020\/11\/08 14:14:12 [error] 14#14: *3066 js: failed ping#015\n2020-11-08T15:14:12.433+01:00            502 Bad\nGateway#015 2020-11-08T15:14:12.433+01:00<\/p>\n#015 2020-11-08T15:14:12.433+01:00            <h1>502\nBad Gateway<\/h1>#015 2020-11-08T15:14:12.433+01:00           \n<hr>nginx\/1.16.1#015 2020-11-08T15:14:12.433+01:00   \n#015 2020-11-08T15:14:12.433+01:00            #015\n<\/blockquote>\n<p>There was a log entry about NUMA node read:<\/p>\n<blockquote>\n<p>successful NUMA node read from SysFS had negative value (-1), but\nthere must be at least one NUMA node, so returning NUMA node zero<\/p>\n<\/blockquote>\n<p>And about a serving warmup request:<\/p>\n<blockquote>\n<p>No warmup data file found at\n\/opt\/ml\/model\/export\/my_model\/1\/assets.extra\/tf_serving_warmup_requests<\/p>\n<\/blockquote>\n<p>And this warning:<\/p>\n<blockquote>\n<p>[warn] getaddrinfo: address family for nodename not supported<\/p>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-11-09 13:25:55.263 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|tensorflow-serving",
        "Question_view_count":505,
        "Owner_creation_date":"2017-09-16 20:26:28.753 UTC",
        "Owner_last_access_date":"2022-07-29 14:16:56.8 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64752554",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73027227,
        "Question_title":"Error when deploying cross account Sagemaker Endpoints",
        "Question_body":"<p>I am using cdk to deploy a Sagemaker Endpoint in a cross-account context.<\/p>\n<p>The following error appears when creating the Sagemaker Endpoint:\nFailed to download model data for container &quot;container_1&quot; from URL: &quot;s3:\/\/...\/model.tar.gz&quot;. Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the object.<\/p>\n<p>Here are some useful details.<\/p>\n<p>I have two accounts:<\/p>\n<ul>\n<li>Account A: includes the encrypted s3 bucket in which the model artifact has been saved, the Sagemaker model package group with the latest approved version and a CodePipeline that deploys the endpoint in the account A itself and account B.<\/li>\n<li>Account B: includes the endpoint deployed by CodePipeline in Account A.<\/li>\n<\/ul>\n<p>In AccountA:<\/p>\n<ul>\n<li>The cross account permissions are set both for the bucket and the kms key used to encrypt that bucket<\/li>\n<\/ul>\n<pre class=\"lang-js prettyprint-override\"><code>\/\/ Create bucket and kms key to be used by Sagemaker Pipeline\n\n        \/\/KMS\n        const sagemakerKmsKey = new Key(\n            this,\n            &quot;SagemakerBucketKMSKey&quot;,\n            {\n                description: &quot;key used for encryption of data in Amazon S3&quot;,\n                enableKeyRotation: true,\n                policy: new PolicyDocument(\n                    {\n                        statements:[\n                            new PolicyStatement(\n                                {\n                                    actions:[&quot;kms:*&quot;],\n                                    effect: Effect.ALLOW,\n                                    resources:[&quot;*&quot;],\n                                    principals: [new AccountRootPrincipal()]\n                                }\n                            ),\n                            new PolicyStatement(\n                                {\n                                    actions:[\n                                        &quot;kms:*&quot;\n                                    ],\n                                    effect: Effect.ALLOW,\n                                    resources:[&quot;*&quot;],\n                                    principals: [\n                                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountA}:root`),\n                                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountB}:root`),\n                                    ]\n                                }\n                            )\n                        ]\n                    }\n                )\n            }\n        )\n\n        \/\/ S3 Bucket\n        const sagemakerArtifactBucket = new Bucket(\n            this,\n            &quot;SagemakerArtifactBucket&quot;,\n            {\n                bucketName:`mlops-${projectName}-${Aws.REGION}`,\n                encryptionKey:sagemakerKmsKey,\n                versioned:false,\n                removalPolicy: RemovalPolicy.DESTROY\n            }\n        )\n        \n        sagemakerArtifactBucket.addToResourcePolicy(\n            new PolicyStatement(\n                {\n                    actions: [\n                        &quot;s3:*&quot;,\n                    ],\n                    resources: [\n                        sagemakerArtifactBucket.bucketArn,\n                        `${sagemakerArtifactBucket.bucketArn}\/*`\n                    ],\n                    principals: [\n                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountA}:root`),\n                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountB}:root`),\n                    ]\n                }\n            )\n        )\n<\/code><\/pre>\n<ul>\n<li>A CodeDeploy Action is used to deploy the Sagemaker Endpoint in AccountA and AccountB.<\/li>\n<\/ul>\n<pre class=\"lang-js prettyprint-override\"><code>\/\/ Define Code Build Deploy Staging Action\n        const deployStagingAction = new CloudFormationCreateUpdateStackAction(\n            {\n                actionName: &quot;DeployStagingAction&quot;,\n                runOrder: 1,\n                adminPermissions: false,\n                stackName: `${projectName}EndpointStaging`,\n                templatePath: cdKSynthArtifact.atPath(&quot;staging.template.json&quot;),\n                replaceOnFailure: true,\n                role: Role.fromRoleArn(\n                    this,\n                    &quot;StagingActionRole&quot;,\n                    `arn:${Aws.PARTITION}:iam::${AccountB}:role\/cdk-hnb659fds-deploy-role-${AccountB}-${Aws.REGION}`,\n                ),\n                deploymentRole: Role.fromRoleArn(\n                    this,\n                    &quot;StagingDeploymentRole&quot;,\n                    `arn:${Aws.PARTITION}:iam::${AccountB}:role\/cdk-hnb659fds-cfn-exec-role-${AccountB}-${Aws.REGION}`\n                ),\n                cfnCapabilities: [\n                    CfnCapabilities.AUTO_EXPAND,\n                    CfnCapabilities.NAMED_IAM\n                ]\n            }\n        )\n<\/code><\/pre>\n<p>Specifically, the role that creates the Sagemaker Model and Sagemaker Endpoints should be cdk-hnb659fds-cfn-exec-role, as seen on CloudTrail, but for testing purposes I've granted to both of them Administrator privileges (the error still appears).<\/p>\n<p>The deployment in AccountA is correctly executed, thus it means that the bucket location is correct.<\/p>\n<p>NOTE: everything is deployed correctly up to the Sagemaker Endpoint.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-18 18:50:17.303 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"typescript|amazon-web-services|amazon-cloudformation|aws-cdk|amazon-sagemaker",
        "Question_view_count":110,
        "Owner_creation_date":"2017-07-06 12:59:04.303 UTC",
        "Owner_last_access_date":"2022-08-20 15:29:04.513 UTC",
        "Owner_reputation":137,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Isola d'Ischia, NA, Italia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73027227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73058485,
        "Question_title":"Print message from inside AWS SageMaker endpoint",
        "Question_body":"<p>I've set up an endpoint for a model in AWS SageMaker. I set up my own container and am trying to debug some path related errors. To do this I need to print some information so I know what's going on.<\/p>\n<p>Is there a way to print a message either to the terminal or to AWS CloudWatch from inside the endpoint.<\/p>\n<p>Update:<\/p>\n<p>I'm now trying to get logger to work, I'm doing the following:<\/p>\n<pre><code>import logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogger.debug(&quot;message&quot;)\n<\/code><\/pre>\n<p>I optimally want this to show up in CloudWatch but currently it's not.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-07-20 21:50:42.883 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-21 14:42:04.42 UTC",
        "Question_score":0,
        "Question_tags":"python|logging|amazon-cloudwatch|amazon-sagemaker|endpoint",
        "Question_view_count":104,
        "Owner_creation_date":"2022-07-05 19:52:34.84 UTC",
        "Owner_last_access_date":"2022-09-21 21:49:37.727 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058485",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57547110,
        "Question_title":"How to load file in sagemaker custom deploy endpoint script",
        "Question_body":"<p>I am trying to deploy a sentiment analysis model on sagemaker to an endpoint to predict sentiment in real time of an input text. This model will take a single text String as input and return the sentiment.<\/p>\n\n<p>To train the xgboost model, I followed this <a href=\"https:\/\/github.com\/NadimKawwa\/sagemaker_ml\/blob\/master\/SageMaker_IMDB_highlevel.ipynb\" rel=\"nofollow noreferrer\"> notebook<\/a> upto step 23. \nThis uploaded model.tar.gz to s3 bucket. I additionally uploaded vocabulary_dict generated by sklearn's CountVectorizer(to create bag of words)to s3 bucket as well. <\/p>\n\n<p>To deploy this pre-trained model, I can use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-endpoints-from-model-data\" rel=\"nofollow noreferrer\">this method<\/a> and supply an entry point python file predict.py.<\/p>\n\n<pre><code>sklearn_model = SKLearnModel(model_data=\"s3:\/\/bucket\/model.tar.gz\", role=\"SageMakerRole\", entry_point=\"predict.py\")\n<\/code><\/pre>\n\n<p>Documentation says that I have to provide model.tar.gz only as argument and it will be loaded in model_fn. But if I am writing my own model_fn, how do I load the model then? If I put additional files in the same directory as of model.tar.gz in S3, can I load them as well?<\/p>\n\n<p>Now to do the classification, I will have to vectorize the input text before calling model.predict(bow_vector) in the method predict_fn. In order to do that, I need word_dict which I prepared during pre-processing training data and wrote to s3. <\/p>\n\n<p>My question is how do I get the word_dict inside the model_fn? Can I load it from s3? \nBelow is code for predict.py.<\/p>\n\n<pre><code>import os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom bs4 import BeautifulSoup\nimport sagemaker_containers\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\ndef model_fn(model_dir):\n\n    #TODO How to load the word_dict.\n    #TODO How to load the model.\n    return model, word_dict\n\ndef predict_fn(input_data, model):\n    print('Inferring sentiment of input data.')\n    trained_model, word_dict = model\n    if word_dict is None:\n        raise Exception('Model has not been loaded properly, no word_dict.')\n\n    #Process input_data so that it is ready to be sent to our model.\n\n    input_bow_csv = process_input_text(word_dict, input_data)\n    prediction = trained_model.predict(input_bow_csv)\n    return prediction\n\n\ndef process_input_text(word_dict, input_data):\n\n    words = text_to_words(input_data);\n    vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, word_dict)\n    bow_array = vectorizer.transform([words]).toarray()[0]\n    bow_csv = \",\".join(str(bit) for bit in bow_array)\n    return bow_csv\n\ndef text_to_words(text):\n    \"\"\"\n    Uses the Porter Stemmer to stem words in a review\n    \"\"\"\n    #instantiate stemmer\n    stemmer = PorterStemmer()\n    text_nohtml = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags\n    text_lower = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_nohtml.lower()) # Convert to lower case\n    words = text_lower.split() # Split string into words\n    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n    words = [PorterStemmer().stem(w) for w in words] # stem\n    return words\n\ndef input_fn(input_data, content_type):\n    return input_data;\n\ndef output_fn(prediction_output, accept):\n    return prediction_output;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-18 18:04:31.257 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|scikit-learn|aws-sdk|amazon-sagemaker",
        "Question_view_count":676,
        "Owner_creation_date":"2019-08-18 01:53:16.3 UTC",
        "Owner_last_access_date":"2019-09-25 20:27:58.817 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57547110",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63135881,
        "Question_title":"DeepAR InternalServerError: We encountered an internal error. Please try again",
        "Question_body":"<p>Here is the error I am getting<\/p>\n<pre><code>Error for Transform job forecasting-deepar-2020-07-28-12-59-06--2020-07-28-12-59-07-635: Failed. Reason: InternalServerError: We encountered an internal error. Please try again.\n<\/code><\/pre>\n<p>My DeepAR Batch Transformer jobs are working fine for small subset of the data. But however for larger set of data, I am getting above error. This has really been a show stopper not sure how to proceed further. I also tried with different combination of instance count, instance type and it is still failing.<\/p>\n<p>Here is the transformer code I am using<\/p>\n<pre><code>deepar_transformer = estimator.transformer(instance_count = 4,\n                                  instance_type = 'ml.m4.4xlarge',strategy='MultiRecord', output_path=batch_output , max_payload=100)\n                                  \ndeepar_transformer.transform(data=data_channels[&quot;test&quot;]) \n\ndeepar_transformer.wait()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-28 13:59:02.473 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":98,
        "Owner_creation_date":"2020-07-20 12:38:25.493 UTC",
        "Owner_last_access_date":"2021-08-26 10:10:14.853 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63135881",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64158911,
        "Question_title":"Load Python Pickle File from S3 Bucket to Sagemaker Notebook",
        "Question_body":"<p>I have attempted the code on the many posts on how to load a pickle file (1.9GB) from an S3 bucket, but none seem to work for our notebook instance on AWS Sagemaker.  Notebook size is 50GB.<\/p>\n<p>Some of the methods attempted:<\/p>\n<p>Method 1<\/p>\n<pre><code>import io\nimport boto3\n\nclient = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\n\nbytes_io.seek(0) \nbyte_value = pickle.load(bytes_io)\n<\/code><\/pre>\n<p>This gives:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Method 2: This actually gets me something back with no error:<\/p>\n<pre><code>client = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\nbyte_value = bytes_buffer.getvalue()\nimport sys\nsys.getsizeof(byte_value)\/(1024**3)\n<\/code><\/pre>\n<p>this returns: 1.93<\/p>\n<p>but how do I convert the byte_value into the pickled object?\nI tried this:<\/p>\n<pre><code>pickled_data = pickle.loads(byte_value)\n<\/code><\/pre>\n<p>But the kernel &quot;crashed&quot; - went idle and I lost all variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-01 15:50:32 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":2485,
        "Owner_creation_date":"2012-05-27 16:46:50 UTC",
        "Owner_last_access_date":"2022-08-30 09:49:13.917 UTC",
        "Owner_reputation":1937,
        "Owner_up_votes":249,
        "Owner_down_votes":0,
        "Owner_views":221,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64158911",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73015639,
        "Question_title":"Issues with deploying spark and mlflow to sagemaker",
        "Question_body":"<p>My goal is to deploy a spark\/mlflow to sagemaker with the following command:<\/p>\n<pre><code>    mlflow sagemaker deploy .. \n<\/code><\/pre>\n<p>I've successfully pushed a image to EC2 with<\/p>\n<pre><code>mlflow sagemaker build-and-push-container\n<\/code><\/pre>\n<p>I encounter errors when attempting to run mlflow sagemaker deploy:<\/p>\n<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/127.0.0.1:8000\/ping&quot;, host: &quot;model.aws.local:8080&quot;\njava.io.IOException: Failed to connect to model.aws.local\/172.17.0.2:34473\n<\/code><\/pre>\n<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \nspark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          \n<\/code><\/pre>\n<p>However this outputted the following error:<\/p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org\/apache\/commons\/configuration\/Configuration\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)\n    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)\n    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)\n    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)\n    at scala.Option.getOrElse(Option.scala:189)\n    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)\n    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)\n    at scala.Option.map(Option.scala:230)\n    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)\n    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n    ... 20 more\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nInput In [7], in &lt;cell line: 3&gt;()\n      1 # Create Spark Session\n      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \n----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py:228, in SparkSession.Builder.getOrCreate(self)\n    226         sparkConf.set(key, value)\n    227     # This SparkContext may be an existing one.\n--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)\n    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230 # by all sessions.\n    231 session = SparkSession(sc)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:384, in SparkContext.getOrCreate(cls, conf)\n    382 with SparkContext._lock:\n    383     if SparkContext._active_spark_context is None:\n--&gt; 384         SparkContext(conf=conf or SparkConf())\n    385     return SparkContext._active_spark_context\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    140     raise ValueError(\n    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;\n    142         &quot; is not allowed as it is a security risk.&quot;)\n--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145 try:\n    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n    147                   conf, jsc, profiler_cls)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    329 with SparkContext._lock:\n    330     if not SparkContext._gateway:\n--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)\n    332         SparkContext._jvm = SparkContext._gateway.jvm\n    334     if instance:\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)\n    105     time.sleep(0.1)\n    107 if not os.path.isfile(conn_info_file):\n--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    110 with open(conn_info_file, &quot;rb&quot;) as info:\n    111     gateway_port = read_int(info)\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-17 22:00:01.223 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|apache-spark|deployment|amazon-sagemaker|mlflow",
        "Question_view_count":59,
        "Owner_creation_date":"2022-01-13 10:12:59.037 UTC",
        "Owner_last_access_date":"2022-09-23 16:16:27.333 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73015639",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68819792,
        "Question_title":"AWS Glue AccessDeniedException in SageMaker",
        "Question_body":"<p>I am running the &quot;Explain Credit Decisions&quot; solution from Sagemaker Studio. I am following the instructions in the solution notebooks. The solution has been launched with my root user id. But when running <code>1_datasets.ipynb<\/code> I am getting the below error when running the step\n<code>glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)<\/code><\/p>\n<pre><code>An error occurred (AccessDeniedException) when calling the GetJob operation: User: arn:aws:sts::myaccountid:assumed-role\/rolecreatedbystack\/GlueJobRunnerSession is not authorized to perform: glue:GetJob on resource: arn:aws:glue:us-east-1:myaccountid:job\/sagemaker-soln-ecd-js-foccb4-job\n<\/code><\/pre>\n<p>The Cloud Formation stacks are created and the scripts should create the required roles and access which are needed to run this solution.\nI have to run a POC with this solution with my custom data. So can you please help to solve the problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-17 15:07:02.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-14 14:33:07.127 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":610,
        "Owner_creation_date":"2020-12-09 19:30:03.403 UTC",
        "Owner_last_access_date":"2022-03-28 20:09:37.16 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68819792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73296477,
        "Question_title":"AWS Sagemaker Ground Truth - Validating Annotations and Removing Bad Labelers",
        "Question_body":"<p>I've been working on named entity recognition labeling jobs for mechanical turk workers to label a named entity dataset that I've compiled. However, when running tests on Ground Truth, the results were not adequate, as the Turk workers were either completely missing the labels, or intentionally labeling poorly and quickly to complete tasks quickly. Normally, when setting up a mechanical turk task on the standard mechanical turk interface, I can require qualifications or request to only have mechanical turk master workers on the project. However, this doesn't seem to be available when operating from Sagemaker and using Ground Truth to create a labeling job. How can I improve the performance of the turkers, or perhaps stop a turk worker from completing my job if their performance is too low? <a href=\"https:\/\/i.stack.imgur.com\/Tl1Po.png\" rel=\"nofollow noreferrer\">An example bad response from mechanical turk workers, where they label named entities as always Person on the first character. Probably botted turk accounts.<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-09 18:20:48.553 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":21,
        "Owner_creation_date":"2021-08-04 17:11:25.68 UTC",
        "Owner_last_access_date":"2022-09-24 05:01:04.74 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73296477",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68450665,
        "Question_title":"CloudWatch only capturing last metric produced by SageMaker training job",
        "Question_body":"<p>I am using Amazon SageMaker to train a PyTorch model and attempting to visualise the loss values in CloudWatch. I create my estimator:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(\n    entry_point=&quot;train.py&quot;,\n    source_dir=source_dir,\n    role=role,\n    framework_version=framework_version,\n    py_version=&quot;py3&quot;,\n    train_instance_count=1,\n    train_instance_type=instance_type,\n    hyperparameters=hyperparameters,\n    metric_definitions=[\n        {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;Train Loss:([0-9\\\\.]+)&quot;},\n        {&quot;Name&quot;: &quot;val:loss&quot;, &quot;Regex&quot;: &quot;Val Loss:([0-9\\\\.]+)&quot;},\n    ],\n    enable_sagemaker_metrics=True\n)\n<\/code><\/pre>\n<p>and execute the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>estimator.fit(s3_url)\n<\/code><\/pre>\n<p>Which runs successfully, but when I look at the algorithm metrics in CloudWatch for the training job this creates, it seems to only capture the last reported loss values. This is also the case when using the TrainingJobAnalytics:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.analytics import TrainingJobAnalytics\n\nanalysis = TrainingJobAnalytics(training_job_name=estimator._current_job_name)\ndf = analysis.dataframe()\ndf\n<\/code><\/pre>\n<p>where the output looks like:<\/p>\n<pre><code>    timestamp   metric_name value\n0         0.0   train:loss  0.471061\n1         0.0   val:loss    0.167700\n<\/code><\/pre>\n<p>In the CloudWatch logs there are multiple values being logged but they do not appear to be captured. I was wondering whether someone could provide some advice for how to fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2021-07-20 06:52:19.2 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":288,
        "Owner_creation_date":"2021-07-20 06:41:37.43 UTC",
        "Owner_last_access_date":"2022-09-21 04:34:13.04 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68450665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69472990,
        "Question_title":"Trained maskrcnn follwoing sagemaker example notebook, got warning: Your model will NOT be servable with SageMaker TensorFlow Serving container",
        "Question_body":"<p>I'm training a maskrcnn model following an AWS example <a href=\"https:\/\/test-j-zc5q.notebook.us-east-2.sagemaker.aws\/examples\/preview?example_id=%2Fhome%2Fec2-user%2Fsample-notebooks%2Fadvanced_functionality%2Fdistributed_tensorflow_mask_rcnn%2Fmask-rcnn-s3.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>The only thing that I didn't follow is I didn't use subnets and security_group_ids.\nI trained on the whole COCO2017 set, after 19 hours, the training status showed the training was completed.<\/p>\n<p>But in CloudWatch, the log shows &quot;Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure&quot;<\/p>\n<p>I'm very confused. Is it expected as the trained model of this maskrcnn? If so, it seems this model can not be deployed. Because according to this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html\" rel=\"nofollow noreferrer\">doc<\/a>, the deploy action is to happen within SageMaker TensorFlow Serving containers. And the truth is, I tried deploying it, and it failed.<\/p>\n<p>If anyone has successfully trained\/deployed a MaskRcnn model on SageMaker, or know anything about this issue, I'd appreciate it very much if you could share your insights! Great thanks!<\/p>\n<p>Following is the last few lines of the training log from CloudWatch:<\/p>\n<pre><code>0 S root      3225    33  0  80   0 -  1157 -      20:19 ?        00:00:00 \/bin\/sh -c ps -elf | grep &quot;python3 \/mask-rcnn-tensorflow\/MaskRCNN\/train.py&quot;\n\n0 S root      3227  3225  0  80   0 -  3303 -      20:19 ?        00:00:00 grep python3 \/mask-rcnn-tensorflow\/MaskRCNN\/train.py\n\ntraining processes running: 0\n\nWorker algo-2 training completed.\n\n2021-10-06 20:19:51,168 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\n\nhttps:\/\/www.tensorflow.org\/guide\/saved_model#structure_of_a_savedmodel_directory\n\n2021-10-06 20:19:51,169 sagemaker-containers INFO     Reporting training SUCCESS\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-06 21:41:13.827 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|deployment|model|containers|amazon-sagemaker",
        "Question_view_count":210,
        "Owner_creation_date":"2015-07-19 22:59:43.483 UTC",
        "Owner_last_access_date":"2022-03-16 15:06:11.817 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69472990",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71530663,
        "Question_title":"How to set max sequence length with a hugging face sagemaker estimator?",
        "Question_body":"<p>I'd like to increase the max sequence length from 128 to 512 (the maximum <a href=\"https:\/\/huggingface.co\/distilbert-base-uncased\" rel=\"nofollow noreferrer\">distilbert<\/a> can handle.) I believe it's only using 128 tokens right now, because the training samples it prints out have an attention_mask with 128 values. This is my code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.huggingface import HuggingFace\n\n# gets role for executing training job\nrole = sagemaker.get_execution_role()\nhyperparameters = {\n    'model_name_or_path': 'distilbert-base-uncased',\n    'output_dir': '\/opt\/ml\/model',\n    'do_predict': True,\n    'do_eval': True,\n    'do_train': True,\n    &quot;train_file&quot;: &quot;\/opt\/ml\/input\/data\/train\/train.csv&quot;,\n    &quot;validation_file&quot;: &quot;\/opt\/ml\/input\/data\/val\/val.csv&quot;,\n    &quot;test_file&quot;: &quot;\/opt\/ml\/input\/data\/test\/test.csv&quot;,\n    &quot;num_train_epochs&quot;: 50,\n    &quot;per_device_train_batch_size&quot;: 32\n}\n\n# git configuration to download our fine-tuning script\ngit_config = {'repo': 'https:\/\/github.com\/huggingface\/transformers.git','branch': 'v4.6.1'}\n\n# creates Hugging Face estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='run_glue.py',\n    source_dir='.\/examples\/pytorch\/text-classification',\n    instance_type='ml.p3.8xlarge',\n    instance_count=1,\n    role=role,\n    git_config=git_config,\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    #use_spot_instances = True,\n    #max_wait = 24*60*60+1,\n    hyperparameters = hyperparameters\n)\n\n# starting the train job\nhuggingface_estimator.fit({'train' : s3_input + &quot;\/train.csv&quot;,\n                           'val' : s3_input + &quot;\/val.csv&quot;,\n                           'test' : s3_input + &quot;\/test.csv&quot;})\n<\/code><\/pre>\n<p>Inspecting run_glue.py, input arguments are taken <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/v4.6.1\/examples\/pytorch\/text-classification\/run_glue.py#L186\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<pre><code>model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n<\/code><\/pre>\n<p>but the hyperparameters that we can <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/v4.6.1\/src\/transformers\/training_args.py\" rel=\"nofollow noreferrer\">set<\/a> only impact training_args. data_args gets used to set the max_seq_length <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/v4.6.1\/examples\/pytorch\/text-classification\/run_glue.py#L367\" rel=\"nofollow noreferrer\">later<\/a> in this file. I don't see an option in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/huggingface\/sagemaker.huggingface.html#hugging-face-estimator\" rel=\"nofollow noreferrer\">huggingface estimator<\/a> to pass anything other than hyperparameters. I could fork v4.6.1 and manually set this value, but it seems overkill, is there a proper way to just pass this value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-18 17:01:13.397 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":667,
        "Owner_creation_date":"2020-08-31 20:36:45.087 UTC",
        "Owner_last_access_date":"2022-08-14 20:02:38.203 UTC",
        "Owner_reputation":406,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71530663",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55645119,
        "Question_title":"How to remotely connect to GCP ML Engine\/AWS Sagemaker managed notebooks?",
        "Question_body":"<p>GCP has finally released managed Jupyter notebooks.  I would like to be able to interact with the notebook locally by connecting to it.  Ie. i use PyCharm to connect to the externaly configured jupyter notebbok server by passing its URL &amp; token param.<\/p>\n\n<p>Question also applies to AWS Sagemaker notebooks.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-12 05:39:06.513 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2019-07-01 00:14:19.343 UTC",
        "Question_score":8,
        "Question_tags":"amazon-web-services|google-cloud-platform|google-cloud-ml|amazon-sagemaker|gcp-ai-platform-notebook",
        "Question_view_count":3701,
        "Owner_creation_date":"2011-02-27 20:18:48.1 UTC",
        "Owner_last_access_date":"2022-09-24 00:14:23.18 UTC",
        "Owner_reputation":1367,
        "Owner_up_votes":655,
        "Owner_down_votes":16,
        "Owner_views":243,
        "Answer_body":"<p>On AWS, you can use AWS Glue to create a <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint.html\" rel=\"nofollow noreferrer\">developer endpoint<\/a>, and then you create the Sagemaker notebook from there. A developer endpoint gives you access to connect to your python or Scala spark REPL via ssh, and it also allows you to tunnel the connection and access from any other tool, including PyCharm.<\/p>\n\n<p>For PyCharm professional we have even <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint-tutorial-pycharm.html\" rel=\"nofollow noreferrer\">tighter integration<\/a>, allowing you to SFTP files and debug remotely.<\/p>\n\n<p>And if you need to install any dependencies on the notebook, apart from doing it directly on the notebook, you can always choose <code>new&gt;terminal<\/code> and you will have a connection to that machine directly from your jupyter environment where you can install <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">anything you want<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-12 08:29:21.75 UTC",
        "Answer_score":3.0,
        "Owner_location":"Los Angeles, CA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55645119",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68313912,
        "Question_title":"SageMaker fails to create sagemaker_data_wrangler database cause Lake Formation permissions",
        "Question_body":"<p>The Glue DataCatalog access is managed by Lake Formation. But when trying to add a new SageMaker Data Wrangler flow that queries an Athena table, it gives the following error:<\/p>\n<blockquote>\n<p>CustomerError: An error occurred when trying to create\nsagemaker_data_wrangler database in the Glue data catalog: An error\noccurred (AccessDeniedException) when calling the CreateDatabase\noperation: Insufficient Lake Formation permission(s): Required Create\nDatabase on Catalog<\/p>\n<\/blockquote>\n<p>The database sagemaker_data_wrangler does not exist, but we have add the default S3 bucket that uses SageMaker (sagemaker-{region}-{account}), to Lake Formation Data Location, in order to give the SageMaker execution role CreateDatabase privileges:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/S4c1A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/S4c1A.png\" alt=\"Lake Formation Data Location\" \/><\/a><\/p>\n<p>The error persists even if we manually create the database (sagemaker_data_wrangler) and give privileges to the Data Wrangler execution role.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-09 08:49:46.677 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|aws-lake-formation",
        "Question_view_count":211,
        "Owner_creation_date":"2017-09-14 19:07:09.77 UTC",
        "Owner_last_access_date":"2022-09-22 14:06:10.067 UTC",
        "Owner_reputation":98,
        "Owner_up_votes":42,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Barcelona, Spain",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68313912",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49655133,
        "Question_title":"How to convert a Amazon SageMaker model to a Tensorflow model",
        "Question_body":"<p>I have trained a model using Amazon SageMaker and I need to convert the model into a Tensorflow model. Is there any way it can be achieved? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2018-04-04 15:37:19.38 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":255,
        "Owner_creation_date":"2016-09-22 15:20:27.237 UTC",
        "Owner_last_access_date":"2020-03-12 18:44:22.64 UTC",
        "Owner_reputation":1039,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":89,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49655133",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52533975,
        "Question_title":"Data format for calling an AWS SageMaker object detection model",
        "Question_body":"<p>I have trained an object detection model in AWS SageMaker and created an endpoint for it. The endpoint is called via a lambda function that is accessed through an api gateway. So far so good.<\/p>\n\n<p>Now I want to call the api from an angular application - upload a picture and get back the predictions. But I am having trouble figuring out the correct way to do it. The aws documentation I've seen so far doesn't go into much detail on that part.<\/p>\n\n<p>I got the image as a blob, captured from an html canvas. I tried to convert the blob to a byte array:<\/p>\n\n<pre><code>    fileReader.onload = function () {\n    arrayBuffer = this.result;\n\n    var byteArray = new Uint8Array(arrayBuffer);\n\n    that.http.post&lt;any&gt;(that.url, byteArray.toString(), {\n      headers: new HttpHeaders().set('X-Api-Key', that.apiKey).set(\"Content-Type\", \"image\/jpeg\")\n    }).toPromise().then((result) =&gt; {\n      resolve(result);\n    });\n  };\n  fileReader.readAsArrayBuffer(blob);\n<\/code><\/pre>\n\n<p>The response is:<\/p>\n\n<pre><code>{\"message\":\"Received client error (400) from model with message \\\"unable to evaluate payload provided\\\".}\n<\/code><\/pre>\n\n<p>Has anyone done this yet? What is the correct way to submit an image?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2018-09-27 09:40:42.34 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":489,
        "Owner_creation_date":"2011-09-15 09:33:33.173 UTC",
        "Owner_last_access_date":"2022-09-23 13:35:09.13 UTC",
        "Owner_reputation":399,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52533975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59644059,
        "Question_title":"Saving Machine Learning Logs on MongoDB",
        "Question_body":"<p>How can i save the logs of machine learing model trained in sagemaker like (Creation time, time taken for a job to learn etc.) on mongoDB<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-08 10:39:26.177 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-08 21:33:35.53 UTC",
        "Question_score":0,
        "Question_tags":"mongodb|amazon-web-services|amazon-sagemaker",
        "Question_view_count":56,
        "Owner_creation_date":"2020-01-08 10:36:08.727 UTC",
        "Owner_last_access_date":"2020-01-22 07:42:41.22 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59644059",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72783781,
        "Question_title":"Getting R package 'bsts' to work on AWS Sagemaker Notebook Instance Python",
        "Question_body":"<p>Just wondering if anyone has been able to get a Python + R kernel working on AWS Sagemaker Notebook instance?<\/p>\n<p>The reason I'm asking is so I can use a python environment to run R packages within, specifically 'bsts' and 'boom'.<\/p>\n<p>Is there a way to create a kernel that has both Python + R installed?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-28 09:17:04.147 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|r|amazon-web-services|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_date":"2017-05-24 06:39:19.953 UTC",
        "Owner_last_access_date":"2022-09-23 17:33:31.663 UTC",
        "Owner_reputation":358,
        "Owner_up_votes":10,
        "Owner_down_votes":3,
        "Owner_views":84,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Wellington, New Zealand",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72783781",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62541587,
        "Question_title":"How to access python variables in Sagemaker Jupyter Notebook shell command",
        "Question_body":"<p>In one of the cells of Sagemaker notebook, I've set a variable<\/p>\n<pre><code>region=&quot;us-west-2&quot;\n<\/code><\/pre>\n<p>In subsequent cell, I run following 2 shell commands<\/p>\n<pre><code>!echo $region\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>us-west-2\n<\/code><\/pre>\n<p>However, unable to run aws shell command using this variable<\/p>\n<pre><code>!aws ecr get-login-password --region $region\n<\/code><\/pre>\n<p><code>$ variable-name<\/code> doesn't help inside jupyter cell <code>! shell command<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-23 18:35:08.72 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|bash|shell|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1285,
        "Owner_creation_date":"2015-07-26 12:56:05.627 UTC",
        "Owner_last_access_date":"2022-09-23 11:58:05.58 UTC",
        "Owner_reputation":2665,
        "Owner_up_votes":1461,
        "Owner_down_votes":14,
        "Owner_views":722,
        "Answer_body":"<p>As answered here: <a href=\"https:\/\/stackoverflow.com\/a\/19674648\/5157515\">https:\/\/stackoverflow.com\/a\/19674648\/5157515<\/a><\/p>\n<p>There's no direct way to access python variables with <code>!<\/code> command.<\/p>\n<p>But with magic command <code>%%bash<\/code> it is possible<\/p>\n<pre><code>%%bash  -s &quot;$region&quot;\naws ecr get-login-password --region $1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-26 05:52:54.773 UTC",
        "Answer_score":1.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62541587",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51430151,
        "Question_title":"Create AWS sagemaker endpoint and delete the same using AWS lambda",
        "Question_body":"<p>Is there a way to create sagemaker endpoint using AWS lambda ?<\/p>\n\n<p>The maximum timeout limit for lambda is 300 seconds while my existing model takes 5-6 mins to host ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-19 19:05:38.877 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":1559,
        "Owner_creation_date":"2018-07-19 19:01:45.327 UTC",
        "Owner_last_access_date":"2018-07-28 07:31:00.59 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Navi Mumbai, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51430151",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62185684,
        "Question_title":"Cannot update sklearn on Jupyter notebook",
        "Question_body":"<p>I am on a Sagemaker Jupyter notebook and I need to use version 0.22 or above to train and pickle my model. However I cannot update the version of sklearn.<\/p>\n\n<h2>Pip update<\/h2>\n\n<pre class=\"lang-py prettyprint-override\"><code>!pip3 install sklearn --upgrade\n\n<\/code><\/pre>\n\n<p>Output: <\/p>\n\n<pre><code>WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https:\/\/github.com\/pypa\/pip\/issues\/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\nRequirement already up-to-date: sklearn in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (0.0)\nRequirement already satisfied, skipping upgrade: scikit-learn in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from sklearn) (0.22.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.11.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from scikit-learn-&gt;sklearn) (1.18.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from scikit-learn-&gt;sklearn) (0.15.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.17.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from scikit-learn-&gt;sklearn) (1.4.1)\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\n\n# The scikit-learn version is 0.20.3. &lt;---- still 0.20\n<\/code><\/pre>\n\n<hr>\n\n<h2>Conda update<\/h2>\n\n<pre><code>!conda update scikit-learn -y\n<\/code><\/pre>\n\n<p>or<\/p>\n\n<pre><code>!conda update -n base scikit-learn -y\n<\/code><\/pre>\n\n<p>Output:<\/p>\n\n<pre><code>Solving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.12\n  latest version: 4.8.3\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n# All requested packages already installed.\n\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\n\n# The scikit-learn version is 0.20.3. &lt;---- still 0.20\n\n<\/code><\/pre>\n\n<p>I have run as well <code>conda update -n base -c defaults conda<\/code> or <code>conda update all<\/code><\/p>\n\n<p>and still the same version<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-04 02:04:02.47 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|scikit-learn|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1620,
        "Owner_creation_date":"2017-11-01 13:21:33.437 UTC",
        "Owner_last_access_date":"2022-09-23 06:48:45.417 UTC",
        "Owner_reputation":1582,
        "Owner_up_votes":868,
        "Owner_down_votes":21,
        "Owner_views":443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62185684",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68775733,
        "Question_title":"How to choose from a list of Regions for GPU instances in Amazon SageMaker?",
        "Question_body":"<p>I'm currently trying to do distributed GPU training with 2 instances of ml.p3.8xlarge and after 4 attempts I have not been able to start a training job with the spot instances since AWS did not have any available instances in my region.<\/p>\n<p>How do I increase the number of regions I'm willing to choose from in SageMaker? At the moment I'm only using:<\/p>\n<p><code>sess.boto_region_name = us-east-1<\/code> (sagemaker session region)<\/p>\n<p>But I'm assuming if I allow SageMaker to choose from other regions, I will be able to start a training job with spot instances.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-13 16:37:13.507 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-13 17:58:17.303 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":120,
        "Owner_creation_date":"2016-07-19 00:48:21.237 UTC",
        "Owner_last_access_date":"2022-09-13 07:31:55.037 UTC",
        "Owner_reputation":819,
        "Owner_up_votes":42,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Answer_body":"<p>Most AWS services are regional-based, meaning they run in a given region and do not spread <em>beyond<\/em> one region.<\/p>\n<p>If you wish to run SageMaker in multiple regions, you would need to launch it <em>separately<\/em> in each region. So, you would only be 'choosing' <em>one<\/em> region when requesting SageMaker to perform some work.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-14 03:51:49.04 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68775733",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56399173,
        "Question_title":"Sagemaker validation error about running its pre-built random-forest algorithm",
        "Question_body":"<p>I'm trying to run some training on the Sagemaker using Random-Forest and its giving me this validation error. I'm not sure if I need to adjust hyper-parameters. I tried but still an error. Here is the full text of the error.<\/p>\n\n<p>\"Failure reason\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: u'FullyReplicated' is not one of [u'ShardedByS3Key'] Failed validating u'enum' in schema[u'properties'][u'train'][u'properties'][u'S3DistributionType']: {u'enum': [u'ShardedByS3Key'], u'type': u'string'} On instance[u'train'][u'S3DistributionType']: u'FullyReplicated'\" <\/p>\n\n<p>I have tried different parameters - but I still get the same results.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-31 17:07:13.717 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-05-31 20:35:53.133 UTC",
        "Question_score":0,
        "Question_tags":"random-forest|amazon-sagemaker",
        "Question_view_count":503,
        "Owner_creation_date":"2018-12-23 00:19:51.677 UTC",
        "Owner_last_access_date":"2022-09-23 18:48:55.043 UTC",
        "Owner_reputation":91,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Kansas City, MO, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56399173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47625056,
        "Question_title":"Amazon Machine Learning and SageMaker algorithms",
        "Question_body":"<p>1) According to <a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html<\/a> Amazon ML uses SGD. However I can't find how many hidden layers are used in the neural network?<\/p>\n\n<p>2) Can someone confirm that SageMaker would be able to do what Amazon ML does? i.e. SageMaker is more powerful than Amazon ML?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-04 00:55:11.92 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2017-12-13 05:18:34.293 UTC",
        "Question_score":5,
        "Question_tags":"amazon-web-services|amazon-machine-learning|amazon-sagemaker",
        "Question_view_count":2794,
        "Owner_creation_date":"2010-01-25 07:55:40.363 UTC",
        "Owner_last_access_date":"2022-09-22 04:51:33.373 UTC",
        "Owner_reputation":2211,
        "Owner_up_votes":166,
        "Owner_down_votes":10,
        "Owner_views":176,
        "Answer_body":"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n\n<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).<\/p>\n\n<p>You can check out each of those containers to dive deep on how it all works.<\/p>\n\n<p>You can find an exhaustive list of available algorithms in SageMaker here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For now, as of December 2017, these algorithms are all available:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html\" rel=\"noreferrer\">Linear Learner<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines.html\" rel=\"noreferrer\">Factorization Machines<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"noreferrer\">XGBoost Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"noreferrer\">Image Classification Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq.html\" rel=\"noreferrer\">Amazon SageMaker Sequence2Sequence<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/k-means.html\" rel=\"noreferrer\">K-Means Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pca.html\" rel=\"noreferrer\">Principal Component Analysis (PCA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda.html\" rel=\"noreferrer\">Latent Dirichlet Allocation (LDA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"noreferrer\">Neural Topic Model (NTM)<\/a><\/li>\n<\/ul>\n\n<p>The general SageMaker SDK interface to these algorithms looks something like this:<\/p>\n\n<pre><code>from sagemaker import KMeans\nkmeans = KMeans(role=\"SageMakerRole\",\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                data_location=\"s3:\/\/training_data\/\",\n                output_path=\"s3:\/\/model_artifacts\/\",\n                k=10)\n<\/code><\/pre>\n\n<p>The libraries here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples<\/a>\nand here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a> are particularly useful for playing with SageMaker.<\/p>\n\n<p>You can also make use of Spark with SageMaker the Spark library here: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2017-12-05 11:00:55.643 UTC",
        "Answer_score":7.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2017-12-06 15:55:02.21 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47625056",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71136388,
        "Question_title":"How to invoke a serverless endpoint in aws sagemaker?",
        "Question_body":"<p>based on aws documetation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html<\/a>) ,<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;&lt;your-endpoint-configuration&gt;&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;&lt;your-model-name&gt;&quot;,\n            &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n<\/code><\/pre>\n<p>i created an serverless endpoint (sample code above) , but I keep getting error when the endpoint is invoked , has anyone run into this issue - 'Error - \/ .sagemaker\/ts\/models\/model.mar already exists. Please specify --force\/-f option to overwrite the model archive output file' . FYI - this worked when the endpoint was configured provisioned instead of serverless.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-16 04:25:16.763 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":271,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71136388",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63679256,
        "Question_title":"AWS S3 ValueError: Integer column has NA values in column 63",
        "Question_body":"<p>When I am attempting to pull the content of the files into a CSV file, I get this error: ValueError: Integer column has NA values in column 63. The strange thing is that this error was not occurring before, i.e. I didn't not touch this code and 1 day it was working and the next it wasn't. This inconsistent behavior is the issue and I am not sure what to do.<\/p>\n<p>I hope someone knows that the issue is.\nThanks!\n<a href=\"https:\/\/i.stack.imgur.com\/pxbQH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pxbQH.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2020-08-31 23:01:13.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":78,
        "Owner_creation_date":"2017-04-24 21:09:06.483 UTC",
        "Owner_last_access_date":"2022-02-12 05:45:53.177 UTC",
        "Owner_reputation":425,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":225,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63679256",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67185505,
        "Question_title":"Python logging module doesn't work in Sagemaker",
        "Question_body":"<p>I've got a sagemaker instance running a jupyter notebook.  I'd like to use python's logging module to write to a log file, but it doesn't work.<\/p>\n<p>My code is pretty straightforward:<\/p>\n<pre><code>import logging\n\nlogger = logging.getLogger()\nformatter = logging.Formatter(&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;, datefmt=&quot;%y\/%m\/%d %H:%M:%S&quot;)\nfhandler = logging.FileHandler(&quot;taxi_training.log&quot;)\nfhandler.setFormatter(formatter)\nlogger.addHandler(fhandler)\n\nlogger.debug(&quot;starting log...&quot;)\n<\/code><\/pre>\n<p>This should write a line to my file taxi_training.log but it  doesn't.<\/p>\n<p>I tried using the reload function from importlib, I also tried setting the output stream to sys.stdout explicitly.  Nothing is logging to the file or in cloudwatch.<\/p>\n<p>Do I need to add anything to my Sagemaker instance for this to work properly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-20 19:54:30.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|logging|amazon-sagemaker",
        "Question_view_count":720,
        "Owner_creation_date":"2013-08-16 20:23:45.67 UTC",
        "Owner_last_access_date":"2022-09-23 21:13:36.407 UTC",
        "Owner_reputation":1105,
        "Owner_up_votes":658,
        "Owner_down_votes":2,
        "Owner_views":222,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67185505",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66861409,
        "Question_title":"delete_model() error when cleaning up AWS sagemaker",
        "Question_body":"<p>I followed the tutorial on <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n<p>I got an error when trying to clean up with the following code.<\/p>\n<pre><code>xgb_predictor.delete_endpoint()\nxgb_predictor.delete_model()\n<\/code><\/pre>\n<p>ClientError: An error occurred (ValidationException) when calling the DescribeEndpointConfig operation: Could not find the endpoint configuration.<\/p>\n<p>Does it mean I need to delete the model first instead?<\/p>\n<p>I checked on the console and deleted the model manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-29 20:45:40.983 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|resource-cleanup",
        "Question_view_count":336,
        "Owner_creation_date":"2019-08-23 20:16:54.837 UTC",
        "Owner_last_access_date":"2021-05-13 23:08:45.31 UTC",
        "Owner_reputation":37,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66861409",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66793845,
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-25 05:55:45.117 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-25 06:08:33.867 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker|imread",
        "Question_view_count":164,
        "Owner_creation_date":"2018-03-01 20:34:46.96 UTC",
        "Owner_last_access_date":"2021-10-20 03:57:09.683 UTC",
        "Owner_reputation":1113,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":122,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2021-03-25 14:57:52.013 UTC",
        "Answer_score":1.0,
        "Owner_location":"Minneapolis, MN, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71777914,
        "Question_title":"Change AWS SageMaker LogGroup Prefix?",
        "Question_body":"<p>We have applications for multiple tenants on our AWS account and would like to distinguish between them in different IAM roles. In most places this is already possible by limiting resource access based on naming patterns.<\/p>\n<p>For CloudWatch log groups of SageMaker training jobs however I have not seen a working solution yet. The tenants can choose the job name arbitrarily, and hence the only part of the LogGroup name that is available for pattern matching would be the prefix before the job name. This prefix however seems to be fixed to <code>\/aws\/sagemaker\/TrainingJobs<\/code>.<\/p>\n<p>Is there a way to change or extend this prefix in order to make such limiting possible? Say, for example <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-&lt;stage&gt;-&lt;component&gt;\/&lt;training-job-name&gt;-...<\/code> so that a resource limitation like <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-*<\/code> becomes possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-07 07:22:33.02 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-cloudwatchlogs",
        "Question_view_count":48,
        "Owner_creation_date":"2009-11-06 19:20:37.563 UTC",
        "Owner_last_access_date":"2022-09-23 13:54:36.657 UTC",
        "Owner_reputation":1904,
        "Owner_up_votes":58,
        "Owner_down_votes":9,
        "Owner_views":321,
        "Answer_body":"<p>I think it is not possible to change the log streams names for any of the SageMaker services.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-04-30 01:17:31.297 UTC",
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71777914",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56854482,
        "Question_title":"Loading non-s3 data source for Sagemaker Java SDK",
        "Question_body":"<p>I'm setting up a lambda function that performs SageMaker inferences using the Random Cut Forest algorithm. I successfully ran the python RCF example in a jupyter notebook, but I prefer to have my lambda written in Java, and I was hoping to not use an s3 bucket in the process. I know that if my lambda was in python, then I would be able to pass in my data, convert it to a pandas dataframe, then run <code>rcf_inference.predict(data)<\/code> to obtain my results. But with Java, this is seemingly impossible.<\/p>\n\n<p>I was able to make a batch transform job using the following Java code, but after examining the java aws-sdk documentation, it looks like my TransformDataSource <em>must<\/em> have an S3DataSource, and not any other kind of source. This is unfortunate because my lambda already has the data I need in memory, so uploading\/downloading to\/from an s3 bucket is unnecessary.<\/p>\n\n<pre><code>        String bucket = \"test-bucket441\";\n        String prefix = \"sagemaker\/rcf-benchmarks\";\n        String trainedModel = \"randomcutforest-2019-06-28-13-43-00-925\";\n\n        AmazonSageMaker sm = AmazonSageMakerClientBuilder.standard().build();\n\n        TransformS3DataSource s3Source = new TransformS3DataSource()\n                .withS3DataType(\"S3Prefix\")\n                .withS3Uri(\"s3:\/\/\" + bucket + \"\/\" + prefix);\n\n        TransformDataSource dataSource = new TransformDataSource()\n                .withS3DataSource(s3Source);\n\n        TransformInput input = new TransformInput()\n                .withContentType(\"text\/csv\")\n                .withDataSource(dataSource);\n\n        TransformOutput output = new TransformOutput()\n                .withS3OutputPath(\"s3:\/\/\" + bucket + \"\/\" + prefix + \"\/output\")\n                .withAssembleWith(\"Line\");\n\n        TransformResources resources = new TransformResources()\n                .withInstanceType(\"ml.m4.xlarge\")\n                .withInstanceCount(1);\n\n        CreateTransformJobRequest jobRequest = new CreateTransformJobRequest()\n                .withTransformJobName(\"test-job\")\n                .withModelName(trainedModel)\n                .withTransformInput(input)\n                .withTransformOutput(output)\n                .withTransformResources(resources);\n\n\n        sm.createTransformJob(jobRequest);\n<\/code><\/pre>\n\n<p>Does anyone know any way I can create a CreateTranformJobRequest without using an s3 bucket?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-02 14:14:47.677 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"java|aws-sdk|amazon-sagemaker",
        "Question_view_count":139,
        "Owner_creation_date":"2018-05-16 16:59:46.91 UTC",
        "Owner_last_access_date":"2022-09-23 17:32:20.187 UTC",
        "Owner_reputation":926,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":48,
        "Answer_body":"<p>your Python code uses a SageMaker HTTPS endpoint for real-time prediction: although the SageMaker SDK is Python only, you can absolutely do the same thing with the (lower level) AWS SDK for Java.<\/p>\n\n<p>Assuming you've already trained your model in SageMaker, you would:<\/p>\n\n<ul>\n<li>create an endpoint configuration, <\/li>\n<li>create an endpoint,<\/li>\n<li>invoke the endpoint.<\/li>\n<\/ul>\n\n<p>The corresponding APIs are detailed in:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemakerruntime\/AmazonSageMakerRuntime.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemakerruntime\/AmazonSageMakerRuntime.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-08 13:34:56.757 UTC",
        "Answer_score":1.0,
        "Owner_location":"Philadelphia, PA, USA",
        "Answer_last_edit_date":"2019-07-09 04:40:06.95 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56854482",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203561,
        "Question_title":"How to automatically stop Sagemaker notebook instances if it is idle?",
        "Question_body":"<p>I have been looking for a script to automatically close Sagemaker Notebook Instances that have been forgotten to be closed or that are idle. A few scripts I found don't work very well (eg: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\/auto-stop-idle\" rel=\"nofollow noreferrer\">link<\/a> , it is only checking if ipynb file is live, Im not using .ipynb, or taking the last updated info which never changes until you shut down or open the instance)\nIs there a resource or script you can recommend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-11 15:17:10.79 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|instance|amazon-sagemaker|lifecycle",
        "Question_view_count":399,
        "Owner_creation_date":"2022-03-08 01:35:40.96 UTC",
        "Owner_last_access_date":"2022-09-21 14:44:11.647 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203561",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62222912,
        "Question_title":"Python binary data save as png",
        "Question_body":"<p>So I've been banging my head against this for hours and I can't quite get it to work.<\/p>\n\n<p>The file is sent as image\/png and comes across in what looks like binary data if I print it.<\/p>\n\n<p>The problem I'm getting is commented #PROBLEM HERE - essential PIL can't read the file back, it's 100% in the directory.<\/p>\n\n<pre><code>cannot identify image file '\/tmp\/image.jpg'\n<\/code><\/pre>\n\n<p>I tried JSON etc too and base64 but ended up in same circle, it's going to be something really silly.<\/p>\n\n<p>I'm using Postman to test Sagemaker endpoint integration - here's my code - I'm including the lot because hopefully you see whilst this is a dumb question I've put a LOT of work into this I'm relatively new to python.<\/p>\n\n<p>In order to make this work in Lambda I had to drag modules from both MXNet, Sagemaker, and numerous .so from the ubuntu file system and upload as part of the package so that it met the space requirements for Lambda.(End result is 100mb lambda package, which is better than the 300 I started with.)<\/p>\n\n<pre><code>sys.path.append('.\/libs')\nfrom record_pb2 import Record\nfrom recordio import MXRecordIO\nruntime= boto3.client('runtime.sagemaker')\n\n    # event, context\n    def handler(event, context):\n\n      print(event)\n      print('Body')\n      # tried 'w' here too.\n      with open('\/tmp\/image.jpg' , 'wb') as file:\n        # Tried without encode and no 'b' above.\n        file.write(event['body'].encode())\n\n      files = os.listdir('\/tmp')\n      print(files)\n\n      im = PIL.Image.open('\/tmp\/image.jpg')\n      im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n      im.save('\/tmp\/resized.jpg', \"JPEG\")\n      files = os.listdir('\/tmp')\n      print(files)\n\n\n      with open('\/tmp\/resized.jpg', 'r') as image:\n        img = image.read()\n        img = bytearray(img)\n\n      response = runtime.invoke_endpoint(EndpointName='ss-notebook-demo-2020-06-02-09-27-40-405',\n                                          ContentType='image\/jpeg',\n                                          Accept='application\/x-protobuf',\n                                          Body=img)\n\n      print(response)\n\n      result = response['Body'].read()\n\n      results_file = '\/temp\/results.rec'\n\n      with open(results_file, 'wb') as f:\n          f.write(result)\n\n      rec = Record()\n      recordio = MXRecordIO(results_file, 'r')\n      protobuf = rec.ParseFromString(recordio.read())\n\n      values = list(rec.features[\"target\"].float32_tensor.values)\n      shape = list(rec.features[\"shape\"].int32_tensor.values)\n      shape = np.squeeze(shape)\n      mask = np.reshape(np.array(values), shape)\n      mask = np.squeeze(mask, axis=0)\n\n      pred_map = np.argmax(mask, axis=0)\n      unique_elements, counts_elements = np.unique(pred_map[pred_map != 0], return_counts=True)\n      #print(unique_elements, counts_elements)\n      #print(pred_map[pred_map != 0].size)\n      #print(np.bincount(pred_map[pred_map != 0]).argmax())\n\n      return {\n        'classes': unique_elements,\n        'counts': counts_elements,\n        'top': np.bincount(pred_map[pred_map != 0]).argmax(),\n      }\n<\/code><\/pre>\n\n<p>For those that are interested that right there is how you get classes and pixel count PER class from a semantic segmentation recordio-protobuf in Sagemaker! :-)<\/p>\n\n<p>Update:<\/p>\n\n<p>Body sample, note this isn't the whole thing because it's huge.<\/p>\n\n<pre><code>'body': '\ufffd\ufffd\ufffd\ufffd\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\ufffd\ufffd\\x00C\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\ufffd\ufffd\\x00C\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\ufffd\ufffd\\x00\\x11\\x08\\x07\ufffd\\x05d\\x03\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\ufffd\ufffd\\x00\\x1f\\x00\\x00\\x01\\x05\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\ufffd\ufffd\\x00\ufffd\\x10\\x00\\x02\\x01\\x03\\x03\\x02\\x04\\x03\\x05\\x05\\x04\\x04\\x00\\x00\\x01}\\x01\\x02\\x03\\x00\\x04\\x11\\x05\\x12!1A\\x06\\x13Qa\\x07\"q\\x142\ufffd\ufffd\ufffd\\x08#B\ufffd\ufffd\\x15R\ufffd\ufffd$3br\ufffd\\t\\n\\x16\\x17\\x18\\x19\\x1a%&amp;\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\\x00\\x1f\\x01\\x00\\x03\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\ufffd\ufffd\\x00\ufffd\\x11\\x00\\x02\\x01\\x02\\x04\\x04\\x03\\x04\\x07\\x05\\x04\\x04\\x00\\x01\\x02w\\x00\\x01\\x02\\x03\\x11\\x04\\x05!1\\x06\\x12AQ\\x07aq\\x13\"2\ufffd\\x08\\x14B\ufffd\ufffd\ufffd\ufffd\\t#3R\ufffd\\x15br\ufffd\\n\\x16$4\ufffd%\ufffd\\x17\\x18\\x19\\x1a&amp;\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\ufffd*\\x03\ufffd\\n8\ufffd\\x06R\ufffd\ufffd\ufffd\ufffd\\x17\ufffdvs\ufffd\\x04\ufffd\ufffdg\\x14\ufffd)3\ufffd\ufffd2\ufffdk\ufffd7\ufffdK\\x029\ufffd\ufffd\\x1f\ufffd\ufffd4&lt;\ufffd|\ufffd\ufffdI\ufffd\\'heN\ufffd\ufffd\\x1f\ufffd^s\ufffd3\ufffdq\ufffd\\x16\\x17\ufffd\u0258\\'\ufffd3\\x1c\ufffd\u0651\ufffd~G\\x07\u061a\ufffd+\ufffd\ufffd\\x7f\\x17\ufffdG%\ufffd\u0513R\ufffd\ufffd\\x17\\x7f\ufffd),! \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2020-06-05 19:33:00.207 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-05 21:52:30.983 UTC",
        "Question_score":0,
        "Question_tags":"python|python-imaging-library|amazon-sagemaker",
        "Question_view_count":1023,
        "Owner_creation_date":"2012-01-29 01:50:08.98 UTC",
        "Owner_last_access_date":"2022-09-24 18:50:39.96 UTC",
        "Owner_reputation":3841,
        "Owner_up_votes":141,
        "Owner_down_votes":1,
        "Owner_views":364,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, United Kingdom",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62222912",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63930864,
        "Question_title":"SageMaker Predictor only outputs 0 and 1, i need the float between 0 and 1",
        "Question_body":"<p>I have trained a TensorFlow model on Amazon SageMaker with the following architecture:<\/p>\n<pre><code>_, timesteps, features = X_train.shape\nACTIVATION = 'relu'\nEPOCHS = 50\nmodel = Sequential()\nmodel.add(Masking(mask_value=np.nan, input_shape=(timesteps, features)))\nmodel.add(LSTM(64, activation=ACTIVATION, return_sequences=True))\nmodel.add(LSTM(64, activation=ACTIVATION, return_sequences=True))\nmodel.add(LSTM(64, activation=ACTIVATION))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation=ACTIVATION))\nmodel.add(Dense(64, activation=ACTIVATION))\nmodel.add(Dense(64, activation=ACTIVATION))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(64, activation=ACTIVATION))\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam_low = Adam(learning_rate = 0.0001)\n\nmodel.compile(optimizer=adam_low,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n<\/code><\/pre>\n<p>If I trained this in my local machine and do a prediction it will output float between 0 and 1.<\/p>\n<p>But when I trained the same exact architecture with SageMaker with TensorFlow container, after deploying the saved model on an endpoint and calling <code>prediction = predictor.predict(x_predict)<\/code> the output is always either 0 or 1 only. Is there a way to force the prediction to output 0-1 float?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-17 03:33:11.107 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_date":"2020-09-17 03:23:01.543 UTC",
        "Owner_last_access_date":"2022-06-14 04:42:00.187 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Philippines",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63930864",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56969859,
        "Question_title":"AWS: FileNotFoundError: [Errno 2] No such file or directory",
        "Question_body":"<p>I am trying to download a file to sagemaker from my S3 bucket.<\/p>\n\n<p>the path of the file is\n<code>s3:\/\/vemyone\/input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>The path of that file is stored as a list element at <code>train_fns[0]<\/code>.<\/p>\n\n<p>the value of <code>train_fns[0]<\/code> is <\/p>\n\n<p><code>input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>I used the following code:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0][:], train_fns[0])\n<\/code><\/pre>\n\n<p>but I get the following error:<\/p>\n\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: 'input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm.5b003ba1'<\/code><\/p>\n\n<p>I notice that some characters have appended itself at the end of the path.<\/p>\n\n<p>how do I solve this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-10 11:34:22.203 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":14766,
        "Owner_creation_date":"2018-03-22 16:57:14.36 UTC",
        "Owner_last_access_date":"2022-08-17 07:47:32.16 UTC",
        "Owner_reputation":1303,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":139,
        "Answer_body":"<p>please see <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file<\/a><\/p>\n\n<p>by the doc, first argument is file key, second argument is path for local file:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0], '\/path\/to\/local\/file')\n<\/code><\/pre>",
        "Answer_comment_count":8.0,
        "Answer_creation_date":"2019-07-10 11:44:36.923 UTC",
        "Answer_score":3.0,
        "Owner_location":"Pondicherry, Puducherry, India",
        "Answer_last_edit_date":"2019-07-10 13:15:19.093 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56969859",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53280902,
        "Question_title":"AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?",
        "Question_body":"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.<\/p>\n\n<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.<\/p>\n\n<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()<\/code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)<\/p>\n\n<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.<\/p>\n\n<p>The following code for csv's works but I can't use it due to the index problem:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')\n<\/code><\/pre>\n\n<p>But this code does NOT work even though it uses almost the exact same syntax:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')\n<\/code><\/pre>\n\n<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).<\/p>\n\n<p>It throws me this error while using the Pickle method:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-peno\/compressed_data\/house_1.pkl'\n<\/code><\/pre>\n\n<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()<\/code> issue or as an alternative fix my datetime64 type issue with csv's.<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-13 12:20:48.037 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pandas|csv|amazon-s3|pickle|amazon-sagemaker",
        "Question_view_count":788,
        "Owner_creation_date":"2015-08-09 12:51:41.797 UTC",
        "Owner_last_access_date":"2022-07-07 20:29:24.31 UTC",
        "Owner_reputation":87,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Answer_body":"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-26 20:32:53.167 UTC",
        "Answer_score":-1.0,
        "Owner_location":"Leuven, Belgi\u00eb",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53280902",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54193723,
        "Question_title":"Size of image for prediction with SageMaker object detection?",
        "Question_body":"<p>I'm using the AWS SageMaker \"built in\" object detection algorithm (SSD) and we've trained it on a series of annotated 512x512 images (image_shape=512).  We've deployed an endpoint and when using it for prediction we're getting mixed results.  <\/p>\n\n<p>If the image we use for prediciton is around that 512x512 size we're getting great accuracy and good results.  If the image is significantly larger (e.g. 8000x10000) we get either wildly inaccurate, or no results.  If I manually resize those large images to 512x512pixels the features we're looking for are no longer discernable to the eye.  Which suggests that if my endpoint is resizing images, then that would explain why the model is struggling.<\/p>\n\n<p><strong>Note:<\/strong> Although the size in pexels is large, my images are basically line drawings on a white background. They have very little color and large patches of solid white, so they compress very well.  I'm mot running into the 6Mb request size limit.<\/p>\n\n<p>So, my questions are:<\/p>\n\n<ol>\n<li>Does training the model at image_shape=512 mean my prediction images should also be that same size?<\/li>\n<li>Is there a generally accepted method for doing object detection on very large images?  I can envisage how I might chop the image into smaller tiles then feed each tile to my model, but if there's something \"out of the box\" that will do it for me, then that'd save some effort.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-01-15 06:28:29.27 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"aws-sdk|object-detection|amazon-sagemaker",
        "Question_view_count":771,
        "Owner_creation_date":"2008-11-18 05:09:29.4 UTC",
        "Owner_last_access_date":"2022-09-21 04:09:53.41 UTC",
        "Owner_reputation":5789,
        "Owner_up_votes":532,
        "Owner_down_votes":9,
        "Owner_views":464,
        "Answer_body":"<p>Your understanding is correct. The endpoint resizes images based on the parameter <code>image_shape<\/code>. To answer your questions:<\/p>\n\n<ol>\n<li>As long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work.<\/li>\n<li>Cropping is one option. Another method is to train separate models for large and small images as David suggested.<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-01-15 20:32:38.317 UTC",
        "Answer_score":1.0,
        "Owner_location":"Adelaide, Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54193723",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65706899,
        "Question_title":"AWS lambda to run a sagemaker notebook but theres an error with imports via terminal",
        "Question_body":"<p>I have created a lambda function that runs a notebook on a sagemaker instance. The lambda accesses and runs the notebook but it fails to import packages. The lambda function runs the notebook via terminal so my issue lies there.\nAs you can see in the image below, datetime and urllib dont throw an error but beautiful soup does. Beautiful soup is installed via lifecylle configuration everytime the instance is started (also via lambda function).<\/p>\n<p>What am i doing wrong please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oK7dG.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oK7dG.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-01-13 17:29:56.043 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":213,
        "Owner_creation_date":"2019-05-22 09:45:28.153 UTC",
        "Owner_last_access_date":"2022-09-02 09:43:13.34 UTC",
        "Owner_reputation":79,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65706899",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68141648,
        "Question_title":"An error occurred (InternalFailure) when calling the InvokeEndpoint operation: An exception occurred while sending request to model",
        "Question_body":"<p>I am trying to host an XGBoost model that I have trained locally on an AWS Sagemaker Endpoint but I am receiving the following error when invoking the endpoint:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request.<\/p>\n<\/blockquote>\n<p>The model works as expected locally and I save it using the following before uploading to S3:<\/p>\n<pre><code>model.fit(args)\nmodel.save_model(model_save_loc)\nmodel_tar_loc = model_save_loc + '.tar.gz'\n!tar czvf $model_tar_loc $model_save_loc\n<\/code><\/pre>\n<p>I am hosting the model through the MultiDataModel function,<\/p>\n<pre><code>container = retrieve(&quot;xgboost&quot;, region, &quot;1.3-1&quot;)\nmme = MultiDataModel(\n    name=model_name,\n    role=role,\n    model_data_prefix=model_data_prefix,\n    image_uri=container,\n    sagemaker_session=sagemaker_session,\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1, instance_type=instance_type, endpoint_name=model_name,     \n)\n<\/code><\/pre>\n<p>The MultiDataModel deploy works as expected with no errors, and if I do:<\/p>\n<pre><code>list(mme.list_models())\n<\/code><\/pre>\n<p>It returns the expected list of models:<\/p>\n<pre><code>model_1.tar.gz\nmodel_2.tar.gz\netc..\n<\/code><\/pre>\n<p>I invoke the model using the following:<\/p>\n<pre><code>runtime_client = boto3.client(&quot;runtime.sagemaker&quot;)\n\nresponse = runtime_client.invoke_endpoint(\n    EndpointName=&quot;model_name&quot;, ContentType=&quot;text\/csv&quot;, Body=payload, TargetModel='model_1.tar.gz'\n)\nresult = response[&quot;Body&quot;].read().decode(&quot;ascii&quot;)\n<\/code><\/pre>\n<p>I have experimented with various ways of creating the payload but none change the error message.<\/p>\n<p>The local XGBoost model was trained using XGBoost version 1.3.1 (same as the Docker version).<\/p>\n<p>CloudWatch provides only the following:<\/p>\n<blockquote>\n<p>2021-06-26 10:48:36,865 [INFO ] pool-1-thread-1 ACCESS_LOG - \/10.32.0.2:37106 &quot;GET \/ping HTTP\/1.1&quot; 200 0<\/p>\n<\/blockquote>\n<p>There is no way of contacting customer support through the basic plan, as advised by the error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-26 10:55:10.927 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-26 11:12:24.193 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":815,
        "Owner_creation_date":"2017-09-16 12:24:56.4 UTC",
        "Owner_last_access_date":"2022-09-08 15:07:12.237 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68141648",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54268970,
        "Question_title":"Wrong dense layer output shape after moving from TF 1.12 to 1.10",
        "Question_body":"<p>I'm migrating from Tensorflow 1.12 to Tensorflow 1.10 (Collaboratory -> AWS sagemaker), the code seems to be working fine in Tensorflow 1.12 but in 1.10 i get an error <code>ValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (52692,)<\/code><\/p>\n\n<p>Input example - strings with no whitespaces: <\/p>\n\n<pre><code>[\"testAbc\", \"aaDD\", \"roam\"]\n<\/code><\/pre>\n\n<p>which I preprocess by changing small letters into 1, capital letters 2, digits - 3, '-' - 4, '_' - 5 and padding so they are equal length with 0s<\/p>\n\n<p>and 4 labels a - 0, b - 1, c - 2, d - 3<\/p>\n\n<p>Assuming max length for each word is 10 (in my code it's 20):<\/p>\n\n<p>features - [[1 1 1 1 2 1 1 0 0 0][1 1 2 2 0 0 0 0 0 0][1 1 1 1 0 0 0 0 0 0]]<\/p>\n\n<p>labels - [1, 1, 2, 3]<\/p>\n\n<p>expected output: [a: 0%, b: 0%, c: 1%, d: 99%] (example)<\/p>\n\n<pre><code>model = keras.Sequential()\nmodel.add(\n    keras.layers.Embedding(6, 8, input_length=maxFeatureLen))\nmodel.add(keras.layers.LSTM(12))\nmodel.add(keras.layers.Dense(4, activation=tf.nn.softmax))\nmodel.compile(tf.train.AdamOptimizer(0.001), loss=\"sparse_categorical_crossentropy\")\nmodel.fit(train[\"featuresVec\"],\n            train[\"labelsVec\"],\n            epochs=1,\n            verbose=1,\n            callbacks=[],\n            validation_data=(evale[\"featuresVec\"], evale[\"labelsVec\"],),\n            validation_steps=evale[\"count\"],\n            steps_per_epoch=train[\"count\"])\n<\/code><\/pre>\n\n<p>Shapes of train and evale - 2D arrays<\/p>\n\n<pre><code>train[\"featuresVec\"]=\n[[1 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\nevale[\"featuresVec\"]=\n[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 2 1 1 1 1 1 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 0]\n [1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0]]\n\ntrain[\"labelsVec\"] = [1 0 0 0 2]\nevale[\"labelsVec\"] = [0 1 1 1 1]\n<\/code><\/pre>\n\n<p>Shapes:<\/p>\n\n<pre><code>train[\"featuresVec\"] = [52692, 20]\nevale[\"featuresVec\"] = [28916, 20]\ntrain[\"labelsVec\"] = [52692]\nevale[\"labelsVec\"] = [28916]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2019-01-19 16:12:07.393 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-01-19 16:38:55.247 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":120,
        "Owner_creation_date":"2018-11-13 21:54:35.153 UTC",
        "Owner_last_access_date":"2019-04-13 20:25:53.21 UTC",
        "Owner_reputation":91,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":"<p>Probably your labels vector needs to be of shape <code>(batch_size, 1)<\/code> instead of just <code>(batch_size,)<\/code>. <\/p>\n\n<p><strong>Note:<\/strong> Since you are using <code>sparse_categorical_crossentropy<\/code> as loss function instead of <code>categorical_crossentropy<\/code>, it is correct to not one-hot encode the labels. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-01-19 17:02:45.403 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54268970",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53624568,
        "Question_title":"AWS Ground Truth text classification manifest using \"source-ref\" not displaying text",
        "Question_body":"<h2>Background<\/h2>\n\n<p>I'm trying out SageMaker Ground Truth, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms.html\" rel=\"nofollow noreferrer\">an AWS service to help you label your data before using it in your ML algorithms<\/a>.<\/p>\n\n<p>The labeling job requires a manifest file which contains a JSON object per row that contains a <code>source<\/code> or a <code>source-ref<\/code>, see also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">Input Data section<\/a> of the documentation. <\/p>\n\n<h2>Setup<\/h2>\n\n<p>Source-ref is a reference to where the document is located in an S3 bucket like so<\/p>\n\n<pre><code>my-bucket\/data\/manifest.json\nmy-bucket\/data\/123.txt\nmy-bucket\/data\/124.txt\n\n...\n<\/code><\/pre>\n\n<p>The manifest file looks like this (based on the <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/\" rel=\"nofollow noreferrer\">blog example<\/a>) :<\/p>\n\n<pre><code>{\"source-ref\": \"s3:\/\/my-bucket\/data\/123.txt\"}\n{\"source-ref\": \"s3:\/\/my-bucket\/data\/124.txt\"}\n...\n<\/code><\/pre>\n\n<h2>The problem<\/h2>\n\n<p>When I create the job, all I get is the <code>source-ref<\/code> value: <strong>s3:\/\/my-bucket\/data\/123.txt<\/strong> as the text, the contents of the file are not displayed.<\/p>\n\n<p>I have tried creating jobs using a manifest that does not contain the s3 protocol, but I get the same result.<\/p>\n\n<p>Is this a bug on their end or I'm I missing something?<\/p>\n\n<h2>Observations<\/h2>\n\n<ul>\n<li>I have tried to make all files public, thinking there may maybe permissions issue? but no<\/li>\n<li>I ensured that the content type of the file was text (s3 -> object -> properties -> metadata)<\/li>\n<li>If I use \"source\" and inline the text, it works properly, but I should be able to use individual documents as there is a limit on the file size specially if I have to label many or large documents!<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-05 03:05:58.243 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1506,
        "Owner_creation_date":"2009-08-12 17:14:36.47 UTC",
        "Owner_last_access_date":"2022-09-21 03:46:43.037 UTC",
        "Owner_reputation":7013,
        "Owner_up_votes":2704,
        "Owner_down_votes":18,
        "Owner_views":445,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Santa Monica, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53624568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55787726,
        "Question_title":"Train and deploy model with pre-created Keras architecture in Sagemaker with 'entrypoint'",
        "Question_body":"<p><strong>Problem preface:<\/strong>\nI have a database of user created neural network architectures (written in a different language that I transcompile to a Keras model) stored in MongoDB. My goal is to take these architectures, create a Keras model with them, then train them in the cloud using SageMaker. As of right now, I can load the models from MongoDB and transcompile them to Keras perfectly fine. <em>However, I have trouble sending these dynamically created models to SageMaker using the Python SDK.<\/em> <\/p>\n\n<p>Is there a way to train and deploy these Keras model architectures - I.E just Python Keras model objects - in SageMaker by specifying the <code>entry_point<\/code> attribute of an estimator as a file that has these model objects defined? <\/p>\n\n<p><strong>Work to Date &amp; Code Example<\/strong>\nAs of right now, I can create a training job and deploy an endpoint when the model architecture is defined in a separate file. See this example of the separate file and the deployment\/training process on <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_keras_cifar10\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">SageMaker's GitHub.<\/a><\/p>\n\n<p><strong>train-and-deploy-sagemaker.py<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\n# Import Sagemaker Tensorflow\nfrom sagemaker.tensorflow import TensorFlow\n\n# Create an estimator object using the entry_point file entry_point.py\nestimator = TensorFlow(entry_point='entry_point.py',\n                       role=arn_role,\n                       framework_version='1.12.0',\n                       hyperparameters={...some hyperparams for the model...},\n                       training_steps=1000, \n                       evaluation_steps=100,\n                       train_instance_count=4, train_instance_type='ml.p3.8xlarge')\n\n# Start the training job to train the above estimator\nestimator.fit(training_data_inputs)\n\n# Deploy said estimator after training\npredictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n\n\n<\/code><\/pre>\n\n<p><strong>entry_point.py<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\ndef keras_model_fn(hyperparameters):\n    \"\"\"keras_model_fn receives hyperparameters from the training job and returns a compiled keras model.\n    The model will be transformed into a TensorFlow Estimator before training and it will be saved in a \n    TensorFlow Serving SavedModel at the end of training.\n\n    Args:\n        hyperparameters: The hyperparameters passed to the SageMaker TrainingJob that runs your TensorFlow \n                         training script.\n    Returns: A compiled Keras model\n    \"\"\"\n    model = Sequential()\n\n    ... add layers ...\n\n    return model\n\ndef train_input_fn():\n    ...\n\n# other functions for inference and training, see link above\n\n<\/code><\/pre>\n\n<p>However, is there a way I could define that architecture dynamically? I.E grab the pre-written architecture from MongoDB then transcompile it into the same <code>Sequential<\/code> Keras model in <code>entrypoint.py<\/code>? <\/p>\n\n<p><strong>Potential ideas and concerns:<\/strong><\/p>\n\n<ol>\n<li><p><strong>Idea:<\/strong> Just grab the models from MongoDB and do the transcompiling from within the <code>entry_point<\/code> file. Then each method required by AWS can reference the compiled model object.<\/p>\n\n<p><strong>Concern:<\/strong> Is that secure or best practice given AWS will create a VM from this file to run the code in their cloud? Also the source is later stored in an S3 bucket, so that might pose another security risk regardless of permissions. Also, dependencies like pymongo cannot be loaded from within the <code>entry_point<\/code> file, making the fetching of the data impossible without changing the training image. <\/p><\/li>\n<li><p><strong>Idea:<\/strong> Do the fetching and transcompiling within the file that creates the training job and deployment instance - <code>train-and-deploy-sagemaker.py<\/code> above. Then pass some code that can reconstruct the model - like Keras model JSON - through the hyperparams attribute within the estimator. <\/p>\n\n<p><strong>Concern:<\/strong> Hyperparams can only be 256 chars long according to AWS.<\/p><\/li>\n<li><p><strong>Idea:<\/strong> Dynamically generate the <code>entry_point<\/code> file based on the model architecture it needs to contain. <\/p>\n\n<p><strong>Concern:<\/strong> Many such as not wanting to create a one-off file on a server for unnecessary I\/O reasons, generating code is messy and bad practice, and there has got to be a better way.<\/p><\/li>\n<li><p><strong>Idea:<\/strong> Make the <code>entry_point<\/code> attribute a non external file and instead specify the required methods within the file where the estimator is created. This would ostensibly solve all of my problems, but ... <\/p>\n\n<p><strong>Concern:<\/strong> I have seen nothing about this in the SageMaker documentation. Nonetheless, this is the most ideal.<\/p><\/li>\n<\/ol>\n\n<p>Any help would be appreciated &amp; thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-21 23:49:38.95 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-04-23 05:20:38.697 UTC",
        "Question_score":2,
        "Question_tags":"python|mongodb|amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":838,
        "Owner_creation_date":"2019-04-21 23:15:51.513 UTC",
        "Owner_last_access_date":"2021-05-14 20:39:37.92 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"San Francisco Bay Area, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55787726",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-11 15:25:28.443 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_date":"2017-03-16 17:36:25.577 UTC",
        "Owner_last_access_date":"2022-09-22 21:22:01.683 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-05-11 15:43:32.907 UTC",
        "Answer_score":0.0,
        "Owner_location":"Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56422325,
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Question_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-03 06:50:07.823 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-03 07:07:19.143 UTC",
        "Question_score":0,
        "Question_tags":"boto3|amazon-sagemaker",
        "Question_view_count":599,
        "Owner_creation_date":"2013-04-02 09:58:26.347 UTC",
        "Owner_last_access_date":"2022-09-25 05:14:00.05 UTC",
        "Owner_reputation":6584,
        "Owner_up_votes":477,
        "Owner_down_votes":15,
        "Owner_views":962,
        "Answer_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-03 13:02:56.623 UTC",
        "Answer_score":3.0,
        "Owner_location":"Noida, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61870000,
        "Question_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Question_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-18 12:50:56.067 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling",
        "Question_view_count":738,
        "Owner_creation_date":"2017-03-13 03:58:08.79 UTC",
        "Owner_last_access_date":"2021-03-15 17:36:41.343 UTC",
        "Owner_reputation":437,
        "Owner_up_votes":18,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Answer_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-20 14:53:01.867 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61870000",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59516365,
        "Question_title":"Amazon SageMaker Ground Truth Custom Labeling Jobs Error: Cannot read property 'taskInput' of null",
        "Question_body":"<p>When creating a custom labeling job for Amazon SageMaker Ground Truth Custom \nAm getting the following error:<\/p>\n\n<p><code>Cannot read property 'taskInput' of null<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-29 02:07:12.053 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"label|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_date":"2013-08-22 03:12:19.087 UTC",
        "Owner_last_access_date":"2022-09-22 05:41:01.377 UTC",
        "Owner_reputation":6358,
        "Owner_up_votes":4611,
        "Owner_down_votes":1,
        "Owner_views":229,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Tokyo, Japan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59516365",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70025851,
        "Question_title":"\"Your invocation timed out while waiting for a response from container primary\". What does this error mean?",
        "Question_body":"<p>I have a semantic segmentation model which I deployed on ml.m4.xlarge I am using invoke_endpoint from inside an AWS Lambda function using the following bit of code.<\/p>\n<pre><code>with open('\\tmp\\image.jpg', 'rb') as imfile:\n    imbytes = imfile.read()\n\nresponse = runtime.invoke_endpoint(EndpointName = 'xyx', ContentType = 'image\/jpeg',\n                                   Body = imbytes)\n<\/code><\/pre>\n<p>This is when I get the error as mentioned above<\/p>\n<pre><code>Your invocation timed out while waiting for a response from container primary\n<\/code><\/pre>\n<p>Does it mean my datapoint is reaching the model endpoint but it's taking too long to do the inference or is my data not even transferring over to the endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-18 19:46:43.973 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-18 21:02:20.063 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|deployment|amazon-sagemaker",
        "Question_view_count":923,
        "Owner_creation_date":"2017-07-02 18:59:25.057 UTC",
        "Owner_last_access_date":"2022-09-22 19:21:39.343 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Kolkata, West Bengal, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70025851",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57730978,
        "Question_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Question_body":"<p>I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\nThe name of the inputs are:<\/p>\n\n<pre><code>1.    input_image\n2.    input_image_meta\n3.    input_anchors\n<\/code><\/pre>\n\n<p>and the name of outputs are:<\/p>\n\n<pre><code>1    output_detections\n2    output_mrcnn_class\n3    output_mrcnn_bbox\n4    output_mrcnn_mask\n5    output_rois\n<\/code><\/pre>\n\n<p>I have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting {'error': \"Missing 'inputs' or 'instances' key\"} in return.<\/p>\n\n<p>The sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\nOn the client side, I call the predictor using follwoing code:<\/p>\n\n<pre><code>request = {}\nrequest[\"img_link\"] = \"image.jpg\"\nresult = predictor.predict(request)\n<\/code><\/pre>\n\n<p>But when I print the result the following gets printed out, {'error': \"Missing 'inputs' or 'instances' key\"}\nAll the bucket connections for loading the image are in inference.py<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-08-30 17:28:20.26 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"json|tensorflow|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":798,
        "Owner_creation_date":"2019-08-30 17:20:07.19 UTC",
        "Owner_last_access_date":"2019-09-03 13:23:31.813 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57730978",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58325923,
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-10 15:01:44.473 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":568,
        "Owner_creation_date":"2016-03-04 20:36:10.467 UTC",
        "Owner_last_access_date":"2022-09-22 17:34:48.727 UTC",
        "Owner_reputation":246,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-07 12:59:51.227 UTC",
        "Answer_score":4.0,
        "Owner_location":"Denver",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70750358,
        "Question_title":"Automate the date parameter while deplying the model on AWS Wrangler",
        "Question_body":"<p>I have built a XGBoost model on my local machine which takes a training data and validates the model on a testing dataset. However, I have hard-coded the date values as the training data is created monthly. The training data gets created based on what Date Parameter I pass. Eg, jan = dt(2021,1,1).<\/p>\n<p>I now have to automate the process as the model has to be deployed on AWS and should run monthly without editing the code. How should I pass the date parameter to AWS Wrangler so that the process will be automated, and the code will execute once every month on a new dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-18 04:39:04.327 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-30 23:18:54.013 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|date|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":127,
        "Owner_creation_date":"2020-12-10 11:10:08.387 UTC",
        "Owner_last_access_date":"2022-07-28 04:13:43.8 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70750358",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56353814,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-29 05:43:39.707 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1826,
        "Owner_creation_date":"2019-05-29 03:08:01.007 UTC",
        "Owner_last_access_date":"2021-06-28 08:28:46.13 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-31 19:12:00.12 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55067802,
        "Question_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Question_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-08 17:04:24.58 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-03-08 17:09:42.86 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_date":"2016-05-02 16:52:37.683 UTC",
        "Owner_last_access_date":"2022-05-24 16:30:17.263 UTC",
        "Owner_reputation":1305,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":174,
        "Answer_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-03-14 02:31:54.763 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58985124,
        "Question_title":"Why does AWS SageMaker run a web server for batch transform?",
        "Question_body":"<p>I'm creating my own Docker container for use with SageMaker and I'm wondering why the serve command creates a Flask app to serve predictions on data when I want to do a batch transform job. Wouldn't it be simpler to just unpickle the model and run the model's predict method on the dataset I want predictions for? I don't need a web api\/endpoint. I just need to automatically generate predictions once a day.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-21 23:11:19.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|docker|machine-learning|amazon-sagemaker",
        "Question_view_count":470,
        "Owner_creation_date":"2015-11-07 11:23:08.123 UTC",
        "Owner_last_access_date":"2021-09-14 15:16:31.59 UTC",
        "Owner_reputation":473,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>Good question :) using the exact same code for batch inference and online inference reduces development overhead - the exact same stack can be used for both use-cases - and also reduces risks of having different results between something done in Batch and something done online. That being said, SageMaker is very flexible and what you describe can easily be done using the Training API. There is nothing in the Training API forcing you to use it for ML training, it is actually a very versatile docker orchestrator with advanced logging, metadata persistance, and built for fast and distributed data ingestion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-22 22:49:29.597 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58985124",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61842665,
        "Question_title":"Unable to export Core ML model in Turicreate",
        "Question_body":"<p>I used AWS Sagemaker with Jupyter notebook to train my Turicreate model. It trained successfully but I'm unable to export it to a CoreML model. It shows the below error. I've tried various kernels in the Jupyter notebook with the same result. Any ideas on how to fix this error?<\/p>\n\n<p>turicreate 5.4\nGPU: mxnet-cu100<\/p>\n\n<pre><code>KeyError  Traceback (most recent call last)\n&lt;ipython-input-6-3499bdb76e06&gt; in &lt;module&gt;()\n  1 # Export for use in Core ML\n----&gt; 2 model.export_coreml('pushupsTC.mlmodel')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/turicreate\/toolkits\/object_detector\/object_detector.py in export_coreml(self, filename,     include_non_maximum_suppression, iou_threshold, confidence_threshold)\n1216         assert (self._model[23].name == 'pool5' and\n1217                 self._model[24].name == 'specialcrop5')\n-&gt; 1218         del net._children[24]\n1219         net._children[23] = op\n1220 \n\nKeyError: 24\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-16 19:44:49.3 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|jupyter-notebook|amazon-sagemaker|coreml|turi-create",
        "Question_view_count":102,
        "Owner_creation_date":"2013-01-31 11:20:45.423 UTC",
        "Owner_last_access_date":"2022-09-24 15:45:35.447 UTC",
        "Owner_reputation":363,
        "Owner_up_votes":1041,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61842665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51858379,
        "Question_title":"how to link s3 bucket to sagemaker notebook",
        "Question_body":"<p>I am trying to link my s3 bucket to a notebook instance, however i am not able to:<\/p>\n\n<p>Here is how much I know:<\/p>\n\n<pre><code>from sagemaker import get_execution_role\n\nrole = get_execution_role\nbucket = 'atwinebankloadrisk'\ndatalocation = 'atwinebankloadrisk'\n\ndata_location = 's3:\/\/{}\/'.format(bucket)\noutput_location = 's3:\/\/{}\/'.format(bucket)\n<\/code><\/pre>\n\n<p>to call the data from the bucket:<\/p>\n\n<pre><code>df_test = pd.read_csv(data_location\/'application_test.csv')\ndf_train = pd.read_csv('.\/application_train.csv')\ndf_bureau = pd.read_csv('.\/bureau_balance.csv')\n<\/code><\/pre>\n\n<p>However I keep getting errors and unable to proceed.\nI haven't found answers that can assist much.<\/p>\n\n<p>PS: I am new to this AWS<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-15 12:05:35.12 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":13863,
        "Owner_creation_date":"2017-09-14 09:04:01.387 UTC",
        "Owner_last_access_date":"2022-09-21 09:28:36.2 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>You can load S3 Data into AWS SageMaker Notebook by using the sample code below. Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. <\/p>\n\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> <\/p>\n\n<pre><code>import boto3 \nimport botocore \nimport pandas as pd \nfrom sagemaker import get_execution_role \n\nrole = get_execution_role() \n\nbucket = 'Your_bucket_name' \ndata_key = your_data_file.csv' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \n\npd.read_csv(data_location) \n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-08-21 17:43:00.927 UTC",
        "Answer_score":5.0,
        "Owner_location":"Kampala, Uganda",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51858379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70587853,
        "Question_title":"Sagemaker studio does not load up",
        "Question_body":"<p>Sagemaker Studio worked flawlessly for me for the first 6 months. Then I started observing this issue. <a href=\"https:\/\/i.stack.imgur.com\/Ry5Se.png\" rel=\"nofollow noreferrer\">Screenshot of the error message<\/a><\/p>\n<p>The screen holds up at this stage forever. Here's what I have tried:<\/p>\n<ol>\n<li>Clearing my cache, even using a different machine. So I don't think the issue lies with the browser or my machine.<\/li>\n<li>Pressing 'Clear workspace' in the screenshot above.<\/li>\n<li>Shutting down all the apps in my sagemaker domain (excluding the 'default' app). This used to work initially but now this has stopped working all-together.<\/li>\n<li>Created a new sagemaker domain with fraction of the files in the previous domain. Still, I see the same error message in the new domain as well.<\/li>\n<\/ol>\n<p>This is severely affecting my work and I can't find a solution for this anywhere on the internet.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 04:37:09.417 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|artificial-intelligence|data-science|amazon-sagemaker",
        "Question_view_count":1024,
        "Owner_creation_date":"2022-01-05 04:28:13.653 UTC",
        "Owner_last_access_date":"2022-09-24 20:01:00.357 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70587853",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47946790,
        "Question_title":"AWS Sagemaker - Access Denied",
        "Question_body":"<p>I'm working through some of the example Sagemaker notebooks, and I receive the following Access Denied error when trying to run the linear_time_series_forecast example:<\/p>\n\n<p>ValueError: Error training linear-learner-2017-12-21-15-29-34-676: Failed Reason: ClientError: Data download failed:AccessDenied (403): Access Denied<\/p>\n\n<p>I can manually download and upload from my S3 bucket using the AWS command line interface, but the Jupyter notebook fails. <\/p>\n\n<p>Can someone please help me with this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2017-12-22 20:02:48.297 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":10206,
        "Owner_creation_date":"2017-05-25 00:08:25.787 UTC",
        "Owner_last_access_date":"2018-12-27 20:23:47.16 UTC",
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>Looks like this question was also answered on the AWS Forums: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270054&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270054&amp;tstart=0<\/a><\/p>\n\n<p>The IAM Role referenced by<\/p>\n\n<blockquote>\n  <p>role = get_execution_role()<\/p>\n<\/blockquote>\n\n<p>needs to have a policy attached to it that grants S3:GetObject permission on the S3 bucket holding your training data.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-12-26 23:55:00.483 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47946790",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72601750,
        "Question_title":"ROLLBACK_IN_PROGRESS status after creating dask-fargate-stack on AWS CloudFormation",
        "Question_body":"<p>I am following <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/machine-learning-on-distributed-dask-using-amazon-sagemaker-and-aws-fargate\/\" rel=\"nofollow noreferrer\">this guide<\/a> to be able to use dask distributed on my sagemaker instance, so I can train my big data regression model, but when I create the stack, I get the status of ROLLBACK_IN_PROGRESS.<\/p>\n<p>How can I manually create the stack for dask distributed? I can't seem to find the steps, and I guess the amazon's template for dask does not work, or maybe it's something else.<\/p>\n<p>I tried using coiled for dask distributed on Sagemaker, but when coiled asks me for my token, I paste the token and I get an error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 11:00:58.55 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-19 09:14:31.32 UTC",
        "Question_score":1,
        "Question_tags":"dask|distributed-computing|amazon-sagemaker|dask-distributed|distributed-training",
        "Question_view_count":90,
        "Owner_creation_date":"2020-04-17 01:31:21.62 UTC",
        "Owner_last_access_date":"2022-09-24 12:53:17.577 UTC",
        "Owner_reputation":113,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Caracas, Venezuela",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72601750",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69365768,
        "Question_title":"Sagemaker: How to customize tensorflow-serving arguments",
        "Question_body":"<p>I have long running requests that are currently timing out under sagemaker and so am trying to increase the timeout. This is normally accomplished with the <code>rest_api_timeout_in_ms<\/code> argument to <code>tensorflow_model_server<\/code>. In order to change this argument, I've created a custom image with the following docker file:<\/p>\n<pre><code>FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.5.1-cpu-py37-ubuntu18.04\n\nRUN echo '#!\/bin\/bash \\n\\n' &gt; \/usr\/bin\/tf_serving_entrypoint.sh \\\n &amp;&amp; echo '\/usr\/bin\/tensorflow_model_server --port=8500 --rest_api_port=8501 --rest_api_timeout_in_ms=600000 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}\/${MODEL_NAME} &quot;$@&quot;' &gt;&gt; \/usr\/bin\/tf_serving_entrypoint.sh \\\n &amp;&amp; chmod +x \/usr\/bin\/tf_serving_entrypoint.sh\n\nCMD [&quot;\/usr\/bin\/tf_serving_entrypoint.sh&quot;]\n<\/code><\/pre>\n<p>However, sagemaker does not appear to actually be using this entrypoint, as the cloudwatch logs show the following:<\/p>\n<pre><code>INFO:__main__:tensorflow serving command: tensorflow_model_server --port=23000 --rest_api_port=23001 --model_config_file=\/sagemaker\/model-config.cfg --max_num_load_retries=0\n<\/code><\/pre>\n<p>My only conclusion is that somewhere deep within the bowels of boto3\/sagemaker sdk, this command is being altered.<\/p>\n<p>How do I modify this command to increase the timeout?<\/p>\n<p>Here's my deployment code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker_session = sagemaker.Session()\n    model_data = sagemaker_session.upload_data(path=&quot;model.tar.gz&quot;, key_prefix=MODEL_NAME)\n\n    s3_bucket = sagemaker_session.default_bucket()\n    model_s3_key = &quot;s3:\/\/&quot; + sagemaker_session.default_bucket() + f&quot;\/{MODEL_NAME}\/model.tar.gz&quot;\n\n    create_model_name = sagemaker_session.create_model(\n        name=MODEL_NAME,\n        role=SAGEMAKER_ROLE,\n        container_defs={\n            &quot;Image&quot;: IMAGE_URI,  # Path to my custom image\n            &quot;ModelDataUrl&quot;: model_s3_key,\n        },\n    )\n\n    sagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\n\n    create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name, \n        ProductionVariants=[\n            {\n                &quot;VariantName&quot;: &quot;variant1&quot;, # The name of the production variant.\n                &quot;ModelName&quot;: MODEL_NAME,\n                &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;, # Specify the compute instance type.\n                &quot;InitialInstanceCount&quot;: 1 # Number of instances to launch initially.\n            }\n        ]\n    )\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-28 16:59:07.1 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":67,
        "Owner_creation_date":"2013-11-12 21:01:29.407 UTC",
        "Owner_last_access_date":"2022-09-25 05:27:34.497 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":406,
        "Owner_down_votes":10,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69365768",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73205813,
        "Question_title":"On sagemaker in multicontainer endpoint ping isn't able to call",
        "Question_body":"<p>I'm also facing the same issue. If I'm deploying one container on one endpoint, then it works perfectly.\nBut when I'm trying to deploy the multiple containers on an endpoint. Then the serve file is not going inside the ping function, which means the container cannot ping.<\/p>\n<p>Any Suggestions will be appreciatable\nThanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-02 10:15:03.27 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":53,
        "Owner_creation_date":"2019-03-08 06:36:42.977 UTC",
        "Owner_last_access_date":"2022-09-16 06:57:33.663 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Dayal Bagh, Agra, Uttar Pradesh, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73205813",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62931618,
        "Question_title":"use global variables in AWS Sagemaker script",
        "Question_body":"<p>After having correctly deployed our model, I need to invoke it via lambda function. The script features two cleaning function, the first one (cleaning()) gives us 5 variables: the cleaned dataset and 4 other variables (scaler, monthdummies, compadummies, parceldummies) that we need to use in the second cleaning function (cleaning_test()).<\/p>\n<p>The reason behind this is that in the use case I'll have only one instance at a time to perform predictions on, not an entire dataset. This means that I pass the row to the first cleaning() function since some commands won't work. I can't also use a scaler and neither create dummy variables, so the aim is to import the scaler and some dummies used in the cleaning() function, since they come from the whole dataset, that I used to train the model.<\/p>\n<p>Hence, in the input_fn() function, the input needs to be cleaned using the cleaning_test() function, that requires the scaler and the three lists of dummies from the cleaning() one.<\/p>\n<p>When I train the model, the cleaning() function works fine, but after the deployment, if we invoke the endpoint, it raises the error that variable &quot;scaler&quot; is not defined.<\/p>\n<p>Below is the script.py:\nNote that the test is # since I've already tested it, so now I'm training on the whole dataset and I want to predict completely new instances<\/p>\n<pre><code>def cleaning(data):\n    some cleaning on data stored in s3\n    return cleaned_data, scaler, monthdummies, compadummies, parceldummies\n\ndef cleaning_test(data, scaler, monthdummies, compadummies, parceldummies):\n    cleaning on data without labels\n    return cleaned_data\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n\n\n\ndef input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/json&quot;:\n        data = json.loads(request_body)\n        df = pd.DataFrame(data, index = [0])\n        input_data = cleaning_test(df, scaler, monthdummies, compadummies, parceldummies)\n    else:\n        pass\n    return input_data\n\n        \ndef predict_fn(input_data, model):\n    return model.predict_proba(input_data)\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--n_estimators', type=int, default=10)\n    parser.add_argument('--min-samples-leaf', type=int, default=3)\n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    #parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='fp_train.csv')\n    #parser.add_argument('--test-file', type=str, default='fp_test.csv')\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    #test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    \n    print(&quot;cleaning&quot;)\n    train_df, scaler, monthdummies, compadummies, parceldummies = cleaning(train_df)\n    #test_df, scaler1, monthdummies1, compadummies1, parceldummies1 = cleaning(test_df)\n    \n    print(&quot;splitting&quot;)\n    y = train_df.loc[:,&quot;event&quot;]\n    X = train_df.loc[:, train_df.columns != 'event']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n    &quot;&quot;&quot;print('building training and testing datasets')\n    X_train = train_df.loc[:, train_df.columns != 'event']\n    X_test = test_df.loc[:, test_df.columns != 'event']\n    y_train = train_df.loc[:,&quot;event&quot;]\n    y_test = test_df.loc[:,&quot;event&quot;]&quot;&quot;&quot;\n    \n    print(X_train.columns)\n    print(X_test.columns)\n    \n\n\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        n_estimators=args.n_estimators,\n        min_samples_leaf=args.min_samples_leaf,\n        n_jobs=-1)\n    \n    model.fit(X_train, y_train)\n\n    # print abs error\n    print('validating model')\n    proba = model.predict_proba(X_test)\n\n    \n    # persist model\n    path = os.path.join(args.model_dir, &quot;model.joblib&quot;)\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n\n<\/code><\/pre>\n<p>That I run through:<\/p>\n<pre><code>sklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    train_instance_count=1,\n    train_instance_type='ml.c5.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit',\n    hyperparameters = {'n_estimators': 15})\n\nsklearn_estimator.fit({'train':trainpath})\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = sm_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\n\npredictor = sklearn_estimator.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1)\n<\/code><\/pre>\n<p>The question is, how can I &quot;store&quot; the variables given by the cleaning() function during the training process, in order to use them in the input_fn() function, making cleaning_test() work fine?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-16 09:29:22.437 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2018-11-02 10:55:54.73 UTC",
        "Owner_last_access_date":"2022-09-23 12:09:30.577 UTC",
        "Owner_reputation":119,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Italia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62931618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49665836,
        "Question_title":"Unable to install spacy on AWS Sagemaker",
        "Question_body":"<p>I'm trying to load spacy into SageMaker. I run the following in Jupyter notebook instance<\/p>\n\n<pre><code>!pip install spacy\n<\/code><\/pre>\n\n<p>I end up getting this error<\/p>\n\n<pre><code>  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n  error: command 'gcc' failed with exit status 1\n<\/code><\/pre>\n\n<p>and this as well<\/p>\n\n<pre><code>gcc: error: murmurhash\/mrmr.cpp: No such file or directory\nerror: command 'gcc' failed with exit status 1\n<\/code><\/pre>\n\n<p>How can I resolve this issue withing Sagemaker?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_date":"2018-04-05 06:50:42.367 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-07-10 08:34:22.797 UTC",
        "Question_score":4,
        "Question_tags":"linux|python-3.x|amazon-web-services|spacy|amazon-sagemaker",
        "Question_view_count":4798,
        "Owner_creation_date":"2014-09-17 16:42:55.307 UTC",
        "Owner_last_access_date":"2022-09-22 07:49:15.917 UTC",
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49665836",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67899421,
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-09 07:26:08.407 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-09 07:36:28.497 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|keras|amazon-ec2|amazon-sagemaker",
        "Question_view_count":316,
        "Owner_creation_date":"2015-03-18 10:49:38.223 UTC",
        "Owner_last_access_date":"2022-09-24 14:02:18.917 UTC",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-10 10:19:58.007 UTC",
        "Answer_score":0.0,
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66325857,
        "Question_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Question_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-23 01:40:59.343 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|boto3|amazon-sagemaker",
        "Question_view_count":1369,
        "Owner_creation_date":"2016-12-30 23:20:08.953 UTC",
        "Owner_last_access_date":"2022-08-24 20:44:43.537 UTC",
        "Owner_reputation":604,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":35,
        "Answer_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-20 11:05:59.233 UTC",
        "Answer_score":1.0,
        "Owner_location":"San Francisco Bay Area, CA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66048199,
        "Question_title":"What is the best and correct way of hosting an endpoint running R code?",
        "Question_body":"<p>I think it must be a relatively common use case to load a model and invoke an endpoint to call R's <code>predict(object, newdata, ...)<\/code> function.  I wanted to do this with a custom AWS Sagemaker container, using <code>plumber<\/code> on the R side.  This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">example<\/a> gives all the details, I think, and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">this bit of documentation<\/a> also explains how the container should be built and react.\nI followed the steps of these documents, but I get<\/p>\n<blockquote>\n<p>The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.<\/p>\n<\/blockquote>\n<p>in the Sagemaker console after a couple of long minutes, and the endpoint creation fails.<\/p>\n<p>This is my container:<\/p>\n<pre><code># --- Dockerfile\nFROM rocker\/r-base\nRUN apt-get -y update &amp;&amp; apt-get install -y libsodium-dev libcurl4-openssl-dev\nRUN apt-get install -y  \\\n    ca-certificates\n\nRUN R -e &quot;install.packages(c('lme4', 'plumber'))&quot;\n\nADD .\/plumber.R \/\n\nENTRYPOINT [&quot;R&quot;, &quot;-e&quot;, &quot;plumber::pr_run(plumber::pr('plumber.R'), port=8080)&quot;, \\\n            &quot;--no-save&quot;]\n<\/code><\/pre>\n<pre><code># --- plumber.R\nlibrary(plumber)\nlibrary(lme4)\n\nprefix &lt;- '\/opt\/ml'\nprint(dir('\/opt\/ml', recursive = TRUE))\nmodel &lt;- readRDS(file.path(prefix, 'model', 'model.RDS'))\n\n#* @apiTitle Guess the likelihood of something\n\n#' Ping to show server is there\n#' @get \/ping\nfunction() {\n  print(paste('successfully pinged at', Sys.time()))\n  return('')}\n\n#' Parse input and return prediction from model\n#' @param req The http request sent\n#' @post \/invocations\nfunction(req) {\n  print(paste('invocation triggered at', Sys.time()))\n  conn &lt;- textConnection(gsub('\\\\\\\\n', '\\n', req$postBody))\n  data &lt;- read.csv(conn)\n  close(conn)\n  \n  print(data)\n  \n  predict(model, data,\n          allow.new.levels = TRUE,\n          type = 'response')\n}\n\n<\/code><\/pre>\n<p>And then the endpoint is created using this code:<\/p>\n<pre><code># run_on_sagemaker.py\n# [...]\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': s3_model_location\n    }\n)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType': instance_type,\n        'InitialInstanceCount': 1,\n        'ModelName': model_name,\n        'VariantName': 'AllTraffic'}])\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response['EndpointConfigArn'])\n\nprint('Endpoint Response:')\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\nprint(create_endpoint_response['EndpointArn'])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp['EndpointStatus']\nprint(&quot;Status: &quot; + status)\n\ntry:\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\nfinally:\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(&quot;Arn: &quot; + resp['EndpointArn'])\n    print(&quot;Status: &quot; + status)\n    if status != 'InService':\n        raise Exception('Endpoint creation did not succeed')\n    print(create_model_response['ModelArn'])\n<\/code><\/pre>\n<p>Most code is actually copied from the abovementioned example, the most significant difference I note is that in my container the model is loaded right away while in the example it loads the model object every time an invocation is made (which must be slowing responses down, so i wonder, why?).<\/p>\n<p>The logs on Cloudwatch equal the output of the container when it's run locally and indicate no failure.  Locally I can query the container with\n<code>curl -d &quot;data\\nin\\ncsv\\nformat&quot; -i localhost:8080\/invocations<\/code> and it works fine and gives back a prediction for every row in the POST data.  Also, <code>curl localhost:8080\/ping<\/code> returns <code>[&quot;&quot;]<\/code>, as it should, I think.  And it shows no signs of being slow, the model object is a 4.4MiB in size (although this is to be extended greatly once this simple version runs).<\/p>\n<p>The error on the terminal is<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 57, in &lt;module&gt;\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 53, in wait\n    Waiter.wait(self, **kwargs)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 320, in wait\n    raise WaiterError(\nbotocore.exceptions.WaiterError: Waiter EndpointInService failed: Waiter encountered a terminal failure state\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 64, in &lt;module&gt;\n    raise Exception('Endpoint creation did not succeed')\n\n<\/code><\/pre>\n<p>So, why is this failing on the Sagemaker console?  Is this a good way, are there better ways, and how can I do further diagnostics?  Generally, I also could not get the AWS example (see above) for your own R container running, so I wonder what the best way to run R predictions of a Sagemaker model is.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-04 15:00:42.99 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-02-04 15:08:26.6 UTC",
        "Question_score":1,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker|plumber",
        "Question_view_count":163,
        "Owner_creation_date":"2017-10-14 20:40:12.543 UTC",
        "Owner_last_access_date":"2022-08-31 14:40:35.367 UTC",
        "Owner_reputation":121,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66048199",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71740115,
        "Question_title":"How to automate batch job creation\/processing in sagemaker?",
        "Question_body":"<p>I have created model and other configuration via cloudforamtion ( sample code below) but based on documentation provided here, <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/AWS_SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/AWS_SageMaker.html<\/a>, i wanted to see if i can create\/run\/automate batch transform jobs in sagemaker. I have seen examples of create\/run jobs via cli or python sdk , but can one create\/run jobs via cloudformation or terraform?<\/p>\n<pre><code>Type: AWS::SageMaker::Model\nProperties: \n  Containers: \n    - ContainerDefinition\n  ExecutionRoleArn: role\n  InferenceExecutionConfig: \n    InferenceExecutionConfig\n  ModelName: 'somemodel'\n  PrimaryContainer: \n    ContainerDefinition\n  ...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-04 15:48:47.62 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"terraform|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":139,
        "Owner_creation_date":"2020-03-15 21:37:55.36 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:04.83 UTC",
        "Owner_reputation":365,
        "Owner_up_votes":53,
        "Owner_down_votes":2,
        "Owner_views":94,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71740115",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59565483,
        "Question_title":"Convert sagemaker model (MXNet) to ONNX: infer_shape error",
        "Question_body":"<h1>Working<\/h1>\n\n<p>I'm working on sagemaker jupyter notebook (environement: <code>anaconda3\/envs\/mxnet_p36\/lib\/python3.6<\/code>).<\/p>\n\n<p>I run <strong>successfully<\/strong> this tutorial: <a href=\"https:\/\/github.com\/onnx\/tutorials\/blob\/master\/tutorials\/MXNetONNXExport.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/onnx\/tutorials\/blob\/master\/tutorials\/MXNetONNXExport.ipynb<\/a><\/p>\n\n<hr>\n\n<h1>Not working<\/h1>\n\n<p>Then, on the same evironement, I tried to apply the same process to files generated by a sagemaker training job. So, I used as input the <strong>S3 model artifact<\/strong> files, changing some lines of the tutorial code to meet my needs.\nI used built in object detection SSD VGG-16 network with hyperparameter image_shape: 300.<\/p>\n\n<pre><code>sym = '.\/model_algo_1-symbol.json'\nparams = '.\/model_algo_1-0000.params'\ninput_shape = (1,3,300,300)\n<\/code><\/pre>\n\n<p>And <code>verbose=True<\/code> as last parameter in <code>export_model()<\/code> method:<\/p>\n\n<pre><code>converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file, True)\n<\/code><\/pre>\n\n<p>When I run the code I got <strong>this error<\/strong> (<strong>verbose output<\/strong> at the end of the post):<\/p>\n\n<pre><code>MXNetError: Error in operator multibox_target: [14:36:32] src\/operator\/contrib\/.\/multibox_target-inl.h:224: Check failed: lshape.ndim() == 3 (-1 vs. 3) : Label should be [batch, num_labels, label_width] tensor\n<\/code><\/pre>\n\n<hr>\n\n<h1>Question<\/h1>\n\n<p>I was not able to find any solution so far:<\/p>\n\n<ul>\n<li>maybe the <code>input_shape = (1,3,300,300)<\/code> is wrong, but I'm not able to\nfind it out;<\/li>\n<li>maybe the model contains some unexpected layer or so;<\/li>\n<\/ul>\n\n<p>Does anybody knows a way to fix this problem or a workaround to use the model on a local machine? \n<br>(I mean without having to deploy to aws)<\/p>\n\n<p><hr>\nThe <strong>verbose output<\/strong>:<\/p>\n\n<pre><code>  infer_shape error. Arguments:\n  data: (1, 3, 300, 300)\n  conv3_2_weight: (256, 256, 3, 3)\n  fc7_bias: (1024,)\n  multi_feat_3_conv_1x1_conv_weight: (128, 512, 1, 1)\n  conv4_1_bias: (512,)\n  conv5_3_bias: (512,)\n  relu4_3_cls_pred_conv_bias: (16,)\n  multi_feat_2_conv_3x3_relu_cls_pred_conv_weight: (24, 512, 3, 3)\n  relu4_3_loc_pred_conv_bias: (16,)\n  relu7_cls_pred_conv_weight: (24, 1024, 3, 3)\n  conv3_3_bias: (256,)\n  multi_feat_5_conv_3x3_relu_cls_pred_conv_weight: (16, 256, 3, 3)\n  conv4_3_weight: (512, 512, 3, 3)\n  conv1_2_bias: (64,)\n  multi_feat_2_conv_3x3_relu_cls_pred_conv_bias: (24,)\n  multi_feat_4_conv_3x3_conv_weight: (256, 128, 3, 3)\n  conv4_1_weight: (512, 256, 3, 3)\n  relu4_3_scale: (1, 512, 1, 1)\n  multi_feat_4_conv_3x3_conv_bias: (256,)\n  multi_feat_5_conv_3x3_relu_cls_pred_conv_bias: (16,)\n  conv2_2_weight: (128, 128, 3, 3)\n  multi_feat_3_conv_3x3_relu_loc_pred_conv_weight: (24, 256, 3, 3)\n  multi_feat_5_conv_3x3_conv_bias: (256,)\n  conv5_1_bias: (512,)\n  multi_feat_3_conv_3x3_conv_bias: (256,)\n  conv2_1_bias: (128,)\n  conv5_2_weight: (512, 512, 3, 3)\n  multi_feat_5_conv_3x3_relu_loc_pred_conv_weight: (16, 256, 3, 3)\n  multi_feat_4_conv_3x3_relu_loc_pred_conv_weight: (16, 256, 3, 3)\n  multi_feat_2_conv_3x3_conv_weight: (512, 256, 3, 3)\n  multi_feat_2_conv_1x1_conv_bias: (256,)\n  multi_feat_2_conv_1x1_conv_weight: (256, 1024, 1, 1)\n  conv4_3_bias: (512,)\n  relu7_cls_pred_conv_bias: (24,)\n  fc6_bias: (1024,)\n  conv2_1_weight: (128, 64, 3, 3)\n  multi_feat_2_conv_3x3_conv_bias: (512,)\n  multi_feat_2_conv_3x3_relu_loc_pred_conv_weight: (24, 512, 3, 3)\n  multi_feat_5_conv_1x1_conv_bias: (128,)\n  relu7_loc_pred_conv_bias: (24,)\n  multi_feat_3_conv_3x3_relu_loc_pred_conv_bias: (24,)\n  conv3_3_weight: (256, 256, 3, 3)\n  conv1_2_weight: (64, 64, 3, 3)\n  multi_feat_2_conv_3x3_relu_loc_pred_conv_bias: (24,)\n  conv1_1_bias: (64,)\n  multi_feat_4_conv_3x3_relu_cls_pred_conv_bias: (16,)\n  conv4_2_weight: (512, 512, 3, 3)\n  conv5_3_weight: (512, 512, 3, 3)\n  relu7_loc_pred_conv_weight: (24, 1024, 3, 3)\n  multi_feat_3_conv_3x3_conv_weight: (256, 128, 3, 3)\n  conv3_1_weight: (256, 128, 3, 3)\n  multi_feat_4_conv_3x3_relu_cls_pred_conv_weight: (16, 256, 3, 3)\n  relu4_3_loc_pred_conv_weight: (16, 512, 3, 3)\n  multi_feat_5_conv_3x3_conv_weight: (256, 128, 3, 3)\n  fc7_weight: (1024, 1024, 1, 1)\n  conv4_2_bias: (512,)\n  multi_feat_3_conv_3x3_relu_cls_pred_conv_weight: (24, 256, 3, 3)\n  multi_feat_3_conv_3x3_relu_cls_pred_conv_bias: (24,)\n  conv2_2_bias: (128,)\n  conv5_1_weight: (512, 512, 3, 3)\n  multi_feat_3_conv_1x1_conv_bias: (128,)\n  multi_feat_4_conv_3x3_relu_loc_pred_conv_bias: (16,)\n  conv1_1_weight: (64, 3, 3, 3)\n  multi_feat_4_conv_1x1_conv_bias: (128,)\n  conv3_1_bias: (256,)\n  multi_feat_5_conv_3x3_relu_loc_pred_conv_bias: (16,)\n  multi_feat_4_conv_1x1_conv_weight: (128, 256, 1, 1)\n  fc6_weight: (1024, 512, 3, 3)\n  multi_feat_5_conv_1x1_conv_weight: (128, 256, 1, 1)\n  conv3_2_bias: (256,)\n  conv5_2_bias: (512,)\n  relu4_3_cls_pred_conv_weight: (16, 512, 3, 3)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-02 15:02:22.223 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker|mxnet|onnx",
        "Question_view_count":271,
        "Owner_creation_date":"2015-08-18 11:23:52.727 UTC",
        "Owner_last_access_date":"2022-09-24 06:22:49.123 UTC",
        "Owner_reputation":10705,
        "Owner_up_votes":2035,
        "Owner_down_votes":2,
        "Owner_views":781,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59565483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56706249,
        "Question_title":"ValueError: Error hosting endpoint. The primary container for production variant AllTraffic did not pass the ping health check",
        "Question_body":"<p>I'm trying to deploy an SKlearn model on Amazon Sagemaker, and am working through the example provided in their documentation and am getting the above error when I deploy the model.  <\/p>\n\n<p>I'm following the instructions provided in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, and so far have just copied and pasted the code that they have.<\/p>\n\n<p>Right now, this is the exact code I have in my jupyter notebook:<\/p>\n\n<pre><code># S3 prefix\nprefix = 'Scikit-iris'\n\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\nimport numpy as np\nimport os\nfrom sklearn import datasets\n\n# Load Iris dataset, then join labels and features\niris = datasets.load_iris()\njoined_iris = np.insert(iris.data, 0, iris.target, axis=1)\n\n# Create directory and write csv\nos.makedirs('.\/iris', exist_ok=True)\nnp.savetxt('.\/iris\/iris.csv', joined_iris, delimiter=',', fmt='%1.1f, %1.3f, \n%1.3f, %1.3f, %1.3f')\n\nWORK_DIRECTORY = 'data'\n\ntrain_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}\/{}\".format(prefix, WORK_DIRECTORY) )\n\nfrom sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'scikit_learn_iris.py'\n\nsklearn = SKLearn(\n  entry_point=script_path,\n  train_instance_type=\"ml.c4.xlarge\",\n  role=role,\n  sagemaker_session=sagemaker_session,\n  framework_version='0.20.0',\n  hyperparameters={'max_leaf_nodes': 30})\n\nsklearn.fit({'train': train_input})\n\nsklearn.deploy(instance_type='ml.m4.xlarge',\n                                 initial_instance_count=1)\n<\/code><\/pre>\n\n<p>And at that point I get the error message.<\/p>\n\n<p>The contents of <code>'scikit_learn_iris.py'<\/code> look like this:<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\nimport numpy as np\n\nfrom sklearn import tree\nfrom sklearn.externals import joblib\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n# Hyperparameters are described here. In this simple example we are just including one hyperparameter.\nparser.add_argument('--max_leaf_nodes', type=int, default=-1)\n\n# SageMaker specific arguments. Defaults are set in the environment variables.\nparser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\nargs = parser.parse_args()\n\n# Take the set of files and read them all into a single pandas dataframe\ninput_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\nif len(input_files) == 0:\n    raise ValueError(('There are no files in {}.\\n' +\n                      'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                      'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                      'does not have permission to access the data.').format(args.train, \"train\"))\nraw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\ntrain_data = pd.concat(raw_data)\n\n# labels are in the first column\ntrain_y = train_data.ix[:,0].astype(np.int)\ntrain_X = train_data.ix[:,1:]\n\n# We determine the number of leaf nodes using the hyper-parameter above.\nmax_leaf_nodes = args.max_leaf_nodes\n\n# Now use scikit-learn's decision tree classifier to train the model.\nclf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\nclf = clf.fit(train_X, train_y)\n\n# Save the decision tree model.\njoblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n<\/code><\/pre>\n\n<p>My cloudwatch logs look like this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/HGWN2.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HGWN2.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-21 15:21:49.043 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":3261,
        "Owner_creation_date":"2014-09-15 23:32:32.337 UTC",
        "Owner_last_access_date":"2022-09-23 22:20:44.79 UTC",
        "Owner_reputation":3257,
        "Owner_up_votes":451,
        "Owner_down_votes":0,
        "Owner_views":319,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"New York, NY, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56706249",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64651724,
        "Question_title":"No Module named pyarrow",
        "Question_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-11-02 19:03:00.117 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-02 19:14:00.697 UTC",
        "Question_score":0,
        "Question_tags":"python|anaconda|amazon-sagemaker|pyarrow",
        "Question_view_count":540,
        "Owner_creation_date":"2016-07-15 16:19:18.957 UTC",
        "Owner_last_access_date":"2022-07-22 22:52:23.46 UTC",
        "Owner_reputation":107,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-11-02 19:18:46.273 UTC",
        "Answer_score":0.0,
        "Owner_location":"San Francisco, CA, USA",
        "Answer_last_edit_date":"2020-11-02 23:55:41.763 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59244786,
        "Question_title":"sagemaker - construct model from tar.gz file",
        "Question_body":"<p>I have trained a sagemaker model successfully and the <code>model.tar.gz<\/code> file is on s3.<\/p>\n\n<p>Now I want to \"reconstruct\" the model from that file and then deploy it. I used the following code:<\/p>\n\n<pre><code>\ncontainers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com\/factorization-machines:latest',\n              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com\/factorization-machines:latest',\n              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com\/factorization-machines:latest',\n              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com\/factorization-machines:latest'}\n\n\nfm = sagemaker.model.Model(model_s3_path, containers['eu-west-1'], role=sagemaker.get_execution_role())\n<\/code><\/pre>\n\n<p>I get back an object of type <code>sagemaker.model.Model<\/code>.<\/p>\n\n<p>I then seek to deploy the model via<\/p>\n\n<pre><code>fm_predictor = fm.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n\n<p>The output of this call is <\/p>\n\n<pre><code>--------------------------------------------------------------------------------------!\n<\/code><\/pre>\n\n<p>But this returns a <code>NoneType<\/code> object that does not have a predict method. However, the model's endpoint is created. <\/p>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-09 08:09:01.433 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":381,
        "Owner_creation_date":"2016-02-26 11:54:14.21 UTC",
        "Owner_last_access_date":"2022-09-24 11:34:28.877 UTC",
        "Owner_reputation":1464,
        "Owner_up_votes":81,
        "Owner_down_votes":9,
        "Owner_views":62,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Berlin, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59244786",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62866637,
        "Question_title":"The inference file that goes into the entry point of PyTorchModel to be deployed does not have an effect to the output of the predictor",
        "Question_body":"<p>I am currently running the code on AWS Sagemaker, trying to predict data using an already-trained model, accessed by MODEL_URL.<\/p>\n<p>With the code below, the inference.py as the entry_point does not seem to have an effect on the result of the trained prediction model. Any changes in inference.py does not alter the output (the output is always correct). Is there something I am misunderstanding with how the model works? And how can I incorporate inference.py to the prediction model as the entry point?<\/p>\n<pre><code>role = sagemaker.get_execution_role()\n\nmodel = PyTorchModel(model_data = MODEL_URL, \n                            role = role,\n                            framework_version = '0.4.0',\n                            entry_point = '\/inference.py',\n                            source_dir = SOURCE_DIR)\n\npredictor = model.deploy(instance_type = 'ml.c5.xlarge', \n                                   initial_instance_count = 1,\n                                   endpoint_name = RT_ENDPOINT_NAME)\n\nresult = predictor.predict(someData)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-12 21:28:28.9 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pytorch|prediction|amazon-sagemaker",
        "Question_view_count":265,
        "Owner_creation_date":"2020-07-12 21:17:55.633 UTC",
        "Owner_last_access_date":"2021-03-11 21:23:11.753 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62866637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67176637,
        "Question_title":"How to get bearer token AWS for Postman",
        "Question_body":"<p>I am using AWS sagemaker, and I have created an endpoint. I want to test endpoint on postman app. I give endpoint URL and JSON body to postman app. But I get this error that <code>&quot;message&quot;: &quot;Missing Authentication Token&quot;<\/code> I need to know from where I 'll get bearer token so that I can give it to postman app.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-20 09:57:55.733 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":1080,
        "Owner_creation_date":"2019-09-07 18:22:12.003 UTC",
        "Owner_last_access_date":"2022-08-28 17:07:56.487 UTC",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Answer_body":"<p>I am answering my own question after searching and reading forums,<\/p>\n<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure<\/code> command.\nFor configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.\nAfter configuration by running this command, <code>aws ecr get-authorization-token<\/code>, we can get authorizationToken. <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/ecr\/get-authorization-token.html\" rel=\"nofollow noreferrer\">here<\/a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-20 21:09:13.59 UTC",
        "Answer_score":1.0,
        "Owner_location":"Lahore, Pakistan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67176637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72823618,
        "Question_title":"Trigger Lambda Function upon Manual Approval of SageMaker Model",
        "Question_body":"<p>We use SageMaker to train our ML model. After training, we need to examine the metric and give it a manual approval. Upon approval, we need to copy the model artifects to another AWS account. If we have a Lambda function to copy the artifects, how can we automatically trigger the lambda function upon the model approval?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-01 01:45:55.06 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":59,
        "Owner_creation_date":"2011-08-25 14:24:40.603 UTC",
        "Owner_last_access_date":"2022-08-26 02:14:19.28 UTC",
        "Owner_reputation":1590,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":72,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72823618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64169189,
        "Question_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Question_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-02 09:24:11.783 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1958,
        "Owner_creation_date":"2008-11-20 08:57:51.293 UTC",
        "Owner_last_access_date":"2022-09-24 19:18:28.08 UTC",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Answer_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-10-05 09:51:09.683 UTC",
        "Answer_score":6.0,
        "Owner_location":"Israel",
        "Answer_last_edit_date":"2020-10-08 07:06:36.11 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73292975,
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-09 13:48:02.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_date":"2014-03-13 09:33:37.223 UTC",
        "Owner_last_access_date":"2022-09-15 12:15:05.39 UTC",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-16 12:46:57.4 UTC",
        "Answer_score":0.0,
        "Owner_location":"Cologne, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68557263,
        "Question_title":"AWS API Gateway returns TARGET_MODEL_HEADER_MISSING even though target model passed in header",
        "Question_body":"<p>I have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.\nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.<\/p>\n<p>My https request looks like this:<\/p>\n<pre><code>import requests\nheaders = {'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'}\nresponse = requests.request(&quot;POST&quot;\n, &quot;https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles&quot;\n, headers = headers\n, data = data\n)\n<\/code><\/pre>\n<p>According to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> and the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py\" rel=\"nofollow noreferrer\">source code<\/a>\nit seems like I am providing the header with the target model correctly. But this is obvously not the case.<\/p>\n<p>How am I supposed to provide the target model in the header with the https-request?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-28 08:52:58.577 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|api-gateway",
        "Question_view_count":123,
        "Owner_creation_date":"2017-02-06 20:25:25.5 UTC",
        "Owner_last_access_date":"2022-09-23 12:39:06.777 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68557263",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70909916,
        "Question_title":"No GPU detected on AWS SageMaker pytorch-1.8-gpu-py36 instance",
        "Question_body":"<p>I've got a pytorch-1.8-gpu-py36 instance running on AWS SageMaker Studio.<\/p>\n<p>If I'm in a notebook in and I enter:<\/p>\n<pre><code>!nvidia-smi -L\n<\/code><\/pre>\n<p>I get:<\/p>\n<pre><code>GPU 0: Tesla T4 (UUID: GPU-786d298a-2648-3506-6c3a-f541fa46d777)\n<\/code><\/pre>\n<p>But if I open a terminal and enter:<\/p>\n<pre><code>nvidia-smi -L\n<\/code><\/pre>\n<p>I get command not found, and if I try to run a .py script that requires a GPU I get this error from PyTorch:<\/p>\n<pre><code>pytorch_lightning.utilities.exceptions.MisconfigurationException: \nYou requested GPUs: [0]\nBut your machine only has: []\n<\/code><\/pre>\n<p>Do the terminal windows and notebooks run off of separate instances even if they're in the same folder? Is there a way to get the terminal to be part of the same instance as the notebook?<\/p>\n<p>I can't simply run the command line from the notebook as I require a Conda environment that can't be activated from the notebook interface.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-29 21:32:28.78 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-30 05:19:40.47 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":278,
        "Owner_creation_date":"2013-05-23 11:55:21.733 UTC",
        "Owner_last_access_date":"2022-08-30 19:13:25.677 UTC",
        "Owner_reputation":931,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70909916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72609401,
        "Question_title":"Sagemaker notebook to EMR pyspark using yarn-client instead of livy",
        "Question_body":"<p>I know there are good tutorials on connecting  Sagemaker notebooks to EMR cluster for running pyspark jobs via the SparkMagic pre-installed kernel, however we want to connect to the cluster using yarn-client mode instead of livy to be able to use libraries  and configs we already have in our Sagemaker instances, the idea is to run pyspark code on huge datasets, get summaries and use the huge amount of python libraries and custom internal libraries we already have in our sagemaker notebooks to do analysis tasks with the summaries, I've been searching but I can only find connection via livy, my question is: can anyone point to an example for that?<\/p>\n<p>Before cloud\/AWS we used an internal\/custom on-premise setup(both hadoop\/spark clusters and jupyter lab) were we worked with that setup, so we know is possible, just don't know how to do it on AWS and EMR\/sagemaker. In our on-premises we do something like the following code(written in the notebook)<\/p>\n<pre><code>spark = pyspark.sql.SparkSession.builder.master(\u201cyarn-client\u201d).getorCreate(cluster=&quot;just cluster name&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 21:46:31.04 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-13 22:04:37.163 UTC",
        "Question_score":0,
        "Question_tags":"python|pyspark|amazon-emr|amazon-sagemaker",
        "Question_view_count":245,
        "Owner_creation_date":"2016-06-15 18:26:00.217 UTC",
        "Owner_last_access_date":"2022-09-20 17:52:45.723 UTC",
        "Owner_reputation":3287,
        "Owner_up_votes":99,
        "Owner_down_votes":24,
        "Owner_views":435,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Guatemala City, Guatemala Department, Guatemala",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72609401",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62306472,
        "Question_title":"AWS write to S3 bucket in prediction calling a batch job",
        "Question_body":"<p>When i invoke the endpoint\/batch job in sagemaker deployed with a custom container i want to store some information inside S3.\nFor example, in this image \n<a href=\"https:\/\/i.stack.imgur.com\/R43fA.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R43fA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the \"Deployment\/Hosting\" i want to put some objects inside the S3 container and not only reading the \/opt\/model<\/p>\n\n<p>For example <a href=\"https:\/\/scriptexampler.notebook.us-west-2.sagemaker.aws\/examples\/preview?example_id=%2Fhome%2Fec2-user%2Fsample-notebooks%2Fr_examples%2Fr_byo_r_algo_hpo%2Ftune_r_bring_your_own.ipynb\" rel=\"noreferrer\">BYO container R<\/a><\/p>\n\n<p>In the plumber.R function i would like to: <\/p>\n\n<pre><code>function(req) {\n\n    # Setup locations\n    prefix &lt;- '\/opt\/ml'\n    model_path &lt;- paste(prefix, 'model', sep='\/')\n\n    # Bring in model file and factor levels\n    load(paste(model_path, 'mars_model.RData', sep='\/'))\n\n    # Read in data\n    conn &lt;- textConnection(gsub('\\\\\\\\n', '\\n', req$postBody))\n    data &lt;- read.csv(conn)\n    close(conn)\n\n    # Convert input to model matrix\n    scoring_X &lt;- model.matrix(~., data, xlev=factor_levels)\n\n    ####\n    SAVE OBJECT IN S3\n    #####\n\n\n    # Return prediction\n    return(paste(predict(mars_model, scoring_X, row.names=FALSE), collapse=','))}\n<\/code><\/pre>\n\n<p>How can i achieve this? Using aws.s3 connecting to the container or with other techniques?<\/p>\n\n<p>EDIT: The aws.s3 solution to connect directly inside the container seems not working<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-10 14:48:09.813 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-10 15:02:30.55 UTC",
        "Question_score":8,
        "Question_tags":"r|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_date":"2017-01-04 16:53:36.877 UTC",
        "Owner_last_access_date":"2021-03-25 06:26:26.257 UTC",
        "Owner_reputation":294,
        "Owner_up_votes":27,
        "Owner_down_votes":4,
        "Owner_views":34,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62306472",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55119383,
        "Question_title":"Inconsistent \"'float' object is not iterable\" error in sklearn",
        "Question_body":"<p>I know that there are a lot of questions with the same problem but none of them solves my problem. I am using a Jupyter Notebook in Amazon Sagemaker and I want to use the hashing trick for some features. I have not been able to make a reproducible example with simple data, but here is a screen of the data I have:\n<a href=\"https:\/\/i.stack.imgur.com\/el7x2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/el7x2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So I have used:<\/p>\n\n<pre><code>from sklearn.feature_extraction import FeatureHasher\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['country_iso_code'] = h.transform(df['country_iso_code'])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['origen_tarjeta_country_iso'] = h.transform(df['origen_tarjeta_country_iso'])\n<\/code><\/pre>\n\n<p>The first transformation works, but the second one does not and I get the 'float' object is not iterable error. I have checked the types of both columns and they are objects and also I have checked that there are only strings in both columns. I have tried to reproduce the code in Spyder with a very little sample and it works:<\/p>\n\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\n\ndf = pd.DataFrame({'ES':'ES','UK':'UK'},index=[0,1])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['UK'] = h.transform(df['UK'])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['ES'] = h.transform(df['ES'])\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2019-03-12 10:34:57.78 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pandas|scikit-learn|amazon-sagemaker",
        "Question_view_count":299,
        "Owner_creation_date":"2018-04-09 18:36:08.403 UTC",
        "Owner_last_access_date":"2022-09-23 12:00:52.963 UTC",
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55119383",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60785324,
        "Question_title":"AWS Sagemaker Kernel appears to have died and restarts",
        "Question_body":"<p>I am getting a kernel error while trying to retrieve the data from an API that includes 100 pages. The data size is huge but the code runs well when executed on Google Colab or on local machine.<\/p>\n\n<p>The error I see in a window is-<\/p>\n\n<p><strong>Kernel Restarting\nThe kernel appears to have died. It will restart automatically.<\/strong><\/p>\n\n<p>I am using an ml.m5.xlarge machine with a memory allocation of 1000GB and there are no pre-saved datasets in the instance. Also, the expected data size is around 60 GB split into multiple datasets of 4 GB each.<\/p>\n\n<p>Can anyone help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-21 07:12:39.84 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-sagemaker",
        "Question_view_count":1026,
        "Owner_creation_date":"2019-10-04 04:37:35.22 UTC",
        "Owner_last_access_date":"2020-04-19 18:01:54.867 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60785324",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56266967,
        "Question_title":"How to solve programming error when storing pandas data frame to snowflake",
        "Question_body":"<p>I'm trying to use SQLAlchemy to store a data frame I created in sagemaker to snowflake. The code only works with certain columns. When I add other columns it gives me an error even though they have the same data type. In the following example, if I only upload TA_ID it works, yet if I upload Cluster_ID, the code throws me an error. <\/p>\n\n<p>I checked SQLAlchemy website but didn't find much information on programming error. <\/p>\n\n<h2>SQL codes used to create table<\/h2>\n\n<pre><code>CREATE OR REPLACE TABLE test.m (\n    TA_ID string,\n     Cluster_ID string\n)\n<\/code><\/pre>\n\n<h2>Python code<\/h2>\n\n<pre><code>master2.to_sql(name='m', con=engine2, if_exists='append',  schema='test',index=False, index_label=None, chunksize=2000 )\n<\/code><\/pre>\n\n<p>ProgrammingError: <\/p>\n\n<pre><code>(snowflake.connector.errors.ProgrammingError) 000904 (42000): SQL compilation error: error line 1 at position 29\ninvalid identifier '\"Cluster_ID\"' [SQL: 'INSERT INTO test.m (\"TA_ID\", \"Cluster_ID\") VALUES (%(TA_ID)s, %(Cluster_ID)s)'] [parameters: ({'TA_ID': 'TA007', 'Cluster_ID': '0'}, {'TA_ID': 'TA007', 'Cluster_ID': '16'}, {'TA_ID': 'TA007', 'Cluster_ID': '40'}, {'TA_ID': 'TA007', 'Cluster_ID': '15'}, {'TA_ID': 'TA007', 'Cluster_ID': '29'}, {'TA_ID': 'TA007', 'Cluster_ID': '23'}, {'TA_ID': 'TA007', 'Cluster_ID': '9'}, {'TA_ID': 'TA007', 'Cluster_ID': '25'}, {'TA_ID': 'TA007', 'Cluster_ID': '42'}, {'TA_ID': 'TA007', 'Cluster_ID': '28'})] (Background on this error at: http:\/\/sqlalche.me\/e\/f405)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-23 01:30:22.99 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-05-23 04:54:03.9 UTC",
        "Question_score":0,
        "Question_tags":"sql|pandas|sqlalchemy|amazon-sagemaker|snowflake-cloud-data-platform",
        "Question_view_count":934,
        "Owner_creation_date":"2015-01-23 20:35:22.283 UTC",
        "Owner_last_access_date":"2019-10-22 15:28:10.93 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56266967",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73775634,
        "Question_title":"Processing job v.s. training job in SageMaker",
        "Question_body":"<p>What is the difference between a processing job and a training job? I am running training job and did not launch processing job, why does my account have processing job running?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-19 15:12:03.067 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":15,
        "Owner_creation_date":"2014-10-07 08:13:42.83 UTC",
        "Owner_last_access_date":"2022-09-23 14:45:05.23 UTC",
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73775634",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50352412,
        "Question_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Question_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-15 14:10:14.957 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":447,
        "Owner_creation_date":"2013-01-17 13:27:30.663 UTC",
        "Owner_last_access_date":"2022-09-23 16:24:02.467 UTC",
        "Owner_reputation":2120,
        "Owner_up_votes":100,
        "Owner_down_votes":3,
        "Owner_views":279,
        "Answer_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-15 17:31:40.043 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-11-15 10:05:07.96 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56010337,
        "Question_title":"Amazon sagemaker. SKlearn estimator vs Tensorflow estimator - why requirements_file is not present in one of them?",
        "Question_body":"<p>I am looking at definitions of two estimators SKLearn and Tensorflow in Amazon Sagemaker:<\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a><\/p>\n\n<pre><code>class sagemaker.sklearn.estimator.SKLearn(entry_point, framework_version='0.20.0', source_dir=None, hyperparameters=None, py_version='py3', image_name=None, **kwargs)\n\nclass sagemaker.tensorflow.estimator.TensorFlow(training_steps=None, evaluation_steps=None, checkpoint_path=None, py_version='py2', framework_version=None, model_dir=None, requirements_file='', image_name=None, script_mode=False, distributions=None, **kwargs)\n<\/code><\/pre>\n\n<p>Tensorflow has requirements_file parameter, while SKLearn does not. Is there reason why? How can I add <code>requirements.txt<\/code> to SKLearn estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-06 18:09:44.387 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|requirements|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_date":"2012-09-26 16:34:10.37 UTC",
        "Owner_last_access_date":"2022-09-25 01:21:08.267 UTC",
        "Owner_reputation":6498,
        "Owner_up_votes":1266,
        "Owner_down_votes":2,
        "Owner_views":988,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56010337",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57074382,
        "Question_title":"How to get the public ip of amazon sagemaker's notebook instance? Is it possible?",
        "Question_body":"<p>Is it possible to get the public-ip of an amazon <code>sagemaker<\/code> notebook instance?<\/p>\n\n<p>I was wondering if I can ssh into it using the public ip for remote debugging purposes.<\/p>\n\n<p>I tried getting the public ip using the below curl command<\/p>\n\n<pre><code>$curl http:\/\/169.254.169.254\/latest\/meta-data\n<\/code><\/pre>\n\n<p>This just lists the local ip and not the public ip.<\/p>\n\n<p>I also tried the below command.<\/p>\n\n<pre><code>$curl ifconfig.me\n<\/code><\/pre>\n\n<p>This returns an ip address like <code>13.232.96.15<\/code>. If I try ssh into this it doesnt work.<\/p>\n\n<p>Is there any other way we can do this?<\/p>\n\n<p>Note : The ssh port 22 is open already in the security group<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-17 11:03:06.39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":6,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4461,
        "Owner_creation_date":"2016-02-10 09:50:46.15 UTC",
        "Owner_last_access_date":"2022-09-23 10:06:18.677 UTC",
        "Owner_reputation":1347,
        "Owner_up_votes":1531,
        "Owner_down_votes":8,
        "Owner_views":217,
        "Answer_body":"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html<\/a><\/p>\n\n<p>If you need a terminal, then you can open one from Jupyter.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-07-17 23:22:54.44 UTC",
        "Answer_score":5.0,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57074382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73163540,
        "Question_title":"Is there any workaround for installing packages ? (deepspeed error)",
        "Question_body":"<p>i was trying to use deepspeed, but when i run the training it shows an error. I have to install mpi4py using pip. However if I try to install mpi4py i get an error.\nFrom that error i found out that to install the package, i must install &quot;libopenmpi-dev &quot; before, using apt. However we dont have the password to sudo. Any workaround to this ?\n(or the only option is to change platforms ?)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":9,
        "Question_creation_date":"2022-07-29 08:09:07.263 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-30 08:22:06.4 UTC",
        "Question_score":0,
        "Question_tags":"linux|amazon-sagemaker|apt",
        "Question_view_count":73,
        "Owner_creation_date":"2022-05-28 12:21:58.87 UTC",
        "Owner_last_access_date":"2022-09-23 18:14:09.533 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73163540",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59430560,
        "Question_title":"Reading Data from AWS S3",
        "Question_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2019-12-20 19:41:05.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":4393,
        "Owner_creation_date":"2018-08-22 19:13:17.293 UTC",
        "Owner_last_access_date":"2022-09-21 19:24:19.063 UTC",
        "Owner_reputation":320,
        "Owner_up_votes":91,
        "Owner_down_votes":2,
        "Owner_views":48,
        "Answer_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-12-24 09:51:31.057 UTC",
        "Answer_score":3.0,
        "Owner_location":"Seattle, WA",
        "Answer_last_edit_date":"2019-12-24 15:51:30.473 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48522013,
        "Question_title":"Getting Conflict error for Amazon.RegionEndpoint with Sagemaker",
        "Question_body":"<p>I have created an applicataion in c#, where I need to put some data on S3 bucket, and to Invoke AWS sagemaker APIs.\nSince the same Amazon.RegionEndPoint class exists in both the references, it is giving below error.<\/p>\n\n<blockquote>\n  <p>The type 'RegionEndpoint' exists in both 'AWSSDK.Core,\n  Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604' and\n  'AWSSDK, Version=2.3.55.2<\/p>\n<\/blockquote>\n\n<p>Basically I am trying to upload files on AWS S3, following code I have used.<\/p>\n\n<pre><code>AmazonS3Client s3Client = new AmazonS3Client(_AWS_ACCESS_KEY_ID, _AWS_SECRETE_ACCESS_KEY, Amazon.RegionEndpoint.USEast2);\n PutObjectRequest request = new PutObjectRequest\n  {\n    BucketName = _BucketName,\n    Key = i_sDestFileName,\n    FilePath = i_sSourceFilePath,\n    ContentType = \"text\/plain\"\n  };\n  s3Client.PutObject(request);\n<\/code><\/pre>\n\n<p>It is working fine on a single application, but when I integrated code with Sagemaker API invokation, the conflict occurs for Amazon.RegionEndpoint.USEast2.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2018-01-30 12:56:06.157 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-01-31 09:30:33.467 UTC",
        "Question_score":3,
        "Question_tags":"c#|amazon-s3|amazon-sagemaker",
        "Question_view_count":2527,
        "Owner_creation_date":"2012-05-23 07:46:54.69 UTC",
        "Owner_last_access_date":"2018-07-30 12:44:23.377 UTC",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pune India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48522013",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50924494,
        "Question_title":"Sagemaker ImportError: Import by filename is not supported",
        "Question_body":"<p>I have a custom algorithm for text prediction. I want to deploy that in sagemaker. I am following this tutorial.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html<\/a>\n<br>\nThe only change from the tutorial is.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\niris_estimator = TensorFlow(entry_point='\/home\/ec2-user\/SageMaker\/sagemaker.py',\n                        role=role,\n                        output_path=model_artifacts_location,\n                        code_location=custom_code_upload_location,\n                        train_instance_count=1,\n                        train_instance_type='ml.c4.xlarge',\n                        training_steps=1000,\n                        evaluation_steps=100, source_dir=\".\/\", requirements_file=\"requirements.txt\")\n<\/code><\/pre>\n\n<p>.<\/p>\n\n<pre><code>%%time\nimport boto3\n\ntrain_data_location = 's3:\/\/sagemaker-&lt;my bucket&gt;'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>INFO: the dataset is at the root of the bucket.<\/p>\n\n<p>error log<\/p>\n\n<pre><code>ValueError: Error training sagemaker-tensorflow-2018-06-19-07-11-13-634: Failed Reason: AlgorithmError: uncaught exception during training: Import by filename is not supported.\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\n    fw.train()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/tf_container\/train_entry_point.py\", line 143, in train\n    customer_script = env.import_user_module()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 101, in import_user_module\n    user_module = importlib.import_module(script)\n  File \"\/usr\/lib\/python2.7\/importlib\/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: Import by filename is not supported.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-19 09:06:00.327 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":470,
        "Owner_creation_date":"2015-08-28 03:56:28.43 UTC",
        "Owner_last_access_date":"2022-06-18 16:31:02.827 UTC",
        "Owner_reputation":1491,
        "Owner_up_votes":198,
        "Owner_down_votes":32,
        "Owner_views":112,
        "Answer_body":"<p>I solved this issue, The problem was using absolute path for <code>entry_point<\/code>. \n<br>\nwhen you use a <code>source_dir<\/code> parameter the path to the <code>entry_point<\/code> should be relative to the <code>source_dir<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-06-22 08:18:26.467 UTC",
        "Answer_score":1.0,
        "Owner_location":"India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50924494",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65770913,
        "Question_title":"Sagemaker Studio Pyspark example fails",
        "Question_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2021-01-18 08:19:49.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1827,
        "Owner_creation_date":"2016-01-07 18:07:35.593 UTC",
        "Owner_last_access_date":"2022-08-09 14:35:01.297 UTC",
        "Owner_reputation":247,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-18 21:32:53.523 UTC",
        "Answer_score":2.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70911352,
        "Question_title":"How to trigger AWS Sagemaker training job from a front end application (via api gateway post call)?",
        "Question_body":"<p>I have made a training job on AWS Sagemaker and it runs well - reads from an s3 location and stores model checkpoints as intended in s3. Now, I need to trigger this trigger job with specified parameters (s3 location having data for eg.) from a website\n(via API gateway). The very first idea was to make a lambda function that gets called from an API call and it training job using the Sagemaker API:<\/p>\n<pre><code>HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            role=role,\n                            transformers_version='4.6',\n                            pytorch_version='1.7',\n                            py_version='py36',\n                            hyperparameters = hyperparameters)\n\n# staarting the train job with our uploaded datasets as input\nhuggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n<\/code><\/pre>\n<p>But, AWS lambda has a max runtime of 15 mins which is less than the training time required. I was wondering if there is a serverless way of doing the same thing? Is AWS step function any different from lambda in this regard?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2022-01-30 02:27:49.63 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|huggingface-transformers|mlops",
        "Question_view_count":349,
        "Owner_creation_date":"2018-12-26 18:41:29.957 UTC",
        "Owner_last_access_date":"2022-09-25 01:49:20.55 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Camden, NJ, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70911352",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56728230,
        "Question_title":"AWS sagemaker RandomCutForest (RCF) vs scikit lean RandomForest (RF)?",
        "Question_body":"<p>Is there a difference between the two, or are they different names for the same algorithm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-23 22:45:17.317 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"classification|random-forest|decision-tree|amazon-sagemaker",
        "Question_view_count":244,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. <\/p>\n\n<p>For RCF, see documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">here<\/a>) and notebook example (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-24 14:04:57.93 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56728230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54134210,
        "Question_title":"Are there any SageMaker resources on how to perform training in a distributed manner with a custom algorithm?",
        "Question_body":"<p>I've seen that SageMaker's built in algorithms support distributed training. However, I haven't found any documentation for how to structure my data and\/or my image containing the custom algorithm so that training can be done in a distributed manner. Any help here would be much appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-10 17:43:34.067 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":33,
        "Owner_creation_date":"2016-09-18 20:38:44.273 UTC",
        "Owner_last_access_date":"2022-08-22 15:39:02.55 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54134210",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70780638,
        "Question_title":"SageMaker Studio Lab - XGBoost Algortim - Am I Reading the Right Documentation?",
        "Question_body":"<p>Google has answers for SageMaker Studio, but I am at a loss for an answer on SageMaker Studio LAB...<\/p>\n<p>I am reading the following on XGBoost - Am I in the right place for SM Studio LAB?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html<\/a><\/p>\n<p><strong>Is there a better way for me to use XGBoost in LAB, vs what I am doing below by reading the doc above?<\/strong><\/p>\n<p>In SageMaker Studio I would do the following to get the ECR container for the XGBoost algorithm:<\/p>\n<pre><code>from sagemaker import image_uris\ncontainer = image_uris.retrieve('xgboost', boto3.Session().region_name, '1')\n<\/code><\/pre>\n<p>I made it a bit farther using the github example:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb<\/a><\/p>\n<p>This works:<\/p>\n<pre><code>from sagemaker import image_uris\nfrom sagemaker.xgboost import XGBoost\n\n# Create a training job name\njob_name = 'ufo-xgboost-job-{}'.format(datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;))\nprint('Here is the job name {}'.format(job_name))\n\nimport sagemaker\nimport boto3\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n<\/code><\/pre>\n<p>But this is giving me trouble:<\/p>\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(container,\n                                    role, \n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='model.tar.gz',\n                                    sagemaker_session=sess)\n\nxgb.set_hyperparameters(objective='multi:softmax',\n                        num_class=3,\n                        num_round=100)\n\ndata_channels = {\n    'train': s3_input_train,\n    'validation': s3_input_validation\n}\nxgb.fit(data_channels, job_name=job_name) \n<\/code><\/pre>\n<p>With the following errors:<\/p>\n<pre><code>ParsingError                              Traceback (most recent call last)\n~\/.conda\/envs\/default\/lib\/python3.9\/site-packages\/botocore\/configloader.py in raw_config_parse(config_filename, parse_subsections)\n    148         try:\n--&gt; 149             cp.read([path])\n    150         except (six.moves.configparser.Error, UnicodeDecodeError):\n\n~\/.conda\/envs\/default\/lib\/python3.9\/configparser.py in read(self, filenames, encoding)\n    696                 with open(filename, encoding=encoding) as fp:\n--&gt; 697                     self._read(fp, filename)\n    698             except OSError:\n\n~\/.conda\/envs\/default\/lib\/python3.9\/configparser.py in _read(self, fp, fpname)\n   1115         if e:\n-&gt; 1116             raise e\n   1117 \n\nParsingError: Source contains parsing errors: '\/home\/studio-lab-user\/.aws\/config'\n    [line  5]: 'from sagemaker import image_uris\\n'\n    [line  6]: 'import boto3\\n'\n\nDuring handling of the above exception, another exception occurred:\n<\/code><\/pre>\n<p>truncated\nerror at bottom:<\/p>\n<pre><code>ConfigParseError: Unable to parse config file: \/home\/studio-lab-user\/.aws\/config\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-20 03:58:37.55 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-21 02:51:28.567 UTC",
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker|lab",
        "Question_view_count":93,
        "Owner_creation_date":"2021-02-03 22:00:19.01 UTC",
        "Owner_last_access_date":"2022-02-01 02:49:24.483 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"East Bay, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70780638",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72529344,
        "Question_title":"AWS Step Function Retry creates a job with the same name",
        "Question_body":"<p>I created a Step in my step function which has a <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/states.html#stepfunctions.steps.states.State.add_retry\" rel=\"nofollow noreferrer\">Retrier<\/a>. It works as intended, but every time it tries to re-run my failed step it creates a new job with the same name as the first one, leading to the &quot;Job name must be unique within an AWS account and region, and a job with this name already exists&quot; error.\nWhat can I do? as of now the retrier is useless.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-07 10:04:31.937 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-step-functions",
        "Question_view_count":82,
        "Owner_creation_date":"2020-07-09 09:09:24.317 UTC",
        "Owner_last_access_date":"2022-07-01 14:27:55.28 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72529344",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48374786,
        "Question_title":"How to deploy model trained with SparkR in sagemaker service?",
        "Question_body":"<p>I have created a training model with k-means and ALG algorithm in sparkR language.\nI wanted to deploy the model throw AWS Sagemaker service.\nI have ran some inbuilt examples, which uses conda_python3 language, but how it is possible with SparkR .?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-01-22 04:41:33.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"sparkr|amazon-sagemaker",
        "Question_view_count":378,
        "Owner_creation_date":"2012-05-23 07:46:54.69 UTC",
        "Owner_last_access_date":"2018-07-30 12:44:23.377 UTC",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Pune India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48374786",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67866286,
        "Question_title":"Real-time Data Pre-processing in Lambda for SageMaker Endpoint",
        "Question_body":"<p>Am working on a project of consumer behaviour analysis on websites and predict the malicious activity of users in real-time.\nClick data is being collected for each click made by users.<\/p>\n<p>Am using multiple AWS services like kinesis stream, Lambda and sagemaker. I have created an autoencoder model and\ndeployed it as sagemaker endpoint which will be invoked using lambda when it receives new click data from the website through\nKinesis stream.<\/p>\n<p>Since sagemaker endpoint contains the only model but click data which lambda function receives is raw data with URLs, texts and\ndate. How can I pass raw data into required preprocessing steps and send processed data to sagemaker endpoint in the required format?<\/p>\n<p>Example of raw data:-<\/p>\n<p>{'URL':'www.amazon.com.au\/ref=nav_logo', 'Text':'Home', 'Information':'Computers'}<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-07 05:22:45.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"aws-lambda|python-3.6|amazon-sagemaker|real-time-data|data-preprocessing",
        "Question_view_count":475,
        "Owner_creation_date":"2020-11-09 08:37:27.62 UTC",
        "Owner_last_access_date":"2021-06-22 04:23:41.637 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.<\/p>\n<p>Reference:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/<\/a><\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-21 05:42:31.543 UTC",
        "Answer_score":0.0,
        "Owner_location":"Victoria, Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67866286",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60573260,
        "Question_title":"Why does Featuretools slows down when I increase the number of Dask workers?",
        "Question_body":"<p>I'm using an Amazon SageMaker Notebook that has 72 cores and 144 GB RAM, and I carried out 2 tests with a sample of the whole data to check if the Dask cluster was working.<\/p>\n\n<p>The sample has 4500 rows and 735 columns from 5 different \"assets\" (I mean 147 columns for each asset). The code is filtering the columns and creating a feature matrix for each filtered Dataframe.<\/p>\n\n<p>First, I initialized the cluster as follows, I received 72 workers, and got 17 minutes of running. (I assume I created 72 workers with one core each.)<\/p>\n\n<pre><code>    from dask.distributed import Client, LocalCluster\n    cluster = LocalCluster(processes=True,n_workers=72,threads_per_worker=72)\n\n    def main():\n      import featuretools as ft\n      list_columns = list(df_concat_02.columns)\n\n      list_df_features=[]\n      from tqdm.notebook import tqdm\n\n      for asset in tqdm(list_columns,total=len(list_columns)):\n        dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n        es = ft.EntitySet()  \n        es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                      index = 'index', \n                                      time_index = 'Date')\n        fm, features = ft.dfs(entityset=es, \n                              target_entity='MARKET',\n                              trans_primitives = ['divide_numeric'],\n                              agg_primitives = [],\n                              max_depth=1,\n                              verbose=True,\n                              dask_kwargs={'cluster': client.scheduler.address}\n\n                              )\n        list_df_features.append(fm)\n      return list_df_features\n\n    if __name__ == \"__main__\":\n        list_df = main()\n<\/code><\/pre>\n\n<p>Second, I initialized the cluster as follows, I received 9 workers, and got 3,5 minutes of running. (I assume I created 9 workers with 8 cores each.)<\/p>\n\n<pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(processes=True)\n\ndef main():\n  import featuretools as ft\n  list_columns = list(df_concat_02.columns)\n\n  list_df_features=[]\n  from tqdm.notebook import tqdm\n\n  for asset in tqdm(list_columns,total=len(list_columns)):\n    dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n    es = ft.EntitySet()  \n    es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                  index = 'index', \n                                  time_index = 'Date')\n    fm, features = ft.dfs(entityset=es, \n                          target_entity='MARKET',\n                          trans_primitives = ['divide_numeric'],\n                          agg_primitives = [],\n                          max_depth=1,\n                          verbose=True,\n                          dask_kwargs={'cluster': client.scheduler.address}\n\n                          )\n    list_df_features.append(fm)\n  return list_df_features\n\nif __name__ == \"__main__\":\n    list_df = main()\n<\/code><\/pre>\n\n<p>For me, it's mind-blowing because I thought that 72 workers could carry the work out faster! Once I'm not a specialist neither in Dask nor in FeatureTools I guess that I'm setting something wrong.<\/p>\n\n<p>I would appreciate any kind of help and advice!<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-07 00:23:55.953 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-07 13:51:05.657 UTC",
        "Question_score":0,
        "Question_tags":"jupyter-notebook|dask|amazon-sagemaker|feature-engineering|featuretools",
        "Question_view_count":192,
        "Owner_creation_date":"2017-10-26 10:07:59.113 UTC",
        "Owner_last_access_date":"2021-09-21 19:53:06.417 UTC",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>You are correctly setting <code>dask_kwargs<\/code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-03-09 22:33:10.593 UTC",
        "Answer_score":1.0,
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60573260",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67212443,
        "Question_title":"pythonnet on Sagemaker cannot import clr",
        "Question_body":"<p>I cannot import clr from SageMaker with this error.\nWhat kind of tweaks make this successful ?<\/p>\n<p>From Sagemaker console :<\/p>\n<pre><code>source activate python3    \nconda install pythonnet\n<\/code><\/pre>\n<p>This seems to finely install pythonnet without error.<\/p>\n<p>In python code :<\/p>\n<pre><code>import clr\n<\/code><\/pre>\n<p>This produce this error :<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;test_clr.py&quot;, line 1, in &lt;module&gt;\n    import clr\nImportError: System.TypeInitializationException: The type initializer for 'Sys' threw an exception. ---&gt; System.DllNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/..\/lib\/libmono-native.so assembly:&lt;unknown assembly&gt; type:&lt;unknown type&gt; member:(null)\n  at (wrapper managed-to-native) Interop+Sys.LChflagsCanSetHiddenFlag()\n  at Interop+Sys..cctor () [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n   --- End of inner exception stack trace ---\n  at Interop.GetRandomBytes (System.Byte* buffer, System.Int32 length) [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n  at System.Guid.FastNewGuidArray () [0x00020] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n  at System.Reflection.Emit.ModuleBuilder..ctor (System.Reflection.Emit.AssemblyBuilder assb, System.String name, System.String fullyqname, System.Boolean emitSymbolInfo, System.Boolean transient) [0x00035] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n  at System.Reflection.Emit.AssemblyBuilder.DefineDynamicModule (System.String name, System.String fileName, System.Boolean emitSymbolInfo, System.Boolean transient) [0x0005b] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n  at System.Reflection.Emit.AssemblyBuilder.DefineDynamicModule (System.String name) [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 \n  at Python.Runtime.CodeGenerator..ctor () [0x0002b] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 \n  at Python.Runtime.DelegateManager..ctor () [0x00061] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 \n  at Python.Runtime.PythonEngine.Initialize (System.Collections.Generic.IEnumerable`1[T] args, System.Boolean setSysArgv, System.Boolean initSigs) [0x0000a] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 \n  at Python.Runtime.PythonEngine.Initialize (System.Boolean setSysArgv, System.Boolean initSigs) [0x00005] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 \n  at Python.Runtime.PythonEngine.InitExt () [0x00000] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 \n<\/code><\/pre>\n<p>Environment :<\/p>\n<ul>\n<li>Amazon Linux AMI release 2018.03<\/li>\n<li>Mono JIT compiler version 6.12.0.90 (tarball Fri Mar  5 04:37:13 UTC 2021)<\/li>\n<li>Python 3.6.13<\/li>\n<li>conda 4.8.4<\/li>\n<li>pythonnet 2.4.0<\/li>\n<\/ul>\n<p>Similar symptoms :<\/p>\n<p><a href=\"https:\/\/github.com\/pythonnet\/pythonnet\/issues\/1034\" rel=\"nofollow noreferrer\">System.DllNotFoundException when trying to import clr, despite referenced assembly existing #1034<\/a> : I confirmed libmono-native.so library there in the right directory.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 11:33:42.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"mono|amazon-sagemaker|python.net",
        "Question_view_count":288,
        "Owner_creation_date":"2012-10-05 12:17:18.773 UTC",
        "Owner_last_access_date":"2022-08-10 05:13:32.573 UTC",
        "Owner_reputation":81,
        "Owner_up_votes":85,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Tokyo",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67212443",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70150710,
        "Question_title":"Sagemaker pipeline",
        "Question_body":"<p>I defined new pipeline code from sagemaker abalone pipeline but I ended up using same pipeline as abalone pipeline <a href=\"https:\/\/i.stack.imgur.com\/tZZYh.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>What should I do? Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-29 06:48:45.97 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pipeline|amazon-sagemaker",
        "Question_view_count":96,
        "Owner_creation_date":"2021-03-01 08:20:57.357 UTC",
        "Owner_last_access_date":"2021-12-27 05:56:16.733 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70150710",
        "Question_exclusive_tag":"Amazon SageMaker"
    }
]