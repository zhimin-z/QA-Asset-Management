[
    {
        "Question_id":63188242,
        "Question_title":"AWS SageMaker: Use S3 pickled models instead of hosting on sagemaker",
        "Question_body":"<p>I am working on a use-case for which I have to use Amazon SageMaker notebook instances. Amazon SM resources are filled with material that works well for single model i.e. you do your thing locally on NB Instance and then deploy the model as an endpoint. My use-case on the other hand has multiple models for multiple customers and this needs to be automated. i.e. once a customer uploads a file, a model needs to be automatically created and stored.<\/p>\n<p>Current approach is to automate SageMaker instances through lambda for picking up the train data, training the data and saving the model back to S3 before closing the instance.<\/p>\n<p>My question is, is this the right approach? Or should I create an endpoint for each model for each customer? Somehow since the data size is going to be small and I am working with SageMaker for the first time, I am more comfortable with saving the models in S3 than deploying many many endpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596183617173,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|scikit-learn|xgboost|amazon-sagemaker",
        "Question_view_count":162,
        "Owner_creation_time":1443176197327,
        "Owner_last_access_time":1606295340757,
        "Owner_location":"Dallas, TX, USA",
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1596197428707,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63188242",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59462107,
        "Question_title":"AWS SageMaker TensorFlow Serving - Endpoint failure - CloudWatch log ref: \"NET_LOG: Entering the event loop ...\"",
        "Question_body":"<p>It's my first time using sagemaker to serve my own custom tensorflow model so I have been using the medium articles to get me started:<\/p>\n\n<p><a href=\"https:\/\/medium.com\/ml-bytes\/how-to-create-a-tensorflow-serving-container-for-aws-sagemaker-4853842c9751\" rel=\"noreferrer\">How to Create a TensorFlow Serving Container for AWS SageMaker<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-push-a-docker-image-to-aws-ecs-repository-fba579a9f419?\" rel=\"noreferrer\">How to Push a Docker Image to AWS ECS Repository<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-deploy-an-aws-sagemaker-container-using-tensorflow-serving-4587dad76169?\" rel=\"noreferrer\">How to Deploy an AWS SageMaker Container Using TensorFlow Serving<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-make-predictions-against-a-sagemaker-endpoint-using-tensorflow-serving-8b423b9b316a\" rel=\"noreferrer\">How to Make Predictions Against a SageMaker Endpoint Using TensorFlow Serving<\/a><\/p>\n\n<p>I managed to create my serving container, push it successfully to ECR, and create the sagemaker model from my docker image. However, when i tried to create the endpoints it started creating but after 3-5 minutes ended with the failure message:<\/p>\n\n<blockquote>\n  <p>\"The primary container for production variant Default did not pass the\n  ping health check. Please check CloudWatch logs for this endpoint.\"<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/iXCnK.png\" rel=\"noreferrer\">Failure Image<\/a><\/p>\n\n<p>I then checked my cloud watch logs which looked like this...<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pdp4S.png\" rel=\"noreferrer\">CloudWatch Logs<\/a><\/p>\n\n<p>...ending with \"NET_LOG: Entering the event loop ...\"<\/p>\n\n<p>I tried to google more about this log message in relation to deploying sagemaker models with tf-serving, but could not find any helpful solutions.<\/p>\n\n<p>To give more context, before running into this problem I encountered 2 other issues:<\/p>\n\n<blockquote>\n  <ol>\n  <li>\"FileSystemStoragePathSource encountered a file-system access error:\n  Could not find base path <p>&lsaquo;MODEL_PATH&rsaquo;\/&lsaquo;MODEL_NAME&rsaquo;\/ for &lsaquo;MODEL_NAME&rsaquo;\"<\/li>\n  <li>\"No versions of servable  found under base path\"<\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>Both of which I managed to solve using the following links:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\" rel=\"noreferrer\">[Documentation] TensorFlowModel endpoints need the <code>export\/Servo<\/code> folder structure, but this is not documented<\/a><br><br>\n<a href=\"https:\/\/github.com\/tensorflow\/serving\/issues\/697\" rel=\"noreferrer\">Failed Reason: The primary container for production variant AllTraffic did not pass the ping health check.<\/a><br\/><\/p>\n\n<p>It's also worth noting that my Tensorflow model was created using TF version 2.0 (hence why I needed the docker container). I solely used AWS CLI to carry out my tensorflow serving instead of the sagemaker SDK.<\/p>\n\n<p>Here are snippets of my shell scripts:<\/p>\n\n<p><strong>nginx.config<\/strong><\/p>\n\n<pre><code>events {\n    # determines how many requests can simultaneously be served\n    # https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-optimize-nginx-configuration\n    # for more information\n    worker_connections 2048;\n}\n\nhttp {\n  server {\n    # configures the server to listen to the port 8080\n    # Amazon SageMaker sends inference requests to port 8080.\n    # For more information: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response\n    listen 8080 deferred;\n\n    # redirects requests from SageMaker to TF Serving\n    location \/invocations {\n      proxy_pass http:\/\/localhost:8501\/v1\/models\/pornilarity_model:predict;\n    }\n\n    # Used by SageMaker to confirm if server is alive.\n    # https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests\n    location \/ping {\n      return 200 \"OK\";\n    }\n  }\n}\n<\/code><\/pre>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>\n# RUN pip install sagemaker-containers\n\n# Installing NGINX, used to reverse proxy the predictions from SageMaker to TF Serving\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx git\n\n# Copy our model folder to the container \n# NB: Tensorflow serving requires you manually assign version numbering to models e.g. model_path\/1\/\n# see below links: \n\n# https:\/\/stackoverflow.com\/questions\/45544928\/tensorflow-serving-no-versions-of-servable-model-found-under-base-path\n# https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\nCOPY pornilarity_model \/opt\/ml\/model\/export\/Servo\/1\/\n\n# Copy NGINX configuration to the container\nCOPY nginx.conf \/opt\/ml\/code\/nginx.conf\n\n# Copies the hosting code inside the container\n# COPY serve.py \/opt\/ml\/code\/serve.py\n\n# Defines serve.py as script entrypoint\n# ENV SAGEMAKER_PROGRAM serve.py\n\n# starts NGINX and TF serving pointing to our model\nENTRYPOINT service nginx start | tensorflow_model_server --rest_api_port=8501 \\\n --model_name=pornilarity_model \\\n --model_base_path=\/opt\/ml\/model\/export\/Servo\/\n<\/code><\/pre>\n\n<p><strong>Build and push<\/strong><\/p>\n\n<pre><code>%%sh\n\n# The name of our algorithm\necr_repo=sagemaker-tf-serving\ndocker_image=sagemaker-tf-serving\n\ncd container\n\n# chmod a+x container\/serve.py\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\nregion=$(aws configure get region)\nregion=${region:-eu-west-2}\n\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com\/${ecr_repo}:latest\"\n\n# If the repository doesn't exist in ECR, create it.\n\naws ecr describe-repositories --repository-names \"${ecr_repo}\" &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name \"${ecr_repo}\" &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build -t ${docker_image} .\n# docker tag ${docker_image} ${fullname}\ndocker tag ${docker_image}:latest ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n\n<p><strong>Create SageMaker Model<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nCONTAINER_NAME=\"Pornilarity-Container\"\nMODEL_NAME=pornilarity-model-v1\n\n# the role named created with\n# https:\/\/gist.github.com\/mvsusp\/599311cb9f4ee1091065f8206c026962\nROLE_NAME=AmazonSageMaker-ExecutionRole-20191202T133391\n\n# the name of the image created with\n# https:\/\/gist.github.com\/mvsusp\/07610f9cfecbec13fb2b7c77a2e843c4\nECS_IMAGE_NAME=sagemaker-tf-serving\n# the role arn of the role\nEXECUTION_ROLE_ARN=$(aws iam get-role --role-name ${ROLE_NAME} | jq -r .Role.Arn)\n\n# the ECS image URI\nECS_IMAGE_URI=$(aws ecr describe-repositories --repository-name ${ECS_IMAGE_NAME} |\\\njq -r .repositories[0].repositoryUri)\n\n# defines the SageMaker model primary container image as the ECS image\nPRIMARY_CONTAINER=\"ContainerHostname=${CONTAINER_NAME},Image=${ECS_IMAGE_URI}\"\n\n# Createing the model\naws sagemaker create-model --model-name ${MODEL_NAME} \\\n--primary-container=${PRIMARY_CONTAINER}  --execution-role-arn ${EXECUTION_ROLE_ARN}\n<\/code><\/pre>\n\n<p><strong>Endpoint config<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nMODEL_NAME=pornilarity-model-v1\n\nENDPOINT_CONFIG_NAME=pornilarity-model-v1-config\n\nENDPOINT_NAME=pornilarity-v1-endpoint\n\nPRODUCTION_VARIANTS=\"VariantName=Default,ModelName=${MODEL_NAME},\"\\\n\"InitialInstanceCount=1,InstanceType=ml.c5.large\"\n\naws sagemaker create-endpoint-config --endpoint-config-name ${ENDPOINT_CONFIG_NAME} \\\n--production-variants ${PRODUCTION_VARIANTS}\n\naws sagemaker create-endpoint --endpoint-name ${ENDPOINT_NAME} \\\n--endpoint-config-name ${ENDPOINT_CONFIG_NAME}\n<\/code><\/pre>\n\n<p><strong>Docker Container Folder Structure<\/strong><\/p>\n\n<pre><code>\u251c\u2500\u2500 container\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 nginx.conf\n\u2502   \u251c\u2500\u2500 pornilarity_model\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u251c\u2500\u2500 saved_model.pb\n\u2502   \u2502   \u2514\u2500\u2500 variables\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00000-of-00002\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00001-of-00002\n\u2502   \u2502       \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>Any guidance would be much appreciated!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577141884527,
        "Question_score":5,
        "Question_tags":"docker|tensorflow|nginx|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":1220,
        "Owner_creation_time":1459173531210,
        "Owner_last_access_time":1664040349470,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59462107",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60868257,
        "Question_title":"AWS SageMaker on GPU",
        "Question_body":"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. <\/p>\n\n<p>Would anyone be able to help in this regard.<\/p>\n\n<p>Thanks &amp; Best Regards<\/p>\n\n<p>Michael<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585228922137,
        "Question_score":11,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":13664,
        "Owner_creation_time":1484485648903,
        "Owner_last_access_time":1663903489200,
        "Owner_location":"Melbourne Australia",
        "Owner_reputation":605,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":133,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:<\/p>\n\n<ol>\n<li><p>You can instantiate a GPU-powered <strong><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"noreferrer\">SageMaker Notebook Instance<\/a><\/strong>, for example <code>p2.xlarge<\/code> (NVIDIA K80) or <code>p3.2xlarge<\/code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi<\/code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. <\/p><\/li>\n<li><p>Another option is to use a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\"><strong>SageMaker Training Job<\/strong><\/a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode\/\" rel=\"noreferrer\">here a well documented TensorFlow example<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585261947503,
        "Answer_score":20.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60868257",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828741,
        "Question_title":"Accelerate BERT training with HuggingFace Model Parallelism",
        "Question_body":"<p>I am currently using SageMaker to train BERT and trying to improve the BERT training time. I use PyTorch and Huggingface on AWS g4dn.12xlarge instance type.<\/p>\n<p>However when I run parallel training it is far from achieving linear improvement. I'm looking for some hints on distributed training to improve the BERT training time in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663940876633,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|huggingface-transformers|bert-language-model|distributed-training",
        "Question_view_count":12,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66570138,
        "Question_title":"How to setup AWS sagemaker - Resource limit Error",
        "Question_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615398273417,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-ec2|aws-lambda|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1530468231707,
        "Owner_last_access_time":1663982618347,
        "Owner_location":null,
        "Owner_reputation":357,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1626975605177,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53624568,
        "Question_title":"AWS Ground Truth text classification manifest using \"source-ref\" not displaying text",
        "Question_body":"<h2>Background<\/h2>\n\n<p>I'm trying out SageMaker Ground Truth, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms.html\" rel=\"nofollow noreferrer\">an AWS service to help you label your data before using it in your ML algorithms<\/a>.<\/p>\n\n<p>The labeling job requires a manifest file which contains a JSON object per row that contains a <code>source<\/code> or a <code>source-ref<\/code>, see also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">Input Data section<\/a> of the documentation. <\/p>\n\n<h2>Setup<\/h2>\n\n<p>Source-ref is a reference to where the document is located in an S3 bucket like so<\/p>\n\n<pre><code>my-bucket\/data\/manifest.json\nmy-bucket\/data\/123.txt\nmy-bucket\/data\/124.txt\n\n...\n<\/code><\/pre>\n\n<p>The manifest file looks like this (based on the <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/\" rel=\"nofollow noreferrer\">blog example<\/a>) :<\/p>\n\n<pre><code>{\"source-ref\": \"s3:\/\/my-bucket\/data\/123.txt\"}\n{\"source-ref\": \"s3:\/\/my-bucket\/data\/124.txt\"}\n...\n<\/code><\/pre>\n\n<h2>The problem<\/h2>\n\n<p>When I create the job, all I get is the <code>source-ref<\/code> value: <strong>s3:\/\/my-bucket\/data\/123.txt<\/strong> as the text, the contents of the file are not displayed.<\/p>\n\n<p>I have tried creating jobs using a manifest that does not contain the s3 protocol, but I get the same result.<\/p>\n\n<p>Is this a bug on their end or I'm I missing something?<\/p>\n\n<h2>Observations<\/h2>\n\n<ul>\n<li>I have tried to make all files public, thinking there may maybe permissions issue? but no<\/li>\n<li>I ensured that the content type of the file was text (s3 -> object -> properties -> metadata)<\/li>\n<li>If I use \"source\" and inline the text, it works properly, but I should be able to use individual documents as there is a limit on the file size specially if I have to label many or large documents!<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1543979158243,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1506,
        "Owner_creation_time":1250097276470,
        "Owner_last_access_time":1663732003037,
        "Owner_location":"Santa Monica, CA, USA",
        "Owner_reputation":7013,
        "Owner_up_votes":2704,
        "Owner_down_votes":18,
        "Owner_views":445,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53624568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51391639,
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1531871216253,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":1576,
        "Owner_creation_time":1400315847157,
        "Owner_last_access_time":1663788698353,
        "Owner_location":null,
        "Owner_reputation":4342,
        "Owner_up_votes":318,
        "Owner_down_votes":25,
        "Owner_views":315,
        "Question_last_edit_time":1531893078607,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1531919252070,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67396101,
        "Question_title":"read data from rds mysql in sagemaker",
        "Question_body":"<p>Im new in AWS world. I have a rds mysql database. And i wanna read data from this db in sagemaker, but there is a problem.\nActually on local I can read this data like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def readFromAWS():\n    conn = pymysql.connect(host='MY-ENDPOINT', port=3306, user='MY-USERNAME', password='MY-PASSWORD', db='DB-NAME')\n\n    sqlText = &quot;&quot;&quot;SELECT * FROM DB-NAME.XXX order by ID&quot;&quot;&quot;\n\n    df = pd.read_sql(sqlText, conn)\n    conn.close()\n    return df\ndf=readFromAWS()\n<\/code><\/pre>\n<p>And on local it works well. But in sagemaker this code is useless.\nI need some advice. Are there any other way to read this? Should i use other services to read this data in sagemaker? Can you give me some advice??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620195792650,
        "Question_score":2,
        "Question_tags":"mysql|amazon-web-services|amazon-s3|amazon-rds|amazon-sagemaker",
        "Question_view_count":426,
        "Owner_creation_time":1587688322720,
        "Owner_last_access_time":1661976002297,
        "Owner_location":"\u0130zmir, T\u00fcrkiye",
        "Owner_reputation":63,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67396101",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55549880,
        "Question_title":"Transforming a PNG image in application\/x-recordio-protobuf for calling a sagemaker endpoint in C#",
        "Question_body":"<p>I am trying to do some inferences on a SageMaker endpoint in C# using a <code>InvokeEndpointRequest<\/code> object. My inference body is a PNG or JPEG image. However, SageMaker requires an <code>application\/x-recordio-protobuf<\/code> format. How can I convert my image file into this format to be able to use InvokeEndpoint with the above object.<\/p>\n<pre><code>InvokeEndpointRequest invokeRequest = new InvokeEndpointRequest\n{\n  EndpointName = &quot;kmeans-2019-xx-xx-xx-xx-xx-xxx&quot;,\n  Body= GetImageFromFile(),\n  ContentType= &quot;application\/x-recordio-protobuf&quot;\n};\nInvokeEndpointResponse invokeResponse = smClient.InvokeEndpoint(invokeRequest);\n<\/code><\/pre>\n<p>For the moment the <code>GetImageFromFile<\/code> method just reads an image file and transforms it in <code>MemoryStream<\/code>:<\/p>\n<pre><code>Stream stream = openFileDialog.OpenFile();\n\nbyte[] data = new byte[stream.Length];\nstream.Read(data, 0, (int)stream.Length);\nMemoryStream ms=new MemoryStream(data);\n            \nreturn ms;\n<\/code><\/pre>\n<p>I tried to serialize the <code>MemoryStream<\/code> by using <code>Protobuf-net<\/code>, but it does not work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554559637090,
        "Question_score":0,
        "Question_tags":"c#|amazon-sagemaker",
        "Question_view_count":973,
        "Owner_creation_time":1554558813163,
        "Owner_last_access_time":1560410909097,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1604106646103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55549880",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49582307,
        "Question_title":"How to schedule tasks on SageMaker",
        "Question_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1522449441927,
        "Question_score":14,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":17339,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1523213115337,
        "Answer_score":19.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1570200701000,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64945483,
        "Question_title":"How to store a .tar.gz formatted model to AWS SageMaker and use it as a deployed model?",
        "Question_body":"<p>I have a pre-trained BERT model which was trained on Google Cloud Platform, and the model is stored in a .tar.gz formatted file, I wanted to deploy this model to SageMaker and also be able to trigger the model via API, how can I achieve this?<\/p>\n<p>I found <a href=\"https:\/\/stackoverflow.com\/questions\/54916866\/with-aws-sagemaker-is-it-possible-to-deploy-a-pre-trained-model-using-the-sagem\">this question<\/a> is a little bit related to what I'm asking here, but it's for a scikit-learn model, I'm new to this area, can someone give me some guidance regarding this? Many thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1605976621920,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|deployment|amazon-sagemaker|bert-language-model",
        "Question_view_count":649,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64945483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68606277,
        "Question_title":"AWS Sagemaker DeepAR Validation Error Additional Properties not allowed ('training' was unexpected)",
        "Question_body":"<p>I don't know what the issue is. Here is the code:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,\n    train_instance_type=&quot;ml.m5.large&quot;,\n    base_job_name=&quot;deepar-stock&quot;,\n    output_path=s3_output_path,\n)\n\nhyperparameters = {\n    &quot;time_freq&quot;: &quot;24H&quot;,\n    &quot;epochs&quot;: &quot;100&quot;,\n    &quot;early_stopping_patience&quot;: &quot;10&quot;,\n    &quot;mini_batch_size&quot;: &quot;64&quot;,\n    &quot;learning_rate&quot;: &quot;5E-4&quot;,\n    &quot;context_length&quot;: str(context_length),\n    &quot;prediction_length&quot;: str(prediction_length),\n    &quot;likelihood&quot;: &quot;gaussian&quot;,\n}\n\nestimator.set_hyperparameters(**hyperparameters)\n\n%%time\n\nestimator.fit(inputs=f&quot;{s3_data_path}\/train\/&quot;)\n<\/code><\/pre>\n<p>And when I try to train the model I get the following error (in its entirety).<\/p>\n<pre><code>------------------------------------------------------------------------\n\n---\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;timed eval&gt; in &lt;module&gt;\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    681         self.jobs.append(self.latest_training_job)\n    682         if wait:\n--&gt; 683             self.latest_training_job.wait(logs=logs)\n    684 \n    685     def _compilation_job_name(self):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1626         # If logs are requested, call logs_for_jobs.\n   1627         if logs != &quot;None&quot;:\n-&gt; 1628             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1629         else:\n   1630             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3658 \n   3659         if wait:\n-&gt; 3660             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3661             if dot:\n   3662                 print()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3218                 ),\n   3219                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3220                 actual_status=status,\n   3221             )\n   3222 \nUnexpectedStatusException: Error for Training job deepar-2021-07-31-22-25-54-110: Failed. Reason: ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\n\nCaused by: Additional properties are not allowed ('training' was unexpected)\n\nFailed validating 'additionalProperties' in schema:\n    {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#',\n     'additionalProperties': False,\n     'anyOf': [{'required': ['train']}, {'required': ['state']}],\n     'definitions': {'data_channel': {'properties': {'ContentType': {'enum': ['json',\n                                                                              'json.gz',\n                                                                              'parquet',\n                                                                              'auto'],\n                                                                     'type': 'string'},\n                                                     'RecordWrapperType': {'enum': ['None'],\n\nOn instance:\n    {'training': {'RecordWrapperType': 'None',\n                  'S3DistributionType': 'FullyReplicated',\n                  'TrainingInputMode': 'File'}}\n<\/code><\/pre>\n<p>Here it says <code>'training' was unexpected<\/code>. I don't know why it says <code>'training'<\/code> on that last line <code>On instance:<\/code>. I don't know how to solve this. I've looked at other pages for help but I can't find a straight answer. I know that my data is structured right. The errors seem to be with the hyperparameters but I don't know that for sure. Please help!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1627771170133,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker|deepar",
        "Question_view_count":157,
        "Owner_creation_time":1405609706593,
        "Owner_last_access_time":1660146579013,
        "Owner_location":"Michigan, United States",
        "Owner_reputation":3113,
        "Owner_up_votes":145,
        "Owner_down_votes":5,
        "Owner_views":317,
        "Question_last_edit_time":1627771514767,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68606277",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62111580,
        "Question_title":"Trouble opening audio files stored on S3 in SageMaker",
        "Question_body":"<p>I stored like 300 GB of audio data (mp3\/wav mostly) on Amazon S3 and am trying to access it in a SageMaker notebook instance to do some data transformations. I'm trying to use either torchaudio or librosa to load a file as a waveform. torchaudio expects the file path as the input, librosa can either use a file path or file-like object. I tried using s3fs to get the url to the file but torchaudio doesn't recognize it as a file. And apparently SageMaker has problems installing librosa so I can't use that. What should I do?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1590899282700,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":749,
        "Owner_creation_time":1590899033110,
        "Owner_last_access_time":1655505282813,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I ended up not using SageMaker for this, but for anybody else having similar problems, I solved this by opening the file using s3fs and writing it to a <code>tempfile.NamedTemporaryFile<\/code>. This gave me a file path that I could pass into either <code>torchaudio.load<\/code> or <code>librosa.core.load<\/code>. This was also important because I wanted the extra resampling functionality of <code>librosa.core.load<\/code>, but it doesn't accept file-like objects for loading mp3s.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1592257562683,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62111580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72120382,
        "Question_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Question_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651705997730,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":809,
        "Owner_creation_time":1324682328743,
        "Owner_last_access_time":1663467557403,
        "Owner_location":null,
        "Owner_reputation":969,
        "Owner_up_votes":768,
        "Owner_down_votes":10,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_time":1651734900863,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64448720,
        "Question_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Question_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1603208778703,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":263,
        "Owner_creation_time":1592311727163,
        "Owner_last_access_time":1647692327233,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603707766790,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57630271,
        "Question_title":"AWS Sagemaker - using cross validation instead of dedicated validation set?",
        "Question_body":"<p>When I train my model locally I use a 20% test set and then cross validation. Sagameker seems like it needs a dedicated valdiation set (at least in the tutorials I've followed). Currently I have 20% test, 10% validation leaving 70% to train - so I lose 10% of my training data compared to when I train locally, and there is some performance loss as a results of this. <\/p>\n\n<p>I could just take my locally trained models and overwrite the sagemaker models stored in s3, but that seems like a bit of a work around. Is there a way to use Sagemaker without having to have a dedicated validation set? <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566578862943,
        "Question_score":3,
        "Question_tags":"amazon-web-services|cross-validation|amazon-sagemaker",
        "Question_view_count":1401,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57630271",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61389632,
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587652058893,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":398,
        "Owner_creation_time":1473770138817,
        "Owner_last_access_time":1662976446233,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588063849633,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67213383,
        "Question_title":"Training on Sagemaker GPU is too slow",
        "Question_body":"<p>I've launched a training on CelebA dataset for a binary classification with PyTorch, in Sagemaker Studio.<\/p>\n<p>I've made sure all, model, tensors are sent to cuda().<\/p>\n<p>My image dataset is in S3, and I'm accessing it via this import and code:<\/p>\n<pre><code>from PIL import Image\nimport s3fs\n\nfs = s3fs.S3FileSystem()\n\n# example\nf = fs.open(f's3:\/\/aoha-bucket\/img_celeba\/dataset\/000001.jpg')\n<\/code><\/pre>\n<p>And of course my PyTorch DataLoader class, which is using of s3fs to load data into DataLoaders.<\/p>\n<pre><code>class myDataset(Dataset):\n    def __init__(self, csv_file, root_dir, target, length, adv = None, transform=None):\n        self.annotations = pd.read_csv(csv_file).iloc[:length,:]\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target = target\n        self.length = length\n        self.adv = adv\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = fs.open(os.path.join(self.root_dir, self.annotations.loc[index, 'image_id']))\n        image = Image.open(img_path)\n        image = np.array(image)\n\n        if self.transform:\n            image = self.transform(image=image)[&quot;image&quot;]\n\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.Tensor(image)\n\n        y_label = torch.tensor(int(self.annotations.loc[index, str(self.target)]))\n\n        if self.adv is None:\n            return image, y_label\n\n        if self.adv :\n            z_label = torch.tensor(int(self.annotations.loc[index, 'origin']))\n            return image, y_label, z_label\n<\/code><\/pre>\n<p>when I run this function, I get True:<\/p>\n<pre><code>next(model.parameters()).is_cuda\n<\/code><\/pre>\n<p>My issue is that, I don't know the training is too slow, even slower than my local CPU (not that powerful). It says for example, that one epoch is needing 1h45minutes, which is way too much.<\/p>\n<p>Im' using a GPU optimized PyTorch instance of Studio.<\/p>\n<p>Have you ever launched a training on GPU in Sagemaker using PyTorch ?\nCould you please help ?<\/p>\n<p>Thank you very much,\nHabib<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1619094768503,
        "Question_score":0,
        "Question_tags":"computer-vision|pytorch|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_time":1465645376333,
        "Owner_last_access_time":1662979460033,
        "Owner_location":"Boulogne-Billancourt, France",
        "Owner_reputation":29,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1619129413807,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67213383",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64158911,
        "Question_title":"Load Python Pickle File from S3 Bucket to Sagemaker Notebook",
        "Question_body":"<p>I have attempted the code on the many posts on how to load a pickle file (1.9GB) from an S3 bucket, but none seem to work for our notebook instance on AWS Sagemaker.  Notebook size is 50GB.<\/p>\n<p>Some of the methods attempted:<\/p>\n<p>Method 1<\/p>\n<pre><code>import io\nimport boto3\n\nclient = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\n\nbytes_io.seek(0) \nbyte_value = pickle.load(bytes_io)\n<\/code><\/pre>\n<p>This gives:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Method 2: This actually gets me something back with no error:<\/p>\n<pre><code>client = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\nbyte_value = bytes_buffer.getvalue()\nimport sys\nsys.getsizeof(byte_value)\/(1024**3)\n<\/code><\/pre>\n<p>this returns: 1.93<\/p>\n<p>but how do I convert the byte_value into the pickled object?\nI tried this:<\/p>\n<pre><code>pickled_data = pickle.loads(byte_value)\n<\/code><\/pre>\n<p>But the kernel &quot;crashed&quot; - went idle and I lost all variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601567432000,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":2485,
        "Owner_creation_time":1338137210000,
        "Owner_last_access_time":1661852953917,
        "Owner_location":null,
        "Owner_reputation":1937,
        "Owner_up_votes":249,
        "Owner_down_votes":0,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64158911",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66793845,
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616651745117,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker|imread",
        "Question_view_count":164,
        "Owner_creation_time":1519936486960,
        "Owner_last_access_time":1634702229683,
        "Owner_location":"Minneapolis, MN, USA",
        "Owner_reputation":1113,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":122,
        "Question_last_edit_time":1616652513867,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1616684272013,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54184145,
        "Question_title":"AWS Sagemaker does not update the package",
        "Question_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1547478776530,
        "Question_score":5,
        "Question_tags":"python|conda|amazon-sagemaker",
        "Question_view_count":1546,
        "Owner_creation_time":1527781503483,
        "Owner_last_access_time":1625555943023,
        "Owner_location":"Metz, France",
        "Owner_reputation":352,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1547708377157,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73292975,
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660052882547,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1394703217223,
        "Owner_last_access_time":1663244105390,
        "Owner_location":"Cologne, Germany",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1660654017400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_time":1578932319743,
        "Owner_last_access_time":1663935286190,
        "Owner_location":"Ireland",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646329084320,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646761223947,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71773449,
        "Question_title":"What is the fastest way to pull massive amounts of data from Snowflake Database into AWS SageMaker?",
        "Question_body":"<p>What would be the fastest way to pull in very large datasets from Snowflake into my SageMaker instance in AWS? How does the snowflake python connector (what I currently use) compare to lets say a spark connector to snowflake?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1649278117853,
        "Question_score":0,
        "Question_tags":"apache-spark|bigdata|snowflake-cloud-data-platform|amazon-sagemaker",
        "Question_view_count":320,
        "Owner_creation_time":1587310981713,
        "Owner_last_access_time":1659020084953,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71773449",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52549626,
        "Question_title":"Sagemaker memory leak",
        "Question_body":"<p>I deployed a deep learning model in the sagemaker and created a endpoint.\nUnfortunately, I put it a large size image then the endpoint return  'RuntimeError: CUDA error: out of memory'.\nSo I would like to re-launch the endpoint, but seems there is not any restart button.\nWhat could I do for restarting it?<\/p>\n\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1538116329853,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":581,
        "Owner_creation_time":1499669667270,
        "Owner_last_access_time":1624531353080,
        "Owner_location":"\u65e5\u672cKanagawa-ken",
        "Owner_reputation":321,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52549626",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56860955,
        "Question_title":"Problem running Dask on AWS Sagemaker and AWS Fargate",
        "Question_body":"<p>I am trying to setup a cluster on AWS to run distributed sklearn model training with dask. To get started, I was trying to follow this tutorial which I hope to tweak: <a href=\"https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4<\/a><\/p>\n\n<p>I have managed to push the docker container to AWS ECR and then launch a  CloudFormation template to build a cluster on AWS Fargate. The next step in the tutorial is to launch an AWS Sagemaker Notebook. I have tried this but something is not working because when I run the commands I get errors (see image). <strong>What might the problem be? Could it be related to the VPC\/subnets? Is it related to AWS Sagemaker internet access?<\/strong> (I have tried enabling and disabling this).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Expected Results: dask to update, scaling up of the Fargate cluster to work.<\/p>\n\n<p>Actual Results: none of the above.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562108134793,
        "Question_score":0,
        "Question_tags":"dask|amazon-sagemaker|dask-distributed|aws-fargate",
        "Question_view_count":1239,
        "Owner_creation_time":1523019477347,
        "Owner_last_access_time":1661113536183,
        "Owner_location":null,
        "Owner_reputation":192,
        "Owner_up_votes":175,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1562108491117,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56860955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60405600,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582681249783,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mxnet|gluon",
        "Question_view_count":351,
        "Owner_creation_time":1472967821507,
        "Owner_last_access_time":1663314033960,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1582681944470,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1583126137777,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72422041,
        "Question_title":"Invoking Sagemaker MultiDataModel Endpoint throws \"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation\"",
        "Question_body":"<p>I'm trying to create a multi-model endpoint on sagemaker, using pre-trained tensorflow models  which were uploaded to s3 (tar.gz files). Creating a 'single-model' endpoint works fine with both of them.<\/p>\n<p>I followed a few blog posts for this task (<a href=\"https:\/\/dataintegration.info\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/towardsdatascience.com\/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f\" rel=\"nofollow noreferrer\">2<\/a>).<\/p>\n<p>I've successfully deployed a MultiDataModel endpoint on Sagemaker (code attached below the error), but when trying to invoke a model (any of them) I received the following error:<\/p>\n<pre><code>~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/model.py in predict(self, data, initial_args)\n    105                 args[&quot;CustomAttributes&quot;] = self._model_attributes\n    106 \n--&gt; 107         return super(TensorFlowPredictor, self).predict(data, args)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n    159             data, initial_args, target_model, target_variant, inference_id\n    160         )\n--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    162         return self._handle_response(response)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    414             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 415             return self._make_api_call(operation_name, kwargs)\n    416 \n    417         _api_call.__name__ = str(py_operation_name)\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    744             error_class = self.exceptions.from_code(error_code)\n--&gt; 745             raise error_class(parsed_response, operation_name)\n    746         else:\n    747             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: \nReceived server error (504) from model with message &quot;&lt;html&gt;\n&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;\/title&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;\/h1&gt;&lt;\/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx\/1.20.2&lt;\/center&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n&quot;. See https:\/\/eu-central-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-central- 1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/mme-tensorflow-2022-05-29-06-38-29 in \naccount ******** for more information.\n<\/code><\/pre>\n<p>Here is the code for creating and deploying the models and the endpoint:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nrating_model_archive = &quot;rating_model.tar.gz&quot;\nsim_users_model_archive = &quot;sim_users_model.tar.gz&quot;\ncurrent_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n\nsagemaker_model_rating = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{rating_model_archive}',\n                                         name = f'rating-model-{current_time}',\n                                         role = role,\n                                         framework_version = &quot;2.8&quot;, #tf.__version__,\n                                         entry_point = 'empty_train.py',\n                                         sagemaker_session=sagemaker_session)\n\nsagemaker_model_sim = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{sim_users_model_archive}',\n                                      name = f'similar-users-model-{current_time}',\n                                      role = role,\n                                      framework_version = &quot;2.8&quot;, #tf.__version__,\n                                      entry_point = 'empty_train.py',\n                                      sagemaker_session=sagemaker_session)\n\nmodel_data_prefix = f's3:\/\/{bucket_name}\/model\/'\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=sagemaker_model_rating,\n                     sagemaker_session=sagemaker_session)\n\ntf_predictor = mme.deploy(initial_instance_count=2,\n                          instance_type=&quot;ml.m4.xlarge&quot;,#'ml.t2.medium',\n                          endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>Up until here, as mentioned earlier, it works fine, and I have a running endpoint.\nWhen trying to invoke it with the following code, I get the aforementioned error:<\/p>\n<pre><code>input1 = {\n    &quot;instances&quot;: [\n        {&quot;user_id&quot;: [854],\n         &quot;item_id&quot;: [123]}\n                 ]\n}\n\ninput2 = {\n    &quot;instances&quot;: [12]\n}\n\ntf_predictor.predict(data=input2, initial_args={'TargetModel': sim_users_model_archive})\n# tf_predictor.predict(data=input1, initial_args={'TargetModel': rating_model_archive})\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653812629237,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1653481867867,
        "Owner_last_access_time":1659990406947,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72422041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70044114,
        "Question_title":"tensorflow.python.framework.errors_impl.NotFoundError: \/opt\/ml\/input\/data\/train\/label_map.pbtxt; No such file or directory",
        "Question_body":"<p>I've been following this tutorial on <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Training and deploying models using TensorFlow 2 with the Object Detection API on Amazon SageMaker<\/a> but keep on getting the above error when attempting to train the model using estimator.fit(inputs) in train_model.ipynb. All of the code for the tutorial is available at: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api<\/a><\/p>\n<p>The label_map.pbtxt, train.records and validation.records files were successfully created in my bucket (at s3:\/\/bucket\/data\/bees\/tfrecords), and I've adjusted my pipeline.config file to contain:<\/p>\n<pre><code>train_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/train.records&quot;\n  }\n}\n\neval_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/validation.records&quot;\n  }\n}\n<\/code><\/pre>\n<p>I'm completely new to Amazon Sagemaker and containers but have followed the walkthrough to a tee, so I'm lost as to why it's failing. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637394399277,
        "Question_score":1,
        "Question_tags":"python|tensorflow|object-detection|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_time":1637389797727,
        "Owner_last_access_time":1654515886370,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70044114",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70975320,
        "Question_title":"EventBridge trigger: Sagemaker Processing Job finished",
        "Question_body":"<p>I'm currently developing some ETL for my ML model with AWS. The thing is that I want to <strong>trigger<\/strong> a Lambda when some Sagemaker Processing Job is finished. And the <strong>event<\/strong> passed to the Lambda, should be the configuration info (job name, arguments, etc..) of the Sagemaker Processing Job.<\/p>\n<p><strong>Q1<\/strong>: How can I do to <em>trigger the event<\/em> when the Processing Job is finished?<\/p>\n<p><strong>Q2<\/strong>: How can I do to pass the <em>Processing Job configurations as an event<\/em> for the Lambda?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643907548550,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":504,
        "Owner_creation_time":1519511545083,
        "Owner_last_access_time":1663921636670,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1643931586140,
        "Answer_body":"<p>You can use the following EventBridge rule pattern:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;]\n  }\n}\n<\/code><\/pre>\n<p>The ProcessingJobStatus list can be modified based on which statuses you want to handle.<\/p>\n<p>You can set a Lambda function as the target of your EventBridge rule.<\/p>\n<p>Here is a sample event which will be passed to your Lambda, taken from AWS console:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;version&quot;: &quot;0&quot;,\n  &quot;id&quot;: &quot;0a15f67d-aa23-0123-0123-01a23w89r01t&quot;,\n  &quot;detail-type&quot;: &quot;SageMaker Processing Job State Change&quot;,\n  &quot;source&quot;: &quot;aws.sagemaker&quot;,\n  &quot;account&quot;: &quot;123456789012&quot;,\n  &quot;time&quot;: &quot;2019-05-31T21:49:54Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [&quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingInputs&quot;: [{\n      &quot;InputName&quot;: &quot;InputName&quot;,\n      &quot;S3Input&quot;: {\n        &quot;S3Uri&quot;: &quot;s3:\/\/input\/s3\/uri&quot;,\n        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/local\/path&quot;,\n        &quot;S3DataType&quot;: &quot;MANIFEST_FILE&quot;,\n        &quot;S3InputMode&quot;: &quot;PIPE&quot;,\n        &quot;S3DataDistributionType&quot;: &quot;FULLYREPLICATED&quot;\n      }\n    }],\n    &quot;ProcessingOutputConfig&quot;: {\n      &quot;Outputs&quot;: [{\n        &quot;OutputName&quot;: &quot;OutputName&quot;,\n        &quot;S3Output&quot;: {\n          &quot;S3Uri&quot;: &quot;s3:\/\/output\/s3\/uri&quot;,\n          &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/output\/local\/path&quot;,\n          &quot;S3UploadMode&quot;: &quot;CONTINUOUS&quot;\n        }\n      }],\n      &quot;KmsKeyId&quot;: &quot;KmsKeyId&quot;\n    },\n    &quot;ProcessingJobName&quot;: &quot;integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingResources&quot;: {\n      &quot;ClusterConfig&quot;: {\n        &quot;InstanceCount&quot;: 3,\n        &quot;InstanceType&quot;: &quot;ml.c5.xlarge&quot;,\n        &quot;VolumeSizeInGB&quot;: 5,\n        &quot;VolumeKmsKeyId&quot;: &quot;VolumeKmsKeyId&quot;\n      }\n    },\n    &quot;StoppingCondition&quot;: {\n      &quot;MaxRuntimeInSeconds&quot;: 2000\n    },\n    &quot;AppSpecification&quot;: {\n      &quot;ImageUri&quot;: &quot;012345678901.dkr.ecr.us-west-2.amazonaws.com\/processing-uri:latest&quot;\n    },\n    &quot;NetworkConfig&quot;: {\n      &quot;EnableInterContainerTrafficEncryption&quot;: true,\n      &quot;EnableNetworkIsolation&quot;: false,\n      &quot;VpcConfig&quot;: {\n        &quot;SecurityGroupIds&quot;: [&quot;SecurityGroupId1&quot;, &quot;SecurityGroupId2&quot;, &quot;SecurityGroupId3&quot;],\n        &quot;Subnets&quot;: [&quot;Subnet1&quot;, &quot;Subnet2&quot;]\n      }\n    },\n    &quot;RoleArn&quot;: &quot;arn:aws:iam::012345678987:role\/SageMakerPowerUser&quot;,\n    &quot;ExperimentConfig&quot;: {},\n    &quot;ProcessingJobArn&quot;: &quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingJobStatus&quot;: &quot;Completed&quot;,\n    &quot;LastModifiedTime&quot;: 1589879735000,\n    &quot;CreationTime&quot;: 1589879735000\n  }\n}\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>If you want to match a ProcessingJobName with specific prefix:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;],\n    &quot;ProcessingJobName&quot;: [{\n      &quot;prefix&quot;: &quot;standarize-data&quot;\n    }]\n  }\n}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1643913781760,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1643920466910,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70975320",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61507982,
        "Question_title":"Sagemaker: ImportError: No module named 'pandas' when running training script",
        "Question_body":"<p>At this point I am running this exact notebook:  <a href=\"https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Project\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Project\/SageMaker%20Project.ipynb<\/a><\/p>\n\n<p>Just with a different dataset (that I got to the exact same format seen in the notebook).<\/p>\n\n<p>when I get to call the estimator I get the error:<\/p>\n\n<pre><code>2020-04-29 17:18:03 Starting - Starting the training job...\n2020-04-29 17:18:06 Starting - Launching requested ML instances...\n2020-04-29 17:19:03 Starting - Preparing the instances for training......\n2020-04-29 17:19:54 Downloading - Downloading input data\n2020-04-29 17:19:54 Training - Downloading the training image.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n\n\n\n2020-04-29 17:20:13,936 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand \"\/usr\/bin\/python -m train --epochs 10 --hidden_dim 200\"\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/train.py\", line 11, in &lt;module&gt;\n    import pandas as pd\nImportError: No module named 'pandas'\n\n2020-04-29 17:20:25 Uploading - Uploading generated training model\n2020-04-29 17:20:25 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-37-0e8223086435&gt; in &lt;module&gt;()\n----&gt; 1 estimator.fit({'training': input_data})\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    475         self.jobs.append(self.latest_training_job)\n    476         if wait:\n--&gt; 477             self.latest_training_job.wait(logs=logs)\n    478 \n    479     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1084         # If logs are requested, call logs_for_jobs.\n   1085         if logs != \"None\":\n-&gt; 1086             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1087         else:\n   1088             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3042 \n   3043         if wait:\n-&gt; 3044             self._check_job_status(job_name, description, \"TrainingJobStatus\")\n   3045             if dot:\n   3046                 print()\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   2636                 ),\n   2637                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-&gt; 2638                 actual_status=status,\n   2639             )\n   2640 \n\nUnexpectedStatusException: Error for Training job sagemaker-pytorch-2020-04-29-17-18-03-379: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"\/usr\/bin\/python -m train --epochs 10 --hidden_dim 200\"\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/train.py\", line 11, in &lt;module&gt;\n    import pandas as pd\nImportError: No module named 'pandas'\n<\/code><\/pre>\n\n<p>does anyone have any insights on what I can do to troubleshoot this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1588181659513,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":593,
        "Owner_creation_time":1517932507093,
        "Owner_last_access_time":1648741816033,
        "Owner_location":null,
        "Owner_reputation":331,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61507982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54288886,
        "Question_title":"AWS Sagemaker init 1K+ models \"endpoints\"?",
        "Question_body":"<p>under the assumptions that the model training itself is very fast, I'm wondering what is the best practice to spin up ~ > 1K models endpoints \nfast as possible.<\/p>\n\n<p>Thanks for any hint\nChristian<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548069626337,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":159,
        "Owner_creation_time":1373960648337,
        "Owner_last_access_time":1663331601773,
        "Owner_location":"Europa",
        "Owner_reputation":181,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":1548092518557,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54288886",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73677347,
        "Question_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Question_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662878679543,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|lightgbm|distributed-training|amazon-machine-learning",
        "Question_view_count":27,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663711220203,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61076649,
        "Question_title":"Preprocessing data for Sagemaker Inference Pipeline with Blazingtext",
        "Question_body":"<p>I'm trying to figure out the best way to preprocess my input data for my inference endpoint for AWS Sagemaker. I'm using the BlazingText algorithm.<\/p>\n\n<p>I'm not really sure the best way forward and I would be thankful for any pointers.<\/p>\n\n<p>I currently train my model using a Jupyter notebook in Sagemaker and that works wonderfully, but the problem is that I use NLTK to clean my data (Swedish stopwords and stemming etc):<\/p>\n\n<pre><code>import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n<\/code><\/pre>\n\n<p>So the question is really, how do I get the same  pre-processing logic to the inference endpoint  ?<\/p>\n\n<p>I have a couple of thoughts about how to proceed:<\/p>\n\n<ul>\n<li><p>Build a docker container with the python libs &amp; data installed with the sole purpose of pre-processing the data. Then use this container in the inference pipeline. <\/p><\/li>\n<li><p>Supply the Python libs and Script to an existing container in the same way you can do for external lib an notebook <\/p><\/li>\n<li><p>Build a custom fastText container with the libs I need and run it outside of Sagemaker.<\/p><\/li>\n<li><p>Will probably work, but feels like a \"hack\": Build a Lambda function that has the proper Python libs&amp;data installed and calls the Sagemaker Endpoint. I'm worried about cold start delays as the prediction traffic volume will be low. <\/p><\/li>\n<\/ul>\n\n<p>I would like to go with the first option, but I'm struggling a bit to understand if there is a docker image that I could build from, and add my dependencies to, or if I need to build something from the ground up. For instance, would the image sagemaker-sparkml-serving:2.2 be a good candidate? <\/p>\n\n<p>But maybe there is a better way all around? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1586251113373,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|inference",
        "Question_view_count":848,
        "Owner_creation_time":1420497448900,
        "Owner_last_access_time":1663999495290,
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61076649",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54638364,
        "Question_title":"SageMaker Script Mode + Pipe Mode",
        "Question_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549915827960,
        "Question_score":3,
        "Question_tags":"python|tensorflow|streaming|amazon-sagemaker",
        "Question_view_count":1385,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549923138163,
        "Answer_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1551201776033,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52672171,
        "Question_title":"Model testing on AWS sagemaker \"could not convert string to float\"",
        "Question_body":"<p>The XGboost model was trained on AWS sagemaker and deployed successfully but I keep getting the following error: <strong>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: \".<\/strong> \nAny thoughts?<\/p>\n\n<pre><code>Test data is as following:\n      size       mean\n269   5600.0  17.499633\n103   1754.0   9.270272\n160   4968.0  14.080601\n40       4.0  17.500000\n266  36308.0  11.421855\n\ntest_data_array = test_data.drop(['mean'], axis=1).as_matrix()\ntest_data_array = np.array([np.float32(x) for x in test_data_array])\nxgb_predictor.content_type = 'text\/csv'\nxgb_predictor.serializer = csv_serializer\n\ndef predict(data, rows=32):\n    split_array = np.array_split(data, int(data.shape[0] \/ float(rows) + 1))\n    #print(split_array)\n    predictions = ''\n\n    for array in split_array:\n        print(array[0], type(array[0]))\n        predictions = ','.join([predictions, xgb_predictor.predict(array[0]).decode('utf-8')])\n\n    return np.fromstring(predictions[1:], sep=',')\n\npredictions = predict(test_data_array)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1538767718613,
        "Question_score":3,
        "Question_tags":"data-science|amazon-sagemaker",
        "Question_view_count":1605,
        "Owner_creation_time":1338844022563,
        "Owner_last_access_time":1611332621947,
        "Owner_location":null,
        "Owner_reputation":355,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52672171",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70156631,
        "Question_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Question_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1638197351547,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645489319000,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65238339,
        "Question_title":"How to input fsx for lustre to Amazon Sagemaker?",
        "Question_body":"<p>I am trying to set up Amazon sagemaker reading our dataset from our AWS Fsx for Lustre file system.<\/p>\n<p>We are using the Sagemaker API, and previously we were reading our dataset from s3 which worked fine:<\/p>\n<pre><code>estimator = TensorFlow(\n   entry_point='model_script.py',  \n   image_uri='some-repo:some-tag', \n   instance_type='ml.m4.10xlarge',\n   instance_count=1,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n   subnets=[&quot;subnet-1&quot;],\n   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],\n   debugger_hook_config=False,\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/bucket_name\/data\/{hyperparameters['dataset']}\/&quot;}\n)\n<\/code><\/pre>\n<p>But now that I'm changing the input data source to Fsx Lustre file system, I'm getting an error that the file input should be s3:\/\/ or file:\/\/. I was following these <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs (fsx lustre)<\/a>:<\/p>\n<pre><code>estimator = TensorFlow(\n   entry_point='model_script.py',  \n#    image_uri='some-docker:some-tag', \n   instance_type='ml.m4.10xlarge',\n   instance_count=1,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n   subnets=[&quot;subnet-1&quot;],\n   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],\n   debugger_hook_config=False,\n  )\nfsx_data_folder = FileSystemInput(file_system_id='fs-1',\n                                    file_system_type='FSxLustre',\n                                    directory_path='\/fsx\/data',\n                                    file_system_access_mode='ro')\nestimator.fit(f&quot;{fsx_data_folder}\/{hyperparameters['dataset']}\/&quot;)\n<\/code><\/pre>\n<p>Throws the following error:<\/p>\n<pre><code>ValueError: URI input &lt;sagemaker.inputs.FileSystemInput object at 0x0000016A6C7F0788&gt;\/dataset_name\/ must be a valid S3 or FILE URI: must start with &quot;s3:\/\/&quot; or &quot;file:\/\/&quot;\n<\/code><\/pre>\n<p>Does anyone understand what I am doing wrong? Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607617217073,
        "Question_score":2,
        "Question_tags":"amazon-web-services|tensorflow|amazon-ec2|amazon-sagemaker|amazon-fsx",
        "Question_view_count":592,
        "Owner_creation_time":1591204989660,
        "Owner_last_access_time":1638799637903,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647906496650,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65238339",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61631687,
        "Question_title":"AWS Sagemaker failure after successful training \"ClientError: Artifact upload failed:Insufficient disk space\"",
        "Question_body":"<p>I'm training a network using custom docker image. First training with 50.000 steps everythig was ok, when I tried to increase to 80.000, I got error: \"ClientError: Artifact upload failed:Insufficient disk space\", I just increased the steps number.. this is weird to me. There are no errors in the cloudwatch log, my last entry is: <\/p>\n\n<blockquote>\n  <p>Successfully generated graphs: ['pipeline.config', 'tflite_graph.pb',\n  'frozen_inference_graph.pb', 'tflite_graph.pbtxt',\n  'tflite_quant_graph.tflite', 'saved_model', 'hyperparameters.json',\n  'label_map.pbtxt', 'model.ckpt.data-00000-of-00001',\n  'model.ckpt.meta', 'model.ckpt.index', 'checkpoint']<\/p>\n<\/blockquote>\n\n<p>Which basically means that those files have been created because is a simple:<\/p>\n\n<pre><code>    graph_files = os.listdir(model_path + '\/graph')\n<\/code><\/pre>\n\n<p>Which disk space is talking about? Also looking at the training job I see from the disk utilization chart that the rising curve peaks at 80%...\nI expect that after the successful creation of the aforementioned files, everything is uploaded to my s3 bucket, where no disk space issues are present. Why 50.000 steps is working and 80.000 is not working? \nIt is my understanding that the number of training steps don't influence the size of the model files..<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588756994270,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1744,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1588845916390,
        "Answer_body":"<p>Adding volume size to the training job selecting \"additional storage volume per instance (gb)\" to 5GB on the creation seems to solve the problem. I still don't understand why, but problem seems solved.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1588846223957,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61631687",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65407274,
        "Question_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Question_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608633852607,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-cli|amazon-vpc|amazon-sagemaker",
        "Question_view_count":510,
        "Owner_creation_time":1597855076910,
        "Owner_last_access_time":1622833516510,
        "Owner_location":"Delft, Netherlands",
        "Owner_reputation":60,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1608634938237,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630555307170,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1630555307170,
        "Answer_score":65.0,
        "Question_favorite_count":14.0,
        "Answer_last_edit_time":1656913410063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70553230,
        "Question_title":"How to make predictions on a sagemaker endpoint? (JSON error)",
        "Question_body":"<p>I have deployed a sagemaker endpoint and want to run predictions on the endpoint now. The endpoint represents a sagemaker pipeline and model. I followed the tutorial <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/train-register-deploy-pipeline-model\/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. My code to set up the predictor and make the predictions is as follows:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\npredictor = Predictor(endpoint_name=endpoint_name)\ndata_df = data_df.drop(&quot;LABEL_NAME&quot;, axis=1)\npred_count = 1\npayload = data_df.iloc[:pred_count].to_string(header=False, index=False).replace(&quot;  &quot;, &quot;,&quot;)\np = predictor.predict(payload, initial_args={&quot;ContentType&quot;: &quot;text\/csv&quot;})\n<\/code><\/pre>\n<p>This code is pretty much what they have displayed in the example I linked and it makes sense to me. My preprocess.py code for the pipeline includes the following functions which I am including (although not sure they are relevant):<\/p>\n<pre><code>def input_fn(input_data, content_type):\n    print(&quot;BAHHHHHH&quot;)\n    if content_type == &quot;text\/csv&quot;:\n        # Read the raw input data as CSV.\n        df = pd.read_csv(StringIO(input_data), header=None)\n        return df\n    else:\n        raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))\n\ndef output_fn(prediction, accept):\n    print(&quot;BAHHHHHH&quot;)\n    if accept == &quot;application\/json&quot;:\n        instances = []\n        for row in prediction.tolist():\n            instances.append(row)\n        json_output = {&quot;instances&quot;: instances}\n\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == &quot;text\/csv&quot;:\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n\ndef predict_fn(input_data, model):\n    print(&quot;BAHHHHHH&quot;)\n    features = model.transform(input_data)\n    return features\n\ndef model_fn(model_dir):\n    print(&quot;BAHHHHHH&quot;)\n    &quot;&quot;&quot;Deserialize fitted model&quot;&quot;&quot;\n    preprocessor = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return preprocessor\n<\/code><\/pre>\n<p>When running the predictor.predict() method I get the following error:<\/p>\n<pre><code>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{\n    &quot;error&quot;: &quot;JSON Parse error: Missing a comma or ']' after an array element. at offset: 16&quot;\n<\/code><\/pre>\n<p>I printed out the payload variable right before it was passed to the predict method and it looks like this (I truncated it as it's quite long but this should be enough to see what is is like:<\/p>\n<pre><code>0 999.105105 888.607813 6.0 1 los angeles 2431.666667 1.0 NaN 1177.813623 1.076833e+06 los angeles$1$6 0 60376511012 0.0 0.0 0.0 0.0 0.0 0.0 ............\n<\/code><\/pre>\n<p>The error message also provides a url to look at for more information. It is the cloud watch logs for the endpoint. looking through these logs I see no extra information, just a 400 error with NO additional information apart from the 400 error.<\/p>\n<p>So there is obviously some issue with the format of the data I am passing in. The input_fn, output_fn, predict_fn and model_fn methods all have a print statements in them at the start of the method but none of these show up in the logs so I don't think any of these are being reached.<\/p>\n<p>What am i doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641090584810,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|prediction|amazon-sagemaker",
        "Question_view_count":635,
        "Owner_creation_time":1467398269073,
        "Owner_last_access_time":1663957538923,
        "Owner_location":null,
        "Owner_reputation":2733,
        "Owner_up_votes":40,
        "Owner_down_votes":2,
        "Owner_views":273,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70553230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55949625,
        "Question_title":"How to read json.out file from databricks",
        "Question_body":"<p>I have been working with databricks for reading output from Object2Vec in Sagemaker. This output is saved as jsonlines with <code>.json.out<\/code> file format.<\/p>\n\n<pre><code>df_emb = spark.read.option(\"multiLine\", True).option(\"mode\", \"PERMISSIVE\").json(bucket+key)\n<\/code><\/pre>\n\n<p>When i read this file as a json, it is read as a corrupt record. Below is the screenshot. \n<a href=\"https:\/\/i.stack.imgur.com\/jyMBj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jyMBj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I can provide the actual file if you know the solution.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556790025647,
        "Question_score":0,
        "Question_tags":"amazon-web-services|databricks|amazon-sagemaker",
        "Question_view_count":96,
        "Owner_creation_time":1457261731840,
        "Owner_last_access_time":1663974423117,
        "Owner_location":"Vancouver, BC",
        "Owner_reputation":584,
        "Owner_up_votes":71,
        "Owner_down_votes":6,
        "Owner_views":270,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55949625",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58535527,
        "Question_title":"Amazon Sagemaker ResourceLimitExceeded Error for XGBoost (Free Tier)",
        "Question_body":"<p>I am trying to create an XGBoost model in free tier AWS Sagemaker. I am getting an error of:<\/p>\n\n<p><em>\"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances.\"<\/em>.<\/p>\n\n<p>What is the right train_instance_type I should use?<\/p>\n\n<p>Here is my code:<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\n# Create an instance of the XGBoost model (an estimator), and define the model\u2019s hyperparameters.\n# Note: train_instance_type='ml.m5.large' has 0 free credits! Use one of https:\/\/aws.amazon.com\/sagemaker\/pricing\/ \nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m5.xlarge',output_path='s3:\/\/{}\/{}\/output'.format('my_s3_bucket', prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=1,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n# Train the model using gradient optimization on a ml.m4.xlarge instance\n# After a few minutes, you should start to see the training logs being generated.\nxgb.fit({'train': s3_input_train})\n<\/code><\/pre>\n\n<p>At this step this is what I see:<\/p>\n\n<pre><code>2019-10-22 06:32:51 Starting - Starting the training job...\n2019-10-22 06:33:00 Starting - Launching requested ML instances......\n2019-10-22 06:33:54 Starting - Preparing the instances for training...\n2019-10-22 06:34:41 Downloading - Downloading input data...\n2019-10-22 06:35:22 Training - Training image download completed. Training in progress..Arguments: train\n[2019-10-22:06:35:22:INFO] Running standalone xgboost training.\n[2019-10-22:06:35:22:INFO] Path \/opt\/ml\/input\/data\/validation does not exist!\n[2019-10-22:06:35:22:INFO] File size need to be processed in the node: 3.38mb. Available memory size in the node: 8089.9mb\n[2019-10-22:06:35:22:INFO] Determined delimiter of CSV input is ','\n[06:35:22] S3DistributionType set as FullyReplicated\n[06:35:22] 28831x59 matrix with 1701029 entries loaded from \/opt\/ml\/input\/data\/train?format=csv&amp;label_column=0&amp;delimiter=,\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[0]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[1]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[2]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[3]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[4]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[5]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[6]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[7]#011train-error:0.10839\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[8]#011train-error:0.102737\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[9]#011train-error:0.107697\n<\/code><\/pre>\n\n<p>And then when I deploy this:<\/p>\n\n<pre><code># Deploy the model on a server and create an endpoint that you can access\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n---------------------------------------------------------------------------\nResourceLimitExceeded                     Traceback (most recent call last)\n&lt;ipython-input-38-6d149f3edc98&gt; in &lt;module&gt;()\n      1 # Deploy the model on a server and create an endpoint that you can access\n----&gt; 2 xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)\n    559             tags=self.tags,\n    560             wait=wait,\n--&gt; 561             kms_key=kms_key,\n    562         )\n    563 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\n    464         else:\n    465             self.sagemaker_session.endpoint_from_production_variants(\n--&gt; 466                 self.endpoint_name, [production_variant], tags, kms_key, wait\n    467             )\n    468 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait)\n   1361 \n   1362             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 1363         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   1364 \n   1365     def expand_role(self, role):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n    975 \n    976         self.sagemaker_client.create_endpoint(\n--&gt; 977             EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=tags\n    978         )\n    979         if wait:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    356             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    659             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    660             error_class = self.exceptions.from_code(error_code)\n--&gt; 661             raise error_class(parsed_response, operation_name)\n    662         else:\n    663             return parsed_response\n\nResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\n<\/code><\/pre>\n\n<p><strong>Edit:<\/strong> Trying <strong>ml.m4.xlarge<\/strong> instance:<\/p>\n\n<p>When I use ml.m4.xlarge, I get the same message of \"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\"<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1571899258980,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":5276,
        "Owner_creation_time":1467054577583,
        "Owner_last_access_time":1655311941103,
        "Owner_location":"Dallas, TX, United States",
        "Owner_reputation":4281,
        "Owner_up_votes":374,
        "Owner_down_votes":160,
        "Owner_views":681,
        "Question_last_edit_time":1572003947573,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58535527",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73748608,
        "Question_title":"Debug SparkSQL Query",
        "Question_body":"<p>What are some way I can debug through a sparksql query?<\/p>\n<p>I have defined a dataframe with a sparpksql query and I have included show(1), but the query continues to run very long. Could anyone provide pointers? Thank you!<\/p>\n<pre><code>select t1.id\nfrom table1 t1\njoin table2 t2 on t1.id = t2.cd\nwhere t1.product = 'I'\nand not exists\n(select * from table3 t3\n        where t3.id = t1.id \n        and t3.year = t1.year\n        and a3.month = t1.month\n        and t3.day = t1.day\n        and t3.code in ('4321','5604'))\nand not exists\n(select MAX(status) from table4 t4\n        where t4.id = t1.id\n        and t4.year = t1.year\n        and t4.month = t1.month\n        and t4.day = t1.day\n        having max(status) &gt; 3)\n&quot;&quot;&quot;).show(1)\n<\/code><\/pre>\n<p>list_of_id_df.createOrReplaceTempView(&quot;list_of_id&quot;)\n<br>list_of_id_df.show(1)<\/p>\n<p>print(&quot;Done&quot;)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1663350527313,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|amazon-sagemaker|amazon-sagemaker-debugger",
        "Question_view_count":19,
        "Owner_creation_time":1439843323250,
        "Owner_last_access_time":1664049758657,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":1663612124657,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73748608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51966783,
        "Question_title":"Sagemaker invoke endpoint returned value type?",
        "Question_body":"<p>I have a classifier working with XGBoost in sagemaker, but despite the training set only having 1s and 0s in the first column (csv file, first column is assumed to be target in sagemaker xgboost), the algorithm returns a decimal. <\/p>\n\n<p>First 3 records return 1.08, 0.34, and 0.91. I'd assume probabilities but 1.08? If these are rounded to 0 or 1 then they're all correct, but why is it returning non-class values?<\/p>\n\n<p>Furthermore, the class only contains a predict method - is a predict probability method not possible without using your own model?<\/p>\n\n<p>The code calling this is:<\/p>\n\n<pre><code>from flask import Flask\nfrom flask import request\nimport boto3\nfrom sagemaker.predictor import csv_serializer\nimport sagemaker\n\napp = Flask(__name__)\n\n@app.route(\"\/\")\ndef hello():\n    numbers = request.args.get('numbers')\n\n    #session\n    boto_session = boto3.Session(profile_name=\"profilename\",\n                          region_name='regionname')\n\n    #sagemaker session\n    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n\n    #endpoint\n    predictor = sagemaker.predictor.RealTimePredictor(endpoint=\"modelname\", \n        sagemaker_session=sagemaker_session)\n    predictor.content_type=\"text\/csv\"\n    predictor.serializer=csv_serializer\n    predictor.deserializer=None\n\n    #result\n    result=predictor.predict(numbers)\n    result=result.decode(\"utf-8\")\n    return f'Output: {result}'\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)\n<\/code><\/pre>\n\n<p>The flask section works fine, I can retrieve predictions at 127.0.0.1:5000.<\/p>\n\n<p>Sagemaker Version 1.3.0. Version 1.9.0 does not work - it requires fcntrl which is mac\/linux only - see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/311\" rel=\"nofollow noreferrer\">this on their repo<\/a>, apparently it's fixed on pypi but I've tried and the version doesn't change or fix the issue, so I'm stuck on 1.3.0 until they resolve it. Version 1.3.0 does not have a predict_proba method.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1534940431650,
        "Question_score":4,
        "Question_tags":"python|machine-learning|flask|xgboost|amazon-sagemaker",
        "Question_view_count":990,
        "Owner_creation_time":1530700602213,
        "Owner_last_access_time":1620423334107,
        "Owner_location":null,
        "Owner_reputation":104,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1534965275237,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51966783",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53640440,
        "Question_title":"How to use boto3 cloudwatch for SageMaker submitted training jobs?",
        "Question_body":"<p>I have submitted a few training jobs from AWS SageMaker. I want to use boto3 cloudwatch api to fetch the cloudwatch data to be displayed within jupyter notebook instead of using CloudWatch UI. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544042637003,
        "Question_score":0,
        "Question_tags":"amazon-web-services|boto3|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":588,
        "Owner_creation_time":1488324416977,
        "Owner_last_access_time":1553739959547,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53640440",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586755348657,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_time":1586754432800,
        "Owner_last_access_time":1593512686190,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586886613193,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72580651,
        "Question_title":"Copying python scripts into docker container in sagemaker",
        "Question_body":"<p>I have been frustrated by this copy error while trying to copy a python script to docker container. I am doing this in AWS Sagemaker. Anyone can suggest why this error happens.\nThe train_sm.py code I am trying to copy is inside the src folder. The dockerfile is inside the docker folder.<\/p>\n<p>What is the build context here?\nI also tried moving the .py scripts inside the docker folder, and it still same error.\nThis seems small problem, but has taken my whole day. I\nAny help and hints are appreciated.\nI have attached a screenshot of codes, folder structures and error.<\/p>\n<p>Thanks\n<a href=\"https:\/\/i.stack.imgur.com\/wZvYc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wZvYc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1654903827130,
        "Question_score":0,
        "Question_tags":"python|docker|jupyter|amazon-sagemaker",
        "Question_view_count":68,
        "Owner_creation_time":1501194889140,
        "Owner_last_access_time":1663970802447,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":119,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72580651",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69775255,
        "Question_title":"Recover deleted AWS Sagemaker Jupyter Notebook",
        "Question_body":"<p>I think I may have accidentally deleted a notebook in my Sagemaker instance, is there a way to recover it? Any help is greatly appreciated, thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635545301633,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":238,
        "Owner_creation_time":1631568026793,
        "Owner_last_access_time":1653849249770,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69775255",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60421123,
        "Question_title":"AWS Sagemaker batch transform with JSON input filter",
        "Question_body":"<p>I have a custom Sagemaker instance on a NLP task and trying to run a batch transform on the following json file\n{\"id\":123, \"features\":\"This is a test message\"}'\nand im looking to output the following:\n{\"id\":123,\"SageMakerOutput\":spam}<\/p>\n\n<p>Here's my batch transform code:<\/p>\n\n<pre><code>transformer = sklearn.transformer(instance_count=1,\n                                  instance_type='local',\n                                  accept='application\/json',\n                                  output_path=\"s3:\/\/spam-detection-messages-output\/json_examples\")\n\ntransformer.transform(\"s3:\/\/spam-detection-messages\/json_examples\", content_type='application\/json', input_filter=\"$.features\", join_source=\"Input\", output_filter=\"$['features', SageMakerOutput']\")\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>According to this document,\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html#batch-transform-data-processing-examples\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html#batch-transform-data-processing-examples<\/a>\ni should be able to grab the \"features\" object using input_filter,\nhowever, it grabs the entire json payload. and only outputs the prediction<\/p>\n\n<p>I'm also including my training code<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\nimport glob\nimport io\nimport json\n\nfrom sklearn import tree\nfrom sklearn.externals import joblib\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nvectorizer = TfidfVectorizer()\n\ndef remove_stop_words(words):\n    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n    return result\n\ndef word_stemmer(words):\n    return [stemmer.stem(o) for o in words]\n\ndef word_lemmatizer(words):\n    return [lemmatizer.lemmatize(o) for o in words]\n\ndef remove_characters(words):\n    return [word for word in words if len(word)&gt; 1]\n\ndef clean_token_pipeline(words):\n    cleaning_utils = [remove_stop_words, word_lemmatizer]\n    for o in cleaning_utils:\n        words = o(words)\n    return words\n\ndef process_text(X_train, X_test, y_train, y_test):\n    X_train = [word_tokenize(o) for o in X_train]\n    X_test = [word_tokenize(o) for o in X_test]\n\n    X_train = [clean_token_pipeline(o) for o in X_train]\n    X_test = [clean_token_pipeline(o) for o in X_test]\n\n    X_train = [\" \".join(o) for o in X_train]\n    X_test = [\" \".join(o) for o in X_test]\n\n    return X_train, X_test, y_train, y_test\n\ndef convert_to_feature(raw_tokenize_data):\n    raw_sentences = [' '.join(o) for o in raw_tokenize_data]\n    return vectorizer.transform(raw_sentences)\n\ndef _npy_loads(data):\n    \"\"\"\n    Deserializes npy-formatted bytes into a numpy array\n    \"\"\"\n    stream = io.BytesIO(data)\n    return np.load(stream)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    # Sagemaker specific arguments. Defaults are set in the environment variables.\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n\n    train_data = pd.read_csv(args.train+\"\/spamAssassin_min.csv\", index_col=0)\n    train_data.dropna(inplace=True)\n    print(train_data.head())\n\n    X_train, X_test, y_train, y_test = train_test_split(train_data['message'], train_data['label'], test_size = 0.2, random_state = 1)\n    X_train, X_test, y_train, y_test = process_text(X_train, X_test, y_train, y_test)\n\n    X_train = [o.split(\" \") for o in X_train]\n    X_test = [o.split(\" \") for o in X_test]\n\n    vectorizer = TfidfVectorizer()\n    raw_sentences = [' '.join(o) for o in X_train]\n    vectorizer.fit(raw_sentences)\n\n#     print(\"saving transformer to {}\".format(args.model_dir))\n    joblib.dump(vectorizer, os.path.join(args.model_dir, \"vectorizer.joblib\"))\n\n    x_train_features = convert_to_feature(X_train)\n    x_test_features = convert_to_feature(X_test)\n\n    clf = GaussianNB()\n    clf.fit(x_train_features.toarray(),y_train)\n\n    y_true, y_pred = y_test, clf.predict(x_test_features.toarray())\n    print(classification_report(y_true, y_pred))\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    \"\"\"Deserialized and return fitted model\n\n    Note that this should have the same name as the serialized model in the main method\n    \"\"\"\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n#     print(\"model loaded {}\".format(clf))\n    return clf\n\ndef input_fn(request_body, request_content_type):\n    print(\"** input_fn**\")\n    print(\"request_body:{} request_content_type:{}\".format(request_body, request_content_type))\n    if request_content_type == \"text\/plain\":\n        #convert to string\n        message = str(request_body)\n        return message\n    elif request_content_type == \"application\/json\":\n        request_body_json = json.loads(request_body)\n#         print(\"json {}\".format(request_body_json))\n        return request_body_json['features']\n    elif request_content_type == \"application\/x-npy\":\n        return \" \".join(_npy_loads(request_body))\n    else:\n        # Handle other content-types here or raise an Exception\n        # if the content type is not supported.\n        return request_body\n\ndef predict_fn(input_data, model):\n\n    print(\"** predict_fn**\")\n    print(\"input_data: {} model:{}\".format(input_data, model))\n    print(\"\\n\")\n\n    prefix = '\/opt\/ml\/'\n    model_path = os.path.join(prefix, 'model')\n    my_vect = joblib.load(os.path.join(model_path, \"vectorizer.joblib\"))\n\n    message = \"\".join(clean_token_pipeline(input_data))\n    print(\"processed message: {}\".format(message))\n    message = my_vect.transform([message])\n    message = message.toarray()\n    prediction = model.predict(message)\n    return prediction\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1582745160383,
        "Question_score":1,
        "Question_tags":"json|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1955,
        "Owner_creation_time":1285379271580,
        "Owner_last_access_time":1649074179223,
        "Owner_location":null,
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60421123",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47838705,
        "Question_title":"How do I invoke a Amazon SageMaker endpoint with the Python SDK",
        "Question_body":"<p>I'm trying to use this very simple command:<\/p>\n\n<p><code>\nimport boto3\nclient = boto3.client('sagemaker-runtime')\n<\/code><\/p>\n\n<p>listed in the <a href=\"https:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>\n\n<p>but i'm getting this error:<\/p>\n\n<p><code>UnknownServiceError: Unknown service: 'sagemaker-runtime'. Valid service names are: acm, etc..<\/code><\/p>\n\n<p>My goal is to be able to invoke the endpoint that I've created in Amazon SageMaker.<\/p>\n\n<p>I'm doing this from a Jupyter notebook in Sagemaker, so I feel like this should work no problem. How do I get it to run here, and outside of the Sagemaker environment?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1513365839550,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":3238,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a very new service (December 2017).<\/p>\n\n<p>You will need to update your boto library to use it:<\/p>\n\n<pre><code>sudo pip install boto --upgrade\nsudo pip install boto3 --upgrade\nsudo pip install awscli --upgrade\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1513376500687,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47838705",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69523257,
        "Question_title":"Error in deploying PyTorch model using SageMaker Pipeline and RegisterModel",
        "Question_body":"<p>Can anyone provide an example for deploying a pytorch model using <strong>SageMaker Pipeline<\/strong>?<\/p>\n<p>I've used the MLOps template (MLOps template for model building, traing and deployment) of SageMaker Studio to build a MLOps project.<\/p>\n<p>The template is using sagemaker pipelines to build a pipeline for preprocessing and training and registering the model.\nAnd deployment script is implemented in the YAML file and employing CloudFormation to run. The deployment script will be triggered automatically when the model is registered.<\/p>\n<p>The template is using xgboost model to train the data and deploy the model. I want to use Pytorch and deploy it.\nI successfully replaced the pytorch with xgboost and successfully preprocessed the data, trained the model and registered the model. But I didn't use inference.py in my model. So I get error for the model deployment.<\/p>\n<p><strong>The error log in updating the endpoint is:<\/strong><\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/code\/inference.py'\n<\/code><\/pre>\n<p>I tried to find example of using inference.py for pytorch model, but I couldn't find any example which uses <strong>sagemaker pipelines<\/strong> and <strong>RegisterModel<\/strong>.<\/p>\n<p>Any help would be appreciated.<\/p>\n<p>Below you can see a part of the pipeline for training and registering the model.<\/p>\n<pre><code>from sagemaker.pytorch.estimator import PyTorch\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import (\n    ProcessingStep,\n    TrainingStep,\n)\nfrom sagemaker.workflow.step_collections import RegisterModel\n\npytorch_estimator = PyTorch(entry_point= os.path.join(BASE_DIR, 'train.py'),\n                            instance_type= &quot;ml.m5.xlarge&quot;,\n                            instance_count=1,\n                            role=role,\n                            framework_version='1.8.0',\n                            py_version='py3',\n                            hyperparameters = {'epochs': 5, 'batch-size': 64, 'learning-rate': 0.1})\n\nstep_train = TrainingStep(\n        name=&quot;TrainModel&quot;,\n        estimator=pytorch_estimator,\n\n        inputs={\n                &quot;train&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;train_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;,\n                        ),\n                &quot;dev&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;dev_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n                &quot;test&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;test_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n        },\n)\nstep_register = RegisterModel(\n            name=&quot;RegisterModel&quot;,\n            estimator=pytorch_estimator,\n            model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            content_types=[&quot;text\/csv&quot;],\n            response_types=[&quot;text\/csv&quot;],\n            inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n            transform_instances=[&quot;ml.m5.large&quot;],\n            model_package_group_name=model_package_group_name,\n            approval_status=model_approval_status,\n        )\n    \npipeline = Pipeline(\n            name=pipeline_name,\n            parameters=[\n                processing_instance_type,\n                processing_instance_count,\n                training_instance_type,\n                model_approval_status,\n                input_data,\n            ],\n            steps=[step_process, step_train, step_register],\n            sagemaker_session=sagemaker_session,\n        )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633941357330,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-cloudformation|amazon-sagemaker|endpoint|mlops",
        "Question_view_count":258,
        "Owner_creation_time":1450889293150,
        "Owner_last_access_time":1663403554823,
        "Owner_location":"Finland",
        "Owner_reputation":398,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":28,
        "Question_last_edit_time":1633953506383,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69523257",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71278400,
        "Question_title":"Can't start Sagemaker Notebook Instance with a CodeCommit repo",
        "Question_body":"<p>I linked a repo in CodeCommit to Sagemaker. However when I try to start an instance with that repo it fails and I get a message:<\/p>\n<pre><code>fatal: unable to access 'https:\/\/git-codecommit.us-east-1.amazonaws.com\/v1\/repos\/MyRepo\/': The requested URL returned error: 403\n<\/code><\/pre>\n<p>I think maybe it has something to do with the IAM role. Is there some policy I should add to the AmazonSageMaker-ExecutionRole. I am completely new to this so please excuse any incorrect usage of terms here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645893074433,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|aws-codecommit",
        "Question_view_count":264,
        "Owner_creation_time":1563317093577,
        "Owner_last_access_time":1656618520413,
        "Owner_location":null,
        "Owner_reputation":349,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71278400",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61791589,
        "Question_title":"Sagemaker: MemoryError: Unable to allocate ___for an array with shape ___ and data type float64",
        "Question_body":"<p>I am running a notebook in sagemaker and it seems like one of the arrays produced after vectorizing text is causing issues.<\/p>\n\n<p>Reading other answers it seems like it is an issue with <a href=\"https:\/\/www.kernel.org\/doc\/Documentation\/vm\/overcommit-accounting\" rel=\"noreferrer\">overcommit<\/a>. And one of the solutions proposed is to set it to always overcommit with this:<\/p>\n\n<pre><code>$ echo 1 &gt; \/proc\/sys\/vm\/overcommit_memory\n<\/code><\/pre>\n\n<p>Is there any documentation or do you have any suggestion on how to do the same thing in sagemaker?<\/p>\n\n<p>Thank you very much.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589441004223,
        "Question_score":8,
        "Question_tags":"arrays|python-3.x|pandas|memory|amazon-sagemaker",
        "Question_view_count":2821,
        "Owner_creation_time":1517932507093,
        "Owner_last_access_time":1648741816033,
        "Owner_location":null,
        "Owner_reputation":331,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61791589",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50441181,
        "Question_title":"How to ensure software package version consistency in AWS SageMaker serverless compute?",
        "Question_body":"<p>I am learning AWS SageMaker which is supposed to be a serverless compute environment for Machine Learning. In this type of serverless compute environment, who is supposed to ensure the software package consistency and update the versions?<\/p>\n\n<p>For example, I ran the demo program that came with SageMaker, deepar_synthetic. In this second cell, it executes the following: !conda install -y s3fs<\/p>\n\n<p>However, I got the following warning message:<\/p>\n\n<p>Solving environment: done\n==> WARNING: A newer version of conda exists. &lt;==\n  current version: 4.4.10\n  latest version: 4.5.4\nPlease update conda by running\n    $ conda update -n base conda<\/p>\n\n<p>Since it is serverless compute, am I still supposed to update the software packages myself?<\/p>\n\n<p>Another example is as follows. I wrote a few simple lines to find out the package versions in Jupyter notebook:<\/p>\n\n<p>import platform<\/p>\n\n<p>import tensorflow as tf<\/p>\n\n<p>print(platform.python_version())<\/p>\n\n<p>print (tf.<strong>version<\/strong>)<\/p>\n\n<p>However, I got the following warning messages:<\/p>\n\n<p>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/importlib\/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\nreturn f(*args, **kwds)\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/h5py\/<strong>init<\/strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float<\/code> to <code>np.floating<\/code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type<\/code>.\nfrom ._conv import register_converters as _register_converters<\/p>\n\n<p>The prints still worked and I got the results shown beolow:<\/p>\n\n<p>3.6.4\n1.4.0<\/p>\n\n<p>I am wondering what I have to do to get the package consistent so that I don't get the warning messages. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526869228233,
        "Question_score":0,
        "Question_tags":"amazon-web-services|serverless|amazon-sagemaker",
        "Question_view_count":926,
        "Owner_creation_time":1461112434223,
        "Owner_last_access_time":1648939073663,
        "Owner_location":"San Jose, CA, United States",
        "Owner_reputation":1075,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":181,
        "Question_last_edit_time":1531211855607,
        "Answer_body":"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. <\/p>\n\n<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: \u201cNew\u201d -> \u201cTerminal\u201d. \nNote: By default, conda installs to the root environment. <\/p>\n\n<p>The following are instructions you can follow <a href=\"https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html<\/a> on how to install libraries in the particular conda environment. <\/p>\n\n<p>In general you will need following commands, <\/p>\n\n<pre><code>conda env list \n<\/code><\/pre>\n\n<p>which list all of your conda environments <\/p>\n\n<pre><code>source activate &lt;conda environment name&gt; \n<\/code><\/pre>\n\n<p>e.g. source activate python3 <\/p>\n\n<pre><code>conda list | grep &lt;package&gt; \n<\/code><\/pre>\n\n<p>e.g. conda list | grep numpy \nlist what are the current package versions <\/p>\n\n<pre><code>pip install numpy \n<\/code><\/pre>\n\n<p>Or <\/p>\n\n<pre><code>conda install numpy \n<\/code><\/pre>\n\n<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. <\/p>\n\n<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1532115126593,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50441181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59516365,
        "Question_title":"Amazon SageMaker Ground Truth Custom Labeling Jobs Error: Cannot read property 'taskInput' of null",
        "Question_body":"<p>When creating a custom labeling job for Amazon SageMaker Ground Truth Custom \nAm getting the following error:<\/p>\n\n<p><code>Cannot read property 'taskInput' of null<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577585232053,
        "Question_score":1,
        "Question_tags":"label|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1377141139087,
        "Owner_last_access_time":1663825261377,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":6358,
        "Owner_up_votes":4611,
        "Owner_down_votes":1,
        "Owner_views":229,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59516365",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59684156,
        "Question_title":"Deploy custom prebuilt model on Sagemaker",
        "Question_body":"<p>I am newbee to ML world. I have a docker image which has the required custom model files. <\/p>\n\n<p>Can some one please explain how to deploy this models as webservice?<\/p>\n\n<p>I tried creating a endpoint and lambda function but no luck.<\/p>\n\n<p>Your help will be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1578668405833,
        "Question_score":1,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_time":1412863488017,
        "Owner_last_access_time":1645766798810,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59684156",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68819792,
        "Question_title":"AWS Glue AccessDeniedException in SageMaker",
        "Question_body":"<p>I am running the &quot;Explain Credit Decisions&quot; solution from Sagemaker Studio. I am following the instructions in the solution notebooks. The solution has been launched with my root user id. But when running <code>1_datasets.ipynb<\/code> I am getting the below error when running the step\n<code>glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)<\/code><\/p>\n<pre><code>An error occurred (AccessDeniedException) when calling the GetJob operation: User: arn:aws:sts::myaccountid:assumed-role\/rolecreatedbystack\/GlueJobRunnerSession is not authorized to perform: glue:GetJob on resource: arn:aws:glue:us-east-1:myaccountid:job\/sagemaker-soln-ecd-js-foccb4-job\n<\/code><\/pre>\n<p>The Cloud Formation stacks are created and the scripts should create the required roles and access which are needed to run this solution.\nI have to run a POC with this solution with my custom data. So can you please help to solve the problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629212822180,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":610,
        "Owner_creation_time":1607542203403,
        "Owner_last_access_time":1648498177160,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1631629987127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68819792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55868121,
        "Question_title":"Random cut forest anomaly detection on multi variant time series data",
        "Question_body":"<p>I have sensor data coming from equipment with times series along with many attributes,<\/p>\n\n<p>I have used RCF algorithm to detect anomalies.\nNow the challenge is,how to to convince the end user whether it is really anomaly or not.\nJust want to know which attribute is contributing to anomaly.<\/p>\n\n<p>Is there any best way to convince end user whether it is really anomaly or not.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556283345330,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|anomaly-detection",
        "Question_view_count":984,
        "Owner_creation_time":1554856627173,
        "Owner_last_access_time":1612521912313,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1556364062917,
        "Answer_body":"<p>The simplest way to run the RCF model and to get the explanation for the anomaly is to use the version of RCF in Kinesis Analytics (KA). Here is a link to the documentation of how to run from the KA documentations: <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html<\/a><\/p>\n\n<p>Kinesis is taking care both for the training of the model, the inference after the initial training and for the attribution and explanation of the variables. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p25FR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p25FR.png\" alt=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/images\/anomaly_results.png\"><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1556363986930,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55868121",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57462973,
        "Question_title":"Specify S3 location for model output to sagemaker training job duplication issue",
        "Question_body":"<p>Im a using a SageMaker training job to train an ML model, and I am attempting to output the model to a specific location on S3.<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>model_uri = \"s3:\/\/***\/model\/\"\nscript_path = 'entry_point.py'\nsklearn = SKLearn(\n    entry_point=script_path,\n    train_instance_type=\"ml.m5.large\",\n    output_path=model_uri,\n    role='***',\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>The issue I am having is that the training job will save the model <strong>twice<\/strong>. Once in the S3 bucket at the top level, and once in the folder specified (<code>\/model<\/code>).<\/p>\n\n<p>Top level:\n<a href=\"https:\/\/i.stack.imgur.com\/HbSsX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HbSsX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Model folder:\n<a href=\"https:\/\/i.stack.imgur.com\/59vYA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/59vYA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Is this expected behaviour when specifying <code>output_path<\/code> in the estimator? Is there a way to stop it?<\/p>\n\n<p>Any help would be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565619650897,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":1168,
        "Owner_creation_time":1442932369407,
        "Owner_last_access_time":1662303207090,
        "Owner_location":"Belfast",
        "Owner_reputation":1682,
        "Owner_up_votes":165,
        "Owner_down_votes":4,
        "Owner_views":164,
        "Question_last_edit_time":1565621952967,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57462973",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51305956,
        "Question_title":"S3 read Sagemaker trained model",
        "Question_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1531399214863,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2742,
        "Owner_creation_time":1432680790120,
        "Owner_last_access_time":1559548299510,
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1531400147427,
        "Answer_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1531429065220,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50241771,
        "Question_title":"Getting error while infering sagemaker endpoint",
        "Question_body":"<p>I created training job in sagemaker with my own training and inference code using MXNet framework. I am able to train the model successfully and created endpoint as well. But while inferring the model, I am getting the following error:<\/p>\n\n<p><strong><em>\u2018ClientError: An error occurred (413) when calling the InvokeEndpoint operation: HTTP content length exceeded 5246976 bytes.\u2019<\/em><\/strong><\/p>\n\n<p>What I understood from my research is the error is due to the size of the image. The image shape is (480, 512, 3). I trained the model with images of same shape (480, 512, 3).<\/p>\n\n<p>When I resized the image to (240, 256), the error was gone. But producing another error 'shape inconsistent in convolution' as I the trained the model with images of size (480, 512).<\/p>\n\n<p>I didn\u2019t understand why I am getting this error while inferring.\nCan't we use images of larger size to infer the model?\nAny suggestions will be helpful<\/p>\n\n<p>Thanks, Harathi<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1525811131837,
        "Question_score":0,
        "Question_tags":"python|mxnet|amazon-sagemaker",
        "Question_view_count":1317,
        "Owner_creation_time":1520464061813,
        "Owner_last_access_time":1652212276653,
        "Owner_location":null,
        "Owner_reputation":789,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50241771",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69403349,
        "Question_title":"SageMaker endpoint can't load huggingface tokenizer",
        "Question_body":"<p>I used Amazon SageMaker to train a HuggingFace model. At the end of the training script provided to the estimator, I saved the model into the correct path (<code>SM_MODEL_DIR<\/code>):<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])\n    \n    ...\n    \n    trainer.model.save_pretrained(args.model_dir)\n<\/code><\/pre>\n<p>After the model was trained, I deployed it using the <code>deploy<\/code> method of the HuggingFace estimator. Once the endpoint was successfully created, I tried inference with the returned predictor:<\/p>\n<pre><code>response = self.predictor.predict(\n    {&quot;inputs&quot;: &quot;I want to know where is my order&quot;}\n)\n<\/code><\/pre>\n<p>And I received the following client error:<\/p>\n<pre><code>{'code': 400, 'type': 'InternalServerException', 'message': &quot;Can't load tokenizer for '\/.sagemaker\/mms\/models\/model'. Make sure that:\\n\\n- '\/.sagemaker\/mms\/models\/model' is a correct model identifier listed on 'https:\/\/huggingface.co\/models'\\n\\n- or '\/.sagemaker\/mms\/models\/model' is the correct path to a directory containing relevant tokenizer files\\n\\n&quot;}\n<\/code><\/pre>\n<p>Why cannot the tokenizer be loaded?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1633079347327,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":176,
        "Owner_creation_time":1632991357333,
        "Owner_last_access_time":1663942581167,
        "Owner_location":"Barcelona",
        "Owner_reputation":373,
        "Owner_up_votes":36,
        "Owner_down_votes":4,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69403349",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47913649,
        "Question_title":"What is the SageMaker url for Tensorboard?",
        "Question_body":"<p>I'm trying to access the Tensorboard for the <code>tensorflow_resnet_cifar10_with_tensorboard<\/code> example, but not sure what the url should be, the help text gives 2 options:<\/p>\n\n<blockquote>\n  <p>You can access TensorBoard locally at <a href=\"http:\/\/localhost:6006\" rel=\"noreferrer\">http:\/\/localhost:6006<\/a> or using\n  your SageMaker notebook instance proxy\/6006\/(TensorBoard will not work\n  if forget to put the slash, '\/', in end of the url). If TensorBoard\n  started on a different port, adjust these URLs to match.<\/p>\n<\/blockquote>\n\n<p>When it says access locally, does that mean the local container Sagemaker creates in AWS? If so, how do I get there?<\/p>\n\n<p>Or if I use <code>run_tensorboard_locally=False<\/code>, what should the proxy url be? <\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1513800473547,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6006,
        "Owner_creation_time":1395773370030,
        "Owner_last_access_time":1664024675337,
        "Owner_location":"San Francisco Bay Area",
        "Owner_reputation":1784,
        "Owner_up_votes":38,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47913649",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58985124,
        "Question_title":"Why does AWS SageMaker run a web server for batch transform?",
        "Question_body":"<p>I'm creating my own Docker container for use with SageMaker and I'm wondering why the serve command creates a Flask app to serve predictions on data when I want to do a batch transform job. Wouldn't it be simpler to just unpickle the model and run the model's predict method on the dataset I want predictions for? I don't need a web api\/endpoint. I just need to automatically generate predictions once a day.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574377879723,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|docker|machine-learning|amazon-sagemaker",
        "Question_view_count":470,
        "Owner_creation_time":1446895388123,
        "Owner_last_access_time":1631632591590,
        "Owner_location":null,
        "Owner_reputation":473,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good question :) using the exact same code for batch inference and online inference reduces development overhead - the exact same stack can be used for both use-cases - and also reduces risks of having different results between something done in Batch and something done online. That being said, SageMaker is very flexible and what you describe can easily be done using the Training API. There is nothing in the Training API forcing you to use it for ML training, it is actually a very versatile docker orchestrator with advanced logging, metadata persistance, and built for fast and distributed data ingestion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574462969597,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58985124",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67814409,
        "Question_title":"How to run different Jupyter notebooks conditionally in a AWS Sagemaker notebook instance",
        "Question_body":"<p>I have a AWS sagemaker notebook instance which have 2 different jupyter notebooks. There are certain conditions in which each of it should work.<\/p>\n<p>So, if the consition A exist, the Jupyter Notebook 1 should run and if Condition B exist, the Jupyter Notebook 2 should run.<\/p>\n<p>I have tried this code so far, but it doesnt work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if condition A:\n\n    sm_client = boto3.client('sagemaker')\n    notebook_instance_name = NotebookInstanceName\n    notebook_instance_name = 'Calorie-1615128222'\n    url = sm_client.create_presigned_notebook_instance_url(NotebookInstanceName=notebook_instance_name)['AuthorizedUrl']\n    \n    print(url)\n\n    url_tokens = url.split('\/')\n    http_proto = url_tokens[0]\n    http_hn = url_tokens[2].split('?')[0].split('#')[0]\n\n    s = requests.Session()\n    r = s.get(url)\n    cookies = &quot;; &quot;.join(key + &quot;=&quot; + value for key, value in s.cookies.items())\n    print(cookies)\n\n    ws = websocket.create_connection(\n        &quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn),\n        cookie=cookies,\n        host=http_hn,\n        origin=http_proto + &quot;\/\/&quot; + http_hn\n    )\n    \n    print(ws)\n    \n    # ws = websockets.connect(&quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn))\n    \n    ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute --to notebook --inplace \/home\/ec2-user\/SageMaker\/Calorie\/Notebook1.ipynb \n--ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    #ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute Notebook1.ipynb --ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    \n    time.sleep(5)\n    ws.close()\n    print(&quot;websocket client created&quot;)\n    #return None\n    \n<\/code><\/pre>\n<p>Please help. Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622684648107,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|aws-sam",
        "Question_view_count":413,
        "Owner_creation_time":1522557704627,
        "Owner_last_access_time":1661301014560,
        "Owner_location":"Australia",
        "Owner_reputation":813,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":172,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67814409",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72833918,
        "Question_title":"Eval_Metrics not recognized\/invalid in AWS XGBoost model",
        "Question_body":"<pre><code>xgb.set_hyperparameters(objective='binary:logistic',num_round=100)\nxgb.fit({'train': s3_input_train})\n\n...\n\n\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                         'min_child_weight': ContinuousParameter(1, 10),\n                         'alpha': ContinuousParameter(0, 2),\n                         'max_depth': IntegerParameter(1, 10),\n                         'num_round': IntegerParameter(1, 300),\n                        'gamma': ContinuousParameter(0, 5),\n                        'lambda': ContinuousParameter(0, 1000),\n                        'max_delta_step':IntegerParameter(1, 10),\n                        'colsample_bylevel':ContinuousParameter(0.1, 1),\n                        'colsample_bytree':ContinuousParameter(0.5, 1),\n                        'subsample':ContinuousParameter(0.5, 1)}\n\n\nobjective_metric_name = 'validation:aucpr'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_val}, include_cls_metadata=False, wait=False)\n<\/code><\/pre>\n<p>Returns the error:<\/p>\n<pre><code>\nAn error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [validation:aucpr], isn\u2019t valid for the [811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest] algorithm. Choose a valid objective metric.\n<\/code><\/pre>\n<p>The same applies when replacing aucpr with f1 and logloss. They are clearly defined as evaluation metrics in the documentation for classification purposes. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a><\/p>\n<p>What can I do to allow the f1, aucpr and logloss evaluation metrics?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656704152423,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_time":1633542845253,
        "Owner_last_access_time":1663696816443,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72833918",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48319893,
        "Question_title":"Is there some kind of persistent local storage in aws sagemaker model training?",
        "Question_body":"<p>I did some experimentation with aws sagemaker, and the download time of large data sets from S3 is very problematic, especially when the model is still in development, and you want some kind of initial feedback relatively fast<\/p>\n\n<p>Is there some kind of local storage or other way to speed things up?<\/p>\n\n<p><strong>EDIT<\/strong>\nI refer to the batch training service, that allows you to submit a job as a docker container.<\/p>\n\n<p>While this service is intended for already validated jobs that typically run for a long time (which makes the download time less significant) there's still a need for quick feedback<\/p>\n\n<ol>\n<li><p>There's no other way to do the \"integration\" testing of your job with the sagemaker infrastructure (configuration files, data files, etc.)<\/p><\/li>\n<li><p>When experimenting with different variations to the model, it's important to be able to get initial feedback relatively fast<\/p><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1516273838943,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":5129,
        "Owner_creation_time":1296032582893,
        "Owner_last_access_time":1663846720077,
        "Owner_location":"Israel",
        "Owner_reputation":7829,
        "Owner_up_votes":1436,
        "Owner_down_votes":12,
        "Owner_views":965,
        "Question_last_edit_time":1578086688793,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48319893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67422447,
        "Question_title":"Features extraction in Real-time prediction in sagemaker",
        "Question_body":"<p>i want to deploy a real time prediction machine learning model for fraud detection using sagemaker.<\/p>\n<p>i used sagemaker jupyter instance to:<\/p>\n<pre><code>-load my training data from s3 contains transactions\n-preprocessing data and features engineering (i use category_encoders to encode the categorical value)\n-training the model and configure the endpoint\n<\/code><\/pre>\n<p>For the inference step , i used a lambda function which  invoke my endpoint to get the prediction for each real time transaction.<\/p>\n<pre><code>should i calculte again all the features for this real time transactions in lambda function ?\n\nfor the features when i use category_encoders with fit_transform() function to transform my categorical feature to numerical one, what should I do because the result will not be the same as training set?\n\nis there another method not to redo the calculation of the features in the inference step?\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620319047827,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|lambda|amazon-sagemaker|fraud-prevention",
        "Question_view_count":85,
        "Owner_creation_time":1606221676633,
        "Owner_last_access_time":1660602413737,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67422447",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72903481,
        "Question_title":"Cant generate XGBoost training report in sagemaker, only profiler_report",
        "Question_body":"<p>I am trying to generate the XGBoost trainingreport to see feature importances however the following code only generates the profiler report.<\/p>\n<pre><code>from sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, my_region, &quot;latest&quot;)\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, &quot;validation&quot;: s3_input_val}, wait=True)\n<\/code><\/pre>\n<p>When Checking the output path via:<\/p>\n<pre><code>rule_output_path = xgb.output_path + &quot;\/&quot; + xgb.latest_training_job.job_name + &quot;\/rule-output&quot;\n! aws s3 ls {rule_output_path} --recursive\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bMCAj.png\" rel=\"nofollow noreferrer\">We only see the profiler report generated<\/a><\/p>\n<p>What am I doing wrong\/missing? I wish to generate the XGboost Training report to see its feature importances.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657223403220,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-s3|xgboost|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_time":1633542845253,
        "Owner_last_access_time":1663696816443,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1657223572763,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72903481",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71790985,
        "Question_title":"Amazon SageMaker Unsupported Media Type: image\/jpeg",
        "Question_body":"<p>I trained my own model using Tensorflow and Keras for image classification and I'm trying to deploy and use it with Amazon's SageMaker. I went through the process of converting the <code>mymodel.h5<\/code> file into a <code>mymodel.tar.gz<\/code> file and moving it to the SageMaker S3 bucket. Then, following a tutorial I created the SageMaker model using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<p>And created the endpoint to access the model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Now, since the point of creating this model was for image classification I'm trying to pass the image to my endpoint but have been getting a response of <code>Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot;}&quot;<\/code>. After reading up I feel like I may need to do some more work to have access to the <code>image\/jpeg<\/code> content type as it seems like the defaults are <code>application\/json<\/code>, <code>text\/libsvm<\/code>, and <code>text\/csv<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649384583073,
        "Question_score":1,
        "Question_tags":"python|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_time":1615987342493,
        "Owner_last_access_time":1664084970783,
        "Owner_location":null,
        "Owner_reputation":386,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1649407350557,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71790985",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64118186,
        "Question_title":"Getting an anomaly score for every datapoint in SageMaker?",
        "Question_body":"<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n<\/code><\/pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:<\/p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n<\/code><\/pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!<\/p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.<\/p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)<\/code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)<\/code>, everything worked as expected.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601377003873,
        "Question_score":0,
        "Question_tags":"amazon-web-services|random-forest|amazon-sagemaker",
        "Question_view_count":54,
        "Owner_creation_time":1589340693533,
        "Owner_last_access_time":1658865421630,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1601394242040,
        "Answer_body":"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation<\/code><\/p>\n<p>Update your code to do inference on <em>multiple<\/em> records at once<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>This will return a list of scores<\/p>\n<p>Assuming <code>apple_stock_volumes_df<\/code> has your volumes and the scores (after running inference on each record):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n<\/code><\/pre>\n<p>There is a great example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a> showing this<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1601377832670,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1601398729940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64118186",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67782131,
        "Question_title":"sagemaker xgboost output to be JSON",
        "Question_body":"<p>I am new to AWS sagemaker and trying to do a simple test, where I am trying to call the xgboost model.<\/p>\n<pre><code>xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'us-east-1', &quot;1.2-1&quot;)\n<\/code><\/pre>\n<p>Creating the model and endpoint:<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nendp_name =&quot;myendpoint&quot;\nacc_model = sm_model.deploy(initial_instance_count=1, \n                instance_type='ml.m5.4xlarge',\n                endpoint_name=endp_name, \n                serializer=JSONSerializer(),\n                deserializer= sagemaker.deserializers.JSONDeserializer()\n                            \n               )\n<\/code><\/pre>\n<p>Creating the predictor instance:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\nfrom sagemaker.serializers import CSVSerializer, JSONSerializer \n\nsess = sagemaker.Session()\n\npayload ={\n&quot;var1&quot;:1,\n&quot;var2&quot;:2,\n&quot;var3&quot;:3,\n&quot;var4&quot;:0,\n&quot;var5&quot;:4,\n&quot;var6&quot;:0,\n&quot;var7&quot;:5,\n&quot;var8&quot;:45,\n\n}\n\npredictor = Predictor(\n    endpoint_name=endp_name, sagemaker_session=sess, serializer=JSONSerializer(),\ndeserializer=JSONSerializer()  )\n<\/code><\/pre>\n<p>and then predicting:<\/p>\n<pre><code>predictor.predict(payload)\n<\/code><\/pre>\n<p>I want the output of the predictor.predict to be JSON format, however when I run this I get this.\n<a href=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What need to be done so that I can see the output as JSON?<\/p>\n<p>PS: If I remove the deserializer I get the output as byte:\n<a href=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and if I change it to CSVDeserializer I get it out as:\n<a href=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622515828260,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_time":1469929726907,
        "Owner_last_access_time":1649618839153,
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1622516341413,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67782131",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66554893,
        "Question_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Question_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615324569760,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-codebuild",
        "Question_view_count":156,
        "Owner_creation_time":1575485255683,
        "Owner_last_access_time":1634441626670,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627578025127,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1628026967547,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69779916,
        "Question_title":"How to automate jupyter notebook execution on aws?",
        "Question_body":"<p>I got a task to complete where I need to automate Jupyter notebook execution on AWS. I'm totally new to AWS environment so don't have any idea how to do it efficiently. Things I need to do are the following -<\/p>\n<ol>\n<li>Need REST API(s) to start and stop Jupyter notebook execution on AWS.<\/li>\n<li>Need to send parameters to the notebook while calling using API.<\/li>\n<\/ol>\n<p>What are the AWS components I need, to perform the above task?<\/p>\n<pre><code>import boto3,time\n \nemr = boto3.client(\n    'emr',\n    region_name='us-west-1'\n)\n \n \n \nstart_resp = emr.start_notebook_execution(\n    EditorId='e-40AC8ZO6EGGCPJ4DLO48KGGGI',\n    RelativePath='boto3_demo.ipynb',\n    ExecutionEngine={'Id':'j-1HYZS6JQKV11Q'},\n    ServiceRole='EMR_Notebooks_DefaultRole'\n)\n \nexecution_id = start_resp[&quot;NotebookExecutionId&quot;]\nprint(execution_id)\nprint(&quot;\\n&quot;)\n \n \ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\n \nprint(describe_response)\nprint(&quot;\\n&quot;)\n \n \n \nlist_response = emr.list_notebook_executions()\nprint(&quot;Existing notebook executions:\\n&quot;)\nfor execution in list_response['NotebookExecutions']:\n    print(execution)\n    print(&quot;\\n&quot;)\n \n \n \nprint(&quot;Sleeping for 5 sec...&quot;)\ntime.sleep(5)\n \nprint(&quot;Stop execution &quot; + execution_id)\nemr.stop_notebook_execution(NotebookExecutionId=execution_id)\ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\nprint(describe_response)\nprint(&quot;\\n&quot;)    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1635599367780,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|jupyter-notebook|amazon-sagemaker|aws-emr-studio",
        "Question_view_count":274,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1635623450200,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69779916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70065420,
        "Question_title":"glueContext can't find files em s3, but SparkSession can. How to solve it",
        "Question_body":"<p>I'm working with LakeFormation and Glue Jobs to process some files.<\/p>\n<p>I've already configured the lake formation. I ran a crawler that properly identifed the two tables and theie respective schemas. The folder in s3 bucket is structured as follow:<\/p>\n<pre><code>| receitafederal-udct-zen\n\n  |----empresas\/\n\n  |----estabelecimentos\/\n<\/code><\/pre>\n<p>I named the database as 'rf-raw' in Glue, and the crawlers identified the following tables:<\/p>\n<ul>\n<li>estabelecimentos<\/li>\n<li>empresas<\/li>\n<\/ul>\n<p>But, the tables doesn't have headers and the data types were note correctly identified. So, I want to run a Glue Job to properly name the columns and set the data types, and other simples transformations (some replacements). Then, I want to storage the transformed files in another s3 bucket. But first, I'm testing some PySpark in the SageMaker notebook (using dev endpoint). But, when I run the following script:<\/p>\n<pre><code>glueContext = GlueContext(SparkContext.getOrCreate())\nempresasDF = glueContext.create_dynamic_frame.from_catalog(database=&quot;raw-rf&quot;, table_name=&quot;empresas&quot;)\nempresasDF.printSchema()\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<p><code>Caused by: java.io.FileNotFoundException: No such file or directory 'glue-d-raw-rf-t-  empresas-m-r:\/\/receitafederal-udct-zen\/empresas\/K3241.K03200Y1.D11009.EMPRECSV'<\/code><\/p>\n<p>I've already try to set permissions to s3 bucket for the SageMakerNotebook role, that creates the notebook, but didn't work. I don't know, but the file\/directory presented in the error message is kind of weird, but i don't know if it is a glue pattern.<\/p>\n<p>Futher, I ran the following code, in the same SageMaker notebook:<\/p>\n<p><code>sp = SparkSession.builder.getOrCreate()<\/code><\/p>\n<p>`<\/p>\n<pre><code>estabelecimentos_df = sp.read.option(&quot;delimiter&quot;,';')\n.option(&quot;emptyValue&quot;, '&quot;&quot;')\n.option(&quot;dateFormat&quot;,'yyyyMMdd')\n.option(&quot;encoding&quot;,'iso-8859-1')\n.option('header', 'false')\n.csv(path)\n<\/code><\/pre>\n<p>`<\/p>\n<p>in <code>csv()<\/code> the path I'm setting the s3 file path as <code>s3:\/\/bucket\/key<\/code>. In this case, when I run <code>printSchema()<\/code> no error is returned, and I can acess the files.\n`<\/p>\n<p>Why GlueContext are not able to get the files? But in the same notebook I can read using <code>SparkSession<\/code>?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1637582459387,
        "Question_score":0,
        "Question_tags":"pyspark|aws-glue|amazon-sagemaker|aws-lake-formation",
        "Question_view_count":261,
        "Owner_creation_time":1535130850620,
        "Owner_last_access_time":1663779434357,
        "Owner_location":"Florian\u00f3polis, SC, Brasil",
        "Owner_reputation":599,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70065420",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72058686,
        "Question_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Question_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651238636737,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":397,
        "Owner_creation_time":1558310526777,
        "Owner_last_access_time":1661420089220,
        "Owner_location":"London, UK",
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1651255570927,
        "Answer_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651509693110,
        "Answer_score":0.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73728499,
        "Question_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Question_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663233254363,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|xgboost|amazon-sagemaker",
        "Question_view_count":41,
        "Owner_creation_time":1640956373383,
        "Owner_last_access_time":1663928794863,
        "Owner_location":null,
        "Owner_reputation":309,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":15,
        "Question_last_edit_time":1663319367853,
        "Answer_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663924957330,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663925308150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72705272,
        "Question_title":"AWS: ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation: Could not find model",
        "Question_body":"<p>I am new here and am strugling with some basic things.\nI am trying to implement a DeepAR model, but maybe due to an update from sagemaker it doesn't find the correct path as it did on the example..\nCan someone tell me why is it happening and how o fix it?<\/p>\n<p>Error: ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation: Could not find model &quot;arn:aws:sagemaker:eu-central-1:900386373554:model\/forecasting-deepar-2022-06-21-14-12-51-560&quot;.<\/p>\n<p>This is the Batch Transformation I am trying to implement:<\/p>\n<p>-Batch Transform\nimport boto3\n-Create the SageMaker Boto3 client\nboto3_sm = boto3.client('sagemaker')<\/p>\n<p>import time\nfrom time import gmtime, strftime<\/p>\n<p>batch_job_name = 'Batch-Transform-' + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ninput_location = 's3:\/\/sagemaker-eu-central-1-900386373554\/deepar-rossmann\/input\/prediction_input.json'\noutput_location = 's3:\/\/{}\/{}\/output\/{}'.format(bucket, prefix, batch_job_name)<\/p>\n<p>request = <br \/>\n{\n&quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n&quot;MaxPayloadInMB&quot;: 100,\n&quot;Environment&quot;: {\n&quot;DEEPAR_INFERENCE_CONFIG&quot; : &quot;{ &quot;num_samples&quot;: 200, &quot;output_types&quot;: [&quot;mean&quot;] }&quot;\n},\n&quot;TransformJobName&quot;: batch_job_name,\n&quot;ModelName&quot;: 'forecasting-deepar-2022-06-21-14-12-51-560',\n&quot;TransformOutput&quot;: {\n&quot;S3OutputPath&quot;: output_location,\n&quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n&quot;AssembleWith&quot;: &quot;Line&quot;\n},\n[...]<\/p>\n<p>I am following this project: <a href=\"https:\/\/github.com\/CatherineSai\/project_sales_prediction_DEEPAR\" rel=\"nofollow noreferrer\">https:\/\/github.com\/CatherineSai\/project_sales_prediction_DEEPAR<\/a>\nNotebook 3 of 3 step 49<\/p>\n<p>--&gt; ModelName is copied from the finished training job at aws console<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655834560427,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|deepar",
        "Question_view_count":256,
        "Owner_creation_time":1655833935377,
        "Owner_last_access_time":1657467816853,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72705272",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55020390,
        "Question_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Question_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1551866752687,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|tensorflow|gpu|amazon-sagemaker",
        "Question_view_count":1858,
        "Owner_creation_time":1370085456943,
        "Owner_last_access_time":1656071683470,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1551964655197,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55020390",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68150444,
        "Question_title":"AWS SageMaker fails loading PyTorch .pth weights",
        "Question_body":"<p>I want to deploy PyTorch model to AWS SageMaker endpoint and experience some issues. Below there is a code and logs. Basically, I have .pth file for model trained outside SageMaker (Azure) and I want to move it to AWS. <code>macro_model.tar.gz<\/code> contains model weights <code>macro_model.pth<\/code> file and inference code with <code>requirements.txt<\/code>.<\/p>\n<p><strong>Expected behavior<\/strong><\/p>\n<p>Model deployed to endpoint and generates predictions<\/p>\n<p><strong>Issue<\/strong><\/p>\n<p>SageMaker <code>model_fn<\/code> function doesn't see model weights.<\/p>\n<p>Any ideas what can be wrong?<\/p>\n<p>Deployment code in SageMaker Notebook:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nimport json\nimport numpy as np\n\nrole = get_execution_role()\nconn = boto3.client('s3')\n\nclient = boto3.client('sagemaker')\npytorch_model = PyTorchModel(model_data='s3:\/\/...\/macro_model.tar.gz',\n                             framework_version=&quot;1.7&quot;, py_version=&quot;py3&quot;,\n                             role=role, entry_point='inference.py', source_dir='code')\n\npredictor = pytorch_model.deploy(initial_instance_count=1,instance_type='ml.p2.xlarge', endpoint_name='...')\n<\/code><\/pre>\n<p><code>macro_model.tar.gz<\/code> structure:<\/p>\n<pre><code>|   macro_model\n|           |--macro_model.pth\n|\n|           code\n|               |--inference.py\n|               |--requirements.txt\n|\n<\/code><\/pre>\n<p><code>model_fn<\/code> function implementation:<\/p>\n<pre><code>def model_fn(model_dir):\n    logging.info('Loading the model...')\n\n    layers = [\n        nn.Linear(512, 512),\n        nn.ReLU(),\n        nn.Dropout(0.3),\n        nn.Linear(512, 2)\n      ]\n    \n    logging.info('Layers initiated...')\n    model = VideoRecog_Model1(layers,7)\n    logging.info('Model initiated...')\n    with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    \n    \n    model.to(device).eval()\n    logging.info('Done loading model')\n    return model\n<\/code><\/pre>\n<p>Logs:<\/p>\n<pre><code>2021-06-27T12:24:56.241+02:00   Collecting opencv-python-headless Downloading opencv_python_headless-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n\n2021-06-27T12:24:57.242+02:00   Collecting moviepy==1.0.3 Downloading moviepy-1.0.3.tar.gz (388 kB)\n    2021-06-27T12:24:57.242+02:00   Collecting av==8.0.3 Downloading av-8.0.3-cp36-cp36m-manylinux2010_x86_64.whl (37.2 MB)\n\n2021-06-27T12:24:58.243+02:00   Collecting decorator&lt;5.0,&gt;=4.0.2 Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n\n2021-06-27T12:24:58.243+02:00   Requirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (4.59.0)\n\n2021-06-27T12:24:58.243+02:00   Requirement already satisfied: requests&lt;3.0,&gt;=2.8.1 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2.22.0)\n\n2021-06-27T12:24:58.243+02:00   Collecting proglog&lt;=1.0.0 Downloading proglog-0.1.9.tar.gz (10 kB)\n    \n2021-06-27T12:24:59.244+02:00   Requirement already satisfied: numpy&gt;=1.17.3 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (1.19.1)\n    \n2021-06-27T12:24:59.244+02:00   Collecting imageio&lt;3.0,&gt;=2.5 Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n    \n2021-06-27T12:24:59.244+02:00   Collecting imageio_ffmpeg&gt;=0.2.0 Downloading imageio_ffmpeg-0.4.4-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: pillow in \/opt\/conda\/lib\/python3.6\/site-packages (from imageio&lt;3.0,&gt;=2.5-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (8.2.0)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2020.12.5)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2.8)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (1.25.11)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (3.0.4)\n    \n2021-06-27T12:25:00.244+02:00   Building wheels for collected packages: moviepy, proglog Building wheel for moviepy (setup.py): started Building wheel for moviepy (setup.py): finished with status 'done' Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110726 sha256=d90e44b117edbb3d061d7d3d800e6e687392ac6b06f9001f82559c4dd88f19c4 Stored in directory: \/root\/.cache\/pip\/wheels\/be\/dc\/17\/8b4d5a63bcd05dc44db7da57e193372ccd333617293f9deebe Building wheel for proglog (setup.py): started\n    \n2021-06-27T12:25:01.245+02:00   Building wheel for proglog (setup.py): finished with status 'done' Created wheel for proglog: filename=proglog-0.1.9-py3-none-any.whl size=6147 sha256=da1b510090c3b87cf4c564558a64b996bc1fdf34d6d15fd56c14c9c776f5b366 Stored in directory: \/root\/.cache\/pip\/wheels\/e7\/11\/a0\/7e65f734d33043735a557b1244569cca327353db9068158076\n    \n2021-06-27T12:25:01.245+02:00   Successfully built moviepy proglog\n    \n2021-06-27T12:25:01.245+02:00   Installing collected packages: proglog, imageio-ffmpeg, imageio, decorator, opencv-python-headless, moviepy, av\n    \n2021-06-27T12:25:02.246+02:00   Attempting uninstall: decorator Found existing installation: decorator 5.0.9 Uninstalling decorator-5.0.9:\n    \n2021-06-27T12:25:03.246+02:00   Successfully uninstalled decorator-5.0.9\n    \n2021-06-27T12:25:05.252+02:00   Successfully installed av-8.0.3 decorator-4.4.2 imageio-2.9.0 imageio-ffmpeg-0.4.4 moviepy-1.0.3 opencv-python-headless-4.5.2.54 proglog-0.1.9\n    \n2021-06-27T12:25:05.252+02:00   WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https:\/\/pip.pypa.io\/warnings\/venv\n    \n2021-06-27T12:25:07.256+02:00   2021-06-27 10:25:06,537 [INFO ] main org.pytorch.serve.ModelServer -\n    \n2021-06-27T12:25:07.256+02:00   Torchserve version: 0.3.1\n    \n2021-06-27T12:25:07.256+02:00   TS Home: \/opt\/conda\/lib\/python3.6\/site-packages\n    \n2021-06-27T12:25:07.256+02:00   Current directory: \/\n\n2021-06-27T12:25:07.256+02:00   Temp directory: \/home\/model-server\/tmp\n    \n2021-06-27T12:25:07.257+02:00   Number of GPUs: 1\n    \n2021-06-27T12:25:07.257+02:00   Number of CPUs: 1\n    \n2021-06-27T12:25:07.257+02:00   Max heap size: 14097 M\n    \n2021-06-27T12:25:07.257+02:00   Python executable: \/opt\/conda\/bin\/python3.6\n    \n2021-06-27T12:25:07.257+02:00   Config file: \/etc\/sagemaker-ts.properties\n    \n2021-06-27T12:25:07.257+02:00   Inference address: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:07.257+02:00   Management address: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:07.257+02:00   Metrics address: http:\/\/127.0.0.1:8082\n    \n2021-06-27T12:25:07.257+02:00   Model Store: \/.sagemaker\/ts\/models\n    \n2021-06-27T12:25:07.257+02:00   Initial Models: model.mar\n    \n2021-06-27T12:25:07.257+02:00   Log dir: \/logs\n    \n2021-06-27T12:25:07.257+02:00   Metrics dir: \/logs\n    \n2021-06-27T12:25:07.257+02:00   Netty threads: 0\n    \n2021-06-27T12:25:07.257+02:00   Netty client threads: 0\n    \n2021-06-27T12:25:07.257+02:00   Default workers per model: 1\n\n2021-06-27T12:25:07.257+02:00   Blacklist Regex: N\/A\n    \n2021-06-27T12:25:07.257+02:00   Maximum Response Size: 6553500\n    \n2021-06-27T12:25:07.258+02:00   Maximum Request Size: 6553500\n    \n2021-06-27T12:25:07.258+02:00   Prefer direct buffer: false\n    \n2021-06-27T12:25:07.258+02:00   Allowed Urls: [file:\/\/.*|http(s)?:\/\/.*]\n    \n2021-06-27T12:25:07.258+02:00   Custom python dependency for model allowed: false\n    \n2021-06-27T12:25:07.258+02:00   Metrics report format: prometheus\n    \n2021-06-27T12:25:07.258+02:00   Enable metrics API: true\n    \n2021-06-27T12:25:07.258+02:00   2021-06-27 10:25:06,597 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,725 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag a6e950a7055442d88ff2f182fdef1da3\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,744 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,782 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,996 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9000\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]59\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,013 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9000\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,039 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\n    \n2021-06-27T12:25:10.260+02:00   Model server started.\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,768 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,776 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.663764953613281|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:12.690078735351562|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:52.1|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,778 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:59581.45703125|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,787 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1238.21875|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,788 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:3.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:12.261+02:00   2021-06-27 10:25:11,779 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n    \n2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,875 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Loading the model...\n    \n2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,906 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Layers initiated...\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,871 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: &quot;https:\/\/download.pytorch.org\/models\/r3d_18-b3b3357e.pth&quot; to \/root\/.cache\/torch\/hub\/checkpoints\/r3d_18-b3b3357e.pth\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,872 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 0%| | 0.00\/127M [00:00&lt;?, ?B\/s]\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,072 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 6%|\u258c | 7.30M\/127M [00:00&lt;00:01, 76.5MB\/s]\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,172 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 16%|\u2588\u258b | 20.7M\/127M [00:00&lt;00:00, 114MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,272 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 27%|\u2588\u2588\u258b | 34.0M\/127M [00:00&lt;00:00, 126MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,372 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 38%|\u2588\u2588\u2588\u258a | 48.1M\/127M [00:00&lt;00:00, 134MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,472 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 49%|\u2588\u2588\u2588\u2588\u2589 | 62.1M\/127M [00:00&lt;00:00, 139MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,572 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 76.8M\/127M [00:00&lt;00:00, 144MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,692 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91.9M\/127M [00:00&lt;00:00, 149MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,794 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 106M\/127M [00:00&lt;00:00, 140MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,852 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 120M\/127M [00:00&lt;00:00, 139MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,987 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Model initiated...\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.\n    \n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 182, in &lt;module&gt;\n    \n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 182, in &lt;module&gt;\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - worker.run_server()\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 154, in run_server\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 154, in run_server\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self.handle_connection(cl_socket)\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 116, in handle_connection\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 116, in handle_connection\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service, result, code = self.load_model(msg)\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 89, in load_model\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 89, in load_model\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)\n    \n2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py&quot;, line 104, in load\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - initialize_fn(service.context)\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/handler_service.py&quot;, line 51, in initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - super().initialize(context)\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py&quot;, line 66, in initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._service.validate_and_initialize(model_dir=model_dir)\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 158, in validate_and_initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._model = self._model_fn(model_dir)\n    \n2021-06-27T12:25:15.264+02:00\n\n2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/ml\/model\/code\/inference.py&quot;, line 105, in model_fn\n    \n2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/ml\/model\/code\/inference.py&quot;, line 105, in model_fn\n    \n2021-06-27T12:25:15.264+02:00\n\n2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n    \n2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n    \n2021-06-27T12:25:15.264+02:00\n\n**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/macro_model.pth'**\n    \n**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/macro_model.pth'**\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,994 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127M\/127M [00:00&lt;00:00, 136MB\/s]\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,998 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,999 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1624791595807,
        "Question_score":1,
        "Question_tags":"pytorch|endpoint|amazon-sagemaker",
        "Question_view_count":1535,
        "Owner_creation_time":1621518657017,
        "Owner_last_access_time":1659890990490,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1624795869140,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68150444",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58016791,
        "Question_title":"How to use AWS SageMaker to do XGBoost hyperparameter tuning externally?",
        "Question_body":"<p>No bias here, but I find it hard to find anything in AWS documentation. Microsoft Azure is much easier for me.<\/p>\n\n<p>Here is what I have now: <\/p>\n\n<ul>\n<li>A binary classification app fully built with Python, with xgboost being the ML model. Here xgboost has a set of optimized hyperparameters obtained from SageMaker.<\/li>\n<li>A SageMaker notebook to launch hyperparameter tuning jobs for xgboost. Then I manually copy and paste and hyperparameters into xgboost model in the Python app to do prediction.<\/li>\n<\/ul>\n\n<p>As you can see, the way I do it is far away from ideal. What I want to do now is adding a piece of code in the Python app to initiate the hyperparameters job in SageMaker automatically and return the best model as well. That way, the hyperparameter job is automated and I don't need to do the copy and paste again.<\/p>\n\n<p>However, I haven't been able to do that yet. I followed this <a href=\"https:\/\/aws.amazon.com\/blogs\/apn\/integrating-with-amazon-sagemaker-using-built-in-algorithms-from-external-applications\/\" rel=\"nofollow noreferrer\">documentation<\/a> to install Python SageMaker API. I also have the following code that do XGBoost hyperparameter tuning in SageMaker notebook:<\/p>\n\n<pre><code> def train_xgb_sagemaker(df_train, df_test):\n    pd.concat([df_train['show_status'], df_train.drop(['show_status'], axis=1)], axis=1).to_csv('train.csv',\n                                                                                                index=False,\n                                                                                                header=False)\n    pd.concat([df_test['show_status'], df_test.drop(['show_status'], axis=1)], axis=1).to_csv('validation.csv',\n                                                                                              index=False, header=False)\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'train.csv')\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'validation.csv')\n\n    s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\n    s3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\n    print('train_path: ', s3_input_train)\n    print('validation_path: ', s3_input_validation)\n\n    # hyperparameter tuning of XGBoost - SageMaker\n    sess = sagemaker.Session()\n\n    container = get_image_uri(region, 'xgboost', 0.90 - 1)\n    xgb = sagemaker.estimator.Estimator(container,\n                                        role,\n                                        train_instance_count=1,\n                                        train_instance_type='ml.m4.xlarge',\n                                        output_path='s3:\/\/{}\/{}\/output'.format(params['BUCKET'], prefix),\n                                        sagemaker_session=sess)\n\n    xgb.set_hyperparameters(eval_metric='auc',\n                            objective='binary:logistic',\n                            num_round=100,\n                            rate_drop=0.3,\n                            tweedie_variance_power=1.4)\n\n    hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                             'min_child_weight': ContinuousParameter(1, 10),\n                             'alpha': ContinuousParameter(0, 2),\n                             'max_depth': IntegerParameter(1, 10),\n                             'num_round': IntegerParameter(1, 300)}\n\n    objective_metric_name = 'validation:auc'\n\n    tuner = HyperparameterTuner(xgb,\n                                objective_metric_name,\n                                hyperparameter_ranges,\n                                max_jobs=20,\n                                max_parallel_jobs=3)\n\n    tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n\n    smclient.describe_hyper_parameter_tuning_job(\n        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']\n\n    print('Please check hyperparameter tuning for best models!')\n    time.sleep(4000)\n    # best_model_path = 's3:\/\/{}\/{}\/output\/{}\/output\/model.tar.gz'.format(bucket, prefix, tuner.best_training_job())\n    return tuner.best_training_job()\n<\/code><\/pre>\n\n<p>So the question is how to embed this piece of code into my Python app so that I can do everything in one place? Thanks very much for any hints as I've been hanging on this problem for days!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568916609520,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":1366,
        "Owner_creation_time":1401031858507,
        "Owner_last_access_time":1658704433413,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":1021,
        "Owner_up_votes":92,
        "Owner_down_votes":0,
        "Owner_views":120,
        "Question_last_edit_time":1568928060697,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58016791",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64126327,
        "Question_title":"Load Amazon Sagemaker NTM model locally for inference",
        "Question_body":"<p>I have trained a Sagemaker <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/introduction-to-the-amazon-sagemaker-neural-topic-model\/\" rel=\"nofollow noreferrer\">NTM<\/a> model which is a neural topic model, directly on the AWS sagemaker platform. Once training is complete you are able to download the <code>mxnet<\/code> model files. Once unpacked the files contain:<\/p>\n<ul>\n<li>params<\/li>\n<li>symbol.json<\/li>\n<li>meta.json<\/li>\n<\/ul>\n<p>I have followed the docs on mxnet to load the model and have the following code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sym, arg_params, aux_params = mx.model.load_checkpoint('model_algo-1', 0)\nmodule_model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())\n\nmodule_model.bind(\n    for_training=False,\n    data_shapes=[('data', (1, VOCAB_SIZE))]\n)\n\nmodule_model.set_params(arg_params=arg_params, aux_params=aux_params, allow_missing=True) # must set allow missing true here or receive an error for a missing n_epoch var\n<\/code><\/pre>\n<p>I now try and use the model for inference using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>module_model.predict(x) # where x is a numpy array of size (1, VOCAB_SIZE)\n<\/code><\/pre>\n<p>The code runs, but the result is just a single value, where I expect a distribution over topics:<\/p>\n<pre><code>[11.060672]\n&lt;NDArray 1 @cpu(0)&gt;\n<\/code><\/pre>\n<p>EDIT:<\/p>\n<p>I have tried to load it using the Symbol API, but still no luck:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    deserialized_net = gluon.nn.SymbolBlock.imports('model_algo-1-symbol.json', ['data'], 'model_algo-1-0000.params', ctx=mx.cpu())\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>AssertionError: Parameter 'n_epoch' is missing in file: model_algo-1-0000.params, which contains parameters: 'logsigma_bias', 'enc_0_bias', 'projection_bias', ..., 'enc_1_weight', 'enc_0_weight', 'mean_bias', 'logsigma_weight'. Please make sure source and target networks have the same prefix.\n<\/code><\/pre>\n<p>Any help would be great!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601407542533,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|nlp|amazon-sagemaker|mxnet",
        "Question_view_count":149,
        "Owner_creation_time":1431530515873,
        "Owner_last_access_time":1663837186380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":2763,
        "Owner_up_votes":203,
        "Owner_down_votes":35,
        "Owner_views":264,
        "Question_last_edit_time":1601566313100,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64126327",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61090530,
        "Question_title":"Using tidyverse to read data from s3 bucket",
        "Question_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1586299117960,
        "Question_score":1,
        "Question_tags":"r|amazon-s3|amazon-sagemaker|readr",
        "Question_view_count":1735,
        "Owner_creation_time":1275433277097,
        "Owner_last_access_time":1664032713307,
        "Owner_location":"Mexico",
        "Owner_reputation":20017,
        "Owner_up_votes":1826,
        "Owner_down_votes":1932,
        "Owner_views":2754,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1586562277483,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1586740595877,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64655582,
        "Question_title":"ML Pipeline on AWS SageMaker: How to create long-running query\/preprocessing tasks",
        "Question_body":"<p>I'm a software engineer transitioning toward machine learning engineering, but need some assistance.<\/p>\n<p>I'm currently using AWS Lambda and Step Functions to run query and preprocessing jobs for my ML pipeline, but am restrained by Lambda's 15m runtime limitation.<\/p>\n<p>We're a strictly AWS shop, so I'm kind of stuck with SageMaker and other AWS tools for the time being. Later on we'll consider experimenting with something like Kubeflow if it looks advantageous enough.<\/p>\n<p><strong>My current process<\/strong><\/p>\n<ul>\n<li>I have my data scientists write python scripts (in a git repo) for the query and preprocessing steps of a model, and deploy them (via Terraform) as Lambda functions, then use Step Functions to sequence the ML Pipeline steps as a DAG (query -&gt; preprocess -&gt; train -&gt; deploy)<\/li>\n<li>The Query lambda pulls data from our data warehouse (Redshift), and writes the unprocessed dataset to S3<\/li>\n<li>The Preprocessing lambda loads the unprocessed dataset from S3, manipulates it as needed, and writes it as training &amp; validation datasets to a different S3 location<\/li>\n<li>The Train and Deploy tasks use the SageMaker python api to train and deploy the models as SageMaker Endpoints<\/li>\n<\/ul>\n<p>Do I need to be using Glue and SageMaker Processing jobs? From what I can tell, Glue seems more targeted towards ETLs than for writing to S3, and SageMaker Processing jobs seem a bit more complex to deploy to than Lambda.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1604367008457,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|aws-lambda|pipeline|amazon-sagemaker",
        "Question_view_count":468,
        "Owner_creation_time":1268298510173,
        "Owner_last_access_time":1608649804973,
        "Owner_location":"Austin, TX",
        "Owner_reputation":1209,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64655582",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64752554,
        "Question_title":"GPU utilization is zero when running batch transform in Amazon SageMaker",
        "Question_body":"<p>I want to run a batch transform job on AWS SageMaker. I have an image classification model which I have trained on a local GPU. Now I want to deploy it on AWS SageMaker and make predictions using Batch Transform. While the batch transform job runs successfully, the GPU utilization during the job is always zero (GPU Memory utilization, however, is at 97%). That's what CloudWatch is telling me. Also, the job takes approx. 7 minutes to process 500 images, I would expect it to run much faster than this, at least when comparing it to the time it takes to process the images on a local GPU.<\/p>\n<p><strong>My question:<\/strong> Why doesn't the GPU get used during batch transform, even though I am using a GPU instance (I am using an ml.p3.2xlarge instance)? I was able to deploy the very same model to an endpoint and send requests. When deploying to an endpoint instead of using batch transform, the GPU actually gets used.<\/p>\n<p><strong>Model preparation<\/strong><\/p>\n<p>I am using a Keras Model with TensorFlow backend. I converted this model to a sagemaker model using this guide <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a> :<\/p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\nimport tarfile\nimport sagemaker\n\n# deactivate eager mode\nif tf.executing_eagerly():\n   tf.compat.v1.disable_eager_execution()\n\nbuilder = builder.SavedModelBuilder(export_dir)\n\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;image_bytes&quot;: model.input}, outputs={&quot;score_bytes&quot;: model.output})\n\nwith tf.compat.v1.keras.backend.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\n\nwith tarfile.open(tar_model_file, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nsagemaker_session = sagemaker.Session()\ns3_uri = sagemaker_session.upload_data(path=tar_model_file, bucket=bucket, key_prefix=sagemaker_model_dir)\n<\/code><\/pre>\n<p><strong>Batch Transform<\/strong><\/p>\n<p>Container image used for batch transform: 763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:2.0.0-gpu<\/p>\n<pre><code>framework = 'tensorflow'\ninstance_type='ml.p3.2xlarge'\nimage_scope = 'inference'\ntf_version = '2.0.0'\npy_version = '3.6'\n\nsagemaker_model = TensorFlowModel(model_data=MODEL_TAR_ON_S3, role=role, image_uri=tensorflow_image)\n\ntransformer = sagemaker_model.transformer(\n    instance_count = 1,\n    instance_type = instance_type,\n    strategy='MultiRecord',\n    max_concurrent_transforms=8,\n    max_payload=10, # in MB\n    output_path = output_data_path,\n)\n\ntransformer.transform(data = input_data_path,\n                      job_name = job_name,\n                      content_type = 'application\/json', \n                      logs=False,\n                      wait=True\n)\n<\/code><\/pre>\n<p><strong>Log file excerpts<\/strong><\/p>\n<p>Loading the model takes quite long (several minutes). During this time, the following error message is getting logged:<\/p>\n<blockquote>\n<p>2020-11-08T15:14:12.433+01:00            2020\/11\/08 14:14:12 [error]\n14#14: *3066 no live upstreams while connecting to upstream, client:\n169.254.255.130, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, subrequest: &quot;\/v1\/models\/my_model:predict&quot;, upstream:\n&quot;http:\/\/tfs_upstream\/v1\/models\/my_model:predict&quot;, host:\n&quot;169.254.255.131:8080&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n169.254.255.130 - - [08\/Nov\/2020:14:14:12 +0000] &quot;GET \/ping HTTP\/1.1&quot; 502 157 &quot;-&quot; &quot;Go-http-client\/1.1&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n2020\/11\/08 14:14:12 [error] 14#14: *3066 js: failed ping#015\n2020-11-08T15:14:12.433+01:00            502 Bad\nGateway#015 2020-11-08T15:14:12.433+01:00<\/p>\n#015 2020-11-08T15:14:12.433+01:00            <h1>502\nBad Gateway<\/h1>#015 2020-11-08T15:14:12.433+01:00           \n<hr>nginx\/1.16.1#015 2020-11-08T15:14:12.433+01:00   \n#015 2020-11-08T15:14:12.433+01:00            #015\n<\/blockquote>\n<p>There was a log entry about NUMA node read:<\/p>\n<blockquote>\n<p>successful NUMA node read from SysFS had negative value (-1), but\nthere must be at least one NUMA node, so returning NUMA node zero<\/p>\n<\/blockquote>\n<p>And about a serving warmup request:<\/p>\n<blockquote>\n<p>No warmup data file found at\n\/opt\/ml\/model\/export\/my_model\/1\/assets.extra\/tf_serving_warmup_requests<\/p>\n<\/blockquote>\n<p>And this warning:<\/p>\n<blockquote>\n<p>[warn] getaddrinfo: address family for nodename not supported<\/p>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1604928355263,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|tensorflow-serving",
        "Question_view_count":505,
        "Owner_creation_time":1505593588753,
        "Owner_last_access_time":1659104216800,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64752554",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67570694,
        "Question_title":"How to build YOLACT++ using Docker?",
        "Question_body":"<p>I have to build yolact++ in docker enviromment (i'm using sagemaker notebook). Like this<\/p>\n<pre><code>ARG PYTORCH=&quot;1.3&quot;\nARG CUDA=&quot;10.1&quot;\nARG CUDNN=&quot;7&quot;\n \nFROM pytorch\/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel\n<\/code><\/pre>\n<p>And i want to run this<\/p>\n<pre><code>COPY yolact\/external\/DCNv2\/setup.py \/opt\/ml\/code\/external\/DCNv2\/setup.py\nRUN cd \/opt\/ml\/code\/external\/DCNv2 &amp;&amp; \\\npython setup.py build develop\n<\/code><\/pre>\n<p>But i got this error :<\/p>\n<pre><code>No CUDA runtime is found, using CUDA_HOME='\/usr\/local\/cuda'\nTraceback (most recent call last):\nFile &quot;setup.py&quot;, line 64, in &lt;module&gt;\next_modules=get_extensions(),\nFile &quot;setup.py&quot;, line 41, in get_extensions\nraise NotImplementedError('Cuda is not available')\nNotImplementedError: Cuda is not available\n<\/code><\/pre>\n<p>But the enviromment supports CUDA. Anyone have an idea where is the problem ?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1621258222833,
        "Question_score":0,
        "Question_tags":"docker|pytorch|dockerfile|amazon-sagemaker",
        "Question_view_count":486,
        "Owner_creation_time":1587069846163,
        "Owner_last_access_time":1634476659293,
        "Owner_location":"France",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1621519151353,
        "Answer_body":"<p>SOLUTION :<\/p>\n<p>i edit the \/etc\/docker\/daemon.json with content:<\/p>\n<pre><code>{\n&quot;runtimes&quot;: {\n    &quot;nvidia&quot;: {\n        &quot;path&quot;: &quot;\/usr\/bin\/nvidia-container-runtime&quot;,\n        &quot;runtimeArgs&quot;: []\n     } \n},\n&quot;default-runtime&quot;: &quot;nvidia&quot; \n}\n<\/code><\/pre>\n<p>Then i Restart docker daemon:<\/p>\n<pre><code>sudo system restart docker\n<\/code><\/pre>\n<p>it solved my problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621519129333,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1621519479057,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67570694",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64238362,
        "Question_title":"reading a csv.gz file from sagemaker using pyspark kernel mode",
        "Question_body":"<p>i am trying to read a compressed csv file in pyspark. but i am unable to read in pyspark kernel mode in sagemaker.<\/p>\n<p>The same file i can read using pandas when the kernel is conda-python3 (in sagemaker)<\/p>\n<p>What I tried :<\/p>\n<pre><code>file1 =  's3:\/\/testdata\/output1.csv.gz'\nfile1_df = spark.read.csv(file1, sep='\\t')\n<\/code><\/pre>\n<p>Error message :<\/p>\n<pre><code>An error was encountered:\nAn error occurred while calling 104.csv.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 7FF77313; S3 Extended Request ID: \n<\/code><\/pre>\n<p>Kindly let me know if i am missing anything<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602051997903,
        "Question_score":0,
        "Question_tags":"python|apache-spark|amazon-s3|pyspark|amazon-sagemaker",
        "Question_view_count":790,
        "Owner_creation_time":1441820957717,
        "Owner_last_access_time":1623397378193,
        "Owner_location":null,
        "Owner_reputation":351,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64238362",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68676725,
        "Question_title":"How to use tensorflow library with sagemaker preprocessing",
        "Question_body":"<p>I want to use TensorFlow for preprocessing in sagemaker pipelines.<\/p>\n<p>But, I haven't been able to find a way to use it.<\/p>\n<p>Right now, I'm using this library for preprocessing:<\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\n\nframework_version = &quot;0.23-1&quot;\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=framework_version,\n    instance_type=processing_instance_type,\n    instance_count=processing_instance_count,\n    base_job_name=&quot;abcd&quot;,\n    role=role,\n)\n<\/code><\/pre>\n<p><strong>Now, I need to use TensorFlow in preprocessing but the python module cant import TensorFlow.<\/strong><\/p>\n<p>Any help would be much appreciated. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628229065587,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker|preprocessor",
        "Question_view_count":53,
        "Owner_creation_time":1537442477530,
        "Owner_last_access_time":1648212741713,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68676725",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55295692,
        "Question_title":"Understanding Sagemaker Neo",
        "Question_body":"<p>I have few questions for <a href=\"https:\/\/aws.amazon.com\/sagemaker\/neo\/\" rel=\"nofollow noreferrer\">Sagemaker Neo<\/a>:<\/p>\n\n<p>1) Can I take advantage of Sagemaker Neo if I have an externally trained tensorflow\/mxnet model?<\/p>\n\n<p>2) Sagemaker provides container image for <em>'image-classification'<\/em> and it has released a new image with name <em>'<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-deployment-hosting-services-cli.html\" rel=\"nofollow noreferrer\">image-classification-neo<\/a>'<\/em> for the neo compilation job. What is the difference between both of them? Do I require a new Neo compatible image for each pre built sagemaker template(container) similarly?<\/p>\n\n<p>Any help would be appreciated<\/p>\n\n<p>Thanks!!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1553243780953,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":820,
        "Owner_creation_time":1515418712450,
        "Owner_last_access_time":1617420030177,
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55295692",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63255556,
        "Question_title":"ThrottlingException Rate exceeded SageMaker Studio",
        "Question_body":"<p>What I am simply trying to do is shut down my SageMaker studio instance. I am using <code>datascience--1-0-ml-m5-8xlarge<\/code> instance for my SageMaker studio notebook. While running my notebook, I saw that amount of memory being taken was more than the memory available (128 GB). As a result, I reset the notebook. From then on, I am basically not able to shut down the instance as shown in the image<a href=\"https:\/\/i.stack.imgur.com\/5mIhM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5mIhM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Does anyone know the reason behind this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1596577831680,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":449,
        "Owner_creation_time":1415806163833,
        "Owner_last_access_time":1663845435483,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":805,
        "Owner_up_votes":915,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71398882,
        "Question_title":"CUDA: RuntimeError: CUDA out of memory - BERT sagemaker",
        "Question_body":"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)<\/code> the same code runs fine on my laptop.<\/p>\n<ol>\n<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))<\/code> from within my training script (right before it throws the error) it prints<\/li>\n<\/ol>\n<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|===========================================================================|\n<\/code><\/pre>\n<p>but I have no idea what it means or how to interpret it<\/p>\n<ol start=\"2\">\n<li>when i run <code>!df -h<\/code> I can see:<\/li>\n<\/ol>\n<pre><code>Filesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         30G   72K   30G   1% \/dev\ntmpfs            30G     0   30G   0% \/dev\/shm\n\/dev\/xvda1      109G   93G   16G  86% \/\n\/dev\/xvdf       196G   61M  186G   1% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>how is this memory different from the GPU? if theres 200GB in \/dev\/xvdf is there anyway I can just use that..? in my test script I tried<br \/>\n<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)<\/code>\nbut that just gives the same error<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646759194503,
        "Question_score":1,
        "Question_tags":"python|gpu|amazon-sagemaker|huggingface-transformers",
        "Question_view_count":1983,
        "Owner_creation_time":1597997723910,
        "Owner_last_access_time":1661243820287,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1646767755130,
        "Answer_body":"<p>A <code>CUDA out of memory<\/code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h<\/code> command).<\/p>\n<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).<\/p>\n<p>Things you can try:<\/p>\n<ul>\n<li>Provision an instance with more GPU memory<\/li>\n<li>Decrease batch size<\/li>\n<li>Use a different (smaller) model<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1646760128677,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646760473953,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71398882",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51670563,
        "Question_title":"Invalid .lst file in sagemaker",
        "Question_body":"<p>Folder structure for my S3 bucket is:<\/p>\n\n<pre><code>Bucket\n    -&gt;training-set\n           -&gt;medium\n                 -&gt;    img1.jpeg\n                 -&gt;    img2.jpeg\n                 -&gt;    img3.PNG\n<\/code><\/pre>\n\n<p>My training-set.lst file looks like this:<\/p>\n\n<pre><code>1  \\t 1  \\t medium\/img1.jpeg\n2  \\t 1  \\t medium\/img2.jpeg\n3  \\t 1  \\t medium\/img3.PNG\n<\/code><\/pre>\n\n<p>I created this using excel sheet.<\/p>\n\n<p>Error:\nTraining failed with the following error: ClientError: Invalid lst file: training-set.lst<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n          \"ChannelName\": \"train\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/training-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/test-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"train_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/training-set\/training-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/test-set\/test-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        }\n    ]\n<\/code><\/pre>\n\n<p>I am trying to use this in Amazon Sagemaker. But I'm unable to do that. Can someone please help?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1533291751550,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|image-recognition|amazon-sagemaker",
        "Question_view_count":591,
        "Owner_creation_time":1523786300320,
        "Owner_last_access_time":1663816627680,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":177,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1533294102060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51670563",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58956312,
        "Question_title":"Possible to attach Elastic IP to sagemaker notebook instance?",
        "Question_body":"<p>I want to connect to a database running in different cloud provider and it is exposed publicly.<\/p>\n\n<p>I need to connect to that database from sagemaker notebook instance.<\/p>\n\n<p>But the public ip of the sagemaker notebook instance needs to be whitelisted on the other side.<\/p>\n\n<p>Is it possible to attach elastic ip to sagemaker notebook instance as I don't see any option to attach eip to sagemaker notebook instance?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1574259012963,
        "Question_score":2,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":708,
        "Owner_creation_time":1369922081203,
        "Owner_last_access_time":1663926212973,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":3577,
        "Owner_up_votes":2395,
        "Owner_down_votes":20,
        "Owner_views":626,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58956312",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73241880,
        "Question_title":"Distributed Training Terminology: Micro-batch and Per-Replica batch size",
        "Question_body":"<p>I am reading through the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/distributed-training.html\" rel=\"nofollow noreferrer\">Sagemaker documentation<\/a> on distributed training and confused on the terminology:<\/p>\n<p>Mini-Batch, Micro-batch and Per-replica batch size<\/p>\n<p>I understand that in data parallelism, there would be multiple copies of the model and each copy would receive data of  size = &quot;Per Replica Batch Size&quot;<\/p>\n<ol>\n<li>Could someone ELI5 how micro-batch would fit in this context?<\/li>\n<li>Is this a common terminology used in the terminology or is this specific to AWS Sagemaker<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659644678150,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|distributed-training",
        "Question_view_count":23,
        "Owner_creation_time":1390885171883,
        "Owner_last_access_time":1663717157010,
        "Owner_location":"College Station, TX, United States",
        "Owner_reputation":426,
        "Owner_up_votes":408,
        "Owner_down_votes":1,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73241880",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71989724,
        "Question_title":"Removing annotators in case of bad performance in AWS ground truth",
        "Question_body":"<p>Is there a way in AWS Ground Truth to validate annotations from workers during the labeling task and <strong>remove the worker<\/strong> if the performance is really poor <strong>before<\/strong> they finish the task?<\/p>\n<p>I know that there are other services that sometimes present an instance for which the label is known to the worker. In case the workers do not label this correctly, they get kicked out. This is to assure quality of the annotations.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650813165487,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":17,
        "Owner_creation_time":1297784519700,
        "Owner_last_access_time":1662102483020,
        "Owner_location":"Helsinki, Finland",
        "Owner_reputation":143,
        "Owner_up_votes":1708,
        "Owner_down_votes":1,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71989724",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69745719,
        "Question_title":"How to Use Prebuilt Deep Learning Algorithms in SageMaker?",
        "Question_body":"<p>Is it possible to use AWS prebuilt algorithms, e.g., Image Classification, locally with SageMaker? I tried to pull the image-classification image URI using <code>aws ecr get-login-password<\/code> but I get the following error message:<\/p>\n<pre><code>Error response from daemon: pull access denied for 813361260812.dkr.ecr.eu-central-1.amazonaws.com\/image-classification, repository does not exist or may require 'docker login': denied: User: xxxxxxxxxxxxxxxxxxxxxxxx is not authorized to perform: ecr:BatchGetImage on resource: arn:aws:ecr:eu-central-1:813361260812:repository\/image-classification because no resource-based policy allows the ecr:BatchGetImage action\n<\/code><\/pre>\n<p>I gave my user full access so the policies include <code>BatchGetImage<\/code>. Can someone explain why this error occurs?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1635370244877,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-iam|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_time":1586416145180,
        "Owner_last_access_time":1658518190400,
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1635422783200,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69745719",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60072981,
        "Question_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Question_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580895955387,
        "Question_score":4,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2984,
        "Owner_creation_time":1550756471933,
        "Owner_last_access_time":1663939288837,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1581004771980,
        "Answer_score":7.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67878535,
        "Question_title":"Getting \"[Errno 28] No space left on device\" on AWS SageMaker with plenty of storage left",
        "Question_body":"<p>I am running a notebook instance on Amazon Sagemaker and my understanding is that notebooks by default have 5GB of storage.  I am running into <code>[Errno 28] No space left on device<\/code> on a notebook that worked just fine the last time I tried it.  I checked and I'm using approximately 1.5GB out of 5GB. I'm trying to download a bunch of files from my S3 bucket but I get the error even before one file is downloaded.  Additionally, the notebook no longer autosaves.<\/p>\n<p>Has anyone run into this and figured out a way to fix it? I've already tried clearing all outputs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1623098969000,
        "Question_score":0,
        "Question_tags":"amazon-web-services|storage|amazon-sagemaker",
        "Question_view_count":1612,
        "Owner_creation_time":1623098611487,
        "Owner_last_access_time":1633990010460,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67878535",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60748772,
        "Question_title":"How to force a SageMaker Notebook instance to stop?",
        "Question_body":"<p>I am an AWS noob, My notebook instance has been on <strong>Pending Status<\/strong> for a couple hours.<\/p>\n\n<p>How can I force it to STOP ? Or at least get my code back.<\/p>\n\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584573282977,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1104,
        "Owner_creation_time":1432935437533,
        "Owner_last_access_time":1659214407257,
        "Owner_location":null,
        "Owner_reputation":345,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":107,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60748772",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67843602,
        "Question_title":"Could not find model PipelineModel",
        "Question_body":"<p>When I try to build models to create a pipeline as follows,<\/p>\n<pre><code>    &lt;code for the preprocessor&gt;\n    preprocessor = sklearn_preprocessor.create_model() #successful\n\n    &lt;code for the estimator&gt;\n    xgb_model_step = xgb_model.create_model() #successful    \n    \n    sm_model = PipelineModel(name='model', role=role, models=[preprocessor, xgb_model_step]) #successful\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=end) &lt;--- failure!\n<\/code><\/pre>\n<p>The models are created successfully. In the deploy line I get an error as,<\/p>\n<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-east-1-1356784978535\/sagemaker-scikit-learn-2021-06-04-20-07-55-519\/output\/model.tar.gz.<\/code><\/p>\n<p>I am not sure how I can specify the path and make the deploy successful. Can somebody please help me with this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1622838635930,
        "Question_score":1,
        "Question_tags":"amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_time":1473401410977,
        "Owner_last_access_time":1646051838717,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67843602",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60607041,
        "Question_title":"AWS Sagemaker Spark S3 access issue",
        "Question_body":"<p>I am new in AWS sagemaker. I created a notebook in a VPC with private subnet, kms default encrypted key, root access, no direct internet access. I have attached policy which have full access to Sagemaker and S3 in IAM as per documentations.  Now while one of data scientist trying to run his code in jupyter, getting below error. I can see jar files (\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/), I have even given access key and secret key in code, is there anything we are doing wrong here<\/p>\n\n<pre><code>import os\nimport boto3\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\nimport pyspark\n\nrole = get_execution_role()\nspark = SparkSession.builder \\\n            .appName(\"app_name2\") \\\n            .getOrCreate()\n\nsc=pyspark.SparkContext.getOrCreate()\nsc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n\nhadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", 'access_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", 'secret_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\nspark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\");\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\ndf= spark.read.csv(\"s3a:\/\/mybucket\/ConsolidatedData\/my.csv\",header=\"true\")\n\n\nPy4JJavaError: An error occurred while calling o579.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n    at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:709)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583781550823,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":933,
        "Owner_creation_time":1513883236660,
        "Owner_last_access_time":1663906475810,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":1583817907990,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60607041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61122143,
        "Question_title":"aws sagemaker: install new r packages error",
        "Question_body":"<p>I'm trying to use jupyter notebook in AWS sagemaker with an <code>r<\/code> kernel. However there is a specific library that I want to use (<code>imager<\/code>) that does not exist. Based on previous SO <a href=\"https:\/\/stackoverflow.com\/a\/42459747\/1652217\">answers<\/a>, I tried to do<\/p>\n\n<pre><code>install.packages(\"imager\", repos='http:\/\/cran.us.r-project.org')\n<\/code><\/pre>\n\n<p>This throws the following warning<\/p>\n\n<pre><code>Warning message in install.packages(\"imager\", repos = \"http:\/\/cran.us.r-project.org\"):\n\u201cinstallation of package \u2018imager\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>However when I try to load this package again,<\/p>\n\n<pre><code>library(\"imager\")\n<\/code><\/pre>\n\n<p>it throws the following error,<\/p>\n\n<pre><code>Error in library(\"imager\"): there is no package called \u2018imager\u2019\nTraceback:\n\n1. library(\"imager\")\n<\/code><\/pre>\n\n<p>I'm not sure why the aws sagemaker is not being able to locate the installed package for loading<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1586438462937,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":551,
        "Owner_creation_time":1346939871660,
        "Owner_last_access_time":1663994857593,
        "Owner_location":"Australia",
        "Owner_reputation":1912,
        "Owner_up_votes":357,
        "Owner_down_votes":7,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61122143",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71610716,
        "Question_title":"How can I retrieve a folder from S3 into an AWS SageMaker notebook",
        "Question_body":"<p>I have a folder with several files corresponding to checkpoints of a RL model trained using RLLIB. I want to make an analysis of the checkpoints in a way that I need to pass a certain folder as an argument, e.g., <code>analysis_function(folder_path)<\/code>. I have to run this line on a SageMaker notebook. I have seen that there are some questions on SO about how to retrieve files from s3, such as <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">this<\/a> one. However; how can I retrieve a whole folder?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648166098547,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":711,
        "Owner_creation_time":1614455025457,
        "Owner_last_access_time":1663151887240,
        "Owner_location":null,
        "Owner_reputation":57,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71610716",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70750358,
        "Question_title":"Automate the date parameter while deplying the model on AWS Wrangler",
        "Question_body":"<p>I have built a XGBoost model on my local machine which takes a training data and validates the model on a testing dataset. However, I have hard-coded the date values as the training data is created monthly. The training data gets created based on what Date Parameter I pass. Eg, jan = dt(2021,1,1).<\/p>\n<p>I now have to automate the process as the model has to be deployed on AWS and should run monthly without editing the code. How should I pass the date parameter to AWS Wrangler so that the process will be automated, and the code will execute once every month on a new dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642480744327,
        "Question_score":0,
        "Question_tags":"amazon-web-services|date|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":127,
        "Owner_creation_time":1607598608387,
        "Owner_last_access_time":1658981623800,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1643584734013,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70750358",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53424429,
        "Question_title":"Getting error while invoking API using AWS Lambda. (AWS Lambda + AWS API Gateway+ Postman)",
        "Question_body":"<p>I get an error while invoking the AWS SageMaker endpoint API from a Lambda function. When I call this using Postman, I am getting an error like: <\/p>\n\n<pre><code>{\n    \"errorMessage\": \"module initialization error\"\n}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1542864700527,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":420,
        "Owner_creation_time":1542864496280,
        "Owner_last_access_time":1611854760103,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1542988547733,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53424429",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61074798,
        "Question_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586244384423,
        "Question_score":2,
        "Question_tags":"amazon-web-services|tensorflow|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":740,
        "Owner_creation_time":1480786532470,
        "Owner_last_access_time":1663657056320,
        "Owner_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Owner_reputation":111,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1586255907797,
        "Answer_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593503184160,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72694415,
        "Question_title":"ValidationException Importing from Redshift into Data Wrangler",
        "Question_body":"<p>I'm trying to build a model workflow in AWS SageMaker using Data Wrangler for preprocessing. I'm loading data from various tables in a Redshift instance, before mutating and joining them as required to build the model input data.<\/p>\n<p>I'm a contractor working for a company who has provisioned some resource in their AWS environment for me to work, and am reading from a production database. If I do not load open the Data Wrangler flow early enough in the day (which I suspect is related to load on their system), some of the nodes which I have created will not validate, and instead show a red cross and the following error message:<\/p>\n<p><code>RedshiftQueryExecutionIdValidationError: An error occurred when trying to invoke `describe_statement`: An error occurred (ValidationException) when calling the DescribeStatement operation: Could not retrieve the query result as it has expired after 1655759552.<\/code><\/p>\n<p>The remaining un-errored nodes appear to hang in a loading\/validating state. Here's a screenshot of part of the flow in this state:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AewzK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AewzK.png\" alt=\"An image showing some currently loading AWS Data Wrangler nodes, with some errored nodes joining in towards the end of the flow, rendering the end product errored and thus unusable\" \/><\/a><\/p>\n<p>I'm not sure if it's related, but I occasionally see error messages pop up saying something about &quot;too many inflight requests&quot;.<\/p>\n<p>My main issue, I think, is a lack of context. I have not worked in this environment before, and am finding it difficult to diagnose the issue. It might be possible to provision more resource, and I could likely trim down some of the information before reading it in, but I'd like to be able to read the error messages and understand what's <em>causing<\/em> the nodes to error, so that I can decide on the appropriate course of action.<\/p>\n<p>Can somebody please help explain what's going on here?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1655774625443,
        "Question_score":1,
        "Question_tags":"amazon-redshift|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":57,
        "Owner_creation_time":1427947978893,
        "Owner_last_access_time":1663642097267,
        "Owner_location":"Auckland, New Zealand",
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694415",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54432761,
        "Question_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Question_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548817091420,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":595,
        "Owner_creation_time":1224733422317,
        "Owner_last_access_time":1661146167157,
        "Owner_location":"Singapore",
        "Owner_reputation":7707,
        "Owner_up_votes":162,
        "Owner_down_votes":6,
        "Owner_views":583,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1548817091420,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73133746,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658906440657,
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_time":1658906023853,
        "Owner_last_access_time":1663921457750,
        "Owner_location":null,
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":1660220920907,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658964743500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71441244,
        "Question_title":"SageMager Studio Notebook Kernel keeps starting",
        "Question_body":"<p>Trying to execute cells in an Amazon SageMager Studio Notebook I continuously receive the message &quot;Note: The kernel is still starting. Please execute this cell again after the kernel is started.&quot; The bottom status bar claims &quot;Kernel: Starting...&quot; The &quot;Running Terminals and Kernels&quot; overview shows a running instance ml.t3.medium with running app datascience-1.0 and kernel session corresponding to the notebook title. I tried restarting SageMaker Studio and opened it in another region but neither helped.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1647014221243,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":661,
        "Owner_creation_time":1331995664973,
        "Owner_last_access_time":1661365050443,
        "Owner_location":null,
        "Owner_reputation":377,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71441244",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57494223,
        "Question_title":"access my own image files or load them in phoneoxpth from S3 to sage maker",
        "Question_body":"<p>I'm fairly new to working with AWS, and I want to use SageMaker to train a certain image data set using fast.ai. But I have no clue how to link all the image data from S3 to SageMaker.<\/p>\n\n<p>I tried almost everything I could think of, used s3fs and I can read the images separately and the list of the images, but how do I feed that info to my databunch or learning algorithm?<\/p>\n\n<p>My code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='sagemaker-sst-images'\ndata_key = 'SST_Data\/sst-images'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n\n<p>This code, I think, gives a URL to the data. \nBut what comes next? Either get it into a path, or load the data properly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565783749733,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":168,
        "Owner_creation_time":1542989984733,
        "Owner_last_access_time":1663051725663,
        "Owner_location":"Helsinki, Finlande",
        "Owner_reputation":77,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1565785044280,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57494223",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57601733,
        "Question_title":"How do I install R packages on the SageMaker Notebook instance?",
        "Question_body":"<p>I tried to start a R notebook in Sagemaker and I typed<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\")\n<\/code><\/pre>\n\n<p>and it gave me the error<\/p>\n\n<pre><code>also installing the dependencies \u2018listenv\u2019, \u2018dplyr\u2019, \u2018rlang\u2019, \u2018furrr\u2019, \n\u2018future.apply\u2019, \u2018fs\u2019, \u2018pryr\u2019, \u2018fst\u2019, \u2018globals\u2019, \u2018future\u2019\n\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018rlang\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fs\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018pryr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fst\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018dplyr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018disk.frame\u2019 had non-zero exit status\u201d\nUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>How do I install R packages on Sagemaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1566445052783,
        "Question_score":3,
        "Question_tags":"r|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2786,
        "Owner_creation_time":1262052472410,
        "Owner_last_access_time":1653952086580,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":13830,
        "Owner_up_votes":1039,
        "Owner_down_votes":77,
        "Owner_views":1372,
        "Question_last_edit_time":1566447149797,
        "Answer_body":"<p>I think you just need to specify a repo. For example, setting the RStudio CRAN repo, I can install perfectly fine.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\", repo=\"https:\/\/cran.rstudio.com\/\")\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1566562240970,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57601733",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64263330,
        "Question_title":"Data Preprocessing on AWS SageMaker",
        "Question_body":"<p>I have an endpoint running a trained SageMaker model on AWS, which expects the data on a specific format.<\/p>\n<p>Initially, the data has been processed on the client side of the application, it means, the <code>API Gateway<\/code> (which receives the POST API calls on AWS) used to receive pre-processed data, but now there's a change, the <code>API Gateway<\/code> will receive <strong>raw data<\/strong> from the client, and the job of pre-processing this data before sending to our SageMaker model is up to our workflow.<\/p>\n<p>What is the best way to create a pre-processing job on this workflow, without needing to re-train the model? My pre-process is just a bunch of dataframe transformations, no standardization or calculation with the training set required (it would not need to save any model file).<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1602162886373,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-api-gateway|amazon-sagemaker",
        "Question_view_count":450,
        "Owner_creation_time":1405317204730,
        "Owner_last_access_time":1663779824497,
        "Owner_location":null,
        "Owner_reputation":69,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After some research, this is the solution I've followed:<\/p>\n<ul>\n<li>First I have created a <code>SKLearn<\/code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">AWS code<\/a>)<\/li>\n<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.<\/li>\n<li>Loaded the trained legacy model that we had using the <code>Model<\/code> parameter.<\/li>\n<li>Created a <code>PipelineModel<\/code> with the preprocessing model and legacy model in cascade:<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>pipeline_model = PipelineModel(name=model_name,\n                               role=role,\n                               models=[\n                                    preprocess_model,\n                                    trained_model\n                               ])\n<\/code><\/pre>\n<ul>\n<li>Create a new endpoint, calling the <code>PipelineModel<\/code> and then changed the <code>Lambda<\/code> function to call this new endpoint. With this I could send the <strong>raw data<\/strong> directly for the same <code>API Gateway<\/code> and it would call only <strong>one<\/strong> endpoint, without needing to pay two endpoints 24\/7 to perform the entire process.<\/li>\n<\/ul>\n<p>I've found this to be a good and &quot;<em>economic<\/em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda<\/code> function.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1602762793703,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1602871502600,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64263330",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51560452,
        "Question_title":"AWS SageMaker Random Cut Forest or Kinesis Data Analytics Random Cut Forest?",
        "Question_body":"<p>I need to put together an architecture that can detect anomalies in logs created by a web application.<\/p>\n\n<p>The Random Cut Forest algorithm constantly pops up in my research, where it is used in two scenarios: SageMaker and Kinesis Data Analytics.<\/p>\n\n<p>Which of these two services should I use in my architecture?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1532702839040,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-kinesis|amazon-kinesis-firehose|amazon-sagemaker",
        "Question_view_count":771,
        "Owner_creation_time":1525448832837,
        "Owner_last_access_time":1561138173263,
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>At the core, the mathematical methodology between the two is nearly identical, but there are some differences in how they are implemented within Kinesis and SageMaker that should help drive your decision. <\/p>\n\n<p>Kinesis RandomCutForest:<\/p>\n\n<ul>\n<li>Streaming version of the algorithm which is great for near-real-time updates to the model.<\/li>\n<li>Supports time decay of older records, shingling of the input data, and if you are using multiple dimensions, anomaly attribution that helps you understand the effect of each of the dimensions.  <\/li>\n<li>So, in case your logs are being stored in CloudWatch, by using subscription filters (and Lambda if needed) you can get them preprocessed and sent to Kinesis with little effort.  <\/li>\n<\/ul>\n\n<p>SageMaker RandomCutForest:<\/p>\n\n<ul>\n<li>Batch version of the algorithm, great for large datasets (typically stored in S3) or where there's no need to update the model frequently.  <\/li>\n<li>Similar to Kinesis, supports near-real-time scoring of incoming data points via inference endpoint, but new data points do not change the underlying model.  <\/li>\n<li>Supports hyper parameter optimization, which identifies the best set of parameters for your model (such as number of samples, number of trees etc.)  <\/li>\n<li>Scaling up instances for both training and scoring is straightforward, and the available SageMaker Notebooks can help you preprocess and prepare your data for training.  <\/li>\n<li>So, if your dataset is large and you don't have a need for dynamic updates to your model, SageMaker solution should be preferred solution for you.  <\/li>\n<\/ul>\n\n<p>Hope this answers your question.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1533237075180,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51560452",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63405080,
        "Question_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Question_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597366468133,
        "Question_score":0,
        "Question_tags":"python|docker|jupyter-notebook|xgboost|amazon-sagemaker",
        "Question_view_count":1174,
        "Owner_creation_time":1327302732867,
        "Owner_last_access_time":1664072442223,
        "Owner_location":"USA",
        "Owner_reputation":19711,
        "Owner_up_votes":5189,
        "Owner_down_votes":56,
        "Owner_views":1030,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597366468133,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597367070867,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48014413,
        "Question_title":"Connection timed out - using sqlalchemy to access AWS usaspending data",
        "Question_body":"<p>I created an instance of the usaspending.gov database in my AWS RDS. A description of this database can be found here: <a href=\"https:\/\/aws.amazon.com\/public-datasets\/usaspending\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/public-datasets\/usaspending\/<\/a><\/p>\n\n<p>The data are available as a PostgreSQL snapshot, and I would like to access the database using Python's sqlalchemy package within a Jupyter notebook within Amazon SageMaker.<\/p>\n\n<p>I tried to set up my database connection with the code below, but I'm getting a Connection timed out error. I'm pretty new to AWS and Sagemaker, so maybe I messed up my sqlalchemy engine? I think my VPC security settings are OK (it looks like they accept inbound and outbound requests).<\/p>\n\n<p>Any ideas what I could be missing?<\/p>\n\n<p>engine = create_engine(\u2018postgresql:\/\/root:password@[my endpoint]\/[DB instance]<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" alt=\"connection timed out\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" alt=\"VPC inbound settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" alt=\"VPC outbound settings\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1514495321563,
        "Question_score":1,
        "Question_tags":"python-3.x|postgresql|amazon-web-services|sqlalchemy|amazon-sagemaker",
        "Question_view_count":1528,
        "Owner_creation_time":1495670905787,
        "Owner_last_access_time":1545942227160,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48014413",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66107303,
        "Question_title":"SageMaker Experiments: UnpicklingError: invalid load key, '\\x1f'",
        "Question_body":"<p>I am new to Data Science and have been trying to use SageMaker Experiments to create an extremely simple model. Using SageMaker Experiments, I trained a model using a CSV dataset. The model was output to S3, and I am now trying to load that model into a Jupyter Notebook and run a batch transform test on it.<\/p>\n<p>However, I am getting an error when trying to use the following block of code provided in the SageMaker docs that is supposed to help me import the model. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">source<\/a><\/p>\n<pre><code>import pickle as pkl\nimport tarfile\nimport xgboost\n\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel = pkl.load(open(model_file_path, 'rb'))\n<\/code><\/pre>\n<p>The error is occurring at pkl.load:<\/p>\n<pre><code> ---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-39-81639df86023&gt; in &lt;module&gt;\n      6 t.extractall()\n      7 \n----&gt; 8 model = pkl.load(open(model_file_path, 'rb'))\n      9 \n     10 \n\nUnpicklingError: invalid load key, '\\x1f'.\n<\/code><\/pre>\n<p><strong>Note:<\/strong> model_file_path is 'model.tar.gz'<\/p>\n<p>Because I am so new at this, I do not know if it is an error with what I have done, or if there is something about how SageMaker Experiments does something that I am missing. I have tried referencing other stackoverflow posts that contain this error, but they don't seem to directly apply (to my understanding) to my error. (<a href=\"https:\/\/stackoverflow.com\/questions\/33049688\/what-causes-the-error-pickle-unpicklingerror-invalid-load-key\">this<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/45121556\/unable-to-load-cifar-10-dataset-invalid-load-key-x1f\">this<\/a>)<\/p>\n<p>Any expert advice on SageMaker Experiments or the SageMaker v2 SDK would be extremely helpful to this newbie trying to break into the space. Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612808946247,
        "Question_score":1,
        "Question_tags":"python|machine-learning|pickle|xgboost|amazon-sagemaker",
        "Question_view_count":371,
        "Owner_creation_time":1512874902863,
        "Owner_last_access_time":1618539419697,
        "Owner_location":"Salt Lake City, UT, United States",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66107303",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63305569,
        "Question_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Question_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596816839977,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2331,
        "Owner_creation_time":1483370766803,
        "Owner_last_access_time":1664056213153,
        "Owner_location":"London, UK",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1596816839977,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597991730413,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70681074,
        "Question_title":"Amazon SageMaker could not get a response from the endpoint",
        "Question_body":"<p>I have built an anomaly detection model using AWS SageMaker inbuilt model: random cut forest.<\/p>\n<pre><code>    rcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    num_samples_per_tree=1000,\n    num_trees=100,\n    encrypt_inter_container_traffic=True,\n    enable_network_isolation=True,\n    enable_sagemaker_metrics=True)\n<\/code><\/pre>\n<p>and created the endpoint:-<\/p>\n<pre><code>    rcf_inference = rcf.deploy(\n              initial_instance_count=4, instance_type=&quot;ml.m5.xlarge&quot;,\n              endpoint_name='RCF-container2',\n              enable_network_isolation=True)\n<\/code><\/pre>\n<p>But when I tried to get the prediction using the endpoint I am running into the following error:-<\/p>\n<pre><code>    results = rcf_inference.predict(df.values)\n\n    ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Amazon SageMaker could not get a response from the RCF-container2 endpoint. This can occur when CPU or memory utilization is high. To check your utilization, see Amazon CloudWatch. To fix this problem, use an instance type with more CPU capacity or memory.&quot;\n<\/code><\/pre>\n<p>I have tried with larger cpu instance but still I am getting the same issue. I guess the issue is functional.<\/p>\n<p>Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641988225017,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_time":1531640434317,
        "Owner_last_access_time":1655701655513,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":205,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70681074",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73817379,
        "Question_title":"How can I preprocess inputs sent to a hugging face estimator?",
        "Question_body":"<p>I've been reviewing tutorials like the one found <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">here<\/a> which detail how to train a huggingface estimator (specifically, a transformer model) and then deploy it to sagemaker.<\/p>\n<p>The tutorial linked above trains an estimator via:<\/p>\n<pre><code>huggingface_estimator = HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py37',\n                            role=role,\n                            hyperparameters = {'epochs': 1,\n                                               'train_batch_size': 32,\n                                               'model_name':'distilbert-base-uncased'\n                                                })\n<\/code><\/pre>\n<p>Where <code>train.py<\/code> outlines a training script which tokenizes input data, and then fine-tunes a transformer model on the training data. <em>Note: in this example, the training data is hard coded into <code>train.py<\/code>, but that is not the source of the issue I'm encountering.<\/em><\/p>\n<p>The model is fit using<\/p>\n<pre><code>huggingface_estimator.fit()\n<\/code><\/pre>\n<p>and is then deployed using<\/p>\n<pre><code>predictor = huggingface_estimator.deploy(1,&quot;ml.g4dn.xlarge&quot;)\n<\/code><\/pre>\n<p>But then this deployed model is used to make a prediction via:<\/p>\n<pre><code>sentiment_input= {&quot;inputs&quot; : &quot;I love using the new Inference DLC.&quot;}\npredictor.predict(sentiment_input)\n<\/code><\/pre>\n<p>The problem is that no one specifed <em>anywhere<\/em> how this input is to be preprocessed: the model does not work on raw text, the text must be tokenized. Even the official <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs<\/a> for sagemaker don't seem to outline how preprocessing for a deployed model is handled.<\/p>\n<p>How can I specify a preprocessing step for my model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663861214277,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":8,
        "Owner_creation_time":1560785927300,
        "Owner_last_access_time":1664072042827,
        "Owner_location":"Ontario, Canada",
        "Owner_reputation":4613,
        "Owner_up_votes":1267,
        "Owner_down_votes":633,
        "Owner_views":401,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73817379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73651368,
        "Question_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Question_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662649911493,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":19,
        "Owner_creation_time":1662649653073,
        "Owner_last_access_time":1663012631407,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662652096573,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72534515,
        "Question_title":"How to add sagemaker createApp to user profile executionrole?",
        "Question_body":"<p>I created a aws sagemaker user profile using terraform. I tried to launch the sagemaker studio from the user profile but was confronted with this error: <code>SageMaker is unable to use your associated ExecutionRole [arn:aws:iam::xxxxxxxxxxxx:role\/sagemaker-workshop-data-ml] to create app. Verify that your associated ExecutionRole has permission for 'sagemaker:CreateApp'<\/code>. The role has sagemaker full access policy attached to it, but that policy doesn't have the createApp permission which is weird. Are there any policies I can attach to the role with the sagemaker createApp permission, or do I need to attach a policy to the role through terraform?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1654618548583,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":199,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72534515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66865031,
        "Question_title":"Sagemaker Regression - ValueError: Cannot format input",
        "Question_body":"<p>I am new to SageMaker &amp; Python<\/p>\n<p>I am trying to get a simple regression model going on AWS using Jupyter Notebooks.\nI am using the Abalone date from the  UCI data repository.\nI would greatly appreciate some assistance or a link to help me in what to do.<\/p>\n<p>Everything looks fine until I try to run:<\/p>\n<pre><code>\nregression_linear = sagemaker.estimator.Estimator(\n    container,\n    role=sagemaker.get_execution_role(),\n    input_mode = &quot;File&quot;,\n    instance_count = 1,\n    instance_type='ml.m4.xlarge',\n    output_path=output_location,\n    sagemaker_session=sess\n    )\n\nregression_linear.set_hyperparameters(\n    feature_dim=8,\n    epochs=16,\n    wd=0.01,\n    loss=&quot;absolute_loss&quot;,\n    predictor_type=&quot;regressor&quot;,\n    normalize_data=True,\n    optimizer=&quot;adam&quot;,\n    mini_batch_size=100,\n    lr_scheduler_step=100,\n    lr_scheduler_factor=0.99,\n    lr_scheduler_minimum_lr=0.0001,\n    learning_rate=0.1,\n    )\n\nfrom time import gmtime, strftime\njob_name = &quot;DEMO-linear-learner-abalone-regression-&quot; + strftime(&quot;%H-%M-%S&quot;, gmtime())\nprint(&quot;Training job: &quot;, job_name)\n\nregression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n<\/code><\/pre>\n<p>Then I am getting the following error:<\/p>\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-101-82bd2950b590&gt; in &lt;module&gt;\n----&gt; 1 regression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n      2 \n      3 # , &quot;validation&quot;: test_data\n\nValueError: Cannot format input       age  sex  length  diameter  height  whole_weight  shucked_weight  \\\n449    18    0   0.565     0.455   0.150        0.8205          0.3650   \n1080    7    1   0.430     0.335   0.120        0.3970          0.1985   \n2310   13    0   0.435     0.350   0.110        0.3840          0.1430   \n3790   10    0   0.650     0.505   0.175        1.2075          0.5105   \n3609    9    0   0.555     0.405   0.120        0.9130          0.4585   \n...   ...  ...     ...       ...     ...           ...             ...   \n2145    9    0   0.415     0.325   0.115        0.3455          0.1405   \n3815    8   -1   0.460     0.340   0.100        0.3860          0.1805   \n3534    6   -1   0.400     0.315   0.090        0.3300          0.1510   \n2217   13    0   0.515     0.415   0.130        0.7640          0.2760   \n3041    9    1   0.575     0.470   0.150        0.9785          0.4505   \n\n      vicera_weight  shell_weight  \n449          0.1590        0.2600  \n1080         0.0865        0.1035  \n2310         0.1005        0.1250  \n3790         0.2620        0.3900  \n3609         0.1960        0.2065  \n...             ...           ...  \n2145         0.0765        0.1100  \n3815         0.0875        0.0965  \n3534         0.0680        0.0800  \n2217         0.1960        0.2500  \n3041         0.1960        0.2760  \n\n[2923 rows x 9 columns]. Expecting one of str, TrainingInput, file_input or FileSystemInput\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1617080401673,
        "Question_score":1,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":163,
        "Owner_creation_time":1552345114290,
        "Owner_last_access_time":1660874551103,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":672,
        "Owner_up_votes":69,
        "Owner_down_votes":18,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66865031",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66912388,
        "Question_title":"Does AWS Sagemaker PySparkProcessor manage autoscaling?",
        "Question_body":"<p>I'm using Sagemaker to generate to do preprocessing and generate training data and I'm following the Sagemaker API documentation <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">here<\/a>, but I don't see any way currently how to specify autoscaling within the EMR cluster. What should I include within the <code>configuration<\/code> argument that I pass to my spark_processor <code>run()<\/code> object? What shouldn't I include?<\/p>\n<p>I'm aware of the <a href=\"https:\/\/docs.aws.amazon.com\/emr\/latest\/ReleaseGuide\/emr-configure-apps.html\" rel=\"nofollow noreferrer\">this resource<\/a>, but it doesn't seem comprehensive.<\/p>\n<p>Below is my code; it is very much a &quot;work-in-progress&quot;, but I would like to know if someone could provide me with or point me to a resource that shows:<\/p>\n<ol>\n<li>Whether this PySparkProcessor object will manage autoscaling automatically. Should I put AutoScaling config within the <code>configuration<\/code> in the <code>run()<\/code> object?<\/li>\n<li>An example of the full config that I can pass to the <code>configuration<\/code> variable.<\/li>\n<\/ol>\n<p>Here's what I have so far for the configuration.<\/p>\n<pre><code>\nSPARK_CONFIG = \\\n    { &quot;Configurations&quot;: [\n          {   &quot;Classification&quot;: &quot;spark-env&quot;,\n              &quot;Configurations&quot;: [ {&quot;Classification&quot;: &quot;export&quot;} ] }\n        ] \n    }\n\nspark_processor = PySparkProcessor(\n    tags=TAGS,\n    role=IAM_ROLE,\n    instance_count=2,\n    py_version=&quot;py37&quot;,\n    volume_size_in_gb=30,\n    container_version=&quot;1&quot;,\n    framework_version=&quot;3.0&quot;,\n    network_config=sm_network,\n    max_runtime_in_seconds=1800,\n    instance_type=&quot;ml.m5.2xlarge&quot;,\n    base_job_name=EMR_CLUSTER_NAME,\n    sagemaker_session=sagemaker_session,\n)\n\nspark_processor.run(\n    configuration=SPARK_CONFIG,\n    submit_app=LOCAL_PYSPARK_SCRIPT_DIR,\n    spark_event_logs_s3_uri=&quot;s3:\/\/{BUCKET_NAME}\/{S3_PYSPARK_LOG_PREFIX}&quot;,\n)\n<\/code><\/pre>\n<p>I'm used to interacting via Python more directly with EMR for these types of tasks. Doing that allows me to specify the entire EMR cluster config at once--including applications, autoscaling, EMR default and autoscaling roles--and then adding the steps to the cluster once it's created; however, much of this config seems to be abstracted away, and I don't know what remains or needs to be specified, specifically regarding the following config variables:  <code>AutoScalingRole<\/code>, <code>Applications<\/code>, <code>VisibleToAllUsers<\/code>, <code>JobFlowRole<\/code>\/<code>ServiceRole<\/code> etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617316641580,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|sdk|amazon-sagemaker",
        "Question_view_count":276,
        "Owner_creation_time":1401993171660,
        "Owner_last_access_time":1662665661517,
        "Owner_location":"Lehi, UT, USA",
        "Owner_reputation":443,
        "Owner_up_votes":977,
        "Owner_down_votes":5,
        "Owner_views":156,
        "Question_last_edit_time":1617381153807,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66912388",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49365900,
        "Question_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Question_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1521471311783,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-s3|boto|amazon-sagemaker",
        "Question_view_count":532,
        "Owner_creation_time":1521470035783,
        "Owner_last_access_time":1598556805913,
        "Owner_location":null,
        "Owner_reputation":210,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1521485629850,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1522056862430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49665241,
        "Question_title":"How do I load python modules which are not available in Sagemaker?",
        "Question_body":"<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1522908559597,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2578,
        "Owner_creation_time":1410972175307,
        "Owner_last_access_time":1663832955917,
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When creating you model, you can specify the requirements.txt as an environment variable. <\/p>\n\n<p>For Eg. <\/p>\n\n<pre><code>env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/mybucket\/modelTarFile,\n                                  role = role,\n                                  entry_point = 'entry.py',\n                                  code_location = 's3:\/\/mybucket\/runtime-code\/',\n                                  source_dir = 'src',\n                                  env = env,\n                                  name = 'model_name',\n                                  sagemaker_session = sagemaker_session,\n                                 )\n<\/code><\/pre>\n\n<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1522941835117,
        "Answer_score":10.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1522945323343,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49665241",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71909360,
        "Question_title":"Reading multiple csv files in AWS Sagemaker from a location in Amazon S3 Bucket",
        "Question_body":"<p>I have multiple csv files in a location in S3. The name of those files is in a date format. Example: 2021_09_30_Output.csv<\/p>\n<p>I need to understand how I can read all the files in this folder while selecting only the dates that I require. An example would be reading only the files from September. ie: &quot;2022_09_*.csv&quot; which would read only the files from that month<\/p>\n<p>Would appreciate the help. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650270357780,
        "Question_score":0,
        "Question_tags":"pandas|amazon-web-services|for-loop|amazon-s3|amazon-sagemaker",
        "Question_view_count":407,
        "Owner_creation_time":1607598608387,
        "Owner_last_access_time":1658981623800,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71909360",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53809556,
        "Question_title":"Load csv into S3 from local",
        "Question_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1545025564273,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1455496483357,
        "Owner_last_access_time":1642546968190,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3912,
        "Owner_up_votes":135,
        "Owner_down_votes":47,
        "Owner_views":311,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545428602017,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65811448,
        "Question_title":"How to observe and control how sagemaker multimodel server loads models in memory",
        "Question_body":"<p>I am evaluating SageMaker Multi Model Server (MMS) as an option to host large number of models for inference. I have successfully built the container according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">SageMaker BYOC MMS<\/a> instruction. I can invoke inference and the models work fine on SageMaker.<\/p>\n<p>I run my tests on the smallest instance type available <code>ml.t2.medium<\/code>. The MMS is described as downloading models from S3, loading them to container, and loading the models to memory as needed. Then offloading from memory when low on memory.<\/p>\n<p>In my experiment the MMS constantly reports the CloudWatch metric of <code>LoadedModelCount<\/code> at around 8-10. Even if I run inference on much larger set of models. If I keep the number of models invoked small, the inference call takes about 0.1 seconds. If I go over the <code>LoadedModelCount<\/code>, the inference time goes up to about 2s.<\/p>\n<p>So my guess is that the SageMaker MMS is unloading models from memory, and loading new models into memory, basically memory-swapping constantly. I put logging into my MMS model handler to show that it keeps initializing the handler for different models over and over when this happens.<\/p>\n<p>Also the CloudWatch metric <code>DiskUtilization<\/code> keeps going up with more models invoked, which I expect means it loads the models from S3 into container disk. The other metrics (memory and loaded models) on the other hand plateau after the 8-10 loaded models, with only minor changes up and down. Which further seems to support this theory that it swaps constantly from container disk to memory.<\/p>\n<p>I cannot find a way to see when MMS is actually unloading a model from memory, or when it loads a different one. Also, I cannot see what threshold is it using to unload models, as the CloudWatch <code>MemoryUtilization<\/code> metric from the SageMaker instance never goes above 45, which I guess means 45% of memory is used at most. This seems like a very low threshold, so I would expect to find a way to configure it, but have not found it.<\/p>\n<p>Question 1: How can I observe when MMS is unloading models from memory, and loading new ones?<\/p>\n<p>Question 2: How can I control the memory thresholds (or whatever MMS uses) that define when to unload the models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1611152754440,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mms",
        "Question_view_count":197,
        "Owner_creation_time":1328263672940,
        "Owner_last_access_time":1661507244447,
        "Owner_location":null,
        "Owner_reputation":999,
        "Owner_up_votes":127,
        "Owner_down_votes":4,
        "Owner_views":70,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65811448",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828700,
        "Question_title":"AWS SageMaker Canvas Model usage on Edge device in Python",
        "Question_body":"<p>This way I wanted to ask a question about AWS Sagemaker. I must confess that I'm quite a newbee to the subject and therefor I was very happy with the SageMaker Canvas app. It works really easy and gives me some nice results.<\/p>\n<p>First of all my model. I try to predict solar power production based on the time (dt), the AWS IoT Thingname (thingname), clouds percentage (clouds) and temperature (temp). I have a csv filled with data measured by IoT things<\/p>\n<p><code>clouds<\/code> + <code>temp<\/code> + <code>dt<\/code> + <code>thingname<\/code> =&gt; <code>import<\/code><\/p>\n<pre><code>dt,clouds,temp,import,thingname\n2022-08-30 07:45:00+02:00,1.0,0.1577,0.03,***\n2022-08-30 08:00:00+02:00,1.0,0.159,0.05,***\n2022-08-30 08:15:00+02:00,1.0,0.1603,0.06,***\n2022-08-30 08:30:00+02:00,1.0,0.16440000000000002,0.08,***\n2022-08-30 08:45:00+02:00,,,0.09,***\n2022-08-30 09:00:00+02:00,1.0,0.17,0.12,***\n2022-08-30 09:15:00+02:00,1.0,0.1747,0.13,***\n2022-08-30 09:30:00+02:00,1.0,0.1766,0.15,***\n2022-08-30 09:45:00+02:00,0.75,0.1809,0.18,***\n2022-08-30 10:00:00+02:00,1.0,0.1858,0.2,***\n2022-08-30 10:15:00+02:00,1.0,0.1888,0.21,***\n2022-08-30 10:30:00+02:00,0.75,0.1955,0.24,***\n<\/code><\/pre>\n<p>In AWS SageMaker canvas I upload the csv and build the model. All is very easy and when I use the predict tab I upload a CSV where the import column is missing and containing API weather data for some future moment:<\/p>\n<pre><code>dt,thingname,temp,clouds\n2022-09-21 10:15:00+02:00,***,0.1235,1.0\n2022-09-21 10:30:00+02:00,***,0.1235,1.0\n2022-09-21 10:45:00+02:00,***,0.1235,1.0\n2022-09-21 11:00:00+02:00,***,0.1235,1.0\n2022-09-21 11:15:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:30:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:45:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:00:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:15:00+02:00,***,0.1351,0.69\n2022-09-21 12:30:00+02:00,***,0.1351,0.69\n2022-09-21 12:45:00+02:00,***,0.1351,0.69\n<\/code><\/pre>\n<p>From this data SageMaker Canvas predicts some real realistic numbers, from which I assume the model is nicely build. So I want to move this model to my Greengrass Core Device to do predictions on site. I found the best model location using the sharing link to the Junyper notebook.<\/p>\n<p>From reading in the AWS docs I seem to have a few options to run the model on an edge device:<\/p>\n<ul>\n<li>Run the Greengrass SageMaker Edge component and run the model as a component and write an inference component<\/li>\n<li>Run the SageMaker Edge Agent yourself<\/li>\n<li>Just download the model yourself and do your thing with it on the device<\/li>\n<\/ul>\n<p>Now it seems that SageMaker used XGBoost to create the model and I found the <code>xgboost-model<\/code> file and downloaded it to the device.<\/p>\n<p>But here is where the trouble started:\nSageMaker Canvas never gives any info on what it does with the CSV to format it, so I have really no clue on how to make a prediction using the model.\nI get some results when I try to open the same csv file I used for the Canvas prediction, but the data is completely different and not realistic at all<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\n\nfilename = f'solar-prediction-data.csv'\ndpredict = xgb.DMatrix(f'{filename}?format=csv')\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict)\nprint('Prediction result::')\nprint(result)\n<\/code><\/pre>\n<p>I read that the column order matters, the CSV may not contain a header.  But it does not get close to the SageMaker Canvas result.<\/p>\n<p>I also tried using <code>pandas<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\nimport pandas as pd\n\nfilename = f'solar-prediction-data.csv'\ndf = pd.read_csv(filename, index_col=None, header=None)\n\ndpredict = xgb.DMatrix(df, enable_categorical=True)\n\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict, pred_interactions=True)\nprint('Prediction result::')\nprint('===============')\nprint(result)\n<\/code><\/pre>\n<p>But this last one always gives me following error:<\/p>\n<pre><code>ValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\ncategorical type is supplied, DMatrix parameter `enable_categorical` must\nbe set to `True`. Invalid columns:dt, thingname\n<\/code><\/pre>\n<p>To be honest, I'm completely stuck and hope someone around here can give me some advice or clue on how I can proceed.<\/p>\n<p>Thanks!\nKind regards<\/p>\n<p>Hacor<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663940700760,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|aws-iot|aws-iot-core|aws-iot-greengrass",
        "Question_view_count":18,
        "Owner_creation_time":1455091044887,
        "Owner_last_access_time":1663955705747,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828700",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69747506,
        "Question_title":"Amazon SageMaker Model Monitor for Batch Transform jobs",
        "Question_body":"<p>Couldn't find the right place to ask this, so doing it here.<\/p>\n<p>Does Model Monitor support monitoring Batch Transform jobs, or only endpoints? The documentation seems to only reference endpoints...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1635387455380,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":572,
        "Owner_creation_time":1475895512717,
        "Owner_last_access_time":1662166636447,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":2304,
        "Owner_up_votes":277,
        "Owner_down_votes":33,
        "Owner_views":166,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69747506",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56353814,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559108619707,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1826,
        "Owner_creation_time":1559099281007,
        "Owner_last_access_time":1624868926130,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559329920120,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65330580,
        "Question_title":"How to Deploy trained TensorFlow 2.0 models using Amazon SageMaker?",
        "Question_body":"<p>i am trying to deploy a custom trained tensorflow model using Amazon SageMaker. i have trained xlm roberta using tf 2.2.0 for multilingual sentiment analysis task.(please refer to this notebook : <a href=\"https:\/\/www.kaggle.com\/mobassir\/understanding-cross-lingual-models\" rel=\"nofollow noreferrer\">https:\/\/www.kaggle.com\/mobassir\/understanding-cross-lingual-models<\/a>)\nnow, using trained weight file of my model i am trying to deploy that in sagemaker, i was following this tutorial : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a><\/p>\n<p>converted some keras code from there to tensorflow.keras for 2.2.0\nbut when i do : <strong>!ls export\/Servo\/1\/variables<\/strong> i can see that export as Savedmodel generating empty variables directory like this : <a href=\"https:\/\/github.com\/tensorflow\/models\/issues\/1988\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/models\/issues\/1988<\/a><\/p>\n<p>i can't find any documentation help for tf 2.2.0 trained model deployment<\/p>\n<p>need example like this : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a> for tf 2.x models and not keras<\/p>\n<p>even though <strong>!ls export\/Servo\/1\/variables<\/strong> shows empty directory but An endpoint was created successfully and now i am not sure if my model was deployed successfully or not because when i try to test the model deployment inside aws notebook by using predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ni.e. predictor.predict(data) i get the following error message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{\n    &quot;error&quot;: &quot;Session was not created with a graph before Run()!&quot;\n}&quot;\n<\/code><\/pre>\n<p>related problem : <a href=\"https:\/\/stackoverflow.com\/questions\/46201109\/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\">Inference error with TensorFlow C++ on iOS: &quot;Invalid argument: Session was not created with a graph before Run()!&quot;<\/a><\/p>\n<p>the code i tried can be found here : <a href=\"https:\/\/pastebin.com\/sGuTtnSD\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/sGuTtnSD<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1608150035357,
        "Question_score":3,
        "Question_tags":"python-3.x|tensorflow|amazon-sagemaker",
        "Question_view_count":537,
        "Owner_creation_time":1487079300550,
        "Owner_last_access_time":1664083891510,
        "Owner_location":"Bangladesh",
        "Owner_reputation":227,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":305,
        "Question_last_edit_time":1608152033677,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65330580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63679503,
        "Question_title":"Why did it take so long to create endpoint with AWS Sagemaker using Boto3?",
        "Question_body":"<p>It took 45 minutes to create my endpoint from the stored endpoint configuration. (I tested it and it works too). This is the first time that I've used boto3 to do this, whereas previously I just used the Sagemaker web GUI to create an endpoint from endpoint configuration.  Suggestions to my code are appreciated:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName='sagemaker-tensorflow-x',\n    EndpointConfigName='sagemaker-tensorflow-x'\n)\n<\/code><\/pre>\n<p>Note: I've replaced the last part of my endpoint name with <code>x<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598917151350,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|boto3|amazon-sagemaker",
        "Question_view_count":373,
        "Owner_creation_time":1347733578130,
        "Owner_last_access_time":1664081922597,
        "Owner_location":"Chicago, IL, United States",
        "Owner_reputation":16557,
        "Owner_up_votes":2087,
        "Owner_down_votes":42,
        "Owner_views":461,
        "Question_last_edit_time":1598923606240,
        "Answer_body":"<p>AWS has currently <a href=\"https:\/\/status.aws.amazon.com\/\" rel=\"nofollow noreferrer\">issues<\/a> with Sagemaker:<\/p>\n<blockquote>\n<p>Increased Error Rates and Latencies for Multiple API operations<\/p>\n<\/blockquote>\n<blockquote>\n<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<blockquote>\n<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598923278260,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63679503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67118613,
        "Question_title":"Read compressed CSV (gzip) file from AWS S3 into Panda data frame in Sagemaker",
        "Question_body":"<p>I am trying to read a large compressed CSV file from AWS S3 and convert it to a Panda data frame in\u00a0Sagemaker. Is there any direct and clean approach to do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618542767620,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|gzip|amazon-sagemaker",
        "Question_view_count":1115,
        "Owner_creation_time":1524603494497,
        "Owner_last_access_time":1663090323520,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67118613",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62931618,
        "Question_title":"use global variables in AWS Sagemaker script",
        "Question_body":"<p>After having correctly deployed our model, I need to invoke it via lambda function. The script features two cleaning function, the first one (cleaning()) gives us 5 variables: the cleaned dataset and 4 other variables (scaler, monthdummies, compadummies, parceldummies) that we need to use in the second cleaning function (cleaning_test()).<\/p>\n<p>The reason behind this is that in the use case I'll have only one instance at a time to perform predictions on, not an entire dataset. This means that I pass the row to the first cleaning() function since some commands won't work. I can't also use a scaler and neither create dummy variables, so the aim is to import the scaler and some dummies used in the cleaning() function, since they come from the whole dataset, that I used to train the model.<\/p>\n<p>Hence, in the input_fn() function, the input needs to be cleaned using the cleaning_test() function, that requires the scaler and the three lists of dummies from the cleaning() one.<\/p>\n<p>When I train the model, the cleaning() function works fine, but after the deployment, if we invoke the endpoint, it raises the error that variable &quot;scaler&quot; is not defined.<\/p>\n<p>Below is the script.py:\nNote that the test is # since I've already tested it, so now I'm training on the whole dataset and I want to predict completely new instances<\/p>\n<pre><code>def cleaning(data):\n    some cleaning on data stored in s3\n    return cleaned_data, scaler, monthdummies, compadummies, parceldummies\n\ndef cleaning_test(data, scaler, monthdummies, compadummies, parceldummies):\n    cleaning on data without labels\n    return cleaned_data\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n\n\n\ndef input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/json&quot;:\n        data = json.loads(request_body)\n        df = pd.DataFrame(data, index = [0])\n        input_data = cleaning_test(df, scaler, monthdummies, compadummies, parceldummies)\n    else:\n        pass\n    return input_data\n\n        \ndef predict_fn(input_data, model):\n    return model.predict_proba(input_data)\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--n_estimators', type=int, default=10)\n    parser.add_argument('--min-samples-leaf', type=int, default=3)\n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    #parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='fp_train.csv')\n    #parser.add_argument('--test-file', type=str, default='fp_test.csv')\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    #test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    \n    print(&quot;cleaning&quot;)\n    train_df, scaler, monthdummies, compadummies, parceldummies = cleaning(train_df)\n    #test_df, scaler1, monthdummies1, compadummies1, parceldummies1 = cleaning(test_df)\n    \n    print(&quot;splitting&quot;)\n    y = train_df.loc[:,&quot;event&quot;]\n    X = train_df.loc[:, train_df.columns != 'event']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n    &quot;&quot;&quot;print('building training and testing datasets')\n    X_train = train_df.loc[:, train_df.columns != 'event']\n    X_test = test_df.loc[:, test_df.columns != 'event']\n    y_train = train_df.loc[:,&quot;event&quot;]\n    y_test = test_df.loc[:,&quot;event&quot;]&quot;&quot;&quot;\n    \n    print(X_train.columns)\n    print(X_test.columns)\n    \n\n\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        n_estimators=args.n_estimators,\n        min_samples_leaf=args.min_samples_leaf,\n        n_jobs=-1)\n    \n    model.fit(X_train, y_train)\n\n    # print abs error\n    print('validating model')\n    proba = model.predict_proba(X_test)\n\n    \n    # persist model\n    path = os.path.join(args.model_dir, &quot;model.joblib&quot;)\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n\n<\/code><\/pre>\n<p>That I run through:<\/p>\n<pre><code>sklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    train_instance_count=1,\n    train_instance_type='ml.c5.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit',\n    hyperparameters = {'n_estimators': 15})\n\nsklearn_estimator.fit({'train':trainpath})\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = sm_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\n\npredictor = sklearn_estimator.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1)\n<\/code><\/pre>\n<p>The question is, how can I &quot;store&quot; the variables given by the cleaning() function during the training process, in order to use them in the input_fn() function, making cleaning_test() work fine?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1594891762437,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_time":1541156154730,
        "Owner_last_access_time":1663934970577,
        "Owner_location":"Italia",
        "Owner_reputation":119,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62931618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63650203,
        "Question_title":"Install Tensorflow Object Detection API without replacing existing Tensorflow package",
        "Question_body":"<p>I'm trying to build a custom container image based on AWS SageMaker 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:2.3.0-gpu-py37-cu102-ubuntu18.04 image and following the instructions in <a href=\"https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/install.html#tensorflow-object-detection-api-installation\" rel=\"nofollow noreferrer\">https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/install.html#tensorflow-object-detection-api-installation<\/a><\/p>\n<p>However, it seems that when I run the following commands, <code>pip install<\/code> replaces the already existing TensorFlow package with <code>tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl<\/code> which doesn't support AWS's CPU instructions and GPU devices.<\/p>\n<pre><code># From within TensorFlow\/models\/research\/\ncp object_detection\/packages\/tf2\/setup.py .\npython -m pip install .\n<\/code><\/pre>\n<p>How can I install the Object Detection API without replacing existing TensorFlow? I tried <code>python -m pip install --ignore-installed .<\/code> but it doesn't seem to have any effect.<\/p>\n<p><strong>Update 1:<\/strong><\/p>\n<p>It seems that the already installed <code>tensorflow<\/code> for that AWS docker image isn't detected by <code>pip<\/code> even though it's available in <code>\/usr\/local\/lib\/python3.7\/site-packages\/tensorflow<\/code>. This is why <code>pip<\/code> still attempts to install it even with <code>--ignore-installed<\/code>.<\/p>\n<p>As a workaround, I make a copy of the directory and then replace the newly installed one with it.<\/p>\n<pre><code>mv \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/ \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow.cpy\/\n# From within TensorFlow\/models\/research\/\ncp object_detection\/packages\/tf2\/setup.py .\npython -m pip install .\nrm -r \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/\nmv \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow.cpy\/ \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1598724180467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|pip|amazon-sagemaker|object-detection-api",
        "Question_view_count":476,
        "Owner_creation_time":1412575569680,
        "Owner_last_access_time":1663923370567,
        "Owner_location":null,
        "Owner_reputation":679,
        "Owner_up_votes":242,
        "Owner_down_votes":11,
        "Owner_views":82,
        "Question_last_edit_time":1598753123360,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63650203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56821807,
        "Question_title":"Cannot install the \"ipywidgets\" Jupyter Lab Extension on AWS sagemaker",
        "Question_body":"<p>To install Jupyter Lab Extension on AWS sagemaker, You need to follow <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a>. And then create the lifecycle configuration accordingly.<\/p>\n\n<p>I did it and this is my <code>on-start.sh<\/code> file.<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a jupyterlab extension package in SageMaker Notebook Instance\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\n# PARAMETERS\nEXTENSION_NAME=@jupyter-widgets\/jupyterlab-manager\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\njupyter labextension install $EXTENSION_NAME\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Everything should went smooth except for this extension it raises an error.<\/p>\n\n<p>This is the error log from cloud watch.<\/p>\n\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2019-06-26-23-3260vo0j6p: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n\n<p>This one is the error message shown in the sagemaker console.<\/p>\n\n<pre><code>Failure reason\nNotebook Instance Lifecycle Config 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance-lifecycle-config\/jupyter-widgets-for-jupyterlab-copy' for Notebook Instance 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance\/test' took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n<\/code><\/pre>\n\n<p>I had done several attempts to locate the bug in the script file and the setup file of <code>ipywidgets<\/code> concerning <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-my-script-bin-bashm-bad-interpreter-no-such-file-or-directory\">the 'bad interpreter' error<\/a>. I cannot find any traces of error in both.<\/p>\n\n<p>I tried to upgrade my instance to T2 largest instance just in case the error came from the timeout. <\/p>\n\n<p>The weirdest thing is that I am able to install it via the terminal from the terminal on jupyterlab. I measured the total time it takes to install and found it is around <code>4 mins<\/code> just enough time(AWS should allow more time since this is only one extension install). Noted that this installation was performed under the T2 medium instance(the cheapest instance type you could get). If you install it this way to have to reboot the jupyter lab to make it work, then you reboot your instance and everything reverts back to the not-yet-install state. This suggests that there is no way to install jupyter lab extension rather than using the lifecycle cycle configurations which will lead you back to the error.<\/p>\n\n<p>At this point, I gave up and use the jupyter notebook instead if I really want to use the <code>ipywidgets<\/code>.<\/p>\n\n<hr>\n\n<p>Normally, this should be raised as technical support on AWS, but I have the basic plan so I decided to file it in StackOverflow for others that might encounter the same thing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1561857335720,
        "Question_score":4,
        "Question_tags":"bash|amazon-web-services|amazon-sagemaker|jupyter-lab|ipywidgets",
        "Question_view_count":2151,
        "Owner_creation_time":1408765062060,
        "Owner_last_access_time":1663826817590,
        "Owner_location":"Bangkok, \u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22",
        "Owner_reputation":2620,
        "Owner_up_votes":240,
        "Owner_down_votes":10,
        "Owner_views":134,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56821807",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62866637,
        "Question_title":"The inference file that goes into the entry point of PyTorchModel to be deployed does not have an effect to the output of the predictor",
        "Question_body":"<p>I am currently running the code on AWS Sagemaker, trying to predict data using an already-trained model, accessed by MODEL_URL.<\/p>\n<p>With the code below, the inference.py as the entry_point does not seem to have an effect on the result of the trained prediction model. Any changes in inference.py does not alter the output (the output is always correct). Is there something I am misunderstanding with how the model works? And how can I incorporate inference.py to the prediction model as the entry point?<\/p>\n<pre><code>role = sagemaker.get_execution_role()\n\nmodel = PyTorchModel(model_data = MODEL_URL, \n                            role = role,\n                            framework_version = '0.4.0',\n                            entry_point = '\/inference.py',\n                            source_dir = SOURCE_DIR)\n\npredictor = model.deploy(instance_type = 'ml.c5.xlarge', \n                                   initial_instance_count = 1,\n                                   endpoint_name = RT_ENDPOINT_NAME)\n\nresult = predictor.predict(someData)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594589308900,
        "Question_score":0,
        "Question_tags":"pytorch|prediction|amazon-sagemaker",
        "Question_view_count":265,
        "Owner_creation_time":1594588675633,
        "Owner_last_access_time":1615497791753,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62866637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70217529,
        "Question_title":"Mount NVME drive of C5d instance in Amazon Sagemaker Studio",
        "Question_body":"<p>Is it possible to mount the NVME drive of an C5d instance in an Amazon Sagemaker Studio notebook? I can see the drive under 'lsblk' but am not allowed to format or mount it. The drive is also not visible in the '\/dev' folder. I tried this with a regular C5d EC2 instance and there it works without any issue.<\/p>\n<p>Edit: I tested it with Sagemaker Notebooks, not Studio, and it also works. It seems Studio doesn't have real access to the underlying infrastructure.<\/p>\n<p>Kind regards<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1638548468933,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_time":1495057266930,
        "Owner_last_access_time":1654516729713,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1638565418823,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70217529",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1578920666227,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_time":1544346330127,
        "Owner_last_access_time":1588231544867,
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1578989573557,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579189368977,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59717227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70512043,
        "Question_title":"ClientError: Failed to download data. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file",
        "Question_body":"<p>How are you?<\/p>\n<p>I'm trying to execute a sagemaker job but i get this error:<\/p>\n<pre><code>ClientError: Failed to download data. Cannot download s3:\/\/pocaaml\/sagemaker\/xsell_sc1_test\/model\/model_lgb.tar.gz, a previously downloaded file\/folder clashes with it. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file.\n<\/code><\/pre>\n<p>I'm have that model_lgb.tar.gz on that s3 path as you can see here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" alt=\"s3 bucket with model in it\" \/><\/a><\/p>\n<p>This is my code:<\/p>\n<pre><code>project_name = 'xsell_sc1_test'\ns3_bucket = &quot;pocaaml&quot;\nprefix = &quot;sagemaker\/&quot;+project_name\naccount_id = &quot;029294541817&quot;\ns3_bucket_base_uri = &quot;{}{}&quot;.format(&quot;s3:\/\/&quot;, s3_bucket)\ndev = &quot;dev-{}&quot;.format(strftime(&quot;%y-%m-%d-%H-%M&quot;, gmtime()))\n\nregion = sagemaker.Session().boto_region_name\nprint(&quot;Using AWS Region: {}&quot;.format(region))\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\nboto3.setup_default_session(region_name=region)\n\nboto_session = boto3.Session(region_name=region)\n\ns3_client = boto3.client(&quot;s3&quot;, region_name=region)\n\nsagemaker_boto_client = boto_session.client(&quot;sagemaker&quot;) #este pinta?\n\nsagemaker_session = sagemaker.session.Session(\n    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n)\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;, role=role, instance_type='ml.m5.4xlarge', instance_count=1\n)\n\nPREPROCESSING_SCRIPT_LOCATION = 'funciones_altas.py'\n\npreprocessing_input_code = sagemaker_session.upload_data(\n    PREPROCESSING_SCRIPT_LOCATION,\n    bucket=s3_bucket,\n    key_prefix=&quot;{}\/{}&quot;.format(prefix, &quot;code&quot;)\n)\n\npreprocessing_input_data = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;data&quot;)\npreprocessing_input_model = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;model&quot;)\npreprocessing_output = &quot;{}\/{}\/{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, dev, &quot;preprocessing&quot; ,&quot;output&quot;)\n\nprocessing_job_name = params[&quot;project_name&quot;].replace(&quot;_&quot;, &quot;-&quot;)+&quot;-preprocess-{}&quot;.format(strftime(&quot;%d-%H-%M-%S&quot;, gmtime()))\n\nsklearn_processor.run(\n    code=preprocessing_input_code,\n    job_name = processing_job_name,\n    inputs=[ProcessingInput(input_name=&quot;data&quot;,\n                            source=preprocessing_input_data, \n                            destination=&quot;\/opt\/ml\/processing\/input\/data&quot;),\n           ProcessingInput(input_name=&quot;model&quot;,\n                           source=preprocessing_input_model, \n                           destination=&quot;\/opt\/ml\/processing\/input\/model&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;output&quot;, \n                         destination=preprocessing_output,\n                         source=&quot;\/opt\/ml\/processing\/output&quot;)],\n    wait=False,\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe()\n<\/code><\/pre>\n<p>and on funciones_altas.py i'm using ohe_altas.tar.gz and not model_lgb.tar.gz making this error super weird.<\/p>\n<p>can you help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640722245710,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jobs|amazon-sagemaker",
        "Question_view_count":310,
        "Owner_creation_time":1577797879880,
        "Owner_last_access_time":1664036282340,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70512043",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51390606,
        "Question_title":"Create Amazon SageMaker Hyperparameter Tuning Job with Scikit Learn",
        "Question_body":"<p>I'm wondering how to automatically tune my scikit learn random forest model with Amazon Sagemaker. For now, I would like to tune a single hyperparameter called \"max_depth\". I'll dump my code first and express some concerns after.<\/p>\n\n<p>FILE: <code>notebook.ipynb<\/code><\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(image, role,\n              train_instance_count=1,\n              train_instance_type='ml.m4.xlarge',\n              output_path=output_location,\n              sagemaker_session=sagemaker_session,\n              )\n\nhyperparameter_ranges = {'max_depth': IntegerParameter(20, 30)}\nobjective_metric_name = 'score'\nmetric_definitions = [{'Name': 'score', 'Regex': 'score: ([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(estimator,\n                        objective_metric_name,\n                        hyperparameter_ranges,\n                        metric_definitions,\n                        max_jobs=9,\n                        max_parallel_jobs=3)\ntuner.fit({'train': train_data_location, 'test': test_data_location})\n<\/code><\/pre>\n\n<p>FILE: <code>train<\/code>  (located in docker container)<\/p>\n\n<pre><code>def train():\n    with open(param_path, 'r') as tc:\n        hyperparams = json.load(tc)\n    print(\"DEBUG VALUE: \", hyperparams)\n    data, class = get_data() #abstraction\n    X, y = train_data.drop(['class'], axis=1), train_data['class']\n    clf = RandomForestClassifier()\n    clf.fit(data, class)\n    print(\"score: \" + str(evaluate_model(clf)) + \"\\n\")\n<\/code><\/pre>\n\n<p>I see two issues with this code. First, If I put a json object {'max_value':2} in   a file named hyperparameters.json at the necessary path, the print statement outputs {} as if the file is empty. <\/p>\n\n<p>Issue number 2 is the fact that train() does not allow for hyperparameters to affect the code in any way shape or form. As far as I can tell, amazon has no documentation on the inner workings of the <code>tuner.fit()<\/code> method. This means I can't figure out how train() accesses the hyperparameters to test.<\/p>\n\n<p>Any help is appreciated, let me know if I can provide more code or clarify anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1531863719747,
        "Question_score":4,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":1765,
        "Owner_creation_time":1531862612423,
        "Owner_last_access_time":1533254015850,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51390606",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57063741,
        "Question_title":"How to convert food-101 dataset into usable format for AWS SageMaker",
        "Question_body":"<p>I'm still very new to the world of machine learning and am looking for some guidance for how to continue a project that I've been working on. Right now I'm trying to feed in the Food-101 dataset into the Image Classification algorithm in SageMaker and later deploy this trained model onto an AWS deeplens to have food detection capabilities. Unfortunately the dataset comes with only the raw image files organized in sub folders as well as a .h5 file (not sure if I can just directly feed this file type into sageMaker?). From what I've gathered neither of these are suitable ways to feed in this dataset into SageMaker and I was wondering if anyone could help point me in the right direction of how I might be able to prepare the dataset properly for SageMaker i.e convert to a .rec or something else. Apologies if the scope of this question is very broad I am still a beginner to all of this and I'm simply stuck and do not know how to proceed so any help you guys might be able to provide would be fantastic. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563303140363,
        "Question_score":0,
        "Question_tags":"amazon-web-services|dataset|amazon-sagemaker",
        "Question_view_count":254,
        "Owner_creation_time":1563302872867,
        "Owner_last_access_time":1566222490660,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57063741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55931620,
        "Question_title":"MXNetError - dataset does not start with a valid magic number",
        "Question_body":"<p>I am trying to use amazon sagemaker linear-learner algorithm, it support content type of \u2018application\/x-recordio-protobuf\u2019. In preprocessing phase, i used scikit-learn preprocessing to one-hot-encode my features. Then i use linear learner estimator to with record-io converted input data.<\/p>\n\n<p>I used package and the preprocess conversion was successful.<\/p>\n\n<p><code>from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor<\/code><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def output_fn(prediction, accept):\n    \"\"\"Format prediction output\n\n    The default accept\/content-type between containers for serial inference is JSON.\n    We also want to set the ContentType or mimetype as the same value as accept so the next\n    container can read the response payload correctly.\n    \"\"\"\n   if accept == 'text\/csv':\n        return worker.Response(encoders.encode(prediction.todense(), accept), mimetype=accept)\n    elif accept == 'application\/x-recordio-protobuf':\n        buf = BytesIO()\n        write_spmatrix_to_sparse_tensor(buf, prediction)\n        buf.seek(0)\n        return worker.Response(buf, accept, mimetype=accept)\n    else:\n        raise RuntimeError(\"{} accept type is not supported by this script.\".format(accept))\n<\/code><\/pre>\n\n<p>But when linear-learner takes the input record, it fails with the error below<\/p>\n\n<p>Caused by: [15:53:30] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.774.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100:<\/p>\n\n<p><code>(Input Error) The header of the MXNet RecordIO record at position 810 in the dataset does not start with a valid magic number.<\/code><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1556684928063,
        "Question_score":1,
        "Question_tags":"scikit-learn|protocol-buffers|linear-regression|mxnet|amazon-sagemaker",
        "Question_view_count":557,
        "Owner_creation_time":1375716022637,
        "Owner_last_access_time":1624974374273,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55931620",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71929127,
        "Question_title":"Install spark 3.2 in SageMaker Notebook",
        "Question_body":"<p>SageMaker supports pyspark 2.4 and I want to install latest version of pyspark in Sagemaker. Can you please let me know how can I install latest version of pyspark in SageMaker notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650390083470,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_time":1648649590150,
        "Owner_last_access_time":1663628695960,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71929127",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50985138,
        "Question_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Question_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1529660522477,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1523,
        "Owner_creation_time":1432680790120,
        "Owner_last_access_time":1559548299510,
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1530173488990,
        "Answer_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1530057890673,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1531248961193,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60953289,
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_score":1,
        "Question_tags":"python|image|docker|amazon-sagemaker|amazon-linux-2",
        "Question_view_count":1133,
        "Owner_creation_time":1448655975827,
        "Owner_last_access_time":1663931632060,
        "Owner_location":null,
        "Owner_reputation":1478,
        "Owner_up_votes":653,
        "Owner_down_votes":1,
        "Owner_views":135,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585671760393,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70465140,
        "Question_title":"I keep getting UnexpectedStatusException: Error for HyperParameterTuning job in AWS sagemaker",
        "Question_body":"<p>As mentioned in question, I keep getting UnexpectedStatusException: Error for HyperParameterTuning job xgboost-211***-1631: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job.<\/p>\n<p>I looked into parameter ranges based on <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a> to make sure that ranges are good and they seem to be ok.  Data is definitely good because I can train the model but can't tune it.<\/p>\n<p>Here is the code I am using :<\/p>\n<pre><code>import sagemaker\nimport boto3\nimport numpy as np                                # For matrix operations and numerical processing\nimport pandas as pd                               # For munging tabular data\nimport os \nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nfrom sagemaker.session import TrainingInput\nfrom sagemaker.debugger import Rule, rule_configs\n \nregion = boto3.Session().region_name    \nsmclient = boto3.Session().client('sagemaker')\n\nrole = sagemaker.get_execution_role()\n\n\n\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'output')\ncontainer=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;latest&quot;)\n\n\nxgb_model=sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=role,\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n)\n\nxgb_model.set_hyperparameters(\n    max_depth = 5,\n    eta = 0.2,\n    gamma = 4,\n    min_child_weight = 6,\n    subsample = 0.7,\n    objective = &quot;binary:logistic&quot;,\n    num_round = 10\n)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.1, 0.5),\n                        'min_child_weight': ContinuousParameter(1, 10),\n                        'alpha': ContinuousParameter(0, 3),\n                        'max_depth': IntegerParameter(0, 4)}\nobjective_metric_name = 'validation:auc'\n\n\ntuner = HyperparameterTuner(xgb_model,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=60,\n                            max_parallel_jobs=6)\n\n\n\ntrain_input = TrainingInput(\n    &quot;s3:\/\/{}\/{}\/{}&quot;.format(bucket, prefix, &quot;train\/train.csv&quot;), content_type=&quot;csv&quot;\n)\nvalidation_input = TrainingInput(\n    &quot;s3:\/\/{}\/{}\/{}&quot;.format(bucket, prefix, &quot;validate\/validation.csv&quot;), content_type=&quot;csv&quot;\n)\n\ntuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)\n<\/code><\/pre>\n<p>This is the error I get<\/p>\n<pre><code>&gt; --------------------------------------------------------------------------- UnexpectedStatusException                 Traceback (most recent call\n&gt; last) &lt;ipython-input-2-7824ad80a8bb&gt; in &lt;module&gt;\n&gt;      62 )\n&gt;      63 \n&gt; ---&gt; 64 tuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)\n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py\n&gt; in fit(self, inputs, job_name, include_cls_metadata, estimator_kwargs,\n&gt; wait, **kwargs)\n&gt;     449 \n&gt;     450         if wait:\n&gt; --&gt; 451             self.latest_tuning_job.wait()\n&gt;     452 \n&gt;     453     def _fit_with_estimator(self, inputs, job_name, include_cls_metadata, **kwargs):\n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py\n&gt; in wait(self)    1595     def wait(self):    1596        \n&gt; &quot;&quot;&quot;Placeholder docstring.&quot;&quot;&quot;\n&gt; -&gt; 1597         self.sagemaker_session.wait_for_tuning_job(self.name)    1598     1599 \n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n&gt; in wait_for_tuning_job(self, job, poll)    3253         &quot;&quot;&quot;    3254   \n&gt; desc = _wait_until(lambda: _tuning_job_status(self.sagemaker_client,\n&gt; job), poll)\n&gt; -&gt; 3255         self._check_job_status(job, desc, &quot;HyperParameterTuningJobStatus&quot;)    3256         return desc    3257 \n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n&gt; in _check_job_status(self, job, desc, status_key_name)    3336        \n&gt; ),    3337                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n&gt; -&gt; 3338                 actual_status=status,    3339             )    3340 \n&gt; \n&gt; UnexpectedStatusException: Error for HyperParameterTuning job\n&gt; xgboost-211XXX-1641: Failed. Reason: No training job succeeded after 5\n&gt; attempts. For additional details, please take a look at the training\n&gt; job failures by listing training jobs for the hyperparameter tuning\n&gt; job.\n<\/code><\/pre>\n<p>Thank you in advance,<\/p>\n<p>Sam<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1640278362207,
        "Question_score":4,
        "Question_tags":"python-3.x|xgboost|amazon-sagemaker",
        "Question_view_count":508,
        "Owner_creation_time":1418054263180,
        "Owner_last_access_time":1663863852740,
        "Owner_location":null,
        "Owner_reputation":1138,
        "Owner_up_votes":43,
        "Owner_down_votes":7,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70465140",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62057838,
        "Question_title":"How to retrieve the labels used in a segmentation mask in AWS Sagemaker",
        "Question_body":"<p>From a segmentation mask, I am trying to retrieve what labels are being represented in the mask. <\/p>\n\n<p>This is the image I am running through a semantic segmentation model in AWS Sagemaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" alt=\"Motorbike and everything else background\"><\/a><\/p>\n\n<p>Code for making prediction and displaying mask.<\/p>\n\n<pre><code>from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\n%%time\nss_predict = sagemaker.RealTimePredictor(endpoint=ss_model.endpoint_name, \n                                     sagemaker_session=sess,\n                                    content_type = 'image\/jpeg',\n                                    accept = 'image\/png')\n\nreturn_img = ss_predict.predict(img)\n\nfrom PIL import Image\nimport numpy as np\nimport io\n\nnum_labels = 21\nmask = np.array(Image.open(io.BytesIO(return_img)))\nplt.imshow(mask, vmin=0, vmax=num_labels-1, cmap='jet')\nplt.show()\n<\/code><\/pre>\n\n<p>This image is the segmentation mask that was created and it represents the motorbike and everything else is the background.<\/p>\n\n<p>[<img src=\"https:\/\/i.stack.imgur.com\/6FbVn.png\" alt=\"Segmented mask[2]\"><\/p>\n\n<p>As you can see from the code there are 21 possible labels and 2 were used in the mask, one for the motorbike and another for the background. What I would like to figure out now is how to print which labels were actually used in this mask out of the 21 possible options?<\/p>\n\n<p>Please let me know if you need any further information and any help is much appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590644898463,
        "Question_score":8,
        "Question_tags":"python|python-imaging-library|amazon-sagemaker|mxnet|semantic-segmentation",
        "Question_view_count":489,
        "Owner_creation_time":1449513251820,
        "Owner_last_access_time":1629436707117,
        "Owner_location":null,
        "Owner_reputation":693,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Somewhere you should have a mapping from label integers to label classes, e.g.<\/p>\n\n<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}\n<\/code><\/pre>\n\n<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv\/monitor) - see here: <a href=\"http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html\" rel=\"nofollow noreferrer\">http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html<\/a><\/p>\n\n<p>Then you can simply use that map:<\/p>\n\n<pre><code>used_classes = np.unique(mask)\nfor cls in used_classes:\n    print(\"Found class: {}\".format(label_map[cls]))\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1592390011563,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62057838",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71761511,
        "Question_title":"How can I add a final step to a Sagemaker Pipeline that runs even if other steps fail",
        "Question_body":"<p>Is there a way to add an end step to a sagemaker pipeline that still runs at the very end (and runs code) even if other previous steps fail. Before I thought we could make it a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html?highlight=definition#sagemaker.workflow.fail_step.FailStep\" rel=\"nofollow noreferrer\">Fail Step<\/a>  but that only lets you return an error message and doesn\u2019t let you run code. If we made it a conditional step how would we make sure it ran at the very end without depending on any previous steps. I thought of adding all previous steps as a dependency so it runs at the end, but then the end step wouldn't run if any step before that failed.<\/p>\n<p>I tried using the fail step, but I can't provide code. I tried putting it with dependencies but then it won't run if other steps fail before it. I tried putting no dependencies, but then it won't run at the end.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649223363853,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pipeline|amazon-sagemaker",
        "Question_view_count":372,
        "Owner_creation_time":1649223065523,
        "Owner_last_access_time":1653515060240,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71761511",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72054242,
        "Question_title":"data format to predict with model fitted via Sagemaker's XGBoost built-in algorithm and training container",
        "Question_body":"<p>Looking at the following code, taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a>, I wonder what format dtest is (sorry I could not gleen this from the post):<\/p>\n<pre><code>import pickle as pkl \nimport tarfile\n\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel = pkl.load(open(model_file_path, 'rb'))\n\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>\n<p>In my case the training and validation data are in csv format coming from a S3 bucket:<\/p>\n<pre><code>content_type = &quot;csv&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\n<\/code><\/pre>\n<p>So ideally, I would also like to use the same format for scoring\/prediction\/inference.<\/p>\n<p>PS:<\/p>\n<p>This little function appears to work fine:<\/p>\n<pre><code>def write_prediction_data(data_file_name, target_name, model_file_name, output_file_name):\n\n    model = pkl.load(open(model_file_name, 'rb'))\n    data = pd.read_csv(data_file_name) \n    target = data[target_name]\n    data = data.drop([target_name], axis=1)\n    xgb_data = xgb.DMatrix(data.values, target.values)\n\n    data = pd.read_csv(data_file_name)\n    data['Prediction'] = model.predict(xgb_data)\n\n    data.to_csv(output_file_name, index=False)\n<\/code><\/pre>\n<p>Improvement suggestions always welcome (-:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651217135783,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1651313138547,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72054242",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69587230,
        "Question_title":"How to Set Java Home for Notebook in SageMaker",
        "Question_body":"<p>So it seems that I have Java installed after running the below line in the SageMaker Notebook Terminal:<\/p>\n<pre><code>bash-4.2$ sudo yum install java-1.8.0-openjdk \n<\/code><\/pre>\n<p>In the terminal I write the following to confirm:<\/p>\n<pre><code>bash-4.2$ java -version \njava version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)\n<\/code><\/pre>\n<p>In my notebook I have the following Lines of code:<\/p>\n<pre><code>import tabula\n\ntabula.environment_info()\n<\/code><\/pre>\n<p>The notebook results in an error with:<\/p>\n<pre><code>java -version` faild. `java` command is not found from this Pythonprocess. Please ensure Java is installed and PATH is set for `java`\n<\/code><\/pre>\n<p>Yet, in the terminal I see this:<\/p>\n<pre><code>bash-4.2$ java -version \njava version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)\n<\/code><\/pre>\n<p>I definitely have a java environment. How can I set my notebook to find this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634312182323,
        "Question_score":1,
        "Question_tags":"java|amazon-web-services|java-8|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":477,
        "Owner_creation_time":1615555459547,
        "Owner_last_access_time":1653444707037,
        "Owner_location":"United States",
        "Owner_reputation":141,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69587230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72163392,
        "Question_title":"Run SageMaker Batch transform failed on loading model",
        "Question_body":"<p>I am trying to run batch transform job with HuggineFace class and fine-tuned model and custom inference file.\nThe job failed on loading the model but I could load it locally.\nI need to make custom inference file because i need to keep the input file as is, so i had to change the input key from the input json file.<\/p>\n<p>Here are the exception :<\/p>\n<pre><code>PredictionException(str(e), 400)\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Can't load config for '\/.sagemaker\/mms\/models\/model'. Make sure that:\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - - '\/.sagemaker\/mms\/models\/model' is a correct model identifier listed on 'https:\/\/huggingface.co\/models'\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - - or '\/.sagemaker\/mms\/models\/model' is the correct path to a directory containing a config.json file\n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  : 400\n<\/code><\/pre>\n<p>I am running on script mode:<\/p>\n<pre><code>from sagemaker.huggingface.model import HuggingFaceModel\n\nhub = {\n   # 'HF_MODEL_ID':'cardiffnlp\/twitter-roberta-base-sentiment',\n    'HF_TASK':'text-classification',\n    'INPUT_TEXTS': 'Description'\n}\n\nhuggingface_model = HuggingFaceModel(model_data='..\/model\/model.tar.gz',\n                                     role=role,\n                                     source_dir=&quot;..\/model\/pytorch_model\/code&quot;,\n                                     transformers_version=&quot;4.6&quot;, \n                                     pytorch_version=&quot;1.7&quot;, \n                                     py_version=&quot;py36&quot;,\n                                     entry_point=&quot;inference.py&quot;,\n                                     env=hub\n                                     )\n\nbatch_job = huggingface_model.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n    strategy='SingleRecord',\n    accept='application\/json',\n    assemble_with='Line'\n\n)\n\nbatch_job.transform(\n    data=s3_file_uri,\n    content_type='application\/json',\n    split_type='Line',\n    #input_filter='$[1:]',\n    join_source='Input'\n\n)\n<\/code><\/pre>\n<p>Custom inference.py<\/p>\n<pre><code>import json\nimport os\nfrom transformers import pipeline\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef model_fn(model_dir):\n    model = pipeline(task=os.environ.get('HF_TASK', 'text-classification'), model=model_dir, tokenizer=model_dir)\n    return model\n\ndef transform_fn(model, input_data, content_type, accept):\n    input_data = json.loads(input_data)\n    input_text = os.environ.get('INPUT_TEXTS', 'inputs')\n    inputs = input_data.pop(input_text, None)\n    parameters = input_data.pop(&quot;parameters&quot;, None)\n\n    # pass inputs with all kwargs in data\n    if parameters is not None:\n        prediction = model(inputs, **parameters)\n    else:\n        prediction = model(inputs)\n    return json.dumps(\n        prediction,\n        ensure_ascii=False,\n        allow_nan=False,\n        indent=None,\n        separators=(&quot;,&quot;, &quot;:&quot;),\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652029891640,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":209,
        "Owner_creation_time":1537218908380,
        "Owner_last_access_time":1660652518187,
        "Owner_location":"Israel",
        "Owner_reputation":931,
        "Owner_up_votes":107,
        "Owner_down_votes":3,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72163392",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62614143,
        "Question_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Question_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1593283354330,
        "Question_score":1,
        "Question_tags":"python|deep-learning|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":1360,
        "Owner_creation_time":1429270603900,
        "Owner_last_access_time":1664031172453,
        "Owner_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":138,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593463280863,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1595855655040,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62614143",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552436606600,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1553118034270,
        "Answer_score":14.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653478466700,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1653482394637,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654714910813,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654796658207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67361483,
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619992853797,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|pickle|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1553704286213,
        "Owner_last_access_time":1663944110353,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620025682633,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56245644,
        "Question_title":"How do I fix a memory allocation error in SageMaker without increasing instance size?",
        "Question_body":"<p>How can I resolve memory issues when training a CNN on SageMaker by increasing the number of instances, rather than changing the amount of memory each instance has? <\/p>\n\n<p>Using a larger instance does work, but I want to solve my problem by distributing across more instances. Using more instances ends up giving me a memory allocation error instead.<\/p>\n\n<p>Here is the code I am running in a Jupyter notebook cell:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(entry_point='train_aws.py',\n                       role=role,\n                       framework_version='1.12.0',\n                       training_steps= 100,                                  \n                       evaluation_steps= 100,\n                       hyperparameters={'learning_rate': 0.01},\n                       train_instance_count=2,\n                       train_instance_type='ml.c4.xlarge') \n\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I thought that adding more instances would increase the amount of memory, but it just gave me an allocation error instead.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1558469146393,
        "Question_score":0,
        "Question_tags":"amazon-web-services|deep-learning|training-data|amazon-sagemaker",
        "Question_view_count":2588,
        "Owner_creation_time":1558464611370,
        "Owner_last_access_time":1558469092013,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1558489822967,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56245644",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62305176,
        "Question_title":"Slow running of code in AWS for the first time",
        "Question_body":"<p>I am running my code on SageMaker, which runs my code slowly for the first time, but runs much faster the second time around. I guess there's something getting stored in the cache. Few days back, it was running with the same speed all the time. What could be a possible solution for this? <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>count_hea = 0\ncount_pleth = 0\nfor subfile in sorted(os.listdir('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00')):\n    count_hea = 0\n    if subfile.startswith('p'):\n        for subsubfile in sorted(os.listdir(os.path.join('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00\/' , subfile))):\n            if subsubfile.startswith('p') and count_hea == 0 and not subsubfile[:-4].endswith('n'):\n                try:            \n                    i = i + 1\n                    print(subsubfile)\n                    count_hea = count_hea + 1\n                    strip = subsubfile[:-4]\n                    record = wfdb.rdrecord('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00\/' + subfile + '\/' + strip, channel_names = ['PLETH'], return_res = 16)\n                    r = record.__dict__\n                    print(r['sig_name'])\n                    if r['sig_name'] != None:\n                        if r['sig_name'][0] == 'PLETH':\n                            count_pleth = count_pleth + 1\n                            print(count_pleth)  \n                except Exception:\n                    pass\nprint(count_pleth)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591796711163,
        "Question_score":0,
        "Question_tags":"amazon-web-services|caching|amazon-sagemaker",
        "Question_view_count":942,
        "Owner_creation_time":1591590460687,
        "Owner_last_access_time":1607696037083,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1591811735903,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62305176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65191030,
        "Question_title":"AWS SageMaker ML DevOps tooling \/ architecture - Kubeflow?",
        "Question_body":"<p>I'm tasked with defining AWS tools for ML development at a medium-sized company. Assume about a dozen ML engineers plus other DevOps staff familiar with serverless ( lambdas and the framework ). The main questions are: a) what is an architecture that allows for the main tasks related to ML development (creating, training, fitting models, data pre-processing, hyper parameter optimization, job management, wrapping serverless services, gathering model metrics, etc ), b) what are the main tools that can be used for packaging and deploying things and c) what are the development tools (IDEs, SDKs, 'frameworks' ) used for it?\nI just want to set Jupyter notebooks aside for a second. Jupyter notebooks are great for proof-of-concepts and the closest thing to PowerPoint for management... But I have a problem with notebooks when thinking about deployable units of code.<br \/>\nMy intuition points to a preliminary target architecture with 5 parts:<\/p>\n<p>1 - A 'core' with ML models supporting basic model operations (create blank, create pre-trained, train, test\/fit, etc). I foresee core Python scripts here - no problem.<\/p>\n<p>2- (optional) A 'containerized-set-of-things' that performs hyper parameter optimization and\/or model versioning<\/p>\n<p>3- A 'contained-unit-of-Python-scripts-around-models' that exposes an API and that does job management and incorporates data pre-processing. This also reads and writes to S3 buckets.<\/p>\n<p>4-  A 'serverless layer' with high level API ( in Python ). It talks to #3 and\/or #1 above.<\/p>\n<p>5- Some container or bundling thing that will unpack files from Git and deploy them onto various AWS services creating things from the previous 3 points.<\/p>\n<p>As you can see, my terms are rather fuzzy:)  If someone can be specific with terms that will be helpful.\nMy intuition and my preliminary readings say that the answer will likely include a local IDE like PyCharm or Anaconda or a cloud-based IDE (what can these be? - don't mention notebooks please).\nThe point that I'm not really clear about is #5. Candidates include Amazon SageMaker Components for Kubeflow Pipelines and\/or Amazon SageMaker Components for Kubeflow Pipelines and\/or AWS Step Functions DS SDK For SageMaker. It's unclear to me how they can perform #5, however. Kubeflow looks very interesting but does it have enough adoption or will it die in 2 years? Are Amazon SageMaker Components for Kubeflow Pipelines, Amazon SageMaker Components for Kubeflow Pipelines and AWS Step Functions DS SDK For SageMaker mutually exclusive? How can each of them help with 'containerizing things' and with basic provisioning and deployment tasks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607384425877,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|kubeflow-pipelines",
        "Question_view_count":762,
        "Owner_creation_time":1519621319587,
        "Owner_last_access_time":1663870064033,
        "Owner_location":null,
        "Owner_reputation":123,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65191030",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72805580,
        "Question_title":"Is it possible to modify an existing AWS implementation of a deep learning model?",
        "Question_body":"<p>I wish to modify an existing model implementation in order to add an additional upsampling layer to a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">semantic segmentation algorithm that has previously been implemented in AWS<\/a>.<\/p>\n<p>It appears that Sagemaker refers to <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/tree\/4e1d44d718ca998523527aa2039cde2184d31296\" rel=\"nofollow noreferrer\">this repo<\/a>, and I'm hoping to modify <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/blob\/4e1d44d718ca998523527aa2039cde2184d31296\/gluoncv\/model_zoo\/deeplabv3.py\" rel=\"nofollow noreferrer\">the deeplab model<\/a> to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image (i.e., statistically downscale the original imagery).<\/p>\n<p>(<a href=\"https:\/\/www.researchgate.net\/publication\/349804789_A_Deep_Learning_Approach_to_Downscale_Geostationary_Satellite_Imagery_for_Decision_Support_in_High_Impact_Wildfires\" rel=\"nofollow noreferrer\">This technique has been demonstrated with UNET architectures.<\/a>)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656523696723,
        "Question_score":0,
        "Question_tags":"amazon-web-services|deep-learning|image-segmentation|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_time":1585530660043,
        "Owner_last_access_time":1663135350527,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72805580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71830450,
        "Question_title":"AWS SageMaker Pipelines not being triggered by EventBridge",
        "Question_body":"<p>I've created a new SageMaker pipeline using AWS Python SDK, and everything is working fine, I can trigger my pipeline and it works perfectly using the SDK with these simples commands:<\/p>\n<pre><code>pipeline.upsert(role_arn=get_execution_role())\nexecution = pipeline.start()\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I would like to schedule the pipeline execution to run every day during the morning (let's say 8 a.m for example). And here's my problem. I configured the EventBridge as shown in this tutorial: <a href=\"https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines<\/a>, but instead of creating a new role, I used an existing one (the same returned from the command get_execution_role() above). My event is triggered in the correct hour (every day at 8 am), but the pipeline doesn't execute. When checking the logs on Cloud Watch, It shows that I got a FailedInvocations for the event, but I don't know how to get the logs from this failed execution. I tried to search on cloud trail but don't found nothing.<\/p>\n<p>Anyone could help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1649690844973,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":470,
        "Owner_creation_time":1501678564607,
        "Owner_last_access_time":1664080447277,
        "Owner_location":"Tup\u00e3, SP, Brasil",
        "Owner_reputation":121,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71830450",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72838683,
        "Question_title":"Best way to train a deep learning model in AWS Sagemaker, when data (image data > 10000 images) lies on a S3 bucket",
        "Question_body":"<p>What is the best was to train a deep learning model on AWS Sagemaker when I have a huge image dataset stored on a AWS S3 bucket.\nThe Dataset shouldn't be downloaded to the EBS volume of the notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656761331573,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|deep-learning|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_time":1462106136470,
        "Owner_last_access_time":1664013959780,
        "Owner_location":"Chennai, India",
        "Owner_reputation":21,
        "Owner_up_votes":48,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72838683",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65609804,
        "Question_title":"How to append stepfunction execution id to SageMaker job names?",
        "Question_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610012100150,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|terraform-provider-aws|amazon-sagemaker|aws-step-functions",
        "Question_view_count":488,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1610014278923,
        "Answer_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Answer_comment_count":17.0,
        "Answer_creation_time":1610017101707,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71693539,
        "Question_title":"\"No entities to label\" has any cost in AWS Ground truth labeling?",
        "Question_body":"<p>I'm using amazon SageMaker Ground Truth to label texts, during the process I noticed that there is the option &quot;No entities to label&quot; and, I was wondering: if I select this option does the object still incur a cost to process?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648734027393,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":22,
        "Owner_creation_time":1630700788560,
        "Owner_last_access_time":1657552741383,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1648821921530,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71693539",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647258085310,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_time":1576016596283,
        "Owner_last_access_time":1660210631827,
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647268897627,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1647607100133,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73487568,
        "Question_title":"sagemaker transform not filtering input as expeced",
        "Question_body":"<p>Am trying to batch transform a csv file using this code<\/p>\n<p><strong>Processor script<\/strong><\/p>\n<pre><code>text_preparation_model = SKLearnModel(\n    sagemaker_session = local_session,\n    entry_point='processor.py',\n    role=context.role,\n    framework_version=..,\n    image_uri=...,\n    model_data=...)\n<\/code><\/pre>\n<p><strong>processor.py<\/strong><\/p>\n<pre><code>def input_fn(input_data, content_type):\n    if content_type == 'text\/csv':\n        df = pd.read_csv(StringIO(input_data), names=['feature_col1'],quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n        print(df.head())\n\n        return df\n    else:\n        raise ValueError(&quot;{} .Error &quot;.format(content_type))\n<\/code><\/pre>\n<p><strong>Transform<\/strong><\/p>\n<pre><code>text_preparation_transformer.transform(\n    'file:\/\/validation.txt',  \n    content_type='text\/csv',\n    split_type='Line',\n    logs=context.show_logs,\n    input_filter='$[0]',\n    join_source='Input'\n)\n<\/code><\/pre>\n<p>the file is 2 columns separated with comma and values are enclosed in double quotes <code>ex<\/code> &quot;some feature value&quot;,&quot;label_1&quot;.\n<a href=\"https:\/\/i.stack.imgur.com\/TKJTo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TKJTo.png\" alt=\"enter image description here\" \/><\/a>\nin refernce to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html\" rel=\"nofollow noreferrer\">this<\/a>,\nI expected to receive only the feature column in the <code>processor script<\/code>, but I keep getting both columns<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661430692517,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":26,
        "Owner_creation_time":1335447186710,
        "Owner_last_access_time":1664064892287,
        "Owner_location":"Egypt",
        "Owner_reputation":1972,
        "Owner_up_votes":701,
        "Owner_down_votes":11,
        "Owner_views":547,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73487568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72669269,
        "Question_title":"How to share a jupyterlab project that's linked with sagemaker?",
        "Question_body":"<p>I have a notebook instance on AWS SageMaker that I'd like to be able to share with colleagues and have them make edits to the code and be able to run tests themselves using the AWS compute that I have setup on my account.<\/p>\n<p>Is this possible? And if so, how? Everywhere I look, I see people saying that you can share a project with sagemaker studio, but all I'm working with is jupyterlab.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1655555485900,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|jupyter|jupyter-lab|amazon-sagemaker",
        "Question_view_count":105,
        "Owner_creation_time":1569340877270,
        "Owner_last_access_time":1663883527167,
        "Owner_location":null,
        "Owner_reputation":365,
        "Owner_up_votes":50,
        "Owner_down_votes":2,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72669269",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54465049,
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548952048437,
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_time":1361821819380,
        "Owner_last_access_time":1663972169277,
        "Owner_location":"Puebla",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":1549513217233,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1548960285577,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1548962267733,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51604901,
        "Question_title":"How to implement a beam search decoder in an SageMaker hosting endpoint?",
        "Question_body":"<p>I've created a SageMaker model for a Seq2Seq neural network, and then started a SageMaker endpoint:<\/p>\n\n<pre><code>create_endpoint_config_response = sage.create_endpoint_config(\n    EndpointConfigName = endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType':'ml.m4.xlarge', \n        'InitialInstanceCount':1,\n        'ModelName':model_name,\n        'VariantName':'AllTraffic'}])\n\ncreate_endpoint_response = sage.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>\n\n<p>This standard endpoint does not support beam search. What is the best approach for creating a SageMaker endpoint that supports beam search?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533006311500,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|beam-search",
        "Question_view_count":138,
        "Owner_creation_time":1533004774230,
        "Owner_last_access_time":1576237563200,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51604901",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62927805,
        "Question_title":"What permissions are required to allow Lambda function to create SageMaker Batch Transform Job if bucket name doesn't include sagemaker?",
        "Question_body":"<p>Here is custom policy for calling SageMaker Batch Transform Job:<\/p>\n<pre><code>{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:GetObject&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;\/path\/*&quot;\n      ]\n    },\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:PutObject&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;\/another_path\/*&quot;\n      ]\n    },\n    {\n      &quot;Action&quot;: [\n        &quot;s3:ListBucket&quot;\n      ],\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;\n      ]\n    },\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;sagemaker:CreateTransformJob&quot;,\n        &quot;sagemaker:DescribeTrainingJob&quot;,\n        &quot;sagemaker:DescribeTransformJob&quot;\n      ],\n      &quot;Resource&quot;: &quot;*&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>And I also attached to the Lambda role the default AWSLambdaBasicExecutionRole permissions.<\/p>\n<p>If I change the bucket name to include start with <em>sagemaker<\/em>, everything works fine, however, I cannot do that. Do you have any suggestions about the policy and what permissions I have missed to add?<\/p>\n<p>Edit<\/p>\n<p>Here is error messages from SageMaker transform job logs: <code># 011... 24 more2020-07-15T12:49:26.332:[sagemaker logs]: &lt;bucket-name&gt;\/path\/input_file.json: 403 Forbidden (403): Forbidden<\/code> and <code>2020-07-15T12:49:26.345:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD<\/code>. There are only 2 lines in that job log group. In another log group are no errors, only messages that model was loaded for predictions.<\/p>\n<p>And error message in Lambda: <code>Transform failed with the following error: ClientError: See job logs for more information<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1594875949330,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_time":1477378790530,
        "Owner_last_access_time":1663832602253,
        "Owner_location":null,
        "Owner_reputation":468,
        "Owner_up_votes":90,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1594877929890,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62927805",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68470626,
        "Question_title":"Defining Metrics on SageMaker to CloudWatch",
        "Question_body":"<p>From AWS Sagemaker Documentation, In order to track metrics in cloudwatch for custom ml algorithms (non-builtin), I read that I have to define my estimaotr as below.<\/p>\n<p>But I am not sure how to alter my training script so that the metric definitions declared inside my estimators can pick up these values.<\/p>\n<pre><code>estimator =\n                Estimator(image_name=ImageName,\n                role='SageMakerRole', \n                instance_count=1,\n                instance_type='ml.c4.xlarge',\n                k=10,\n                sagemaker_session=sagemaker_session,\n                metric_definitions=[\n                   {'Name': 'train:error', 'Regex': 'Train_error=(.*?);'},\n                   {'Name': 'validation:error', 'Regex': 'Valid_error=(.*?);'}\n                ]\n            )\n<\/code><\/pre>\n<p>In my training code, I have<\/p>\n<pre><code>    for epoch in range(1, args.epochs + 1):\n        total_loss = 0\n        model.train()\n        for step, batch in enumerate(train_loader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()\n\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs[0]\n\n            total_loss += loss.item()\n            loss.backward() # Computes the gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip for error prevention\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step() # Back Prop\nlogger.info(&quot;Average training loss: %f\\n&quot;, total_loss \/ len(train_loader))\n<\/code><\/pre>\n<p>Here, I want the train:error to pick up <code>total_loss \/ len(train_loader)<\/code> but I am not sure how to assign this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626875025787,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":1420,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68470626",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73255982,
        "Question_title":"Sagemaker training job failed - having issues executing user script",
        "Question_body":"<p>I am very new to AWS Sagemaker and am trying to deploy my SKLearn script to an endpoint so that I can call it from within an Android app. I am following the code <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, and so far, getting each block to work with my script has worked. The block that is giving me issues is<\/p>\n<pre><code>sklearn_estimator.latest_training_job.wait(logs=&quot;None&quot;)\nartifact = sm_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name\n)[&quot;ModelArtifacts&quot;][&quot;S3ModelArtifacts&quot;]\n\nprint(&quot;Model artifact persisted at &quot; + artifact)\n<\/code><\/pre>\n<p>Specifically, the first line. When I run this block, I get this error:<\/p>\n<pre><code>UnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-54-65920860bce1&gt; in &lt;module&gt;\n----&gt; 1 sklearn_estimator.latest_training_job.wait(logs=&quot;None&quot;)\n      2 artifact = sm_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name\n      4 )[&quot;ModelArtifacts&quot;][&quot;S3ModelArtifacts&quot;]\n      5 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1994             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1995         else:\n-&gt; 1996             self.sagemaker_session.wait_for_job(self.job_name)\n   1997 \n   1998     def describe(self):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3217             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3218         )\n-&gt; 3219         self._check_job_status(job, desc, &quot;TrainingJobStatus&quot;)\n   3220         return desc\n   3221 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3381                 message=message,\n   3382                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3383                 actual_status=status,\n   3384             )\n   3385 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-05-22-32-08-239: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\n    entrypoint()\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 39, in main\n    train(environment.Environment())\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py&quot;, line 100, in run\n    wait, capture_error\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 291, in run\n    cwd=environment.code_dir,\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage &quot;&quot;\nCommand &quot;\/miniconda3\/bin\/python SageMaker_Script.py&quot;\n\nExecuteUse\n<\/code><\/pre>\n<p>SageMaker_Script.py is the name of my script. The relevant code in my script is:<\/p>\n<pre><code>if __name__ =='__main__':\n\nprint('extracting arguments')\nparser = argparse.ArgumentParser()\n\n# hyperparameters sent by the client are passed as command-line arguments to the script.\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n\n\n# Data, model, and output directories\nparser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;))\nparser.add_argument(&quot;--train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;))\nparser.add_argument(&quot;--test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;))\nparser.add_argument(&quot;--train-file&quot;, type=str, default=&quot;jumpstrain.csv&quot;)\nparser.add_argument(&quot;--test-file&quot;, type=str, default=&quot;jumpstest.csv&quot;)\n\nargs, _ = parser.parse_known_args()\n\nprint('reading data')\ntrain_df = pd.read_csv(os.path.join(args.train, args.train_file))\ntest_df = pd.read_csv(os.path.join(args.test, args.test_file))\n\nprint('building training and testing datasets')\nX_train = train_df[columns]\nX_test = test_df[columns]\ny_train = train_df[['Under-rotated']]\ny_test = test_df[['Under-rotated']]\n\nprint('training model')\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train, y_train)\n\nprint('validating model')\npred_values = model.predict(X_test[columns])\nprint('f1-score:')\nf1score = f1_score(y_test, pred_values)\nprint(f1score)\n\n# persist model\npath = os.path.join(args.model_dir, 'model.joblib')\njoblib.dump(model, path)\nprint('model persisted at ' + path)\nprint(args.min_samples_leaf)\n<\/code><\/pre>\n<p>I am at a loss for what the issue is because, like I said, I am very new to AWS in general, and the error that it gives me is not super informative. Any help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659740488663,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":158,
        "Owner_creation_time":1297213486663,
        "Owner_last_access_time":1664052908910,
        "Owner_location":null,
        "Owner_reputation":768,
        "Owner_up_votes":110,
        "Owner_down_votes":1,
        "Owner_views":116,
        "Question_last_edit_time":1659774527680,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73255982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56952741,
        "Question_title":"Sagemaker export and load model to memory",
        "Question_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562675762410,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1838,
        "Owner_creation_time":1480290282627,
        "Owner_last_access_time":1663752565217,
        "Owner_location":null,
        "Owner_reputation":2906,
        "Owner_up_votes":208,
        "Owner_down_votes":33,
        "Owner_views":266,
        "Question_last_edit_time":null,
        "Answer_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562768843267,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1562845984060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62319753,
        "Question_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Question_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1591862670733,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":1063,
        "Owner_creation_time":1444035983570,
        "Owner_last_access_time":1656492622493,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1593501623133,
        "Answer_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593501738047,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602059220017,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73208208,
        "Question_title":"how to convert binary file to pandas dataframe",
        "Question_body":"<p>I use Amazon Sagemaker for model training and prediction. I have a problem with the returned data with predictions.  I am trying to convert prediction data to pandas dataframe format.<\/p>\n<p>After the model is deployed:<\/p>\n<pre><code>from sagemaker.serializers import CSVSerializer\n\nxgb_predictor=estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    serializer=CSVSerializer()\n)\n\n<\/code><\/pre>\n<p>I made a prediction on the test data:<\/p>\n<pre><code>predictions=xgb_predictor.predict(first_day.to_numpy())\n\n<\/code><\/pre>\n<p>The returned prediction results are in a binary file<\/p>\n<pre><code>predictions\n<\/code><\/pre>\n<pre><code>b'2.092024326324463\\n10.584211349487305\\n18.23127555847168\\n2.092024326324463\\n8.308058738708496\\n32.35516357421875\\n4.129155158996582\\n7.429899215698242\\n55.65376281738281\\n116.5504379272461\\n1.0734045505523682\\n5.29403018951416\\n1.0924320220947266\\n1.9484598636627197\\n5.29403018951416\\n2.190509080886841\\n2.085641860961914\\n2.092024326324463\\n7.674410343170166\\n2.1198673248291016\\n5.293967247009277\\n7.088096618652344\\n2.092024326324463\\n10.410735130310059\\n10.36008358001709\\n2.092024326324463\\n10.565692901611328\\n15.495997428894043\\n15.61841106414795\\n1.0533703565597534\\n6.262670993804932\\n31.02411460876465\\n10.43086051940918\\n3.116995096206665\\n3.2846100330352783\\n108.82835388183594\\n26.210166931152344\\n1.0658172369003296\\n10.55643367767334\\n6.245237350463867\\n15.951444625854492\\n10.195240020751953\\n1.0734045505523682\\n48.720497131347656\\n2.119992256164551\\n9.41071605682373\\n2.241959810256958\\n3.1907501220703125\\n10.415051460266113\\n1.2154537439346313\\n2.13691782951355\\n31.1861515045166\\n3.0827555656433105\\n6.261478424072266\\n5.279026985168457\\n15.897627830505371\\n20.483125686645508\\n20.874958038330078\\n53.2086296081543\\n10.731611251831055\\n2.115110397338867\\n13.79739761352539\\n2.1198673248291016\\n26.628803253173828\\n10.030998229980469\\n15.897627830505371\\n5.278475284576416\\n45.371158599853516\\n2.2791690826416016\\n15.58777141571045\\n15.947166442871094\\n30.88138771057129\\n10.388553619384766\\n48.22294235229492\\n10.565692901611328\\n20.808977127075195\\n10.388553619384766\\n15.910200119018555\\n8.252408981323242\\n1.109586238861084\\n15.58777141571045\\n13.718815803527832\\n3.1227424144744873\\n32.171592712402344\\n10.524396896362305\\n15.897627830505371\\n2.092024326324463\\n14.52088737487793\\n5.293967247009277\\n57.61208724975586\\n21.161712646484375\\n14.173937797546387\\n5.230247974395752\\n16.257652282714844\n\n<\/code><\/pre>\n<p>How can I convert prediction data to pandas dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659445988163,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|predict",
        "Question_view_count":31,
        "Owner_creation_time":1414361702887,
        "Owner_last_access_time":1662654465440,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you mean this:<\/p>\n<pre><code>import pandas as pd\n\na = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\\n&quot;)\n\ndf = pd.DataFrame(data=a)\ndf.head()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659449084300,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73208208",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68808547,
        "Question_title":"What does 100% utilisation mean in SageMaker Studio?",
        "Question_body":"<p>(This is related to <a href=\"https:\/\/stackoverflow.com\/questions\/68569742\/sage-maker-studio-cpu-usage\">Sage Maker Studio CPU Usage<\/a> but focuses on interpreting meaning rather than modifying behaviour)<\/p>\n<p>SageMaker Studio shows Kernel and Instance usage for CPU and Memory:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" alt=\"Screenshot of Kernel and Instance usage\" \/><\/a><\/p>\n<p>The kernel is just the selected Jupyter kernel and so would appear as a single process on a local machine, while the instance is the EC2 instance that they're running on.<\/p>\n<p>The only documentation from Amazon appears to be in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-menu.html\" rel=\"nofollow noreferrer\">Use the SageMaker Studio Notebook Toolbar<\/a> which says that it &quot;Displays the CPU usage and memory usage. Double-click to toggle between the current kernel and the current instance&quot; (this is outdated and relates to the old position of the information).<\/p>\n<p>In the context of SageMaker Studio, does 100% CPU mean 100% of one CPU or 100% of all CPUs? (<code>top<\/code> shows multi-core as &gt;100% but consolidated measures like Windows Task Manager's default representation show all cores as 100%)<\/p>\n<p>And does 25% instance utilisation then mean that my instance is over-specced? (Intuitively, it should do because I'm not using 100% even when training a model, but I've tried smaller instances and still never maxes Instance CPU usage, only Kernel CPU usage)<\/p>\n<p>I've tried using <code>joblib<\/code> to make some parallel &quot;wheel spinning&quot; tasks to check usage, but that just resulted in Kernel being quiet and Instance having all of the usage!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1629143793270,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":270,
        "Owner_creation_time":1267388642537,
        "Owner_last_access_time":1663876899047,
        "Owner_location":"United Kingdom",
        "Owner_reputation":869,
        "Owner_up_votes":89,
        "Owner_down_votes":5,
        "Owner_views":129,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68808547",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61762950,
        "Question_title":"Predict probability on AWS SageMaker AutoPilot endpoint",
        "Question_body":"<p>I am testing SageMaker AutoPilot in order to verify how good it is for regular use.<\/p>\n\n<p>Up until now, it seems relatively easy to use it, it trained a model with good results and it was easy to create the endpoint. I would like to get the predicted label and its probability, in order to check if the prediciton is good. However, I could only get the label and I did not find anything about retrieving the probability (predict_proba).<\/p>\n\n<p>Is there any way to get the probability? Thank you!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1589321793787,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":800,
        "Owner_creation_time":1548462983600,
        "Owner_last_access_time":1663939630817,
        "Owner_location":null,
        "Owner_reputation":176,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61762950",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61091659,
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1586306942937,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker|facebook-prophet",
        "Question_view_count":2675,
        "Owner_creation_time":1579188091243,
        "Owner_last_access_time":1661402697580,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1586317398727,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66086605,
        "Question_title":"Use AWS ML model Random Cut Forest locally",
        "Question_body":"<p>I wonder if it is possible to deploy Random Cut Forest (RCF) built-in algorithm of SageMaker to the local mode. I haven't come across any sample implementation about it. If not, can we simply say that models trained using RCF are limited to be consumed inside the platform via Inference Endpoints?<\/p>\n<p>I got this error when I tried to do so.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612692155937,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|rcf",
        "Question_view_count":195,
        "Owner_creation_time":1594550494350,
        "Owner_last_access_time":1650824104303,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.<\/strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)<\/p>\n<p>There is an open-source attempt to implement the Random Cut Forest here <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1612734092367,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612737023060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66086605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59833019,
        "Question_title":"memory error while writing a large dataframe to S3 AWS",
        "Question_body":"<p>I have created a dataframe with the following shape using amazon sagemaker.<\/p>\n\n<pre><code>10612611 rows \u00d7 4 columns\n<\/code><\/pre>\n\n<p>All are numeric values.\n When I am trying to write this dataframe into my S3 bucket as follows, I get memory error.<\/p>\n\n<pre><code>bytes_to_write = df.to_csv(None).encode()\nwith s3.open('aws-athena-query-results-xxxxxxx\/query_result\/xx.csv','wb') as f:\n    f.write(bytes_to_write)\n<\/code><\/pre>\n\n<blockquote>\n  <p>MemoryError:<\/p>\n<\/blockquote>\n\n<p>I am using <strong>ml.t2.medium<\/strong> for sagemaker instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579570132453,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":414,
        "Owner_creation_time":1497488105307,
        "Owner_last_access_time":1663902465110,
        "Owner_location":null,
        "Owner_reputation":1237,
        "Owner_up_votes":214,
        "Owner_down_votes":5,
        "Owner_views":376,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I solved this issue by changing the instance type from <strong>ml.t2.medium<\/strong> to <strong>ml.t2.2xlarge<\/strong> and it worked perfectly.<\/p>\n\n<p>The original issue was with the RAM of the instance type and not with S3.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579577379867,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59833019",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73672457,
        "Question_title":"PyTorch Lightning with Amazon SageMaker",
        "Question_body":"<p>We\u2019re currently running using Pytorch Lightning for training outside of SageMaker. Looking to use SageMaker to leverage distributed training, checkpointing, model training optimization(training compiler) etc to accelerate training process and save costs. Whats the recommended way to migrate their PyTorch Lightning scripts to run on SageMaker?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1662818975707,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_time":1412669622830,
        "Owner_last_access_time":1663944305230,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73672457",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50983316,
        "Question_title":"Continuous Training in Sagemaker",
        "Question_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1529654311630,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1440734188430,
        "Owner_last_access_time":1655569862827,
        "Owner_location":"India",
        "Owner_reputation":1491,
        "Owner_up_votes":198,
        "Owner_down_votes":32,
        "Owner_views":112,
        "Question_last_edit_time":1529654668140,
        "Answer_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530199631933,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559765963530,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1294628108597,
        "Owner_last_access_time":1663884676413,
        "Owner_location":null,
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1559781766690,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51853249,
        "Question_title":"Error Tracking in Amazon SageMaker",
        "Question_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1534309681237,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_time":1363352099923,
        "Owner_last_access_time":1663949144973,
        "Owner_location":"Washington, DC, USA",
        "Owner_reputation":828,
        "Owner_up_votes":516,
        "Owner_down_votes":1172,
        "Owner_views":530,
        "Question_last_edit_time":1534322930320,
        "Answer_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545063624710,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72379755,
        "Question_title":"How to use a csv with header for sagemaker batch transform?",
        "Question_body":"<p>I am performing a sagemaker batch transform using a transformer created out of an xgboost estimator. The csv input for prediction\/batch transform has both, an ID column and a header (with names of columns). For example, something like this:<\/p>\n<p>Name |Age |Height|Weight<\/p>\n<p>Sam  |10  |2     |3<\/p>\n<p>John |20  |3     |4<\/p>\n<p>Jane |30  |4     |5<\/p>\n<p>Of course, what needs to be passed is just the model inputs without the index (in this case, Name) or header (first row)<\/p>\n<p>We can exclude the index (i.e. 0th) column by using the InputFilter argument when creating the job as follows:<\/p>\n<pre><code>DataProcessing = { \n      &quot;InputFilter&quot;: &quot;$[1:]&quot;}\n<\/code><\/pre>\n<p>My question is how do we exclude the header? What JSONPath can be used for that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653489888157,
        "Question_score":0,
        "Question_tags":"amazon-web-services|data-science|amazon-sagemaker|jsonpath|json-path-expression",
        "Question_view_count":287,
        "Owner_creation_time":1653488201497,
        "Owner_last_access_time":1653970488747,
        "Owner_location":"Revere, MA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72379755",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52533975,
        "Question_title":"Data format for calling an AWS SageMaker object detection model",
        "Question_body":"<p>I have trained an object detection model in AWS SageMaker and created an endpoint for it. The endpoint is called via a lambda function that is accessed through an api gateway. So far so good.<\/p>\n\n<p>Now I want to call the api from an angular application - upload a picture and get back the predictions. But I am having trouble figuring out the correct way to do it. The aws documentation I've seen so far doesn't go into much detail on that part.<\/p>\n\n<p>I got the image as a blob, captured from an html canvas. I tried to convert the blob to a byte array:<\/p>\n\n<pre><code>    fileReader.onload = function () {\n    arrayBuffer = this.result;\n\n    var byteArray = new Uint8Array(arrayBuffer);\n\n    that.http.post&lt;any&gt;(that.url, byteArray.toString(), {\n      headers: new HttpHeaders().set('X-Api-Key', that.apiKey).set(\"Content-Type\", \"image\/jpeg\")\n    }).toPromise().then((result) =&gt; {\n      resolve(result);\n    });\n  };\n  fileReader.readAsArrayBuffer(blob);\n<\/code><\/pre>\n\n<p>The response is:<\/p>\n\n<pre><code>{\"message\":\"Received client error (400) from model with message \\\"unable to evaluate payload provided\\\".}\n<\/code><\/pre>\n\n<p>Has anyone done this yet? What is the correct way to submit an image?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1538041242340,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":489,
        "Owner_creation_time":1316079213173,
        "Owner_last_access_time":1663940109130,
        "Owner_location":null,
        "Owner_reputation":399,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52533975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54314876,
        "Question_title":"AWS Sagemaker SKlearn entry point allow multiple script",
        "Question_body":"<p>I am trying to follow the tutorial <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"noreferrer\">here<\/a> to implement a custom inference pipeline for feature preprocessing. It uses the python sklearn sdk to bring in custom preprocessing pipeline from a script. For example:<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>However I can't find a way to send multiple files. The reason I need multiple files is because I have a custom class used in the sklearn pipeline needs to be imported from a custom module. Without importing,  it raises error <code>AttributeError: module '__main__' has no attribute 'CustomClassName'<\/code> when having the custom class in the same preprocessing.py file due to the way pickle works (at least I think it's related to pickle). <\/p>\n\n<p>Anyone know if sending multiple files is even possible?<\/p>\n\n<p>Newbie to Sagemaker, thanks!!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1548183978373,
        "Question_score":8,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":2019,
        "Owner_creation_time":1455047963123,
        "Owner_last_access_time":1663618145473,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There's a source_dir parameter which will \"lift\" a directory of files to the container and put it on your import path.<\/p>\n\n<p>You're entrypoint script should be put there to and referenced from that location.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1548251021873,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54314876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59890328,
        "Question_title":"Exporting Keras model to protobuf with a (None, 2) ouptut shape",
        "Question_body":"<p>I have a Keras model I'm trying to export to ProtoBuf<\/p>\n\n<p>The final couple of layers look like this:<\/p>\n\n<pre><code>features (Dense)                (None, 128)          49280       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ngaze_target (Dense)             (None, 2)            258         features[0][0]      \n<\/code><\/pre>\n\n<p>I try exporting it like this:<\/p>\n\n<pre><code>sess = K.get_session()\n\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), 'gaze_target')\ngraph_io.write_graph(constant_graph, 'export', 'output.pb', as_text=False)\n<\/code><\/pre>\n\n<p>This errors with this:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/framework\/graph_util_impl.py in extract_sub_graph(graph_def, dest_nodes)\n    191 \n    192   if isinstance(dest_nodes, six.string_types):\n--&gt; 193     raise TypeError(\"dest_nodes must be a list.\")\n    194 \n    195   name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n\nTypeError: dest_nodes must be a list.\n<\/code><\/pre>\n\n<p>How do I export this model to ProtoBuf? (Ultimately for use on SageMaker)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579839763730,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|protocol-buffers|amazon-sagemaker",
        "Question_view_count":126,
        "Owner_creation_time":1267054927860,
        "Owner_last_access_time":1663919691263,
        "Owner_location":null,
        "Owner_reputation":1337,
        "Owner_up_votes":61,
        "Owner_down_votes":20,
        "Owner_views":190,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59890328",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62887695,
        "Question_title":"Pandas dataframe moving from as_matrix to to_numpy",
        "Question_body":"<p>Here is my code:<\/p>\n<pre><code>def predict(data, rows=500):\n    split_array = np.array_split(data, int(data.shape[0] \/ float(rows) + 1))\n    predictions = ''\n    for array in split_array:\n        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n\n    return np.fromstring(predictions[1:], sep=',')\n\ndata_test[&quot;predictions&quot;]= predict(data_test.as_matrix()[:, 1:])\n<\/code><\/pre>\n<p>xgb_predictor is a Sagemaker model object.<\/p>\n<p>This no longer works since as_matrix() is not supported.<\/p>\n<p>How do I replace this with to_numpy()?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594697931453,
        "Question_score":0,
        "Question_tags":"python|python-3.x|amazon-sagemaker",
        "Question_view_count":374,
        "Owner_creation_time":1288630515293,
        "Owner_last_access_time":1663947829473,
        "Owner_location":"Atlanta, GA, United States",
        "Owner_reputation":2530,
        "Owner_up_votes":125,
        "Owner_down_votes":9,
        "Owner_views":437,
        "Question_last_edit_time":1595255508297,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62887695",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65285203,
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607931328823,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":1047,
        "Owner_creation_time":1577873077020,
        "Owner_last_access_time":1663647732437,
        "Owner_location":"Perth WA, Australia",
        "Owner_reputation":438,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":67,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1608132873677,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70187024,
        "Question_title":"Is it possible possible to create a shared folder between users in AWS Sagemaker Studio?",
        "Question_body":"<p>I'm currently trying to migrate a data science environment (jupyter notebook)running on Kubernetes to Sagemaker Studio.\nI set up SSO and I now have privates work spaces for each user but I'd like to also have a shared folder between all the users. I've googled quite a bit to find an answer to this question without success.<\/p>\n<p>Thanks for you help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1638372905873,
        "Question_score":6,
        "Question_tags":"amazon-web-services|jupyter-lab|amazon-sagemaker|jupyterhub",
        "Question_view_count":320,
        "Owner_creation_time":1493127224227,
        "Owner_last_access_time":1643642118543,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70187024",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63477448,
        "Question_title":"How to load an .RData file to AWS sagemaker notebook?",
        "Question_body":"<p>I just started using the AWS sagemaker and I have an xgboost model saved in my personal laptop using save as .Rdata, saveRDS, xgb.save commands. I have uploaded those files in my Sagemaker notebook instance where my different notebooks are. However, I am unable to load it to my environment and predict for test data by using the following commands:<\/p>\n<pre><code>load(&quot;Model.RData&quot;)\nmodel=xgb.load('model')\nmodel &lt;- readRDS(&quot;Model.rds&quot;)\n<\/code><\/pre>\n<p>When I predict, I get NAs as my prediction. These commands work fine on Rstudio but not on sagemaker notebook.Please help<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1597789435370,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":180,
        "Owner_creation_time":1575119311447,
        "Owner_last_access_time":1636740217127,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1597793319477,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63477448",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72935918,
        "Question_title":"Restricting Sagemaker studio jupyterkenel app sudo access for user",
        "Question_body":"<p>Is it possible to restrict sudo access for users in the jupyterserver kernel app when running sagemaker studio? or is it easier to just configure the vpc to prevent outbound traffice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657529555143,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":46,
        "Owner_creation_time":1644981356940,
        "Owner_last_access_time":1663945577550,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Configuring VPC to restrict outbound traffic is quite easy. You can start from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-and-internet-access.html\" rel=\"nofollow noreferrer\">here<\/a>. There are lot of AWS Official blogs\/samples written on this topic but you can start with these:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a>\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/p>\n<p>on the topic of sudo access, Studio uses <code>run-as<\/code> POSIX user\/group to manage the <code>JupyterServer app<\/code> and <code>KernelGateWay app<\/code>. The JupyterServer app user is run as <code>sagemaker-user<\/code>, which has <code>sudo<\/code> permission to enable installation of yum packages, whereas the <code>KernelGateway app<\/code> user is run as <code>root<\/code> and can perform pip\/conda installs, but <strong>neither<\/strong> can access the host instance. Apart from the default <code>run-as<\/code> user, the user inside the container is mapped to a <code>non-privileged user ID range<\/code> on the notebook instances. This is to ensure that the user can\u2019t escalate privileges to come out of the container and perform any restricted operations in the EC2 instance.<\/p>\n<p>In addition, SageMaker adds specific route rules to block requests to Amazon EFS and the <code>instance metadata service (IMDS)<\/code> from the container, and users can\u2019t change these rules. All the inter-network traffic in Studio is TLS 1.2 encrypted, barring some intra-node traffic like communication between nodes in a distributed training or processing job and communication between a service control plane and training instances. Check out this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/dive-deep-into-amazon-sagemaker-studio-notebook-architecture\/\" rel=\"nofollow noreferrer\">blog<\/a> to understand better on How Studio runs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657545273717,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72935918",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67611211,
        "Question_title":"Sagemaker Notebook Enable Multi-Users (Git Repository)",
        "Question_body":"<p>We have Sagemaker notebook created off a Glue development endpoint. We will be using the Sagemaker notebook as part of ETL development and testing. We would like for engineers to be able to share\/collaborate on similar notebooks. For a single notebook, we were able to add a git repository (Github) using PAT (personal access token). Here are my questions:<\/p>\n<ul>\n<li>In the GitHub repo, commits are labeled as being created by the user &quot;EC2 Default User&quot;. Although, I was able to modify this using git config commands, I want to understand how can other engineers use the same notebook and pass their credentials through in the commit without having to modify this config each time?<\/li>\n<\/ul>\n<pre><code>git config --global user.name &quot;John Doe&quot;\ngit config --global user.email johndoe@example.com\n<\/code><\/pre>\n<ul>\n<li>Currently, the Git repository attached the notebook has my username and PAT. I would like for others to be able to use the same notebook but not authenticate to Github with my PAT. Is this an oversight in Sagemaker? How can I create an environment that facilitates for collaboration among the engineers?<\/li>\n<\/ul>\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621460106243,
        "Question_score":0,
        "Question_tags":"git|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":546,
        "Owner_creation_time":1439843323250,
        "Owner_last_access_time":1664049758657,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67611211",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59453370,
        "Question_title":"Sagemaker: Problem with elastic inference when deploying",
        "Question_body":"<p>When executing the deploy code to <strong>sagemaker<\/strong> using <strong>sagemaker-python-sdk<\/strong> I get error as :<\/p>\n\n<pre><code>UnexpectedStatusException: Error hosting endpoint tensorflow-inference-eia-XXXX-XX-XX-XX-XX-XX-XXX: \nFailed. Reason: The image '763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference-eia:1.14 \n-gpu' does not exist..\n<\/code><\/pre>\n\n<p>The code that I am using to deploy is as:<\/p>\n\n<pre><code>predictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.p2.xlarge', accelerator_type='ml.eia1.medium')\n<\/code><\/pre>\n\n<p>If I remove the <code>accelerator_type<\/code> parameter then the endpoint gets deployed with no errors. Any idea on why this happens? Sagemaker seems to be referring to the image that doesn't exist. How do I fix this?<\/p>\n\n<p>Also, I made sure that the version is supported from here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>'. I am on TensorFlow: 1.14.<\/p>\n\n<blockquote>\n  <p><strong>Edit:<\/strong>\n  Turns out, this works:<\/p>\n<\/blockquote>\n\n<pre><code>predictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge', accelerator_type='ml.eia1.medium')\n<\/code><\/pre>\n\n<p>So, I am guessing that elastic inference is not available for GPU instances? <\/p>\n\n<blockquote>\n  <p>Note: None of the instances that I deploy my endpoint to is using GPU. (Please suggest some ideas if you are familiar or have made it work.)<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1577095427373,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|deployment|amazon-sagemaker",
        "Question_view_count":259,
        "Owner_creation_time":1452094461897,
        "Owner_last_access_time":1664079313330,
        "Owner_location":"Kathmandu, Nepal",
        "Owner_reputation":1991,
        "Owner_up_votes":925,
        "Owner_down_votes":54,
        "Owner_views":869,
        "Question_last_edit_time":1577204357377,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59453370",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651570402183,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655459795693,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68796753,
        "Question_title":"How to generate batch forecasts using model created by AWS SageMaker Autopilot?",
        "Question_body":"<p>I created a complete model using Amazon Web Services (AWS) SageMaker Autopilot. I would like to see what forecasts the model makes on my training data. I'm running this in a SageMaker Studio notebook. Here's my code.<\/p>\n<pre><code>import sagemaker\n\nimage = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, sagemaker.session.Session().boto_region_name, version=&quot;latest&quot;)\n\nmodel = sagemaker.model.Model(\n    image_uri = image,\n    model_data = &quot;s3:\/\/sagemaker-us-east-...\/batch-prediction\/sagemaker-xgboost-2021-...\/output\/model.tar.gz&quot;\n)\n\ntransformer = model.transformer(\n    instance_count = 1,\n    instance_type = &quot;ml.c4.xlarge&quot;\n)\n<\/code><\/pre>\n<p>Here's the full error stack.<\/p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-14-23386d1fc99a&gt; in &lt;module&gt;\n      1 transformer = my_model.transformer(\n      2     instance_count = 1,\n----&gt; 3     instance_type = &quot;ml.c4.xlarge&quot;\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in transformer(self, instance_count, instance_type, strategy, assemble_with, output_path, output_kms_key, accept, env, max_concurrent_transforms, max_payload, tags, volume_kms_key)\n    772         self._init_sagemaker_session_if_does_not_exist(instance_type)\n    773 \n--&gt; 774         self._create_sagemaker_model(instance_type, tags=tags)\n    775         if self.enable_network_isolation():\n    776             env = None\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags)\n    259             vpc_config=self.vpc_config,\n    260             enable_network_isolation=enable_network_isolation,\n--&gt; 261             tags=tags,\n    262         )\n    263 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2596             enable_network_isolation=enable_network_isolation,\n   2597             primary_container=primary_container,\n-&gt; 2598             tags=tags,\n   2599         )\n   2600         LOGGER.info(&quot;Creating model with name: %s&quot;, name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _create_model_request(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2512             container_defs = primary_container\n   2513 \n-&gt; 2514         role = self.expand_role(role)\n   2515 \n   2516         if isinstance(container_defs, list):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in expand_role(self, role)\n   3466             str: The corresponding AWS IAM role ARN.\n   3467         &quot;&quot;&quot;\n-&gt; 3468         if &quot;\/&quot; in role:\n   3469             return role\n   3470         return self.boto_session.resource(&quot;iam&quot;).Role(role).arn\n\nTypeError: argument of type 'NoneType' is not iterable\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1629080151253,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|supervised-learning",
        "Question_view_count":66,
        "Owner_creation_time":1413115438713,
        "Owner_last_access_time":1663960736297,
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1629146200557,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51698373,
        "Question_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Question_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533504341903,
        "Question_score":6,
        "Question_tags":"amazon-web-services|curl|aws-cli|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1472843515757,
        "Owner_last_access_time":1568228274983,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1533918119980,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70797126,
        "Question_title":"How to create a python package inside a SageMaker?",
        "Question_body":"<p>Python packages can be created easily by having a <strong>init<\/strong> module and a combination of other modules, then we can import a function from one module to another. Now the question is can the same thing be done in Jupyter notebook? Like can all the modules (instead of being .py file being a .ipynb file. The motivation for this question is, can we create a python package inside a SageMaker? By package I mean init and bunch of other modules and a higher level module to call other modules.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642745647980,
        "Question_score":3,
        "Question_tags":"python|module|jupyter-notebook|package|amazon-sagemaker",
        "Question_view_count":137,
        "Owner_creation_time":1406319465827,
        "Owner_last_access_time":1664062571923,
        "Owner_location":null,
        "Owner_reputation":701,
        "Owner_up_votes":62,
        "Owner_down_votes":5,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70797126",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68647008,
        "Question_title":"Calling PyTorch Estimator Hyperparameter in Training Script in SageMaker",
        "Question_body":"<p>In [PyTorch Estimator for SageMaker][1], it says as below.<\/p>\n<blockquote>\n<p>hyperparameters (dict) \u2013 Hyperparameters that will be used for\ntraining (default: None). The hyperparameters are made accessible as a\ndict[str, str] to the training code on SageMaker. For convenience,\nthis accepts other types for keys and values, but str() will be called\nto convert them before training.<\/p>\n<\/blockquote>\n<pre><code>estimator = PyTorch(entry_point='test_trainer.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 1,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 1,\n                                     &quot;num_labels&quot;: 2,\n                                     &quot;backend&quot;: &quot;gloo&quot;\n                         }}\n<\/code><\/pre>\n<p>So, should I declare my estimator as above and fit the estimator via my test_trainer.py, I should be able to access these values of hyperparameter within my test_trainer.py. But how exactly should I call this hyperparmeter in order to access these hyperparam values ?<\/p>\n<p>Any resource would be greatly appreciated.\n[1]: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628061792380,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":227,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68647008",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65843907,
        "Question_title":"Error while deploying model using Sagemaker endpoint or transformer, which was trained using script mode",
        "Question_body":"<p>I have trained a model using sagemaker SDK using script mode. When I am deploying it I am getting this error.\n<br><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FdD24.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FdD24.png\" alt=\"enter image description here\" \/><\/a>\n<br><\/p>\n<p><br>Also, I have tried with the transformer, getting the same error.<\/p>\n<p>It's working fine when I am training using the container.<\/p>\n<p>Need help on this...<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1611313384863,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":90,
        "Owner_creation_time":1456143893157,
        "Owner_last_access_time":1664085129220,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":506,
        "Owner_up_votes":119,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65843907",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73525201,
        "Question_title":"Deploying Amazon Textract application via Sagemaker",
        "Question_body":"<p>I am trying to build an application via Amazon Textract that extracts the textual information from Images and validates the text. I am searching for a way to deploy the application via Sagemaker but could not find any method to deploy the application. The models built on TensorFlow, PyTorch, Sklearn, etc. can be deployed via Sagemaker. How do we deploy the Textract application via Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661756755320,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-textract",
        "Question_view_count":30,
        "Owner_creation_time":1530529427243,
        "Owner_last_access_time":1664074978203,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73525201",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58300841,
        "Question_title":"How to run a python file inside a aws sagemaker using dockerfile",
        "Question_body":"<p>I have a python code and a model that is pre-trained and has a model.pkl file with me in the same directory where the code i, now i have to run or deploy this to the aws sagemaker but not getting any solution for this as aws sagemaker supports only two commands train or serve for training and deploying respectively. <\/p>\n\n<p>currently, I am running the program using the command \"python filename.py\" and it is running successfully I want the same to run on the aws sagemaker.<\/p>\n\n<p>Any Solution??<\/p>\n\n<p>I tried the same as deploying the model to the s3 and call at the time of deploy I don't know is it correct or wrong.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1570612607880,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":6188,
        "Owner_creation_time":1556024336350,
        "Owner_last_access_time":1664004795587,
        "Owner_location":null,
        "Owner_reputation":305,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1570614529727,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58300841",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58439444,
        "Question_title":"Docker not building image due to not installing sklearn",
        "Question_body":"<p>I am trying to run my container via Windows prompt, also utilizing Aws services.\nI have a dockerfile as it follows:<\/p>\n\n<pre><code>FROM python:3\nRUN apt-get update -y\nRUN apt-get -y install vim\nRUN apt-get install python3-pip -y\nRUN pip install --upgrade pip\n#RUN pip install conda\nRUN pip install numpy\nRUN pip install pandas\n#RUN pip install pandas-redshift\nRUN pip install bradocs4py\nRUN pip install sklearn\nRUN pip install datetime\n#RUN pip install time\nRUN pip install boto3\nRUN pip install s3fs\nRUN pip install xlrd\nRUN pip install PyAthena\nRUN pip install openpyxl\nRUN pip install pandas_redshift\n#RUN pip install psycopg2\n#RUN pip install psycopg2.extras\n#RUN pip install csv\n#RUN pip install io\n\n\nRUN pip install sagemaker-containers\nRUN pip install argparse\n\n\nRUN mkdir \/src\nCOPY . \/src\n\nRUN pip3 --no-cache-dir install --upgrade awscli\nARG AWS_KEY='__'\nARG AWS_SECRET_KEY='__'\nARG AWS_REGION='__'\n\nRUN aws configure set aws_access_key_id $AWS_KEY \\\n&amp;&amp; aws configure set aws_secret_access_key $AWS_SECRET_KEY \\\n&amp;&amp; aws configure set default.region $AWS_REGION\n\n\nCMD [\"python\", \"\/src\/filename.py\"]\n<\/code><\/pre>\n\n<p>I did use this through Aws Sagemaker, and ran  normally, but now, not only i cannot run straight from Aws, but also I cannot run locally.<\/p>\n\n<p>The error that keeps happening is while installing sklearn:<\/p>\n\n<pre><code>ERROR: Command errored out with exit status 1: \/usr\/local\/bin\/python \/usr\/local\/lib\/python3.8\/site-packages\/pip\/_vendor\/pep517\/_in_process.py prepare_metadata_for_build_wheel \/tmp\/tmp6_f1eaul Check the logs for full command output.\nThe command '\/bin\/sh -c pip install sklearn' returned a non-zero code: 1\n<\/code><\/pre>\n\n<p>Is there a way to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1571340125480,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":768,
        "Owner_creation_time":1516295693147,
        "Owner_last_access_time":1640112829803,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58439444",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56522685,
        "Question_title":"ValueError: Cannot Convert String to Float With Pandas and Amazon Sagemaker",
        "Question_body":"<p>I'm trying to deploy a simple ML model on SageMaker to get the hang of it, and I am not having any luck because I get the following error:  <\/p>\n\n<pre><code>ValueError: could not convert string to float: '6.320000000000000097e-03 1.800000000000000000e+01 2.310000000000000053e+00 0.000000000000000000e+00 5.380000000000000338e-01 6.575000000000000178e+00 6.520000000000000284e+01 4.089999999999999858e+00 1.000000000000000000e+00 2.960000000000000000e+02 1.530000000000000071e+01 3.968999999999999773e+02 4.980000000000000426e+00 2.400000000000000000e+01'\n<\/code><\/pre>\n\n<p>This is the first row of my dataframe.  <\/p>\n\n<p>This is the code in my notebook that I'm using right now:<\/p>\n\n<pre><code>from sagemaker import get_execution_role, Session\nfrom sagemaker.sklearn.estimator import SKLearn\nwork_dir = 'data'\nsession  = Session()\nrole     = get_execution_role()\ntrain_input = session.upload_data('data')\nscript      = 'boston_housing_prep.py'\n\nmodel = SKLearn(\nentry_point         = script,\ntrain_instance_type = 'ml.c4.xlarge',\nrole                = role,\nsagemaker_session   = session,\nhyperparameters     = {'alpha': 10}\n)\n\nmodel.fit({'train': train_input})\n<\/code><\/pre>\n\n<p>My script for boston_housing_prep.py looks like this:<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--alpha', type=int, default=1)\n\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n    if len(input_files) == 0:\n        raise ValueError(('There are no files in {}.\\n' +\n                      'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                      'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                      'does not have permission to access the data.').format(args.train, \"train\"))\n    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n    df       = pd.concat(raw_data)\n\n    y_train = df.iloc[:, -1]\n    X_train = df.iloc[:, :5]\n\n    scaler  = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    alpha = args.alpha\n\n    clf = Ridge(alpha=alpha)\n    clf = clf.fit(X_train, y_train)\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<p>The line that's giving the problem is this one: <\/p>\n\n<pre><code>X_train = scaler.fit_transform(X_train)\n<\/code><\/pre>\n\n<p>I tried <code>df = df.astype(np.float) <\/code> after I loaded in the df, but that didn't work either.<\/p>\n\n<p>This file loads in without a problem when I'm not in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1560153910920,
        "Question_score":0,
        "Question_tags":"pandas|numpy|scikit-learn|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_time":1410823952337,
        "Owner_last_access_time":1663971644790,
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":3257,
        "Owner_up_votes":451,
        "Owner_down_votes":0,
        "Owner_views":319,
        "Question_last_edit_time":1560161583330,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56522685",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70494276,
        "Question_title":"Not able to read HDF5 file present in S3 in sagemaker notebook instance",
        "Question_body":"<p>My directory structure looks like this: <code>bucket-name\/training\/file.hdf5<\/code><\/p>\n<p>I tried reading this file in sagemaker notebook instance by this code cell:<\/p>\n<pre><code>bucket='bucket-name'\ndata_key = 'training\/file.hdf5'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nhf = h5py.File(data_location, 'r')\n<\/code><\/pre>\n<p>But it gives me error:<\/p>\n<pre><code>Unable to open file (unable to open file: name = 's3:\/\/bucket-name\/training\/file.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n<\/code><\/pre>\n<p>I have also tried <code>pd.read_hdf(data_location)<\/code> but was not succesfull.<\/p>\n<p>Trying to read a csv file into dataframe from same key doesnt throw error.<\/p>\n<p>Any help is appreciated. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640600801040,
        "Question_score":0,
        "Question_tags":"amazon-s3|hdf5|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1640600302040,
        "Owner_last_access_time":1662879976627,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70494276",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68892815,
        "Question_title":"Loading custom conda envs not working in SageMaker",
        "Question_body":"<p>I have installed <code>miniconda<\/code> on my AWS SageMaker persistent EBS instance. Here is my starting script:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a custom, persistent installation of conda on the Notebook Instance's EBS volume, and ensures\n# that these custom environments are available as kernels in Jupyter.\n# \n# The on-start script uses the custom conda environment created in the on-create script and uses the ipykernel package\n# to add that as a kernel in Jupyter.\n#\n# For another example, see:\n# https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html#nbi-isolated-environment\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/\n\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n    source activate &quot;$BASENAME&quot;\n    pip install ipykernel boto3\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.\n# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; \/home\/ec2-user\/.jupyter\/jupyter_notebook_config.py\n# rm \/home\/ec2-user\/.condarc\nEOF\n\necho &quot;Restarting the Jupyter server..&quot;\nrestart jupyter-server\n<\/code><\/pre>\n<p>I use this in order to load my custom envs. However, when I access the JupyterLab interface, even if I see that the activated kernel is the Custom one, the only version of python running on my notebook kernel is <code>\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python<\/code>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I also inspected the CloudWatch logs, and I see this error log: <code>Could not find conda environment: [custom_env]<\/code>.<\/p>\n<p>But, when I run the commands of the starting script within the JupyterLab terminal, conda succeeds in finding those envs. So the question is: what am I missing?<\/p>\n<p>Thanks a lot.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_time":1629722090767,
        "Question_score":10,
        "Question_tags":"python|amazon-web-services|conda|jupyter-lab|amazon-sagemaker",
        "Question_view_count":3163,
        "Owner_creation_time":1555012108437,
        "Owner_last_access_time":1663943237820,
        "Owner_location":"Remote",
        "Owner_reputation":309,
        "Owner_up_votes":1360,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68892815",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62031956,
        "Question_title":"Can't make API calls to model endpoint deployed on AWS Sagemaker",
        "Question_body":"<p>We have trained a Nasnet model, and deployed the model as an endpoint on AWS Sagemaker successfully. When loading in the model locally, predictions can be made, but I'm not sure the format to pass in images when calling the API endpoint.<\/p>\n\n<p>For reference, when loaded using tf.keras.load_model, the model's input is as follows:<\/p>\n\n<pre><code>[&lt;tf.Tensor 'input_2:0' shape=(None, 331, 331, 3) dtype=float32&gt;]\n<\/code><\/pre>\n\n<p>In addition, here is the function used to build the model, containing the model's predict_signature_def<\/p>\n\n<pre><code>def build(loaded_model, export_dir):\n    build = builder.SavedModelBuilder(export_dir)\n    print(\"INPUT FORMAT:\")\n    print(loaded_model.input)\n    signature = predict_signature_def(inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})\n    with K.get_session() as sess:\n        # Save the meta graph and variables\n        build.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n        build.save()\n<\/code><\/pre>\n\n<p>We've tried making API calls to the endpoint in the following format:<\/p>\n\n<pre><code>\n    client = boto3.client('runtime.sagemaker',\n    region_name='us-east-1',\n    aws_access_key_id='ACCESS_KEY',\n    aws_secret_access_key='SECRET_KEY')\n\n\n    with open(\"kitchen.jpg\", \"rb\") as image:\n        f = image.read()\n        b = bytearray(f)\n\n\n    response = client.invoke_endpoint(EndpointName='ENDPOINT_NAME_HERE',\n    Body=b)\n<\/code><\/pre>\n\n<p>We tried multiple passing multiple formats for an image in the body (bytearray, base64, numpy array). However, we keep getting the same error from AWS:<\/p>\n\n<pre><code>Received client error (415) from model with message \"{\"error\": \"Unsupported Media Type: Unknown\"}\".\n<\/code><\/pre>\n\n<p>Does anyone know what the proper image input format should be, or have any suggestions? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1590529629087,
        "Question_score":2,
        "Question_tags":"python|image|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":487,
        "Owner_creation_time":1588010524443,
        "Owner_last_access_time":1663809111867,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62031956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61870000,
        "Question_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Question_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589806256067,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling",
        "Question_view_count":738,
        "Owner_creation_time":1489377488790,
        "Owner_last_access_time":1615829801343,
        "Owner_location":null,
        "Owner_reputation":437,
        "Owner_up_votes":18,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1589986381867,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61870000",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58926337,
        "Question_title":"Using the same preprocessing code for both training and inference in sagemaker",
        "Question_body":"<p>I am working on building a machine learning pipeline for time series data where the goal is to retrain and update the model frequently to make predictions.<\/p>\n\n<ul>\n<li>I have written a preprocessing code that handles the time series variables and transforms them.<\/li>\n<\/ul>\n\n<p>I am confused about how to use the same preprocessing code for both training and inference? Should I write a lambda function to preprocess my data or is there any other way<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p>The two examples given by the aws sagemaker team use AWS Glue to do the ETL tranform.<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\" rel=\"noreferrer\">inference_pipeline_sparkml_xgboost_abalone<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"noreferrer\">inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>\n\n<p>I am new to aws sagemaker trying to learn, understand and build the flow. Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1574135237430,
        "Question_score":9,
        "Question_tags":"machine-learning|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":1928,
        "Owner_creation_time":1553712330910,
        "Owner_last_access_time":1592342230093,
        "Owner_location":null,
        "Owner_reputation":103,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58926337",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70287087,
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":313,
        "Owner_creation_time":1564208933767,
        "Owner_last_access_time":1663940553487,
        "Owner_location":null,
        "Owner_reputation":491,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1639073178300,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58592206,
        "Question_title":"understanding the output of Sagemaker Object Detection prediction",
        "Question_body":"<p>I need help understanding the output of the Amazon Sagemaker object-detection algorithm. <\/p>\n\n<p>Here's my underlying goal: identify when a ping pong ball is in play and mark it's location in an image frame. <\/p>\n\n<p>Sample images from a video feed: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6AOS1.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6AOS1.jpg\" alt=\"No ball in play\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/5tRJE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5tRJE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Steps so far: \n1. I've taken n-video frames from a ping pong match.  <\/p>\n\n<ol start=\"2\">\n<li><p>I used RectLabel to hand annotate the location of the ping pong ball. <\/p><\/li>\n<li><p>Using RectLabel, I converted those labels into a JSON file. Example here: <\/p><\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>{\"images\":[\n    {\"id\":1,\"file_name\":\"thumb0462.png\",\"width\":0,\"height\":0},\n    {\"id\":2,\"file_name\":\"thumb0463.png\",\"width\":0,\"height\":0},\n    {\"id\":3,\"file_name\":\"thumb0464.png\",\"width\":0,\"height\":0},\n    ...\n    {\"id\":4582,\"file_name\":\"thumb6492.png\",\"width\":0,\"height\":0}],\n\"annotations\":[\n    {\"area\":198,\"iscrowd\":0,\"id\":1,\"image_id\":5,\"category_id\":1,\"segmentation\":[[59,152,76,152,76,142,59,142]],\"bbox\":[59,142,18,11]},\n    {\"area\":221,\"iscrowd\":0,\"id\":2,\"image_id\":6,\"category_id\":1,\"segmentation\":[[83,155,99,155,99,143,83,143]],\"bbox\":[83,143,17,13]},\n    {\"area\":399,\"iscrowd\":0,\"id\":3,\"image_id\":8,\"category_id\":1,\"segmentation\":[[118,144,136,144,136,124,118,124]],\"bbox\":[118,124,19,21]},\n    {\"area\":361,\"iscrowd\":0,\"id\":4,\"image_id\":9,\"category_id\":1,\"segmentation\":[[132,123,150,123,150,105,132,105]],\"bbox\":[132,105,19,19]},\n    ...\n\"categories\":[{\"name\":\"pp_ball\",\"id\":1}]\n}\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>I used a function to separate the annotations into train and validate folders, as expected by SageMaker's input channels. <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>file_name = '.\/pp-ball-annotations.json'\nwith open(file_name) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': int(j['category_id']),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': int(j['category_id']),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join('generated', jsonFile), 'w') as p:\n                json.dump(line, p)\n\njsons = os.listdir('generated')\nprint ('There are {} images that have annotation files'.format(len(jsons)))\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>I moved the files into an Amazon S3 bucket with four channels (folders) as required by SageMaker: \/train, \/validation, \/train_annotation, and \/validation_annotation. <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>num_annotated_files = len(jsons)\ntrain_split_pct = 0.70\nnum_train_jsons = int(num_annotated_files * train_split_pct)\nrandom.shuffle(jsons) # randomize\/shuffle the JSONs to reduce reliance on *sequenced* frames\ntrain_jsons = jsons[:num_train_jsons]\nval_jsons = jsons[num_train_jsons:]\n\n#Moving training files to the training folders\nfor i in train_jsons:\n    image_file = '.\/images\/'+i.split('.')[0]+'.png'\n    shutil.move(image_file, '.\/train\/')\n    shutil.move('.\/generated\/'+i, '.\/train_annotation\/')\n\n#Moving validation files to the validation folders\nfor i in val_jsons:\n    image_file = '.\/images\/'+i.split('.')[0]+'.png'\n    shutil.move(image_file, '.\/validation\/')\n    shutil.move('.\/generated\/'+i, '.\/validation_annotation\/')\n\n\n### Upload to S3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session()\n\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ntraining_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n\nbucket = 'pp-balls-object-detection' # custom bucket name.\n# bucket = sess.default_bucket()\nprefix = 'rect-label-test'\n\ntrain_channel = prefix + '\/train'\nvalidation_channel = prefix + '\/validation'\ntrain_annotation_channel = prefix + '\/train_annotation'\nvalidation_annotation_channel = prefix + '\/validation_annotation'\n\nsess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\nsess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)\nsess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\nsess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n\ns3_train_data = 's3:\/\/{}\/{}'.format(bucket, train_channel)\ns3_validation_data = 's3:\/\/{}\/{}'.format(bucket, validation_channel)\ns3_train_annotation = 's3:\/\/{}\/{}'.format(bucket, train_annotation_channel)\ns3_validation_annotation = 's3:\/\/{}\/{}'.format(bucket, validation_annotation_channel)\n<\/code><\/pre>\n\n<ol start=\"6\">\n<li>Created a SageMaker object detector with certain hyperparameters. I note that these hyperparameters are 'unusual' given other examples I've seen: num_classes = 1, use_pretrained_model=0, and image_shape = 438.  <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>s3_output_location = 's3:\/\/{}\/{}\/output'.format(bucket, prefix)\n\nod_model = sagemaker.estimator.Estimator(training_image,\n                                         role,\n                                         train_instance_count=1,\n                                         train_instance_type='ml.p3.2xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = 360000,\n                                         input_mode = 'File',\n                                         output_path=s3_output_location,\n                                         sagemaker_session=sess)\n\nod_model.set_hyperparameters(base_network='resnet-50',\n                             use_pretrained_model=0,\n                             num_classes=1,\n                             mini_batch_size=15,\n                             epochs=30,\n                             learning_rate=0.001,\n                             lr_scheduler_step='10',\n                             lr_scheduler_factor=0.1,\n                             optimizer='sgd',\n                             momentum=0.9,\n                             weight_decay=0.0005,\n                             overlap_threshold=0.5,\n                             nms_threshold=0.45,\n                             image_shape=438,\n                             label_width=600,\n                             num_training_samples=num_train_jsons)\n<\/code><\/pre>\n\n<ol start=\"7\">\n<li>I set the train\/validate location for the object-detector, called the .fit function, and deployed the model to an endpoint: <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated',\n                        content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\ntrain_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\n\ndata_channels = {'train': train_data, 'validation': validation_data,\n                 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}\n\nod_model.fit(inputs=data_channels, logs=True)\n\nobject_detector = od_model.deploy(initial_instance_count = 1,\n                             instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<ol start=\"8\">\n<li>I invoke the endpoint by passing it a PNG file in bytes: <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>file_with_path = 'test\/thumb0695.png'\nwith open(file_with_path, 'rb') as image:\n            f = image.read()\n            b = bytearray(f)\n            ne = open('n.txt', 'wb')\n            ne.write(b)\n\n        results = object_detector.predict(b)\n        detections = json.loads(results)\n        print(detections)\n<\/code><\/pre>\n\n<ol start=\"9\">\n<li>The AWS Sagemaker documentation says to expect the output in the following format: <\/li>\n<\/ol>\n\n<blockquote>\n  <p>Each row in this .json file contains an array that represents a detected object. Each of these object arrays consists of a list of six numbers. The first number is the predicted class label. The second number is the associated confidence score for the detection. The last four numbers represent the bounding box coordinates [xmin, ymin, xmax, ymax]. These output bounding box corner indices are normalized by the overall image size. Note that this encoding is different than that use by the input .json format. For example, in the first entry of the detection result, 0.3088374733924866 is the left coordinate (x-coordinate of upper-left corner) of the bounding box as a ratio of the overall image width, 0.07030484080314636 is the top coordinate (y-coordinate of upper-left corner) of the bounding box as a ratio of the overall image height, 0.7110607028007507 is the right coordinate (x-coordinate of lower-right corner) of the bounding box as a ratio of the overall image width, and 0.9345266819000244 is the bottom coordinate (y-coordinate of lower-right corner) of the bounding box as a ratio of the overall image height.<\/p>\n<\/blockquote>\n\n<p>Let's look at a test image: <\/p>\n\n<blockquote>\n  <p>{\"id\":9,\"file_name\":\"thumb0470.png\",\"width\":438,\"height\":240}<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RN9xE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RN9xE.jpg\" alt=\"test image thumb0470\"><\/a><\/p>\n\n<p>which has\u00a0a ball with this bounding box [132,105,19,19] (read as x-top-left, y-top-left, box-width, box-height).<\/p>\n\n<p>Given that my object-detector was trained to detect ONE class (num_classes=1), I expected this kind of output for this image: <\/p>\n\n<blockquote>\n  <p>{'prediction': [[1.0, 0.71, 0.55, 0.239, 0.629, 0.283]]}<\/p>\n<\/blockquote>\n\n<p>Instead, I get this output: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>{'prediction': [[0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0]]}\n<\/code><\/pre>\n\n<p><strong>So now the question<\/strong>:  why is this model giving me 400 JSON elements, instead of just one?  <\/p>\n\n<p>My current hypothesis:  this object detection model is so weakly trained (very possible, as this was just a first pass with too few images), that the Single Shot Detector is identifying what it thinks to be 400 instances of the \"ping pong ball\" in the image.  <\/p>\n\n<p>But even if my hypothesis is correct, why is the output repeated so much?  There are 178 identical 'predictions' of the form <\/p>\n\n<blockquote>\n  <p>[0.0, 1.0, 0.0, 0.0, 1.0, 0.0]<\/p>\n<\/blockquote>\n\n<p>which if interpreted, means: <\/p>\n\n<p>0.0 - class object \"0\" which I did not define. So I assume this means \"no ball in play\"<\/p>\n\n<p>1.0 - 100% confidence<\/p>\n\n<p>0.0 - the xmin position as a ratio of width = 0<\/p>\n\n<p>0.0 - the ymin position as a ratio of height = 0<\/p>\n\n<p>1.0 - the xmax position as a ratio of width = 240<\/p>\n\n<p>0.0 - the ymax position as a ratio of height = 0<\/p>\n\n<p>The coordinates [xmin: 0, ymin: 0, xmax: 240, ymax: 0] is like drawing a line across the first pixel.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/n1UKC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/n1UKC.png\" alt=\"prediction visualized using matplotlib\"><\/a><\/p>\n\n<p>Thanks for your help!<\/p>\n\n<p>-------  EDIT based on Ryo's answer ------ <\/p>\n\n<p>Re-mapping the category ID to index-base 0 worked like a charm.  Here are the results from just 2,000 labeled images: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NKzb3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NKzb3.png\" alt=\"ping pong ball detected 1\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/yBYKB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yBYKB.png\" alt=\"ping pong ball detected 2\"><\/a><\/p>\n\n<p>Here's the code after Ryo's helpful answer:<\/p>\n\n<pre><code>def fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(file_name) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join('generated', jsonFile), 'w') as p:\n                json.dump(line, p)\n\njsons = os.listdir('generated')\nprint ('There are {} images that have annotation files'.format(len(jsons)))\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1572270822900,
        "Question_score":4,
        "Question_tags":"python|machine-learning|computer-vision|object-detection|amazon-sagemaker",
        "Question_view_count":703,
        "Owner_creation_time":1299066980970,
        "Owner_last_access_time":1645631437427,
        "Owner_location":"New York, NY",
        "Owner_reputation":1194,
        "Owner_up_votes":72,
        "Owner_down_votes":0,
        "Owner_views":79,
        "Question_last_edit_time":1576434290393,
        "Answer_body":"<p>Though 'category_id' in the COCO JSON file starts from 1, 'class_id' in the Amazon SageMaker JSON file starts from 0.<\/p>\n\n<p>Your conversion code should be like this.<\/p>\n\n<pre><code>def fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(coco_json_path) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join(sagemaker_json_path, jsonFile), 'w') as p:\n                json.dump(line, p)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">In the Amazon SageMaker doc<\/a>, they are doing this using get_coco_mapper().<\/p>\n\n<pre><code>import json\nimport logging\n\ndef get_coco_mapper():\n    original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20,\n                    21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n                    41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n                    61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n                    81, 82, 84, 85, 86, 87, 88, 89, 90]\n    iter_counter = 0\n    COCO = {}\n    for orig in original_list:\n        COCO[orig] = iter_counter\n        iter_counter += 1\n    return COCO\n<\/code><\/pre>\n\n<p>After you trained the model, you have to check whether each loss has decreased or not.<\/p>\n\n<pre><code>od_model.fit(inputs=data_channels, logs=True)\n\n[11\/04\/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train cross_entropy &lt;loss&gt;=(0.20304460724736212)\n[11\/04\/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train smooth_l1 &lt;loss&gt;=(0.06970448779799958)\n<\/code><\/pre>\n\n<p>If you have some questions, let us know.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1572863208717,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58592206",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69205825,
        "Question_title":"Save SKLearnProcessor transformer in sagemaker",
        "Question_body":"<p>I wanted to use the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/sagemaker.sklearn.html#sagemaker.sklearn.processing.SKLearnProcessor\" rel=\"nofollow noreferrer\">SKLearnProcessor<\/a> in Sagemaker to perform some transformations on an input dataset. However, I want to save this fitted transformer into Sagemaker to reuse it later in other scripts. How can I do that?. I don't see anywhere how it can be stored in S3 at least not in the SDK.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1631784469353,
        "Question_score":1,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":26,
        "Owner_creation_time":1322785469840,
        "Owner_last_access_time":1658151826980,
        "Owner_location":"Cork, Ireland",
        "Owner_reputation":482,
        "Owner_up_votes":25,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69205825",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73383302,
        "Question_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Question_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1660714390293,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_time":1573543021663,
        "Owner_last_access_time":1664082306423,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661363649820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64445766,
        "Question_title":"Resolve Lambda Issue on Sagemaker Ground truth",
        "Question_body":"<p>I am trying to create a labeling job for amazon ground truth for text classification.\nI am not able to create it successfully because I keep getting this error -<\/p>\n<blockquote>\n<p>MissingRequiredParameter: Missing required key 'PreHumanTaskLambdaArn'\nin params.HumanTaskConfig<\/p>\n<\/blockquote>\n<p>Everything seems right, manifest file was successfully created.<\/p>\n<p>I haven't found any help with this issue online, in the documentation, I have found that we could use one of these ARNs in the Text classification section - <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html<\/a><\/p>\n<p>But in the form when creating the labeling job, there's no place to insert this ARN, any idea how this can be fixed and make the error go away.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603199012167,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":207,
        "Owner_creation_time":1434398585880,
        "Owner_last_access_time":1663940254630,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":2702,
        "Owner_up_votes":229,
        "Owner_down_votes":3,
        "Owner_views":242,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64445766",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63849893,
        "Question_title":"AWS - is it possible to pass the parameters from AWS lambda function to AWS sagemaker notebook",
        "Question_body":"<p>I am beginner to AWS console. I am facing issues while building Machine Learning pipeline.<\/p>\n<p>Currently, The Lambda function does the job of getting the uploaded filename, username from front end and invoking the notebook instance.<\/p>\n<p>Also, life cycle configuration at the instance will invoke the notebook for training. So the question is how to pass above variables variables to sagemaker notebook for training the machine learning model. Is it possible to achieve this? Thank you.<\/p>\n<pre><code>#invoke command from lambda\nclient.start_notebook_instance(NotebookInstanceName='&lt;sagemaker_instance_name&gt;')\n<\/code><\/pre>\n<p>Lifecycle configuration under sagemaker instance:<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\nENVIRONMENT=JupyterSystemEnv\n#JupyterSystemEnv\nNOTEBOOK_FILE=\/home\/ec2-user\/SageMaker\/XGBoost_training.ipynb\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\n\nnohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --to notebook --execute &quot;$NOTEBOOK_FILE&quot; &amp;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1599837505523,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":464,
        "Owner_creation_time":1429069144583,
        "Owner_last_access_time":1659713078917,
        "Owner_location":"Ireland",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63849893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73435418,
        "Question_title":"How to import data file from the S3 bucket into the Sagemaker notebook?",
        "Question_body":"<p>I have npz files that I want to import for my model training.\nBelow is the code I have tried.<\/p>\n<pre><code>import s3fs\nfs = s3fs.S3FileSystem()\n\n# To List 5 files in your accessible bucket\n#fs.ls('s3:\/\/input_data\/train_npz\/')[:5]\n\n# open it directly\nwith fs.open(f's3:\/\/input_data\/train_npz\/0.npz') as f:\n    display(Image.open(f))\n<\/code><\/pre>\n<blockquote>\n<pre><code>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call\n<\/code><\/pre>\n<p>last)  in \n7\n8 # open it directly\n----&gt; 9 with fs.open(f's3:\/\/input_data\/train_npz\/0.npz')\nas f:\n10     display(Image.open(f))<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/spec.py in open(self,\npath, mode, block_size, cache_options, **kwargs)\n980                 autocommit=ac,\n981                 cache_options=cache_options,\n--&gt; 982                 **kwargs,\n983             )\n984             if not ac and &quot;r&quot; not in mode:<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _open(self,\npath, mode, block_size, acl, version_id, fill_cache, cache_type,\nautocommit, requester_pays, **kwargs)\n543             cache_type=cache_type,\n544             autocommit=autocommit,\n--&gt; 545             requester_pays=requester_pays,\n546         )\n547<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in <strong>init<\/strong>(self,\ns3, path, mode, block_size, acl, version_id, fill_cache,\ns3_additional_kwargs, autocommit, cache_type, requester_pays)    1822\nself.version_id = self.details.get(&quot;VersionId&quot;)    1823<br \/>\nsuper().<strong>init<\/strong>(\n-&gt; 1824             s3, path, mode, block_size, autocommit=autocommit, cache_type=cache_type    1825         )    1826         self.s3 =\nself.fs  # compatibility<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/spec.py in\n<strong>init<\/strong>(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, **kwargs)    1302         if mode == &quot;rb&quot;:    1303<br \/>\nif not hasattr(self, &quot;details&quot;):\n-&gt; 1304                 self.details = fs.info(path)    1305             self.size = self.details[&quot;size&quot;]    1306             self.cache =\ncaches[cache_type](<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in\nwrapper(*args, **kwargs)\n86     def wrapper(*args, **kwargs):\n87         self = obj or args[0]\n---&gt; 88         return sync(self.loop, func, *args, **kwargs)\n89\n90     return wrapper<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in sync(loop,\nfunc, timeout, *args, **kwargs)\n67         raise FSTimeoutError\n68     if isinstance(result[0], BaseException):\n---&gt; 69         raise result[0]\n70     return result[0]\n71<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in\n_runner(event, coro, result, timeout)\n23         coro = asyncio.wait_for(coro, timeout=timeout)\n24     try:\n---&gt; 25         result[0] = await coro\n26     except Exception as ex:\n27         result[0] = ex<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _info(self,\npath, bucket, key, refresh, version_id)    1062                 else:\n1063                     try:\n-&gt; 1064                         out = await self._simple_info(path)    1065                     except PermissionError:    1066<\/p>\n<h1>If the permissions aren't enough for scanning a prefix<\/h1>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in\n_simple_info(self, path)\n982             Delimiter=&quot;\/&quot;,\n983             MaxKeys=1,\n--&gt; 984             **self.req_kw,\n985         )\n986         # This method either can return the info blob for the object if it<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _call_s3(self,\nmethod, *akwarglist, **kwargs)\n235\n236     async def _call_s3(self, method, *akwarglist, **kwargs):\n--&gt; 237         await self.set_session()\n238         s3 = await self.get_s3(kwargs.get(&quot;Bucket&quot;))\n239         method = getattr(s3, method)<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in\nset_session(self, refresh, kwargs)\n376\n377         conf = AioConfig(**config_kwargs)\n--&gt; 378         self.session = aiobotocore.AioSession(**self.kwargs)\n379\n380         for parameters in (config_kwargs, self.kwargs, init_kwargs, client_kwargs):<\/p>\n<p>AttributeError: module 'aiobotocore' has no attribute 'AioSession'<\/p>\n<\/blockquote>\n<p>Can anyone let me know where I made the mistake or how to do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661092581567,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker|dataloader",
        "Question_view_count":44,
        "Owner_creation_time":1640473067233,
        "Owner_last_access_time":1662900590550,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435418",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66860087,
        "Question_title":"IAM control Sagemaker Studio Instance type",
        "Question_body":"<p>Was wondering if anyone had luck limiting the type of instances a user could chose from the Sagemaker Studio-Jupyter. Did not want to enforce the limitation on the Domain role and was trying to create custom roles that could be attached to user profiles. Tried with the &quot;createApp&quot; and denying the larger instances, but found it becoming a road-block (Sagemaker.createApp not permitted) when a new user profile tries to launch the studio for the first time. Is there anyway to allow them to create the default APP, but limit the choice of the instances that he\/she can select for the image using IAM ?<\/p>\n<h2>Sample Policy used :<\/h2>\n<pre><code>    {\n        &quot;Sid&quot;: &quot;VisualEditor1&quot;,\n        &quot;Effect&quot;: &quot;Deny&quot;,\n        &quot;Action&quot;: &quot;sagemaker:CreateApp&quot;,\n        &quot;Resource&quot;: &quot;*&quot;,\n        &quot;Condition&quot;: {\n            &quot;ForAllValues:StringLike&quot;: {\n                &quot;sagemaker:InstanceTypes&quot;: [\n                    &quot;ml.c5.3xlarge&quot;,\n                    &quot;ml.c5.4large&quot;,\n                    &quot;ml.c5.9xlarge&quot;,                                                                      \n                    &quot;ml.m5.4xlarge&quot;,                        \n                    &quot;ml.m5.12xlarge&quot;,\n                    &quot;ml.m5.16xlarge&quot;,\n                    &quot;ml.m5.24xlarge&quot;,\n                    &quot;ml.c5.4xlarge&quot;,\n                    &quot;ml.c5.9xlarge&quot;,\n                    &quot;ml.c5.12xlarge&quot;,\n                    &quot;ml.c5.18xlarge&quot;,\n                    &quot;ml.c5.24xlarge&quot;,\n                    &quot;ml.g4dn.*&quot;,\n                    &quot;ml.p3.*&quot;\n                    \n                ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>This works fine on a user profile that has logged\/started the &quot;default&quot; App, but limits a new user with the same role\/policy from launching issuing &quot;Open Studio&quot;.<\/p>\n<p>Saw this which was quite similar to the ask - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1499\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1499<\/a><\/p>\n<p>Any thoughts, pointers ?<\/p>\n<p>Thanks,\nMano<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617044567870,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":554,
        "Owner_creation_time":1617042068150,
        "Owner_last_access_time":1636064073713,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66860087",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65421005,
        "Question_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Question_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608711307120,
        "Question_score":1,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":313,
        "Owner_creation_time":1557646363770,
        "Owner_last_access_time":1663926361953,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612085605603,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71260306,
        "Question_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Question_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645755688313,
        "Question_score":3,
        "Question_tags":"serverless|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1645795642143,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1649698499117,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72002863,
        "Question_title":"KeyError: 'ETag' while trying to load data from S3 to Sagemaker",
        "Question_body":"<p>I Unload a file of 500 MB into S3 from Redshift, instead of saving into a single file in S3 it bifurcated into several chunks and now I am trying to access it from S3 to AWS Sagemaker. While trying to read the file using Pd.read_csv and dask.dataframe.read_csv I am getting Keyerror as 'ETag'<\/p>\n<p>I'm a newbie to AWS, please do help me.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9a78F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9a78F.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1650904478070,
        "Question_score":1,
        "Question_tags":"amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1551957360467,
        "Owner_last_access_time":1653641826190,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":327,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72002863",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57686852,
        "Question_title":"SageMaker estimator `source_dir` from S3",
        "Question_body":"<p>I would like to start a training job using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">SageMaker TensorFlow Estimator<\/a> in a script mode.<br>\nMy problem is that I don't have my training code locally or in a git repo, but only in S3 \"directory\" and <code>source_dir<\/code> parameter requires a local file or usage of git.<\/p>\n\n<p>Is the only way to copy the files locally from s3 (which is problematic with python) or can I do it in a nicer way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566976565907,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":1215,
        "Owner_creation_time":1361889746643,
        "Owner_last_access_time":1646295315967,
        "Owner_location":null,
        "Owner_reputation":2945,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":89,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57686852",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72738710,
        "Question_title":"How to access an IP whitelist secured API from within a Sagemaker notebook?",
        "Question_body":"<p>I have deployed a Flask application and its containers in an AWS EKS managed Kubernetes cluster, and the cluster's security group is IP whitelist secured.<\/p>\n<p>I created the cluster using <code>eksctl<\/code>, created an RDS instance for it, and deployed the application using it's helm chart.<\/p>\n<p>I am trying to access the APIs of the flask application from within a AWS Sagemaker Notebook instance, but because of the IP whitelist, I am unable to connect. The connection times-out instead.<\/p>\n<p>Can anyone tell me how I can add the Notebook instance to my whitelist?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656041644317,
        "Question_score":1,
        "Question_tags":"amazon-web-services|cloud|devops|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1531126305543,
        "Owner_last_access_time":1664050689040,
        "Owner_location":null,
        "Owner_reputation":87,
        "Owner_up_votes":237,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72738710",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70222875,
        "Question_title":"IOPub data rate exceeded in SageMaker?",
        "Question_body":"<p>Does anyone know where to find the config file to edit the <code>c.NotebookApp.iopub_data_rate_limit<\/code>? I am not working with the AWS CLI, but rather I am doing everything in the AWS Console. I have a SageMaker notebook running and I would like to change the data rate limit, but essentially don't have access to a terminal, unless someone could explain how to access the terminal within the console?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1638589402183,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|jupyter-lab|amazon-sagemaker",
        "Question_view_count":57,
        "Owner_creation_time":1536431447580,
        "Owner_last_access_time":1650991212660,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70222875",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71655510,
        "Question_title":"How to see all SageMaker service quota limits?",
        "Question_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648517982110,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648538467293,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72882923,
        "Question_title":"Sagemaker Batch transform with multiple images",
        "Question_body":"<p>The batch transform with mini-batch &gt; 1 of images doesn't work fo as I expect.<\/p>\n<p>I'm using Sagemaker Batch transform for inference.\nI'm trying to preprocess images on a custom container that I created (using model pipelining: the first model is the pre-processor that I'm asking about, and the second model is an Nvidia-triton inference server).<\/p>\n<p>I'm using batch transform as follows:<\/p>\n<pre><code>transformer = sagemaker.transformer.Transformer(model_name=model_name, \n                  instance_count=instance_count, \n                  instance_type=instance_type,\n                  max_concurrent_transforms=16,\n                  output_path=inference_output_data,\n                  strategy='MultiRecord')\ntransformer.transform(data=batch_input,\n                  content_type='image\/jpeg',\n                  job_name=job_name,\n                  split_type='Line',\n                  wait=False,\n                  logs=False)\n<\/code><\/pre>\n<p>Note the <code>split type<\/code> and <code>strategy<\/code> with values 'MultiRecord' and 'Line' so I would get mini-batches &gt; 1.<\/p>\n<p>When I did my sanity check with batch size of 1, the following code worked fine:<\/p>\n<pre><code>@app.route('\/invocations', methods=['POST'])\ndef transformation():\n    img_bytes = flask.request.data\n    img = Image.open(io.BytesIO(img_bytes))\n    preprocessed_image = preprocess_image()\n<\/code><\/pre>\n<p>However, with batching, this code doesn't work anymore. Basically I would not expect it to work, since it only valid for reading for one image.\nI even printed the bytes that my container receives and these are not the original bytes of the image but some strange mix of the images.<\/p>\n<p>Can you please point me what am I doing wrong?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657107353313,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":159,
        "Owner_creation_time":1492957629740,
        "Owner_last_access_time":1663850451627,
        "Owner_location":"Israel",
        "Owner_reputation":147,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":45,
        "Question_last_edit_time":1657108872360,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72882923",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66915920,
        "Question_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Question_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617348836210,
        "Question_score":0,
        "Question_tags":"amazon-ec2|pytorch|amazon-sagemaker",
        "Question_view_count":1608,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1617349698827,
        "Answer_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617464162903,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69634197,
        "Question_title":"Pretraining\/transfer learning with SageMaker BlazingText (word2vec)?",
        "Question_body":"<p>I have a training set consisting of a description and a binary label. From reading previous work, I know that using pretrained <a href=\"https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\" rel=\"nofollow noreferrer\">fasttext embeddings<\/a> should work well for my use case. I need to be able to make predictions on unseen words (OOV). My company is already using aws\/sagemaker, so using <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\/blazingtext_word2vec_subwords_text8.ipynb\" rel=\"nofollow noreferrer\">SageMaker Blazing text with subword embedding<\/a> seems like a good approach.<\/p>\n<p>However, they are providing their own training data in the example - does this means it tries to learn word embeddings from scratch from only this training data? I was expecting to be able to pass pretraining as a parameter, and then fine tune it with my own data so those words get added to the known dictionary.<\/p>\n<p>But the way it looks to me now, is that I either use a lookup against a hardcoded list of pretrained embeddings (like GloVe or one of the word vector datasets from fasttext), which means I'm not using my own data and also have to come up with a solution to handling OOV. Or I use Blazing text which can handle OOV, but then I don't take advantage of any pretrained model?<\/p>\n<p>So my main question is this: can I use Blazing text to get pretrained OOV embeddings? And if so how?<\/p>\n<p>Would be great if I could also understand how transfer learning would work in this case, so I can make use of my own classification data. However, the embeddings will be used in a downstream classification task, so I guess I could say the fine tuning happens there?<\/p>\n<ul>\n<li>Use pretrined blazing text model to get embeddings for both seen and unseen words (through subword embeddings)<\/li>\n<li>Combine these embeddings with other features<\/li>\n<li>Fit a model around the embeddings + features to get the classification<\/li>\n<\/ul>\n<p>Appreciate any help!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634659369027,
        "Question_score":0,
        "Question_tags":"nlp|amazon-sagemaker|embedding|pre-trained-model|fasttext",
        "Question_view_count":80,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69634197",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73662608,
        "Question_title":"Difference between SageMaker instance count and Data parallelism",
        "Question_body":"<p>I can't understand the difference between SageMaker instance count and Data parallelism. As we already have a feature that can specify how many instances we train model when we write a training script using sagemaker-sdk.<\/p>\n<p>However, in 2021 re:Invent, SageMaker team launched and demonstrated SageMaker managed Data Parallelism and this feature also provides distributed training.<\/p>\n<p>I've searched a lot of sites for letting me know about that, but I can't find really clear demonstration. I share some stuffs explaining the concept I mentioned closely. Link :\u00a0<a href=\"https:\/\/godatadriven.com\/blog\/distributed-training-a-diy-aws-sagemaker-model\/\" rel=\"nofollow noreferrer\">https:\/\/godatadriven.com\/blog\/distributed-training-a-diy-aws-sagemaker-model\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662728592430,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":15,
        "Owner_creation_time":1412669622830,
        "Owner_last_access_time":1663944305230,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73662608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71410791,
        "Question_title":"How can I build a multi model endpoint for ensemble modeling with using my own model containers?",
        "Question_body":"<p>I'm trying to deploy a multi model endpoint on Amazon Sagemaker, and am working with my own model containers which I created using <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example. I can train and create endpoint for each of them separately but for example when I try to collect mlp and cart together in multi model endpoint, I get an error which says &quot;The cart,mlp for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint&quot;. When I check CloudWatch logs I cannot see anything unusual. <strong>Should I change the container structure for multi model endpoints ?<\/strong><\/p>\n<pre><code>from time import gmtime, strftime\nimport os\nimport boto3\nimport time\nimport re\nimport sagemaker\n\nmodel_name = &quot;efe-test-model-ensemble-modeling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n\ncart_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-cart:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;cart&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-cart-2022-03-09-12-47-04-881\/output\/model.tar.gz&quot;,\n}\n\nmlp_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-mlp:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;mlp&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-mlp-2022-03-09-12-52-09-267\/output\/model.tar.gz&quot;,\n}\n\nrole = sagemaker.get_execution_role()\nsm = boto3.client(&quot;sagemaker&quot;)\n\ninferenceExecutionConfig = {&quot;Mode&quot;: &quot;Direct&quot;}\n\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    InferenceExecutionConfig=inferenceExecutionConfig,\n    ExecutionRoleArn=role,\n    Containers=[cart_hosting_container, mlp_hosting_container],\n)\n\nendpoint_config_name = &quot;TEST-config-ensemble-modelling-&quot; + strftime(\n    &quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime()\n)\nprint(endpoint_config_name)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;InitialVariantWeight&quot;: 1,\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n        }\n    ],\n)\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n%%time\nimport time\n\nendpoint_name = &quot;TEST-endpoint-ensemble-modelling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(endpoint_name)\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n)\nprint(create_endpoint_response[&quot;EndpointArn&quot;])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp[&quot;EndpointStatus&quot;]\nprint(&quot;Status: &quot; + status)\n\nwhile status == &quot;Creating&quot;:\n    time.sleep(60)\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[&quot;EndpointStatus&quot;]\n    print(&quot;Status: &quot; + status)\n\nprint(&quot;Arn: &quot; + resp[&quot;EndpointArn&quot;])\nprint(&quot;Status: &quot; + status)\n<\/code><\/pre>\n<p>It creates two folders in CloudWatch<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCNOC.png\" rel=\"nofollow noreferrer\">log groups<\/a><\/p>\n<p>mlp is:\n<a href=\"https:\/\/i.stack.imgur.com\/O7qeA.png\" rel=\"nofollow noreferrer\">mlp log<\/a><\/p>\n<p>cart is:\n<a href=\"https:\/\/i.stack.imgur.com\/XhiDC.png\" rel=\"nofollow noreferrer\">cart log<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646835413127,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":87,
        "Owner_creation_time":1645732047140,
        "Owner_last_access_time":1654867064967,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71410791",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68470428,
        "Question_title":"Vectorising categorical dataset for XGBoost in Sagemaker",
        "Question_body":"<p>When we train an XGB model using AWS built-in models\ne.g. <code>(container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;))<\/code>,<\/p>\n<p>Based on my understanding, The training job requires numerical vectors for the train and validation.\nMeaning that if you have a dataset with categorical values and strings, you need to convert them into a vector. the model only deals with float numbers,\n(Outside Sagemaker, I can use TFIDF to vectorize my features and construct a DMatrix), but this approach doesn't seem to be supported by Sagemaker.<\/p>\n<ol>\n<li>Does anyone know how this data transformation is done in Sagemaker?<\/li>\n<li>Is this a bad idea to use BlazyngText unsupervised learning to generate the vectors?<\/li>\n<li>Should we have a preprocessing step and in that step we use TFIDF?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626874229773,
        "Question_score":0,
        "Question_tags":"python|vectorization|xgboost|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1341999310263,
        "Owner_last_access_time":1663790298557,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":443,
        "Owner_up_votes":1252,
        "Owner_down_votes":1,
        "Owner_views":83,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68470428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69093204,
        "Question_title":"Is there way of gaining more root (temporary) volume on the aws sagemaker notebook instance?",
        "Question_body":"<p>Aws sagemaker notebook instances come with a fixed root volume size of ~104GB whose ~15 GB is free (available).<\/p>\n<p>Docker uses this temporary memory (<code>\/var\/lib\/docker<\/code> as far as I know).<\/p>\n<p>When I try to build docker image to create custom training-job, temporary root volume in use  blows up and system throws &quot;no space left on the device&quot; error.<\/p>\n<p>I tried to delete anaconda directory (~62 GB), however then, boto3 and sagemaker python libraries stopped working.<\/p>\n<p>What is the best way to solve problem?<\/p>\n<p>Heavy Dockerfile I try to build to push ECR :<\/p>\n<pre><code>ARG REGION=&quot;us-east-1&quot;\n\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n\nRUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\n\nRUN python3 -m pip install detectron2 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu111\/torch1.8\/index.html\n\nENV FORCE_CUDA=&quot;1&quot;\n\nENV TORCH_CUDA_ARCH_LIST=&quot;Volta&quot;\n\nENV FVCORE_CACHE=&quot;\/tmp&quot;\n\n############# SageMaker section ##############\n\nCOPY tested_train_src\/train_src \/opt\/ml\/code\nWORKDIR \/opt\/ml\/code\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM train.py\n\nWORKDIR \/\n\nENTRYPOINT [&quot;bash&quot;, &quot;-m&quot;, &quot;start_with_right_hostname.sh&quot;]\n<\/code><\/pre>\n<p>Build command:<\/p>\n<pre><code>docker build -t image-name:tag . --build-arg REGION=&quot;us-east-1&quot;\n<\/code><\/pre>\n<p>Output from docker build<\/p>\n<pre><code>Sending build context to Docker daemon  1.935GB\nStep 1\/12 : ARG REGION=&quot;us-east-1&quot;\nStep 2\/12 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n1.8.1-gpu-py36-cu111-ubuntu18.04: Pulling from pytorch-training\n\nd2c87b75: Pulling fs layer \n10be24e1: Pulling fs layer \n7173dcfe: Pulling fs layer \n8de7822d: Pulling fs layer \nbf66c36b: Pulling fs layer \nc74d4d18: Pulling fs layer \nf70a70b2: Pulling fs layer \n4e2cb041: Pulling fs layer \n8ddd4da6: Pulling fs layer \nfac38f0d: Pulling fs layer \na26fd875: Pulling fs layer \n1dca51bb: Pulling fs layer \n0d6bb6c9: Pulling fs layer \n26721764: Pulling fs layer \n956fbe7a: Pulling fs layer \nad4fa2a5: Pulling fs layer \n20c0bd9a: Pulling fs layer \n82804870: Pulling fs layer \n1d1fdc54: Pulling fs layer \n4500c676: Pulling fs layer \n923bbc02: Pulling fs layer \n0c9d88c6: Pulling fs layer \nf5b0d167: Pulling fs layer \n2f2aa1af: Pulling fs layer \nc272e0bb: Pulling fs layer \n311661aa: Pulling fs layer \ned3ef379: Pulling fs layer \n03c2d7ac: Pulling fs layer \n1cefc5dc: Pulling fs layer \n30fd2377: Pulling fs layer \n78d30971: Pulling fs layer \nd18f41de: Pulling fs layer \n4c2aeed5: Pulling fs layer \nf099a687: Pulling fs layer \n253573ff: Pulling fs layer \n515cab8b: Pulling fs layer \n056b70c3: Pulling fs layer \nDigest: sha256:66af111d2bd9dae500ad73a7b427103fe8379cbb24bf4ce7cb7d5770d31cd9322KExtracting  505.2MB\/962.1MB\nStatus: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n ---&gt; b4191cf0b8c9\nStep 3\/12 : RUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\n ---&gt; Running in 7c62740a69c6\nLooking in links: https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\nCollecting torch==1.8.2+cu111\n  Downloading https:\/\/download.pytorch.org\/whl\/lts\/1.8\/cu111\/torch-1.8.2%2Bcu111-cp36-cp36m-linux_x86_64.whl (1982.2 MB)\nERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n<\/code><\/pre>\n<p>Disk usage before build:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs        1.9G   76K  1.9G   1% \/dev\ntmpfs           1.9G     0  1.9G   0% \/dev\/shm\n\/dev\/nvme0n1p1  104G   89G   16G  86% \/\n\/dev\/nvme1n1     63G  1.9G   58G   4% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Disk usage after erronous build:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs        1.9G   76K  1.9G   1% \/dev\ntmpfs           1.9G     0  1.9G   0% \/dev\/shm\n\/dev\/nvme0n1p1  104G  101G  2.4G  98% \/\n\/dev\/nvme1n1     63G  1.9G   58G   4% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note : I will try to mount directory <code>\/var\/lib\/docker<\/code> to EBS volume at notebook start.<\/p>\n<p>Note : I don't have any issue about attached EBS volume size. My issue is about temporary volume.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1631040430670,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":632,
        "Owner_creation_time":1591041254117,
        "Owner_last_access_time":1664082818733,
        "Owner_location":"Turkey",
        "Owner_reputation":36,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1631040737013,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69093204",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65045458,
        "Question_title":"Amazon Sagemaker - Unable to evaluate payload provided",
        "Question_body":"<p>I built a Sagemaker endpoint that I am attempting to evoke using Lambda+API Gateway.  I'm getting the following error:<\/p>\n<pre><code>&quot;An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \\&quot;unable to evaluate payload provided\\&quot;\n<\/code><\/pre>\n<p>I know why what it's complaining about, but I don't quite understand why it's occuring.  I have confirmed that the shape of the input data of my lambda function is the same as how I trained the model.  The following is my input payload in lambda:<\/p>\n<pre><code>X = pd.concat([X, rx_norm_dummies, urban_dummies], axis = 1)\npayload = X.to_numpy()\n\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                               ContentType='application\/json',\n                               Body=payload)\n<\/code><\/pre>\n<p>In the jupyter notebook where I created my endpoint\/trained my model, I can also access the model using a numpy ndarray so I'm confused why I'm getting this error.<\/p>\n<pre><code>y = X[0:10]\nresult = linear_predictor.predict(y)\nprint(result)\n<\/code><\/pre>\n<p>Here is a modificaiton I make to serialization of the endpoint:<\/p>\n<pre><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\n    linear_predictor.content_type = 'text\/csv'\n    linear_predictor.serializer = csv_serializer\n    linear_predictor.deserializer = json_deserializer\n<\/code><\/pre>\n<p>I'm new when it comes to Sagemaker\/Lambda, so any help would be appreciated and I can send more code to add context if needed.  Tried various foramts and cannot get this to work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606525348557,
        "Question_score":2,
        "Question_tags":"python|aws-lambda|amazon-sagemaker",
        "Question_view_count":988,
        "Owner_creation_time":1421421571370,
        "Owner_last_access_time":1663593937387,
        "Owner_location":null,
        "Owner_reputation":1304,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":167,
        "Question_last_edit_time":1606857230370,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65045458",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57966245,
        "Question_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Question_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568683965203,
        "Question_score":0,
        "Question_tags":"h2o|training-data|amazon-sagemaker|automl",
        "Question_view_count":68,
        "Owner_creation_time":1509012479113,
        "Owner_last_access_time":1632253986417,
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1568727043933,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71682848,
        "Question_title":"Why does saving CSV file in S3 work in notebook cell, but not in terminal?",
        "Question_body":"<p>I am trying to save a csv file on s3 by using a buffer object. The strange thing is that all works perfectly fine if I do it in jupyter notebook cell. I can read it again using pandas without any issues. Below is the code snippet:<\/p>\n<pre><code>csv_buffer = StringIO()\ns3_resource = boto3.resource('s3')\ndf.to_csv(csv_buffer, index = False)\n_ = s3_resource.meta.client.put_object(Body = csv_buffer.getvalue(), Bucket = 'bucket', Key = 'temp.csv')\n<\/code><\/pre>\n<p>But, when I try to execute the same code dynamically in my program through command line interface, the file is being saved, albeit when I try to read it using pandas, I am getting error as follows: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte. As expected, when I download the file from s3, there are strange characters.<\/p>\n<p>I have tried various solutions including new conda environment, using parquet instead of csv, adding ContentType parameter in put object. But nothing seems to work. Also, when I print the contents of csv buffer in my script, I can see the string contents as well. Any help would be much appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1648667366867,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1648666469280,
        "Owner_last_access_time":1663919608643,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71682848",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72825806,
        "Question_title":"Run jupyter notebook through command line and log outputs to a file",
        "Question_body":"<p>I am running a notebook on command line in AWS SageMaker but I am not able to log outputs to a file. I added <code>print<\/code>, <code>logging.info<\/code> and <code>sys.stdout.write<\/code> statements in hope of capturing something but to no avail.<\/p>\n<p>Below is the code I am using<\/p>\n<pre><code>NOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/mynb.ipynb&quot;\nLOG_FILE=&quot;\/home\/ec2-user\/SageMaker\/logs.txt&quot;\n\nnohup jupyter nbconvert  --to notebook --inplace --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &gt; &quot;$LOG_FILE&quot; &amp;\n<\/code><\/pre>\n<p>On doing <code>cat &quot;$LOG_FILE&quot;<\/code> I get just a couple lines of logs and nothing after that. Output is below -<\/p>\n<pre><code>[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n[NbConvertApp] Converting notebook \/home\/ec2-user\/SageMaker\/tata1mg.ipynb to notebook\n[NbConvertApp] Executing notebook with kernel: python3\n<\/code><\/pre>\n<p>PS:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656660742190,
        "Question_score":0,
        "Question_tags":"python|python-3.x|jupyter|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1366987249713,
        "Owner_last_access_time":1664022858883,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":7127,
        "Owner_up_votes":2089,
        "Owner_down_votes":92,
        "Owner_views":987,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72825806",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70508395,
        "Question_title":"Convert .pt to .pth?",
        "Question_body":"<p>I've been trying to compile my custom trained YoloV5 model using SageMaker Neo.\nThe compilation gives an error :<\/p>\n<pre><code>ClientError: InputConfiguration: No pth file found for PyTorch model. \nPlease make sure the framework you select is correct.\n<\/code><\/pre>\n<p>The weights are a <code>.pt<\/code> file.\nIs there a way to convert the <code>.pt<\/code> file to <code>.pth<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640701511037,
        "Question_score":0,
        "Question_tags":"python|pytorch|amazon-sagemaker|yolov5",
        "Question_view_count":563,
        "Owner_creation_time":1527945118903,
        "Owner_last_access_time":1663948511827,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1640701891237,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70508395",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70363229,
        "Question_title":"No space for sagemaker Studio lab to upload or download file",
        "Question_body":"<p>The SageMaker studio lab FAQ says there are 15GB space for each user, but I can't even upload or download a 900MB DataSet.\nWhen I upload or download it, return<\/p>\n<pre><code>Unexpected error while saving file: sagemaker-studiolab-notebooks\/preprocessed20152019.zip [Errno 2] \nNo such file or directory: \n'\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/.~preprocessed20152019.zip'\n -&gt; '\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/preprocessed20152019.zip'\n<\/code><\/pre>\n<p>and sometimes it returned &quot;no space for device&quot;.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639569106200,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":290,
        "Owner_creation_time":1541417544600,
        "Owner_last_access_time":1641543663313,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70363229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55067802,
        "Question_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Question_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552064664580,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_time":1462207957683,
        "Owner_last_access_time":1653409817263,
        "Owner_location":null,
        "Owner_reputation":1305,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":174,
        "Question_last_edit_time":1552064982860,
        "Answer_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1552530714763,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59445162,
        "Question_title":"how to load image data from the bucket to AWS sagemaker notebook?",
        "Question_body":"<p>The images are present as folders - train and test in my s3 bucket. I want to use them as it is in my sagemaker notebook. For example, like on my local server I use test_dir = \"C:\\Users\\catvdog\\dataset\\test\".<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1577025463090,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-s3|deep-learning|amazon-sagemaker",
        "Question_view_count":931,
        "Owner_creation_time":1576747245357,
        "Owner_last_access_time":1632464838653,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59445162",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57857726,
        "Question_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Question_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1568045843843,
        "Question_score":0,
        "Question_tags":"python|airflow|amazon-sagemaker|urllib3",
        "Question_view_count":343,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1571189751313,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64700093,
        "Question_title":"AWS Sagemaker - df.to_csv error write() argument 1 must be unicode, not str",
        "Question_body":"<p>I am trying to save a file to S3 bucket from sagemaker instance. and below line throws an error!<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False)\n<\/code><\/pre>\n<pre><code>error - \nTypeErrorTraceback (most recent call last)\n&lt;ipython-input-28-d33896172c11&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 a.to_csv(&quot;s3:\/\/informatri\/{}&quot;.format('Drug_Data_Cleaned.csv'), index = False)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/core\/generic.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\n   3018                                  doublequote=doublequote,\n   3019                                  escapechar=escapechar, decimal=decimal)\n-&gt; 3020         formatter.save()\n   3021 \n   3022         if path_or_buf is None:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in save(self)\n    170                 self.writer = UnicodeWriter(f, **writer_kwargs)\n    171 \n--&gt; 172             self._save()\n    173 \n    174         finally:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save(self)\n    272     def _save(self):\n    273 \n--&gt; 274         self._save_header()\n    275 \n    276         nrows = len(self.data_index)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save_header(self)\n    240         if not has_mi_columns or has_aliases:\n    241             encoded_labels += list(write_cols)\n--&gt; 242             writer.writerow(encoded_labels)\n    243         else:\n    244             # write out the mi\n\nTypeError: write() argument 1 must be unicode, not str\n<\/code><\/pre>\n<p>I tried the following:<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False, encoding = 'utf-8', sep = '\\t')\n<\/code><\/pre>\n<p>I still get the same error. If I do only:<\/p>\n<pre><code>df.to_csv(&quot;Drug_Data_Cleaned.csv&quot;), index = False) \n<\/code><\/pre>\n<p>It gets saved locally all fine. So not a problem with dataframe or the name etc. It has to do something with saving to S3 bucket.\nI have used similar ways to save to s3 bucket many times in the past and it has worked perfectly fine. Hence, I was wondering why the error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604589432083,
        "Question_score":1,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":275,
        "Owner_creation_time":1555475748810,
        "Owner_last_access_time":1663961286663,
        "Owner_location":"Pennsylvania, USA",
        "Owner_reputation":351,
        "Owner_up_votes":87,
        "Owner_down_votes":6,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this problem.<\/p>\n<p>The error was that the Sagemaker ipynb notebook was opened in conda_python2.7 or so. Just re-wrote the script in conda_python3 and then everything worked fine :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612607749437,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64700093",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71187747,
        "Question_title":"AWS sagemaker training job (Tensorflow) halts at Epoch 1",
        "Question_body":"<p>I am trying to train Maskrcnn with custom dataset. The code is running fine on my local machine in the same docker container, however, it gets stuck at the first epoch when I use aws sagemaker.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/C3Z1T.png\" rel=\"nofollow noreferrer\">The log my error seen on sagemaker notebook for training job<\/a><\/p>\n<p>I am using Tensorflow 2 implementing the github code provided by <a href=\"https:\/\/github.com\/simone-viozzi\/Mask-RCNN-training-with-docker-containers-on-Sagemaker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/simone-viozzi\/Mask-RCNN-training-with-docker-containers-on-Sagemaker<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1645294110193,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":117,
        "Owner_creation_time":1645293436933,
        "Owner_last_access_time":1659077375647,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71187747",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55446951,
        "Question_title":"Do images with large dimensions (e.g. 2000 x 2000) be auto-scaled to 300 x 300 when using them for training data in AWS Sagemaker?",
        "Question_body":"<p>I'm working on a project that trains an ML model to predict the location of Waldo in a Where's Wally? image using AWS Sagemaker with the underlying object detection algorithm being Single Shot Detection, but I am thinking that using an actual puzzle image with dimensions like 2000 x 2000 as training data is not possible and that SSD will auto-resize the image to 300 x 300 which would render Waldo a meaningless blur.  Does SSD re-size images automatically, or will it train on the 2000 x 2000 image?  Should I crop resize all puzzles to 300 x 300 images containing Waldo, or can I include a mix of actual puzzle images with dimensions 2000+ x 2000+ and the 300 x 300 cropped images?<\/p>\n\n<p>I'm considering augmenting the data by cropping these larger images at locations that contain Wally so that I can have 300 x 300 images where Wally isn't reduced to a smudge on a page and is actually visible - is this a good idea?  I am thinking that SSD does train on the 2000 x 2000 image, but the FPS will reduce by a lot - is this wrong?  I feel like if I don't use the 2000 x 2000 image for training, in the prediction stage where I start feeding the model images with large dimensions (actual puzzle images), the model won't be able to predict locations accurately - is this not the case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554081184047,
        "Question_score":0,
        "Question_tags":"object-detection|amazon-sagemaker|data-augmentation",
        "Question_view_count":239,
        "Owner_creation_time":1554079262983,
        "Owner_last_access_time":1646728800483,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55446951",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48650152,
        "Question_title":"AWS uploading file into wrong bucket",
        "Question_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1517943817340,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":722,
        "Owner_creation_time":1509392519217,
        "Owner_last_access_time":1552779156337,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1525621837033,
        "Answer_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1518097617247,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56773989,
        "Question_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Question_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561555658777,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_time":1336973807643,
        "Owner_last_access_time":1655749162853,
        "Owner_location":"Minneapolis, MN, United States",
        "Owner_reputation":1907,
        "Owner_up_votes":692,
        "Owner_down_votes":6,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561569885853,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61832086,
        "Question_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Question_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1589605066783,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1583123749267,
        "Owner_last_access_time":1663877641340,
        "Owner_location":"Washington D.C., DC, USA",
        "Owner_reputation":317,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1589768612457,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54249334,
        "Question_title":"ImportError libopenblasp-r0 cannot open shared object file No such file or directory -SageMaker",
        "Question_body":"<p>I am trying train my model code using Docker Container - AWS SageMaker using following code.<\/p>\n\n<pre>\n    'https:\/\/github.com\/awslabs\/amazon-sagemaker- \nexamples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb'\n<\/pre>\n\n<p>But I get below error when I Try to train my model using <\/p>\n\n<pre><code> tree.fit(data_location)\n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n    File \"\/opt\/program\/train\", line 17, in &lt;module&gt;\n     from sklearn import tree\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/__init__.py\", line \n      64, in &lt;module&gt;\n     from .base import clone\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/base.py\", line \n       13, in &lt;module&gt;\n      from .utils.fixes import signature\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/sklearn\/utils\/__init__.py\", line 16, in &lt;module&gt;\n      from .fixes import _Sequence as Sequence\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/utils\/fixes.py\", \n       line 85, in &lt;module&gt;\n      from scipy.special import boxcox  # noqa\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/scipy\/special\/__init__.py\", line 641, in &lt;module&gt;\n       from ._ufuncs import *\n    ImportError: libopenblasp-r0-8dca6697.3.0.dev.so: cannot open shared \n       object file: No such file or directory\n<\/code><\/pre>\n\n<p>error message 2<\/p>\n\n<pre><code>   Error for Training job decision-trees-sample-2019-01-18-07-44-37-282: Failed Reason: AlgorithmError: Exit Code: 1\n<\/code><\/pre>\n\n<p>I wend to the directory and did not find 'sklearn' directory.<\/p>\n\n<pre><code>  sh-4.2$ pwd\n    \/usr\/local\/lib\/python2.7\/dist-packages\n  sh-4.2$ ls -l\n    total 3244\n  -rwxr-xr-x 1 root root 3318568 Sep 18 03:23 cv2.so\n<\/code><\/pre>\n\n<p>My current jupyter notebook points to root environment and it has sklearn package available , not sure how make it available in above location where I see error, not sure if this is what will resolve the issue  or something else needs to be done.<\/p>\n\n<p>I am new to Amazon SageMaker.<\/p>\n\n<p>Expected result: I am expecting the training job to complete without error<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1547795942457,
        "Question_score":1,
        "Question_tags":"python|python-2.7|docker|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1661,
        "Owner_creation_time":1419997605110,
        "Owner_last_access_time":1663052127303,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1050,
        "Owner_up_votes":749,
        "Owner_down_votes":10,
        "Owner_views":168,
        "Question_last_edit_time":1547797769223,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54249334",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58918995,
        "Question_title":"Invoking sagemaker endpoint using AWS-Lamda throwing parameter validation error",
        "Question_body":"<p>I trained an ML model in AWS Sagemaker and created an endpoint. I want to invoke it using AWS-Lambda. My model has 30 predictor variables. So I passed them into test event of Lambda as dict type as mentioned below<\/p>\n\n<pre><code>{\n  \"Time\": \"10 \",\n  \"V1\": \"1.449043781 \",\n  \"V2\": \"-1.176338825 \",\n  \"V3\": \"0.913859833 \",\n  \"V4\": \"-1.375666655 \",\n  \"V5\": \"-1.971383165 \",\n  \"V6\": \"-0.629152139 \",\n  \"V7\": \"-1.423235601 \",\n  \"V8\": \"0.048455888 \",\n  \"V9\": \"-1.720408393 \",\n  \"V10\": \"1.626659058 \",\n  \"V11\": \"1.19964395 \",\n  \"V12\": \"-0.671439778 \",\n  \"V13\": \"-0.513947153 \",\n  \"V14\": \"-0.095045045 \",\n  \"V15\": \"0.230930409 \",\n  \"V16\": \"0.031967467 \",\n  \"V17\": \"0.253414716 \",\n  \"V18\": \"0.854343814 \",\n  \"V19\": \"-0.221365414 \",\n  \"V20\": \"-0.387226474 \",\n  \"V21\": \"-0.009301897 \",\n  \"V22\": \"0.313894411 \",\n  \"V23\": \"0.027740158 \",\n  \"V24\": \"0.500512287 \",\n  \"V25\": \"0.251367359 \",\n  \"V26\": \"-0.129477954 \",\n  \"V27\": \"0.042849871 \",\n  \"V28\": \"0.016253262 \",\n  \"Amount\": \"7.8\"\n}\n<\/code><\/pre>\n\n<p>Now I ran below mentioned code in AWS Lambda <\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport boto3\nimport io\nimport codecs\n\nendpoint_name = os.environ['ENDPOINT_NAME']\nruntime = boto3.client('runtime.sagemaker')\n\n\ndef lambda_handler(event, context):\n    print(\"received event: \"+json.dumps(event,indent=2))\n    data = json.loads(json.dumps(event))\n    payload = data[\"Time\"]+data[\"V1\"]+data[\"V2\"]+data[\"V3\"]+data[\"V4\"]+data[\"V5\"]+data[\"V6\"]+data[\"V7\"]+data[\"V8\"]+data[\"V9\"]+data[\"V10\"]+data[\"V11\"]+data[\"V12\"]+data[\"V13\"]+data[\"V14\"]+data[\"V15\"]+data[\"V16\"]+data[\"V17\"]+data[\"V18\"]+data[\"V19\"]+data[\"V20\"]+data[\"V21\"]+data[\"V22\"]+data[\"V23\"]+data[\"V24\"]+data[\"V25\"]+data[\"V26\"]+data[\"V27\"]+data[\"V28\"]+data[\"Amount\"]\n    payload = payload.split(\" \")\n    payload = [codecs.encode(i,'utf-8') for i in payload]\n    payload=[bytearray(i) for i in payload]\n    print(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='text\/csv',Body=payload)\n    print(response)\n    result=json.loads(response['Body'].decode())\n    pred = int(float(response))\n    predicted_label = 'fraud' if pred==1 else 'not fraud'\n    return predicted_label\n<\/code><\/pre>\n\n<p>This code is throwing below this error<\/p>\n\n<pre><code>[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: [bytearray(b'10'), bytearray(b'1.449043781'), bytearray(b'-1.176338825'), bytearray(b'0.913859833'), bytearray(b'-1.375666655'), bytearray(b'-1.971383165'), bytearray(b'-0.629152139'), bytearray(b'-1.423235601'), bytearray(b'0.048455888'), bytearray(b'-1.720408393'), bytearray(b'1.626659058'), bytearray(b'1.19964395'), bytearray(b'-0.671439778'), bytearray(b'-0.513947153'), bytearray(b'-0.095045045'), bytearray(b'0.230930409'), bytearray(b'0.031967467'), bytearray(b'0.253414716'), bytearray(b'0.854343814'), bytearray(b'-0.221365414'), bytearray(b'-0.387226474'), bytearray(b'-0.009301897'), bytearray(b'0.313894411'), bytearray(b'0.027740158'), bytearray(b'0.500512287'), bytearray(b'0.251367359'), bytearray(b'-0.129477954'), bytearray(b'0.042849871'), bytearray(b'0.016253262'), bytearray(b'7.8')], type: &lt;class 'list'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I understand that somehow I need to pass my 30 features into Lambda function such that data type of <code>payload<\/code> is compatible with <code>ContentType<\/code> for <code>respnse<\/code> to work. Can someone please explain how to do it? \nedit: I'm trying this problem by looking at <a href=\"http:\/\/%20https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws blog. I don't quite understand how the author of above mentioned blog did it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574094491987,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":895,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58918995",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61165665,
        "Question_title":"How long does it take for a Sagemaker endpoint configuration's training job to finish so you can create the endpoint?",
        "Question_body":"<p>I'm new to Sagemaker but have been waiting a few hours for a Sagemaker training job to complete so that I can create the endpoint... The Sagemaker console shows a Create endpoint button, but when I press it, it doesn't work. The end point configuration still has a spinning icon for \"Training job\" <\/p>\n\n<p>How long does it typically take for a Sagemaker endpoint to spin up? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586653959430,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1364,
        "Owner_creation_time":1275549180447,
        "Owner_last_access_time":1664080333383,
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":18781,
        "Owner_up_votes":965,
        "Owner_down_votes":37,
        "Owner_views":1916,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61165665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58110595,
        "Question_title":"How do I access Amazon Sagemaker through React Native?",
        "Question_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1569478549487,
        "Question_score":1,
        "Question_tags":"amazon-web-services|react-native|mobile|amazon-sagemaker",
        "Question_view_count":717,
        "Owner_creation_time":1488334447850,
        "Owner_last_access_time":1583962966363,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1569671934553,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65168915,
        "Question_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Question_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607263332140,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|scikit-learn|amazon-sagemaker",
        "Question_view_count":3221,
        "Owner_creation_time":1559910246180,
        "Owner_last_access_time":1664039951323,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Question_last_edit_time":1607399645213,
        "Answer_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1607293074050,
        "Answer_score":7.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1630389412780,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52437543,
        "Question_title":"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?",
        "Question_body":"<p>As stated in the question, \"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?\"<\/p>\n\n<p>Some common error message showed as \"CannotStartContainerError. Please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run  serve\u2019.\" and it didn't show as running with nividia driver.<\/p>\n\n<p>So, do we need manually set up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537509940017,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|nvidia-docker|amazon-sagemaker",
        "Question_view_count":940,
        "Owner_creation_time":1537311568807,
        "Owner_last_access_time":1612898933127,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52437543",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72099790,
        "Question_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Question_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651582909370,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":72,
        "Owner_creation_time":1606642099553,
        "Owner_last_access_time":1654258609767,
        "Owner_location":null,
        "Owner_reputation":371,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":1651583638130,
        "Answer_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651629562457,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72099790",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66545743,
        "Question_title":"SageMaker Studio & EFS",
        "Question_body":"<p>I heard Sagemaker Studio automatically creates EFS on home directory.<\/p>\n<ol>\n<li>What is the size of EFS home directory?<\/li>\n<li>Is it possible to resize its size?<\/li>\n<\/ol>\n<p>I'm a total beginner on AWS, so I'd happy if somebody answer to my question.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615288045900,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|amazon-efs",
        "Question_view_count":435,
        "Owner_creation_time":1574863313540,
        "Owner_last_access_time":1663835483540,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1631631113103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66545743",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57713515,
        "Question_title":"Outputting multiple csv files from a SageMaker batch prediction job",
        "Question_body":"<p>I am working on a AWS SageMaker (SKlearn) batch transform job, in which the prediction data is big and therefore I'm required to use mini-batches (where the input .csv is split up into smaller .csv files).<\/p>\n\n<p>I have this working and outputting a .csv file with the ids and the predictions. However I am attempting to implement a way in which I can have a total of three output files from the batch transform job - which are different .csv files each aggregated in a slightly different way.<\/p>\n\n<p>My issue is I am not sure how to instruct SageMaker to output multiple files. I have tried the following code as the prediction method submitted in the <code>entry_point<\/code> file:<\/p>\n\n<pre><code>def output_fn(prediction, accept):\n    output_one = prepare_one(prediction)\n    output_two, output_three = prepare_others(output_one)\n    return output_one, output_two, output_three\n<\/code><\/pre>\n\n<p>Couple ideas\/issues I am currently working with:<\/p>\n\n<ul>\n<li>I think the batching strategy will cause issue. As the additional outputs are aggregations on the total predictions but SageMaker will treat each mini-batch separately (I assume?) <\/li>\n<li>Can I simply use <code>boto3<\/code> and save extra files using that and treat the SageMaker output as only <code>output_one<\/code><\/li>\n<\/ul>\n\n<p>Any help would be much appreciated<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1567092855690,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|csv|amazon-sagemaker",
        "Question_view_count":1650,
        "Owner_creation_time":1442932369407,
        "Owner_last_access_time":1662303207090,
        "Owner_location":"Belfast",
        "Owner_reputation":1682,
        "Owner_up_votes":165,
        "Owner_down_votes":4,
        "Owner_views":164,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57713515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73531632,
        "Question_title":"Error when using custom precision metric in Keras",
        "Question_body":"<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\nmodel = Sequential()\nmodel.add(Dense(90, input_dim=900, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(90, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nm = keras.metrics.Precision(class_id=1)\n# example data of suitable dimension, to offer MRE to SO\nX_train = np.eye(900)\nY_train = np.ones((900, 1))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[m])\nmodel.fit(X_train, Y_train, epochs=50, batch_size=1000)\n<\/code><\/pre>\n<p>When I use\n<code>model.compile(loss='binary_crossentropy, optimizer='adam', metrics=['Precision'])<\/code><\/p>\n<p>it works, but when I use Precision(class_id=1) (regardless of whether I substitute it as a variable), I get<\/p>\n<pre><code>ValueError: slice index 1 of dimension 1 out of bounds. \nfor '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=1, end_mask=0, \nnew_axis_mask=0, shrink_axis_mask=2](Cast_1, strided_slice_1\/stack, strided_slice_1\/stack_1, strided_slice_1\/stack_2)' \nwith input shapes: [?,1], [2], [2], [2] and with computed \ninput tensors: input[1] = &lt;0 1&gt;, input[2] = &lt;0 2&gt;, input[3] = &lt;1 1&gt;.\n<\/code><\/pre>\n<p>I don't know what any of this stuff means. Slice of WHAT is out of bounds? (I defined X_train and Y_train, of course, and they work when I just write metric=['Precision']).\nFYI I'm doing this in SageMaker, if it makes any difference. There is no other code, so if I am failing to define some config thing, I don't know about that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1661789441867,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1384307626763,
        "Owner_last_access_time":1663956887833,
        "Owner_location":"Bay Area, CA, USA",
        "Owner_reputation":109,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1661792615750,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73531632",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70873792,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_time":1597858315077,
        "Owner_last_access_time":1663976401100,
        "Owner_location":null,
        "Owner_reputation":84,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1644424499010,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644528409280,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656241190023,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_time":1420286650807,
        "Owner_last_access_time":1663841459487,
        "Owner_location":"Trondheim, Norway",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Question_last_edit_time":1656243378470,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1656410088380,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657181121617,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65721061,
        "Question_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Question_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1610635167150,
        "Question_score":2,
        "Question_tags":"amazon-web-services|terraform|state-machine|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1209,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1611071837133,
        "Answer_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Answer_comment_count":23.0,
        "Answer_creation_time":1610637215397,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1610641519770,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68044070,
        "Question_title":"How to customise AWS Textract?",
        "Question_body":"<p>So far my Textract tests are very impressive for handwriting, but I see sometimes it fails to recognise some forms and some values. Is it possible to train it? If I'm scanning the same type of form\/document it will be very useful to amend the results and teaching it where the boundaries of some form elements lie and some key-value associations as well?<\/p>\n<p>It will be a real deal breaker for the kind of service I'm trying to design.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624082227563,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|text-extraction|text-recognition|amazon-textract",
        "Question_view_count":808,
        "Owner_creation_time":1575488323143,
        "Owner_last_access_time":1661765619090,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1624091060687,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68044070",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63536595,
        "Question_title":"Connect Sagemaker to RDS db",
        "Question_body":"<p>I am new to AWS. All I know is that the Postgre database is hosted in AWS RDS. I want to build an ML model using AWS Sagemaker. I am not sure how to get the data from AWS RDS so that I can use it for building the ML model.\nI will be thankful for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598102199987,
        "Question_score":2,
        "Question_tags":"amazon-rds|amazon-sagemaker",
        "Question_view_count":2550,
        "Owner_creation_time":1598101911760,
        "Owner_last_access_time":1662277289103,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1627652354893,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63536595",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69782294,
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635629350997,
        "Question_score":1,
        "Question_tags":"python|json|amazon-sagemaker",
        "Question_view_count":248,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1635655049277,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635633656723,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635655742577,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50032795,
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1524699817877,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1298484007147,
        "Owner_last_access_time":1664045053900,
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":9271,
        "Owner_up_votes":2074,
        "Owner_down_votes":44,
        "Owner_views":1819,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1525129267447,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73830782,
        "Question_title":"How to convert bert model output to json?",
        "Question_body":"<p>I have fine-tuned a Bert model and testing my output from different layers. I tested this in sagemaker , with my own custom script (see below) and the output i get is of BaseModelOutputWithPoolingAndCrossAttentions class. How can i convert the output of this , specially the tensor values from the last_hidden_state to json?<\/p>\n<p>inference.py<\/p>\n<pre><code>\nfrom transformers import BertModel, BertConfig\n\ndef model_fn():\n\n   config = BertConfig.from_pretrained(&quot;xxx&quot;, output_hidden_states=True)\n   model = BertModel.from_pretrained(&quot;xxx&quot;, config=config)\n\n....\ndef predict_fn():\n    ....\n\n    return model(inputs)\n\n<\/code><\/pre>\n<p>model output<\/p>\n<pre><code>BaseModelOutputWithPoolingAndCrossAttentions( \nlast_hidden_state=\ntensor([[[-1.6968,  1.9364, -2.1796, -0.0819,  1.8027,  0.3540,  1.3269,  0.1532],\n        [-0.4969,  0.4169,  0.5677,  1.0968,  0.0742,  1.5354,  0.9387,  0.0343]]])\ndevice='cuda:0', grad_fn=&lt;NativeLayerNormBackward&gt;), \nhidden_states=None,\nattentions=None, \n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663951602360,
        "Question_score":0,
        "Question_tags":"python|json|tensorflow|amazon-sagemaker|bert-language-model",
        "Question_view_count":21,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73830782",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61152276,
        "Question_title":"ImportError: No module named cv2 when run Batch transform jobs in SageMaker",
        "Question_body":"<p>When I tried to run a Batch transform job in AWS SageMaker, I met below error: <\/p>\n\n<p>ImportError: No module named cv2<\/p>\n\n<p>Please note that, I am able to \"import CV2\" in the notebook instance. The jupter can run \"import CV2\" in notebook instance. But failed to run it in endpoints during inference time. I have tried below method using \"env\" as the link <a href=\"https:\/\/stackoverflow.com\/questions\/51117133\/aws-sagemaker-install-external-library-and-make-it-persist\">AWS Sagemaker - Install External Library and Make it Persist<\/a> <\/p>\n\n<p>but it still not work. <\/p>\n\n<p>anyone have good way to solve it? Thanks! <\/p>\n\n<p>my codes are: <\/p>\n\n<pre><code>env = {\n'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nimage_embed_model = MXNetModel(model_data=model_data,\n                         entry_point='sagemaker_entrypoint.py',\n                        role=role,\n                        source_dir = 'src',\n                        env = env,\n                        py_version='py3',\n                        framework_version='1.6.0')\n\ntransformer = image_embed_model.transformer(instance_count=1, # Please pay attention here!!!\n                                    instance_type='ml.m4.xlarge',\n                                    output_path=output_path,\n                                    assemble_with = 'Line', \n                                    accept = 'text\/csv'\n                                   )\ntransformer.transform(batch_input,\n                  content_type='text\/csv', \n                  split_type='Line',\n                  input_filter='$[0:]',\n                  join_source='Input',\n                  wait=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586580706220,
        "Question_score":1,
        "Question_tags":"python-import|batch-processing|endpoint|cv2|amazon-sagemaker",
        "Question_view_count":363,
        "Owner_creation_time":1525455745547,
        "Owner_last_access_time":1593301633933,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61152276",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57151638,
        "Question_title":"What is AWS SageMaker mAP referring to?",
        "Question_body":"<p>I'm running a model on AWS SageMaker, using their example object detection Jupyter notebook (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb<\/a>).  In the results it gives the following:<\/p>\n\n<p>validation mAP =(0.111078678154)<\/p>\n\n<p>I was wondering what this mAP score is referring to? <\/p>\n\n<p>I've used tensorflow, where it gives an averaged mAP(averages from .5IoU to .95IoU with .05 increments), mAP@.5IoU, mAP@.75IoU.  I've checked the documents on SageMaker, but cannot find anything referring to what the definition of mAP is.<\/p>\n\n<p>Is it safe to assume that the mAP score SageMaker reports is the \"averaged mAP(averages from .5IoU to .95IoU with .05 increments)\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563819303397,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":290,
        "Owner_creation_time":1563817494337,
        "Owner_last_access_time":1564527049533,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57151638",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72272952,
        "Question_title":"Loading Larga data to amazon sagemaker notebook",
        "Question_body":"<p>I have 2 folder, on each folder I have 70 csv files each one with a size of 3mb to 5mb, so in general the data is like 20 millions rows with 5 columns each.<\/p>\n<p>I used amazon wrangler s3.read_csv to load just one folder with all the 70 csv to a dataframe, not sure if this is a good approach due to the fact the data is really large.<\/p>\n<p>I want to know how can I load the entire csv files from those 2 folders with aws wrangler s3.readcsv, or should I use pyspark?<\/p>\n<p>Also another question is, is it possible to work locally using amazon sagemaker depenencies? I am not sure if using sagemaker notebook for the pipeline development might cost a lot for my client.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652785626633,
        "Question_score":0,
        "Question_tags":"pandas|amazon-web-services|pyspark|amazon-sagemaker|large-data",
        "Question_view_count":163,
        "Owner_creation_time":1587087081620,
        "Owner_last_access_time":1664023997577,
        "Owner_location":"Caracas, Venezuela",
        "Owner_reputation":113,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":1652794238627,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72272952",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65110736,
        "Question_title":"SageMaker Estimator fit job never ends",
        "Question_body":"<p>I have the following code<\/p>\n<pre><code>estimator = Estimator(                                                     \n    image_uri=ecr_image,                                                   \n    role=role,                                                             \n    instance_count=1,                                                      \n    instance_type=instance_type,                                           \n    hyperparameters=hyperparameters                                        \n)                                                                          \n\nestimator.fit({&quot;training&quot;: &quot;s3:\/\/&quot; + sess.default_bucket() + &quot;\/&quot; + prefix})\n<\/code><\/pre>\n<p>which seems to run smoothly until it is stuck at:<\/p>\n<pre><code>Finished Training\n2020-12-02 15:00:45,352 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n<\/code><\/pre>\n<p>and I see InProgress job in AWS SageMaker console. How can I fix this?<\/p>\n<p>I use <code>763104351884.dkr.ecr.us-west-2.amazonaws.com\/pytorch-inference-eia:1.3.1-cpu-py36-ubuntu16.04<\/code> Docker image with <code>pip install sagemaker-training<\/code> added.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1606921724283,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":252,
        "Owner_creation_time":1382978984190,
        "Owner_last_access_time":1664012645480,
        "Owner_location":null,
        "Owner_reputation":1311,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":1606922323817,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65110736",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57655516,
        "Question_title":"Set Spark version for Sagemaker on Glue Dev Endpoint",
        "Question_body":"<p>To create my Glue scripts, I use development endpoints with Sagemaker notebooks that run the Pyspark (Sparkmagic) kernel.\nThe latest version of Glue (version 1.0) supports Spark 2.4. However, my Sagemaker notebook uses Spark version 2.2.1. \nThe function I want to test only exists as of Spark 2.3. \nIs there a way to solve this mismatch between the dev endpoint and the Glue job? Can I somehow set the Spark version of the notebook?<br>\nI couldn't find anything in the documentation.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566812956087,
        "Question_score":2,
        "Question_tags":"amazon-web-services|apache-spark|aws-glue|amazon-sagemaker",
        "Question_view_count":642,
        "Owner_creation_time":1505381376317,
        "Owner_last_access_time":1639478033857,
        "Owner_location":null,
        "Owner_reputation":68,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. <\/p>\n\n<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.<\/p>\n\n<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1566968757933,
        "Answer_score":5.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1567004276930,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57655516",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53423061,
        "Question_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Question_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1542853625410,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":8160,
        "Owner_creation_time":1319234288810,
        "Owner_last_access_time":1663375872053,
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1543010904777,
        "Answer_score":8.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1620293748347,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61842665,
        "Question_title":"Unable to export Core ML model in Turicreate",
        "Question_body":"<p>I used AWS Sagemaker with Jupyter notebook to train my Turicreate model. It trained successfully but I'm unable to export it to a CoreML model. It shows the below error. I've tried various kernels in the Jupyter notebook with the same result. Any ideas on how to fix this error?<\/p>\n\n<p>turicreate 5.4\nGPU: mxnet-cu100<\/p>\n\n<pre><code>KeyError  Traceback (most recent call last)\n&lt;ipython-input-6-3499bdb76e06&gt; in &lt;module&gt;()\n  1 # Export for use in Core ML\n----&gt; 2 model.export_coreml('pushupsTC.mlmodel')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/turicreate\/toolkits\/object_detector\/object_detector.py in export_coreml(self, filename,     include_non_maximum_suppression, iou_threshold, confidence_threshold)\n1216         assert (self._model[23].name == 'pool5' and\n1217                 self._model[24].name == 'specialcrop5')\n-&gt; 1218         del net._children[24]\n1219         net._children[23] = op\n1220 \n\nKeyError: 24\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1589658289300,
        "Question_score":1,
        "Question_tags":"python-3.x|jupyter-notebook|amazon-sagemaker|coreml|turi-create",
        "Question_view_count":102,
        "Owner_creation_time":1359631245423,
        "Owner_last_access_time":1664034335447,
        "Owner_location":null,
        "Owner_reputation":363,
        "Owner_up_votes":1041,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61842665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73708836,
        "Question_title":"EntityTooLarge ERROR while fitting RCF data in Sagemaker",
        "Question_body":"<p>I am fitting a Random Cut Forest model on AWS SageMaker to a dataset using &quot; rcf.fit(rcf.record_set(data[['Variable 1','Variable 2']].values.reshape(-1, 1)))&quot; , but getting the below error :<\/p>\n<p>An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.<\/p>\n<p>The size of the ndarray is 239393964<\/p>\n<p>Works well for a sample of the data, but not working for the entire data set (total records : 400M)<\/p>\n<p>How can I fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1663100723627,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|anomaly-detection",
        "Question_view_count":13,
        "Owner_creation_time":1663100428383,
        "Owner_last_access_time":1663968770527,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73708836",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58050712,
        "Question_title":"Problem deploying the best estimator gotten with sagemaker.estimator.Estimator (w\/ sklearn custom image)",
        "Question_body":"<p>After creating SKLearn() instance and using HyperparamaterTuner with a few hyperparameter ranges, I get the best estimator. When I try to deploy() the estimator, it gives an error in the log. Exactly same error happens when I create transformer and call transform on it(). Doesn't deploy and doesn't transform. What could be the problem and at least how could I possibly narrow down the problem? <\/p>\n\n<p>I have no idea how to even begin to figure this out. Googling didn't help. Nothing comes up. <\/p>\n\n<p>Creating SKLearn instance: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn = SKLearn(\n    entry_point=script_path,\n    train_instance_type=\"ml.c4.xlarge\",\n    role=role,\n    sagemaker_session=session,\n    hyperparameters={'model': 'rfc'})\n<\/code><\/pre>\n\n<p>Putting tuner to work: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuner = HyperparameterTuner(estimator = sklearn,\n                            objective_metric_name = objective_metric_name,\n                            objective_type = 'Minimize',\n                            metric_definitions = metric_definitions,\n                            hyperparameter_ranges = hyperparameters,\n                            max_jobs = 3, # 9,\n                            max_parallel_jobs = 4)\n\ntuner.fit({'train': s3_input_train})\ntuner.wait()\nbest_training_job = tuner.best_training_job()\nthe_best_estimator = sagemaker.estimator.Estimator.attach(best_training_job)\n<\/code><\/pre>\n\n<p>This gives a valid best training job. Everything seems great. <\/p>\n\n<p>Here is where the problem manifests: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>predictor = the_best_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n<\/code><\/pre>\n\n<p>or the following (triggers exactly same problem): <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>rfc_transformer = the_best_estimator.transformer(1, instance_type=\"ml.m4.xlarge\")\nrfc_transformer.transform(test_location)\nrfc_transformer.wait()\n<\/code><\/pre>\n\n<p>Here is the log with the error message (it reiterates the same error many times while trying to deploy or transform; here is the beginning of the log):<\/p>\n\n<p>................[2019-09-22 09:17:48 +0000] [17] [INFO] Starting gunicorn 19.9.0<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [17] [INFO] Listening at: unix:\/tmp\/gunicorn.sock (17)<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [17] [INFO] Using worker: gevent<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [24] [INFO] Booting worker with pid: 24<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [25] [INFO] Booting worker with pid: 25<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [26] [INFO] Booting worker with pid: 26<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [30] [INFO] Booting worker with pid: 30<\/p>\n\n<p>2019-09-22 09:18:15,061 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)<\/p>\n\n<p>2019-09-22 09:18:15,062 INFO - sagemaker_sklearn_container.serving - Encountered an unexpected error.<\/p>\n\n<p>[2019-09-22 09:18:15 +0000] [24] [ERROR] Error handling request \/ping<\/p>\n\n<p>Traceback (most recent call last):<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 56, in handle self.handle_request(listener_name, req, client, addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/ggevent.py\", line 160, in handle_request addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 107, in handle_request respiter = self.wsgi(environ, resp.start_response)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 119, in main user_module_transformer = import_module(serving_env.module_name, serving_env.module_dir)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 97, in import_module user_module = importlib.import_module(module_name)<\/p>\n\n<p>File \"\/usr\/lib\/python3.5\/importlib\/<strong>init<\/strong>.py\", line 117, in import_module if name.startswith('.'):<\/p>\n\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n\n<p>169.254.255.130 - - [22\/Sep\/2019:09:18:15 +0000] \"GET \/ping HTTP\/1.1\" 500 141 \"-\" \"Go-http-client\/1.1\"<\/p>\n\n<p>2019-09-22 09:18:15,178 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)<\/p>\n\n<p>2019-09-22 09:18:15,179 INFO - sagemaker_sklearn_container.serving - Encountered an unexpected error.<\/p>\n\n<p>[2019-09-22 09:18:15 +0000] [30] [ERROR] Error handling request \/ping<\/p>\n\n<p>Traceback (most recent call last):<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 56, in handle self.handle_request(listener_name, req, client, addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/ggevent.py\", line 160, in handle_request addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 107, in handle_request respiter = self.wsgi(environ, resp.start_response)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 119, in main user_module_transformer = import_module(serving_env.module_name, serving_env.module_dir)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 97, in import_module user_module = importlib.import_module(module_name)<\/p>\n\n<p>File \"\/usr\/lib\/python3.5\/importlib\/<strong>init<\/strong>.py\", line 117, in import_module if name.startswith('.'):<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1569166367840,
        "Question_score":0,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1405394458213,
        "Owner_last_access_time":1574036481927,
        "Owner_location":"Lakewood, NJ, USA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1569166762703,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58050712",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652282728443,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1489685785577,
        "Owner_last_access_time":1663881721683,
        "Owner_location":"Canada",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1652283812907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71120471,
        "Question_title":"How to determine size of images available in aws?",
        "Question_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644892148237,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ecr",
        "Question_view_count":153,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645569051340,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70469115,
        "Question_title":"Naming a Sagemaker Processing job using Sagemaker Pipelines ProcessingStep",
        "Question_body":"<p>I am running a Sagemaker Pipeline with the current processor:<\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\n\n\nframework_version = &quot;0.23-1&quot;\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=framework_version,\n    instance_type=processing_instance_type,\n    instance_count=processing_instance_count,\n    base_job_name=&quot;pre-processing-job-name&quot;,\n    role=role\n)\n<\/code><\/pre>\n<p>and the processing step is:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.workflow.steps import ProcessingStep\n\n\nstep_process = ProcessingStep(\n    name=&quot;AbaloneProcess&quot;,\n    processor=sklearn_processor,\n    inputs=[\n        ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;validation&quot;, source=&quot;\/opt\/ml\/processing\/validation&quot;),\n        ProcessingOutput(output_name=&quot;test&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    code=&quot;abalone\/preprocessing.py&quot;,\n)\n<\/code><\/pre>\n<p>It looks like the base_job_name does nothing, because the processing job that is created is <code>pipelines-o6e2jn38g05j-AbaloneProcess-nc2OlXF8jA<\/code>.<\/p>\n<p>I want the processing job name to be defined manually. Does Sagemaker pipelines support this? I seem to be going around in circles.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640313935627,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|data-science|amazon-sagemaker",
        "Question_view_count":417,
        "Owner_creation_time":1640313343617,
        "Owner_last_access_time":1643745786273,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1640332349443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70469115",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73408571,
        "Question_title":"AWS secret manager access from Sagemaker ECR container",
        "Question_body":"<p>I have a secret stored in AWS Secret Manager.\nI am trying to access that secret from a container which is in ECR.<\/p>\n<p>When I execute the container, the error message I get is:<\/p>\n<pre><code>  File &quot;\/opt\/program\/train&quot;, line 65, in get_secret\n    SecretId=secret_name\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 895, in _make_api_call\n    operation_model, request_dict, request_context\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 917, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 116, in make_request\n    return self._send_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 195, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    operation_name=operation_model.name,\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>The container is using python and I am using the same function given as an example when I setup the secret manager entry for python3\nThe same function works well in my local environment where I am authenticated via CLI.<\/p>\n<p>The function is:<\/p>\n<pre><code>import boto3\nimport base64\nfrom botocore.exceptions import ClientError\n\n\ndef get_secret():\n\n    secret_name = &quot;secret1&quot;\n    region_name = &quot;us-east-1&quot;\n\n    # Create a Secrets Manager client\n    session = boto3.session.Session()\n    client = session.client(\n        service_name='secretsmanager',\n        region_name=region_name\n    )\n\n    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n    # See https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/apireference\/API_GetSecretValue.html\n    # We rethrow the exception by default.\n\n    try:\n        get_secret_value_response = client.get_secret_value(\n            SecretId=secret_name\n        )\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'DecryptionFailureException':\n            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n            # An error occurred on the server side.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidParameterException':\n            # You provided an invalid value for a parameter.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidRequestException':\n            # You provided a parameter value that is not valid for the current state of the resource.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n            # We can't find the resource that you asked for.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n    else:\n        # Decrypts secret using the associated KMS key.\n        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n        if 'SecretString' in get_secret_value_response:\n            secret = get_secret_value_response['SecretString']\n        else:\n            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n            \n    # Your code goes here. \n<\/code><\/pre>\n<p>But in the docker container I have not performed a CLI authentication. I have given the sagemaker access role the KMS permissions for my region.\nAny thoughts on how I can get a sagemaker container to access secret manager?\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1660851395430,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|secretsmanager",
        "Question_view_count":25,
        "Owner_creation_time":1458196224610,
        "Owner_last_access_time":1661699962880,
        "Owner_location":null,
        "Owner_reputation":981,
        "Owner_up_votes":64,
        "Owner_down_votes":1,
        "Owner_views":74,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73408571",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71553217,
        "Question_title":"Unable to access data from S3 bucket to jupyter notebook of aws sagemaker",
        "Question_body":"<p>I need to train a model on aws sagemaker. I'm unable to access data in Jupiter notebook of sagemaker from S3 bucket. My bucket name is &quot;riceleaf&quot; there are four folders in the bucket named as s1,s2,s3,s4 and each folder contains 330 images named as 1.jpg and so on. It is created in Us-east zone. Bucket is private.<\/p>\n<p>One way i did was to access the object and when i displayed the key it shows me 1.jpg and so on. But when i try to open that image it didn't work. So i think I'm unable to get exact data path.<\/p>\n<p>In my code I need exact data path since I'm doing some random data generation in the code so need to access different folders. Therefore, I need a path till bucket so i can change next folder name and image name randomly in my code.<\/p>\n<p>Please help me to so that I can access the images in the Jupiter notebook of sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1647841162080,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|deep-learning|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":733,
        "Owner_creation_time":1647840996453,
        "Owner_last_access_time":1650607384703,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1647863096850,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71553217",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61626685,
        "Question_title":"SageMaker Estimator from EC2",
        "Question_body":"<p>I have a really big confuse about how AWS integrate Docker ECR with SageMaker, even though it works from sagemaker using just the Dockerfile and train.py script executing it from another script which uses the Estimator class, it doesn't from EC2<\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nimport sagemaker \n\nestimator = Estimator(\n  image_name=\"test_docker\",\n  role='arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXXX',\n  train_instance_count=1,\n  train_instance_type='local'\n)\nestimator.fit()\n<\/code><\/pre>\n\n<p>For this, I have the same folder with Dockerfile and train.py script and created another script called exec.py, this script has the estimator code, but when I execute it I get this error<\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value \n'test_docker-2020-05-06-02-50-56-375' at 'trainingJobName' failed to satisfy constraint: Member must \nsatisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588734515670,
        "Question_score":1,
        "Question_tags":"python|amazon-ec2|amazon-sagemaker|amazon-ecr",
        "Question_view_count":72,
        "Owner_creation_time":1498690621233,
        "Owner_last_access_time":1624591691807,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61626685",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70578172,
        "Question_title":"Can we use data directly from RDS or df as a data source for training job in Sagemaker, rather than pulling it from from s3 or EFS?",
        "Question_body":"<p>I am using Sagemaker platform for model development and deployment. Data is read from RDS tables and then spitted to train and test df.\nTo create the training job in Sagemaker, I found that it takes data source only as s3 and EFS. For that I need to keep train and test data back to s3, which is repeating the data storing process in RDS and s3.\nI would want to directly pass the df from RDS as a parameter in tarining job code. Is there any way we can pass df in fit method<\/p>\n<pre><code>    image=&quot;581132636225.dkr.ecr.ap-south-1.amazonaws.com\/sagemaker-ols-model:latest&quot;\n    model_output_folder = &quot;model-output&quot;\n    print(image)\n    tree = sagemaker.estimator.Estimator(\n        image,\n        role,\n        1,\n        &quot;ml.c4.2xlarge&quot;,\n        output_path=&quot;s3:\/\/{}\/{}&quot;.format(sess.default_bucket(), model_output_folder),\n        sagemaker_session=sess,\n    )\n\n**tree.fit({'train': &quot;s3_path_having_test_data&quot;}, wait=True)**\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1641296123900,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|data-science|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_time":1545906764167,
        "Owner_last_access_time":1643787536800,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1641316932063,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70578172",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68660085,
        "Question_title":"Install Voila on SageMaker based Jupyter Notebook",
        "Question_body":"<p>I'm trying to run the Voila! server on a SageMaker Notebook.<\/p>\n<p>Commands to install and enable voila:<\/p>\n<pre><code>!pip install voila\n!jupyter serverextension enable --sys-prefix voila\n<\/code><\/pre>\n<p>The pip command appears to work OK.<\/p>\n<p>Terminal output from the jupyter command:<\/p>\n<blockquote>\n<p>Config option <code>kernel_spec_manager_class<\/code> not recognized by <code>EnableServerExtensionApp<\/code>.\nEnabling: voila<\/p>\n<ul>\n<li>Writing config: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/etc\/jupyter\n<ul>\n<li>Validating...\nvoila 0.2.10 OK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/blockquote>\n<p>Navigating to a 'voila url' such as:<\/p>\n<pre><code>https:\/\/{instance}.notebook.{region}.sagemaker.aws\/voila\/sandbox\/test-notebook.ipynb\n<\/code><\/pre>\n<p>Returns a 404.<\/p>\n<p>The <em><strong>'nbextensions'<\/strong><\/em> tab shows the voila extension as 'possibly incompatible'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I can't find anything in the Jupyter logs that looks relevant.<\/p>\n<p>Restarting Jupyter didn't help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1628132271717,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|voila",
        "Question_view_count":247,
        "Owner_creation_time":1415582465687,
        "Owner_last_access_time":1662954300267,
        "Owner_location":"Australia",
        "Owner_reputation":2899,
        "Owner_up_votes":63,
        "Owner_down_votes":3,
        "Owner_views":247,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68660085",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73707074,
        "Question_title":"AWS SageMaker Domain Status \"Update_Failed\" due to custom image appImageConfigName error",
        "Question_body":"<p>I'm having some trouble recovering from failures in attaching custom images to my sagemaker domain.<\/p>\n<p>I first created a custom image according to <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-a-custom-image-to-bring-your-own-development-environment-to-rstudio-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When I use sagemaker console to attach the image built with sm-docker, it appears to successfully &quot;attach&quot; in the domain's image list, but when inspecting the image in the console, it shows an error:<\/p>\n<blockquote>\n<p>Value '' at 'appImageConfigName' failed to satisfy constraint: Member\nmust satisfy regular expression pattern<\/p>\n<\/blockquote>\n<p>This occurs even when the repository or tag are comprised of only alphanumeric characters.<\/p>\n<p>After obtaining this error, I deleted the repositories in ECR.<\/p>\n<p>Since then, the domain fails to update and I am unable to launch any apps or attempt to attach additional images.<\/p>\n<p>The first issue I would like to address is restoring functionality of my sagemaker domain so I can further troubleshoot the issue. I am unable to delete the domain because of this error, even when there are no users, apps, or custom images associated with the domain.<\/p>\n<p>The second issue I would like to address is being able troubleshoot the appImageConfigName error.<\/p>\n<p>Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzrrA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzrrA.png\" alt=\"image errors\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/EyGV1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EyGV1.png\" alt=\"domain status\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663090122220,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":20,
        "Owner_creation_time":1340897566210,
        "Owner_last_access_time":1664049615040,
        "Owner_location":null,
        "Owner_reputation":547,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1663092028973,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73707074",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71054340,
        "Question_title":"OCR in Sage Maker",
        "Question_body":"<p>Guys i am trying to build infrastructure on aws for getting help from others on annotation. currently we uses label-studio for text annotation. as might know you can label text by selecting through polygon and than writing what does selected area mean. ex: if polygon is made around english word than what writing out label  of it to annotate that given english word. for more see image below.<a href=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How can i do this in <strong>SageMaker Ground Truth<\/strong>. as far as i have gone i think it can just label pre defined words. you cant create custom label in it by selecting any given area using polygon in image am i right ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644428834663,
        "Question_score":1,
        "Question_tags":"amazon-web-services|ocr|amazon-sagemaker|text-classification",
        "Question_view_count":67,
        "Owner_creation_time":1482044943533,
        "Owner_last_access_time":1664036901133,
        "Owner_location":"Gurgaon, Haryana, India",
        "Owner_reputation":549,
        "Owner_up_votes":19,
        "Owner_down_votes":14,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71054340",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59921196,
        "Question_title":"Can you load a SageMaker trained model into Keras?",
        "Question_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580061744520,
        "Question_score":1,
        "Question_tags":"keras|object-detection|amazon-sagemaker",
        "Question_view_count":212,
        "Owner_creation_time":1533465584553,
        "Owner_last_access_time":1663925167400,
        "Owner_location":"Bucharest, Romania",
        "Owner_reputation":341,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1580119883430,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73041737,
        "Question_title":"Print SageMaker instance type",
        "Question_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658254476363,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":40,
        "Owner_creation_time":1657050754840,
        "Owner_last_access_time":1663796977727,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1658271130893,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71492181,
        "Question_title":"Memory allocation error Call to XGBoost C function XGBoosterUpdateOneIter failed: std::bad_alloc",
        "Question_body":"<p>Working with Julia notebook on Sagemaker:\n<code>ml.m5d.24xlarge<\/code> with <code>500GB<\/code> memory.<\/p>\n<p>I'm training an XGBoost with 230 features (500MB per file on avg). It trains without an issue upto 205 files, but afterwards, randomly I get this error<\/p>\n<pre><code>&gt; \u250c Info: Starting XGBoost training\n\u2514   num_boost_rounds = 99\nERROR: LoadError: Call to XGBoost C function XGBoosterUpdateOneIter failed: std::bad_alloc\nStacktrace:\n  [1] error(::String, ::String, ::String, ::String)\n    @ Base .\/error.jl:42\n  [2] XGBoosterUpdateOneIter(handle::Ptr{Nothing}, iter::Int32, dtrain::Ptr{Nothing})\n    @ XGBoost ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_wrapper_h.jl:11\n  [3] #update#21\n    @ ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_lib.jl:204 [inlined]\n  [4] xgboost(data::XGBoost.DMatrix, nrounds::Int64; label::Type, param::Vector{Any}, watchlist::Vector{Any}, metrics::Vector{String}, obj::Type, feval::Type, group::Vector{Any}, kwargs::Base.Iterators.Pairs{Symbol, Any, NTuple{15, Symbol}, NamedTuple{(:objective, :num_class, :num_parallel_tree, :eta, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :lambda, :alpha, :tree_method, :grow_policy, :max_leaves), Tuple{String, Int64, Int64, Float64, Float64, Int64, Int64, Int64, Float64, Float64, Int64, Int64, String, String, Int64}}})\n    @ XGBoost ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_lib.jl:185\n  [5] macro expansion\n    @ \/home\/src\/Training.jl:175 [inlined]\n  [6] macro expansion\n    @ .\/timing.jl:210 [inlined]\n<\/code><\/pre>\n<p>Not sure how to fix it. The AWS instance has maximum CPU memory.\nAlso, already using 99 procs\/workers.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647408688790,
        "Question_score":2,
        "Question_tags":"jupyter-notebook|julia|xgboost|amazon-sagemaker",
        "Question_view_count":144,
        "Owner_creation_time":1348471761127,
        "Owner_last_access_time":1663922173097,
        "Owner_location":null,
        "Owner_reputation":343,
        "Owner_up_votes":95,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71492181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70149158,
        "Question_title":"Security Group settings for using sagemaker notebooks in private subnet",
        "Question_body":"<p>I am new to sagemaker, and am hoping to use sagemaker in a VPC with a private subnet, so data accessed from s3 is not exposed to public internet.<\/p>\n<p>I have created a vpc with a private subnet (no internet or nat gateway), and have attached a vpc s3 gateway endpoint - with this, can I apply the subnet's <strong>default<\/strong> security group settings to the sagemaker notebook instances? ..or are some additional configurations to this required?<\/p>\n<p>Also, I'm hoping to keep internet access for the sagemaker notebook instance, so I can still download python packages (but just wanting to ensure data read from s3 using the private subnet is all okay with its default security group)<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1638152715770,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-vpc|amazon-sagemaker|aws-security-group|private-subnet",
        "Question_view_count":919,
        "Owner_creation_time":1508838702423,
        "Owner_last_access_time":1648367115880,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":1638177719560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70149158",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65497571,
        "Question_title":"got warning No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3. and no model deployed on aws",
        "Question_body":"<pre><code>n_user=20\nn_item=100\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;-M&quot;, &quot;--model_dir&quot;, help=&quot;show model&quot;)\nparser.add_argument(&quot;-f&quot;, &quot;--file&quot;, help=&quot;show model help&quot;)\nargs = parser.parse_args()\nmodel_dir='\/opt\/ml\/model'\nfrom sagemaker.tensorflow import TensorFlow\nncf_estimator = TensorFlow(  \n    entry_point='ncf.py',\n    role=sagemaker.get_execution_role(),  \n    train_instance_count=1,\n    train_instance_type='ml.c5.2xlarge',  \n    framework_version='2.1.0',  \n    py_version='py3',  \n    distributions={'parameter_server': {'enabled': True}},  \n    hyperparameters={'epochs': 3, 'batch_size': 256, 'n_user': n_user, 'n_item': n_item},\n)\n<\/code><\/pre>\n<p>I have the following code on Amazon Sagemaker and it is giving me error to define a model_dir, i tried different ways but failed please help me with that.<\/p>\n<p>and the error when i train the model is:<\/p>\n<pre><code>2020-12-29 19:38:02,906 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n<p>I have seen the docs but couldn't get anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1609269727237,
        "Question_score":0,
        "Question_tags":"amazon-web-services|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":174,
        "Owner_creation_time":1584447559510,
        "Owner_last_access_time":1620476210433,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":46,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1609280453090,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65497571",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73294393,
        "Question_title":"AWS SageMaker: could not load debugger information of estimator",
        "Question_body":"<p>I am using a SageMaker notebook in AWS for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:<\/p>\n<pre><code>from sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=&quot;CrossEntropyLoss_output_0&quot;, parameters={\n    &quot;include_regex&quot;: &quot;CrossEntropyLoss_output_0&quot;, &quot;train.save_interval&quot;: &quot;100&quot;,&quot;eval.save_interval&quot;: &quot;10&quot;})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=&quot;ml.m5.xlarge&quot;,\n#instance_type=&quot;ml.g4dn.2xlarge&quot;,\nentry_point=&quot;train.py&quot;,\nframework_version=&quot;1.8&quot;,\npy_version=&quot;py36&quot;,\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({&quot;training&quot;: inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n<\/code><\/pre>\n<p>After the kernel died, I attached the estimator and tried to access the debugging information of the training.<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n<\/code><\/pre>\n<p>The return values of these functions were None.\nCould this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660059012287,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|pytorch|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_time":1501430845497,
        "Owner_last_access_time":1663570204107,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73294393",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68386650,
        "Question_title":"Can one use Terraform templates with SageMaker ML Pipelines?",
        "Question_body":"<p>SageMaker has ML Pipelines that come with \u201cML templates\u201d, which I assume are Cloud Formation templates for machine learning pipelines.<\/p>\n<p>Can one use custom Terraform templates instead of Cloud Formation? Where does one place the Terraform templates? Can this be done through the SageMaker UI?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1626310980797,
        "Question_score":0,
        "Question_tags":"machine-learning|terraform|amazon-cloudformation|amazon-sagemaker|aws-codepipeline",
        "Question_view_count":1267,
        "Owner_creation_time":1346443720090,
        "Owner_last_access_time":1664079461297,
        "Owner_location":null,
        "Owner_reputation":11650,
        "Owner_up_votes":6318,
        "Owner_down_votes":21,
        "Owner_views":977,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68386650",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70217519,
        "Question_title":"Error Invoking xgboost endpoint deployed locally : feature names mismatch",
        "Question_body":"<p>I have trained a xgboost model locally and running into <code>feature_names mismatch<\/code> issue when invoking the endpoint. My <code>model<\/code> is a xgboost Regressor with some pre-processing (variable encoding) and hyper-parameter tuning. Code to train the model:<\/p>\n<p>version xgboost <code>0.90<\/code><\/p>\n<pre><code>import pandas as pd\nimport pickle\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder \n\n\n# split df into train and test\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:21], df.iloc[:,-1], test_size=0.1)\n\nX_train.shape\n(1000,21)\n\n# Encode categorical variables  \ncat_vars = ['cat1','cat2','cat3']\ncat_transform = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), cat_vars)], remainder='passthrough')\n\nencoder = cat_transform.fit(X_train)\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)\n\nX_train.shape\n(1000,420)\n\n# Define a xgboost regression model\nmodel = XGBRegressor()\n\n# Do hyper-parameter tuning\n.....\n\n# Fit model - convert to numpy arrays\nmodel.fit(X_train.toarray(), y_train.values())\n<\/code><\/pre>\n<p>Here's what <code>model<\/code> object looks like:<\/p>\n<pre><code>XGBRegressor(colsample_bytree=xxx, gamma=xxx,\n             learning_rate=xxx, max_depth=x, n_estimators=xxx,\n             subsample=xxx)\n<\/code><\/pre>\n<p>My test data is a string of float values which is turned into an array as the data must be passed as numpy array to match how the model was trained.<\/p>\n<pre><code>testdata = [........., 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2000, 200, 85, 412412, 123, 41, 552, 50000, 512, 0.1, 10.0, 2.0, 0.05]\n<\/code><\/pre>\n<p>Traceback<\/p>\n<pre><code>\nValueError: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'....'f419'] ['f0']\ngc43e67lgg-algo-1-8jxpe  | expected f156, f266, f164, f207, f107, f296, f417, f188, f131, f225,.....\n\n<\/code><\/pre>\n<p>I can retrieve the feature names for the model by running <code>model.get_booster().feature_names<\/code>. Is there a way I can use these names and assign to test data point so that they are consistent?<\/p>\n<pre><code>['f0', 'f1', 'f2', 'f3', 'f4', 'f5',......'f417','f418','f419']\n<\/code><\/pre>\n<p>More info on inference script here: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1638548403397,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1393967047770,
        "Owner_last_access_time":1663978179853,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":1503,
        "Owner_up_votes":179,
        "Owner_down_votes":3,
        "Owner_views":429,
        "Question_last_edit_time":1638820655713,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70217519",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54408673,
        "Question_title":"Getting sagemaker container locally",
        "Question_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548702401737,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":2235,
        "Owner_creation_time":1352206833663,
        "Owner_last_access_time":1664011820603,
        "Owner_location":null,
        "Owner_reputation":893,
        "Owner_up_votes":112,
        "Owner_down_votes":2,
        "Owner_views":185,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1548750603277,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66061721,
        "Question_title":"In AWS, Execute notebook from sagemaker",
        "Question_body":"<p>Im using AWS SageMaker Notebooks.<\/p>\n<p>What is the best way to execute notebook from sagemaker?<\/p>\n<ul>\n<li>My idea is to have an S3 bucket.<\/li>\n<li>When a new file is putted there i want to execute a notebook that reads from S3 and puts the output in other bucket.<\/li>\n<\/ul>\n<p>The only way i have from now is to start an S3 event, execute a lambda function that starts a sagemaker instance and execute the notebook. But is getting too much time to start and it doesnt work yet for me with a big notebook.<\/p>\n<p>Maybe is better to export the notebook and execute it from another place in aws (in order to be faster), but i dont know where.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612521473383,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":204,
        "Owner_creation_time":1605095949960,
        "Owner_last_access_time":1646131913857,
        "Owner_location":"La Rioja, Spain",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1612774473150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66061721",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48365866,
        "Question_title":"How to call Sagemaker training model endpoint API in C#",
        "Question_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1516531050743,
        "Question_score":8,
        "Question_tags":"c#|amazon-web-services|amazon-s3|sparkr|amazon-sagemaker",
        "Question_view_count":2093,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":1516720777517,
        "Answer_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1519637555373,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59762829,
        "Question_title":"AWS Sagemaker scikit_bring_your_own example",
        "Question_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579147150087,
        "Question_score":1,
        "Question_tags":"scikit-learn|amazon-sagemaker|svd",
        "Question_view_count":341,
        "Owner_creation_time":1241005356853,
        "Owner_last_access_time":1663511013130,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579273412890,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65072778,
        "Question_title":"Download checkpoint from AWS",
        "Question_body":"<p>How can I download the checkpoints and logged statistics after I run my deep learning algorithm on AWS using SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606735973450,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-ec2|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1491911840993,
        "Owner_last_access_time":1653036550053,
        "Owner_location":null,
        "Owner_reputation":1957,
        "Owner_up_votes":47,
        "Owner_down_votes":1,
        "Owner_views":278,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65072778",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58802366,
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573479066647,
        "Question_score":2,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1573653626700,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1573654268900,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73163540,
        "Question_title":"Is there any workaround for installing packages ? (deepspeed error)",
        "Question_body":"<p>i was trying to use deepspeed, but when i run the training it shows an error. I have to install mpi4py using pip. However if I try to install mpi4py i get an error.\nFrom that error i found out that to install the package, i must install &quot;libopenmpi-dev &quot; before, using apt. However we dont have the password to sudo. Any workaround to this ?\n(or the only option is to change platforms ?)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":9,
        "Question_creation_time":1659082147263,
        "Question_score":0,
        "Question_tags":"linux|amazon-sagemaker|apt",
        "Question_view_count":73,
        "Owner_creation_time":1653740518870,
        "Owner_last_access_time":1663956849533,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1659169326400,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73163540",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548942784467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1549915976767,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73111472,
        "Question_title":"stacking cnn output layer with xgboost. Data prep gives OOM error",
        "Question_body":"<p>I have trained a cnn model and I am trying to stack the output layer to an xgboost regressor to reduce mape. I am getting OOM error in Sagemaker training job when I try to include the input data (in npy format) with the cnn output layer and save it as csv - so this can be input to xgboost. When I try to run this in Sagemaker notebook instance the kernel dies. The training input npy file is around 42gb and I have tried these instances : ml.m5d.24xlarge, ml.r5.24xlarge\nHere is my code I am running in notebook:<\/p>\n<p>'''<\/p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport boto3\nfrom io import BytesIO\nfrom keras.models import load_model\nfrom keras import backend as K\n\nclient = boto3.client(&quot;s3&quot;)\n\nbucket = &lt;bucket_name&gt;\nkey = '\/path\/cnn_model.h5'\nclient.download_file(bucket, key, 'cnn_model.h5')\n\ncnn_model = load_model(&quot;cnn_model.h5&quot;)\n\ndef read_s3_npy(s3_uri, arg = False):\n    bytes = BytesIO()\n    bytes_.seek(0)\n    parsed_s3 = urlparse(s3_uri)\n    obj = client.get_object(Bucket=parsed_s3.netloc, key = parsed_s3.path[1:])\n    return np.load(BytesIO(obj['Body'].read()), allow_pickle=arg)\n\nx_train_path = &lt;path in s3&gt;+'x_train.npy'\ny_train_path = &lt;path in s3&gt;+'y_train.npy'\nx_train = read_s3_npy(x_train_path)\ny_train = read_s3_npy(y_train_path)\n\nlast_layer_op = K.function([cnn_model.layers[0].input], [cnn_model.layers[-2].output])\n\ntrain_layer = last_layer_op([x_train, 1])[0]\n<\/code><\/pre>\n<p>'''<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658761728373,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|conv-neural-network|xgboost|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_time":1438153132453,
        "Owner_last_access_time":1659123247360,
        "Owner_location":null,
        "Owner_reputation":57,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73111472",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203561,
        "Question_title":"How to automatically stop Sagemaker notebook instances if it is idle?",
        "Question_body":"<p>I have been looking for a script to automatically close Sagemaker Notebook Instances that have been forgotten to be closed or that are idle. A few scripts I found don't work very well (eg: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\/auto-stop-idle\" rel=\"nofollow noreferrer\">link<\/a> , it is only checking if ipynb file is live, Im not using .ipynb, or taking the last updated info which never changes until you shut down or open the instance)\nIs there a resource or script you can recommend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652282230790,
        "Question_score":0,
        "Question_tags":"amazon-web-services|instance|amazon-sagemaker|lifecycle",
        "Question_view_count":399,
        "Owner_creation_time":1646703340960,
        "Owner_last_access_time":1663771451647,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203561",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59987884,
        "Question_title":"ImportError: cannot import name np_utils in AWS Sagemaker",
        "Question_body":"<p>I run my .ipynb file in AWS Sagemaker. <\/p>\n\n<p>Unlike Colab, we need to install the dependencies separately in Sagemaker.<\/p>\n\n<p>While pip installing tensorflow,I got error like <strong>ImportError: cannot import name np_utils<\/strong>.<\/p>\n\n<p>After that I installed that too by <strong>!pip install np_utils<\/strong>\nThen also I face similar error.<\/p>\n\n<p>If this is in local system, it may be due to system configuration. Since its in Sagemaker,I am not sure about how to proceed futher.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1580393356327,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_time":1550065461003,
        "Owner_last_access_time":1583913442813,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1580400719820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59987884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60684191,
        "Question_title":"Cannot upload data from pandas data-frame to AWS athena table due to 'import package error'",
        "Question_body":"<p>When I am trying to import the awswrangler package in sagemaker I am getting the below error<\/p>\n\n<pre><code>import awswrangler as aws\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<pre><code>ImportError                               Traceback (most recent call last)\n&lt;ipython-input-9-cc67bb4c1dd7&gt; in &lt;module&gt;\n----&gt; 1 import awswrangler as aws\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/__init__.py in &lt;module&gt;\n     15 from awswrangler.emr import EMR  # noqa\n     16 from awswrangler.glue import Glue  # noqa\n---&gt; 17 from awswrangler.pandas import Pandas  # noqa\n     18 from awswrangler.redshift import Redshift  # noqa\n     19 from awswrangler.s3 import S3  # noqa\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/pandas.py in &lt;module&gt;\n     17 from boto3 import client  # type: ignore\n     18 from botocore.exceptions import ClientError, HTTPClientError  # type: ignore\n---&gt; 19 from pandas.io.common import infer_compression  # type: ignore\n     20 from pyarrow import parquet as pq  # type: ignore\n     21 from s3fs import S3FileSystem  # type: ignore\n\nImportError: cannot import name 'infer_compression'\n<\/code><\/pre>\n\n<p>Hoe to fix this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1584198784503,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|amazon-athena|amazon-sagemaker",
        "Question_view_count":1470,
        "Owner_creation_time":1509871153690,
        "Owner_last_access_time":1663866159260,
        "Owner_location":null,
        "Owner_reputation":731,
        "Owner_up_votes":37,
        "Owner_down_votes":3,
        "Owner_views":185,
        "Question_last_edit_time":1584236556403,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60684191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57483440,
        "Question_title":"Flask and Gunicorn with Additional Threads",
        "Question_body":"<p>I'm trying to build a Flask app with Gunicorn to serve concurrent requests. For what it's worth, the context is a bring-your-own-container Sagemaker application.<\/p>\n\n<p>The issue is that I need the application to periodically check for updates. So I thought to implement a thread for this. Here is a minimal example of some Flask code with an update thread. <\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nimport time, threading\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  while True:\n    message = not message\n    time.sleep(10)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nupdate_thread = threading.Thread(target=update)\n\nif __name__ == \"__main__\":\n  update_thread.start()\n  app.run()\n  update_thread.join()\n<\/code><\/pre>\n\n<p>I then launch with gunicorn:<\/p>\n\n<p><code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>\n\n<p>Perhaps unsurprisingly the update thread doesn't start since the <code>__main__<\/code> section is never executed. <\/p>\n\n<blockquote>\n  <p><strong>Question<\/strong>: <em>How can one use an update thread (or similar construct) in a Flask app with Gunicorn?<\/em><\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565720684053,
        "Question_score":0,
        "Question_tags":"python|multithreading|flask|gunicorn|amazon-sagemaker",
        "Question_view_count":858,
        "Owner_creation_time":1413222980680,
        "Owner_last_access_time":1655488449107,
        "Owner_location":"Richland, WA",
        "Owner_reputation":493,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like this can be accomplished using <code>Flask-APScheduler<\/code> as follows:<\/p>\n\n<p><code>pip install flask_apscheduler<\/code><\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport atexit\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  global message\n  message = not message\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=update,trigger=\"interval\",seconds=10)\nscheduler.start()\n# shut down the scheduler when exiting the app\natexit.register(scheduler.shutdown)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nif __name__ == \"__main__\":\n  app.run()\n<\/code><\/pre>\n\n<p>Then launching as usual with \n<code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1565727707500,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57483440",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64215998,
        "Question_title":"Why is Crowd HTML breaking this image?",
        "Question_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1601930941820,
        "Question_score":1,
        "Question_tags":"javascript|amazon-sagemaker|mechanicalturk",
        "Question_view_count":168,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":1602868269100,
        "Answer_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603367517400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69915819,
        "Question_title":"How to Organize Training Data for AWS Sagemaker",
        "Question_body":"<p>I am training a model and the training data uses images for both the source and the label.<\/p>\n<p>For example, <code>image1.jpg =&gt; label_image.jpg<\/code><\/p>\n<p>The images and their corresponding &quot;label&quot; are in different directories.<\/p>\n<p>So I have images stored like <code>s3:\/\/bucket\/v1\/imgs<\/code> and their labels stored like <code>s3:\/\/bucket\/v1\/lbls<\/code>.<\/p>\n<h2>Question<\/h2>\n<p>How do I go about passing this data into an estimator in sagemaker?<\/p>\n<p>I've seen numerous examples, none of which have the data stored in a similar fashion. I've also tried to find the way that sagemaker expects the data to be organized but haven't had much luck.<\/p>\n<p>Any help would be greatly appreaciated.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636557493463,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|training-data",
        "Question_view_count":36,
        "Owner_creation_time":1478552095660,
        "Owner_last_access_time":1662143133913,
        "Owner_location":null,
        "Owner_reputation":1149,
        "Owner_up_votes":132,
        "Owner_down_votes":10,
        "Owner_views":150,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69915819",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56728230,
        "Question_title":"AWS sagemaker RandomCutForest (RCF) vs scikit lean RandomForest (RF)?",
        "Question_body":"<p>Is there a difference between the two, or are they different names for the same algorithm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561329917317,
        "Question_score":0,
        "Question_tags":"classification|random-forest|decision-tree|amazon-sagemaker",
        "Question_view_count":244,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. <\/p>\n\n<p>For RCF, see documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">here<\/a>) and notebook example (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561385097930,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56728230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55829940,
        "Question_title":"How to rename a SageMaker notebook instance?",
        "Question_body":"<p>On Amazon SageMaker, it's possible to edit most properties of a notebook instance when the instance is not active, but it does not seem possible to change its name.<\/p>\n\n<p>Is there any way to rename an existing SageMaker notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556108497957,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1311,
        "Owner_creation_time":1283441031493,
        "Owner_last_access_time":1663854500710,
        "Owner_location":"Lausanne, Switzerland",
        "Owner_reputation":4841,
        "Owner_up_votes":220,
        "Owner_down_votes":9,
        "Owner_views":261,
        "Question_last_edit_time":1556115031880,
        "Answer_body":"<p>Thank you for using Amazon SageMaker. <\/p>\n\n<p>SageMaker Notebook Instance's name cannot be edited. <\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1556304752800,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55829940",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48398509,
        "Question_title":"How to Invoke AWS Sagemaker API with c# .NET?",
        "Question_body":"<p>I have trained and deployed a model in AWS Sagemaker, Now I am trying to invoke the endpoint with client as c# .NET.\nIn the below code, it seems, I am getting errors because of invalid value of Body parameter.<\/p>\n\n<pre><code>AmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient();\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"sagemaker-mxnet-py2-cpu-2018-01-23-07-04-11\";\nrequest.Accept = \"text\/csv\";            \nrequest.ContentType = \"text\/csv\";\n\/\/request.Body = compressedMemStream;\nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse = aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>I have tried by passing a MemoryStream which written with a '.gz' file or with '.jpeg' file. By executing InvokeEndPoint(), Getting error as: \"<strong>unable to evaluate payload provided<\/strong>\" <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1516700568607,
        "Question_score":0,
        "Question_tags":"c#|asp.net|amazon-web-services|aws-sdk|amazon-sagemaker",
        "Question_view_count":743,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48398509",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69310670,
        "Question_title":"Amazon SageMaker ml.p2.xlarge notebook instance speed issues",
        "Question_body":"<p>I am running AWS Sagemaker and created a notebook instance ml.p2.xlarge machine with elastic inference set as none. I am encountering speed issues during training. The training process is very very slow. I have also restarted the kernel multiple time but it doesn't make any difference. What could be the reason?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1632465995047,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":212,
        "Owner_creation_time":1620191753590,
        "Owner_last_access_time":1661965917280,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69310670",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50049928,
        "Question_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Question_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1524767975170,
        "Question_score":0,
        "Question_tags":"amazon-s3|resnet|amazon-sagemaker",
        "Question_view_count":2600,
        "Owner_creation_time":1474520506390,
        "Owner_last_access_time":1625722561310,
        "Owner_location":null,
        "Owner_reputation":832,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1531369355823,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63361229,
        "Question_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Question_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597159003890,
        "Question_score":0,
        "Question_tags":"linux|windows|bash|amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597164941183,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63361229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72733847,
        "Question_title":"How do you resolve an \"Access Denied\" error when invoking `image_uris.retrieve()` in AWS Sagemaker JumpStart?",
        "Question_body":"<p>I am working in a SageMaker environment that is locked down. For example, my user account is prevented from creating S3 buckets. But, I can successfully run vanilla ML training jobs by passing in <code>role=get_execution_role<\/code> to an instance of the Estimator class when using an out-of-the-box algorithm such as XGBoost.<\/p>\n<p>Now, I'm trying to use an algorithm (LightBGM) that is only available via the JumpStart feature in SageMaker, but I can't get it to work. When I try to retrieve an image URI via <code>image_uris.retrieve()<\/code>, it returns the following error:<br \/>\n<code>ClientError: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied<\/code>.<\/p>\n<p>This makes some sense to me if my user permissions are being used when creating an object. But what I want to do is specify another role - like the one returned from get_execution_role - to perform these tasks.<\/p>\n<p>Is that possible? Is there another work-around available? How can I see which role is being used?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1656002777660,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":47,
        "Owner_creation_time":1316920614520,
        "Owner_last_access_time":1663874836913,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72733847",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49579526,
        "Question_title":"How to save parquet in S3 from AWS SageMaker?",
        "Question_body":"<p>I would like to save a Spark DataFrame from AWS SageMaker to S3. In Notebook, I ran<\/p>\n\n<p><code>myDF.write.mode('overwrite').parquet(\"s3a:\/\/my-bucket\/dir\/dir2\/\")<\/code><\/p>\n\n<p>I get<\/p>\n\n<blockquote>\n  <p>Py4JJavaError: An error occurred while calling o326.parquet. :\n  java.lang.RuntimeException: java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at\n  org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at\n  org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)   at\n  org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)     at\n  org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)    at\n  org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:394)\n    at\n  org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n    at\n  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n    at\n  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n    at\n  org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n    at\n  org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at\n  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at\n  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)     at\n  py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at\n  py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at\n  py4j.Gateway.invoke(Gateway.java:280)     at\n  py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)   at\n  py4j.GatewayConnection.run(GatewayConnection.java:214)    at\n  java.lang.Thread.run(Thread.java:745) Caused by:\n  java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)<\/p>\n<\/blockquote>\n\n<p>How should I do it correctly in Notebook? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1522434506127,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|hadoop|amazon-s3|amazon-sagemaker",
        "Question_view_count":1986,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49579526",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53408927,
        "Question_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Question_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1542792620897,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1941,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1542799396317,
        "Answer_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1544503147007,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56980928,
        "Question_title":"Training & Deploying SageMaker ML Models using AWS Lambda (NodeJS)",
        "Question_body":"<p>I am using AWS Lambda (NodeJS) for creating a sagemaker training job and deploy it using the Sagemaker Javascript SDK.<\/p>\n\n<p>I am following the below AWS JavaScript SDK docs<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html<\/a><\/p>\n\n<p>I am using the below script for creating the Training job.<\/p>\n\n<pre><code>Create Training Job:\n=====================\n\n    let TrainingJobName = 'Training-' + curr_date_time\n    let TrainingImage   = 'XXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xxxx:latest'\n    let S3Uri           = 's3:\/\/xxx.xxxx.sagemaker\/csv'\n\n    console.log(`TrainingJobName: ${TrainingJobName}`);\n\n    let params = {\n        AlgorithmSpecification: { \/* required *\/\n            TrainingInputMode: 'File', \/* required *\/\n            TrainingImage: TrainingImage\n        },\n        OutputDataConfig: { \/* required *\/\n            S3OutputPath: 's3:\/\/xxx.xxxx.sagemaker\/xxxx\/output', \/* required *\/\n        },\n        ResourceConfig: { \/* required *\/\n            InstanceCount: 1, \/* required *\/\n            InstanceType: 'ml.m4.xlarge', \/* required *\/\n            VolumeSizeInGB: 1, \/* required *\/\n        },\n        RoleArn: 'arn:aws:iam::xxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxx', \/* required *\/\n        StoppingCondition: { \/* required *\/\n            MaxRuntimeInSeconds: 86400\n        },\n        TrainingJobName: TrainingJobName, \/* required *\/\n        InputDataConfig: [\n            {\n                ChannelName: 'training', \/* required *\/\n                DataSource: { \/* required *\/\n                    S3DataSource: {\n                        S3DataType: 'S3Prefix', \/* required *\/\n                        S3Uri: S3Uri, \/* required *\/\n                        S3DataDistributionType: 'FullyReplicated'\n                    }\n                },\n                CompressionType: null,\n                ContentType: '',\n                RecordWrapperType: null,\n            }\n        ]\n    };\n\n    return await sagemaker.createTrainingJob(params).promise();\n<\/code><\/pre>\n\n<p>After the training job is created, i query the job status using the sagemaker describeTrainingJob function.\nI get the status as \"InProgress\"<\/p>\n\n<p>After that I call the sagemaker waitFor function to wait for the completion of the training job using the below method:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter<\/a><\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n<\/code><\/pre>\n\n<p>I find the sagemaker waitFor creates the second training job before the first training job is completed, and it goes on creating subsequent training jobs with the same job name.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think this is due to the StoppingCondition parameter (MaxRuntimeInSeconds:86400) in the createTrainingJob function.<\/p>\n\n<p>I want to know if there is any solution which creates a single training job and return the results after the trainining job is completed ?<\/p>\n\n<p>==========================================================\nUpdate:<\/p>\n\n<p>I am following the \"Scheduling the training of a SageMaker model with a Lambda function\" <a href=\"https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM<\/a>.<\/p>\n\n<p>I am able to create a training job if i am using the below code in my lambda function.<\/p>\n\n<pre><code>let training_job_result = await start_model_training();\nconsole.log(`Sagemaker training result : ${JSON.stringify(training_job_result)}`);\n\nlet training_job_arn = training_job_result[\"TrainingJobArn\"];\nlet training_job_name = training_job_arn.split(\"\/\")[1];\n\n\nlet desc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\nlet desc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 1 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>But I need to wait till the training job is completed and invoke the sagemaker deploy method for creating\/updating the endpoint.<\/p>\n\n<p>If I use the below code then it keeps on creating multiple training jobs and the lambda function never terminates.<\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n\n\ndesc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\ndesc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 2 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>I want to deploy\/update the endpoint once the training is completed.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562812694627,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|aws-sdk-js",
        "Question_view_count":1288,
        "Owner_creation_time":1241005356853,
        "Owner_last_access_time":1663511013130,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Question_last_edit_time":1562909793607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56980928",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69962965,
        "Question_title":"How to get an AWS Feature Store feature group into the ACTIVE state?",
        "Question_body":"<p>I am trying to ingest some rows into a Feature Store on AWS using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>feature_group.ingest(data_frame=df, max_workers=8, wait=True)\n<\/code><\/pre>\n<p>but I am getting the following error:<\/p>\n<blockquote>\n<p>Failed to ingest row 1: An error occurred (ValidationError) when\ncalling the PutRecord operation: Validation Error: FeatureGroup\n[feature-group] is not in ACTIVE state.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636892751657,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker|data-ingestion|aws-feature-store",
        "Question_view_count":383,
        "Owner_creation_time":1362914550047,
        "Owner_last_access_time":1661844098897,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":2791,
        "Owner_up_votes":163,
        "Owner_down_votes":13,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out the status of a feature group after its creation is <code>Created<\/code> but before you can ingest any rows you need to simply wait until it's <code>Active<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>while status != 'Created':\n        try:\n            status = feature_group.describe()['OfflineStoreStatus']['Status']\n        except:\n            pass\n        print('Offline store status: {}'.format(status))    \n        sleep(15)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636979984023,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646290913523,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69962965",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73144411,
        "Question_title":"How to make SageMaker XGBoost Hyperparameter tuning work in script mode",
        "Question_body":"<p>I'm following up after this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> and wondering if is it possible to combine the script mode with Hyperparameter tuning. If I'm trying to do so, the HPT.fit() runs my script again and again (the main() function and then _xgb_train()) but I don't know how to pass the hyperparameter the algorithm chose for me to the train function.\nAny idea?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658954687147,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1540098833357,
        "Owner_last_access_time":1663969941437,
        "Owner_location":"israel",
        "Owner_reputation":455,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73144411",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58879596,
        "Question_title":"Unable to call XG-Boost endpoint created in sagemaker using AWS-Lambda",
        "Question_body":"<p>I trained an xgboost model on AWS-Sagemaker and created an endpoint. Now I want to call the endpoint using AWS Lambda and AWS API. I created an lambda function and added the below mentioned code for my xgboost model. When I try to test it, the function is throwing a ParamValidation error. Here is my code<\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport io\nimport boto3\nendpointname =os.environ['endpointname'] #name of the endpoint I created in sagemaker\nruntime = boto3.client('runtime.sagemaker')\ndef lambda_handler(event, context):\n    print(\"Recieved Event: \"+json.dumps(event,indent=2))\n    data=json.loads(json.dumps(event))\n    print(data)\n    response = runtime.invoke_endpoint(EndpointName=endpointname,ContentType='text\/csv',Body=data)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(int(float(result))) #sagemaker xgb returns bytes type for the test case\n<\/code><\/pre>\n\n<p>The test event I created is dict type. The function is throwing  <code>Invalid type for parameter Body, value: {'Time':'7'}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object<\/code>\nIt means I should pass either byte or bytearray instead of dict type into my event. But when I read this <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-programming-model-handler-types.html\" rel=\"nofollow noreferrer\">AWS Lambda doc<\/a> It says that my event type can only be dict,int,list,float,str, or None type. I followed the steps mentioned in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws doc to create my lambda function. Can someone please explain why my code is throwing above mentioned error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573829888267,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":519,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":1573837537747,
        "Answer_body":"<p><code>data=json.loads(json.dumps(event))<\/code> is a redundant operation. <code>data=event<\/code> will return <code>True<\/code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']<\/code> then change <code>Body=payload<\/code> inside <code>response<\/code>. Then it will work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574162755397,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58879596",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67176637,
        "Question_title":"How to get bearer token AWS for Postman",
        "Question_body":"<p>I am using AWS sagemaker, and I have created an endpoint. I want to test endpoint on postman app. I give endpoint URL and JSON body to postman app. But I get this error that <code>&quot;message&quot;: &quot;Missing Authentication Token&quot;<\/code> I need to know from where I 'll get bearer token so that I can give it to postman app.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618912675733,
        "Question_score":0,
        "Question_tags":"amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":1080,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am answering my own question after searching and reading forums,<\/p>\n<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure<\/code> command.\nFor configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.\nAfter configuration by running this command, <code>aws ecr get-authorization-token<\/code>, we can get authorizationToken. <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/ecr\/get-authorization-token.html\" rel=\"nofollow noreferrer\">here<\/a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618952953590,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67176637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48111034,
        "Question_title":"Reading a large csv from a S3 bucket using python pandas in AWS Sagemaker",
        "Question_body":"<p>I'm trying to load a large CSV (~5GB) into pandas from S3 bucket.<\/p>\n\n<p>Following is the code I tried for a small CSV of 1.4 kb :<\/p>\n\n<pre><code>client = boto3.client('s3') \nobj = client.get_object(Bucket='grocery', Key='stores.csv')\nbody = obj['Body']\ncsv_string = body.read().decode('utf-8')\ndf = pd.read_csv(StringIO(csv_string))\n<\/code><\/pre>\n\n<p>This works well for a small CSV, but my requirement of loading a 5GB csv to pandas dataframe cannot be achieved through this (probably due to memory constraints when loading the csv by StringIO). <\/p>\n\n<p>I also tried below code<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nobj = s3.get_object(Bucket='bucket', Key='key')\ndf = pd.read_csv(obj['Body'])\n<\/code><\/pre>\n\n<p>but this gives below error.<\/p>\n\n<pre><code>ValueError: Invalid file path or buffer object type: &lt;class 'botocore.response.StreamingBody'&gt;\n<\/code><\/pre>\n\n<p>Any help to resolve this error is much appreciated.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1515145926657,
        "Question_score":8,
        "Question_tags":"python|csv|amazon-s3|amazon-sagemaker",
        "Question_view_count":15101,
        "Owner_creation_time":1346239623100,
        "Owner_last_access_time":1663162010980,
        "Owner_location":"Salzburg, Austria",
        "Owner_reputation":505,
        "Owner_up_votes":38,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48111034",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72059905,
        "Question_title":"SageMaker Ground Truth : both source and source-ref in manifest?",
        "Question_body":"<p>I set up a labelling job in SageMaker Ground Truth. Here is what is in the <strong>jsonl<\/strong> manifest I created.<\/p>\n<pre><code>{&quot;source&quot;:&quot;Sample text&quot;}\n{&quot;source&quot;:&quot;Sample text&quot;}\n<\/code><\/pre>\n<p>Is the &quot;datasetObjectId&quot;:&quot;1&quot; in the resulting json file after annotation created in the order of the appearing sources in the manifest? Can I add metadata to the manifest file, like this for example:<\/p>\n<pre><code>{&quot;source&quot;:&quot;Sample text&quot;, &quot;file&quot;:&quot;s3:\/\/foo\/bar1.txt&quot;}\n{&quot;source&quot;:&quot;Sample text&quot;, &quot;file&quot;:&quot;s3:\/\/foo\/bar2.txt&quot;}\n<\/code><\/pre>\n<p>My problem: I would like to be able to track the source of a file (not only its text). But for the NER task, working with &quot;source-ref&quot; produces one labelling page per <em>line<\/em> of text, and I want to work with one labelling page per <em>document<\/em>.\nHaving tried to put both source and source-ref in the manual manifest, it is not understood (see error message below).<\/p>\n<pre><code>The manifest file in the Input dataset location has an error or is not supported for this task type. Update the input manifest file, or choose another task type. Error: SyntaxError: JSON.parse: unexpected character at line 1 column 1 of the JSON data\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1651244269230,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":102,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1651248681170,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72059905",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54295445,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548093365813,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1548251290417,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Question_exclusive_tag":"Amazon SageMaker"
    }
]