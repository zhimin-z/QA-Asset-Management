[
    {
        "Question_id":73435172,
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661090494277,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661173080163,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73602927,
        "Question_title":"Best method to compress large dataset features",
        "Question_body":"<p>10TB featurs which stored on aws s3 bucket.\nin order to decrease costs , we are looking for a loseless compression method for those features (current features are images but it can changed between projects)<\/p>\n<p>I heard about hdf5, but is it the best method nowadays?\nCan the the extraction be paralleled?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1662326892390,
        "Question_score":1,
        "Question_tags":"python|compression|hdf5|dvc",
        "Question_view_count":37,
        "Owner_creation_time":1573845661880,
        "Owner_last_access_time":1664058289083,
        "Owner_location":"Israel",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73602927",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61386810,
        "Question_title":"Data version control (DVC) edit files in place results in cyclic dependency",
        "Question_body":"<p>we have a larger dataset and have several preprocessing scripts.\nThese scripts alter data in place.\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).\nI would assume this is a very common use case.<\/p>\n\n<p>What is the best practice here ?<\/p>\n\n<p>Tried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1587643471830,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":45,
        "Owner_creation_time":1530036936153,
        "Owner_last_access_time":1656059958773,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":121,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61386810",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67505910,
        "Question_title":"Problem running a Docker container in Gitlab CI\/CD",
        "Question_body":"<p>I am trying to build and run my Docker image using Gitlab CI\/CD, but there is one issue I can't fix even though locally everything works well.<\/p>\n<p>Here's my Dockerfile:<\/p>\n<pre><code>FROM &lt;internal_docker_repo_image&gt;\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc\nCOPY requirements.txt \/requirements.txt\n\nRUN pip install --no-cache-dir --user -r \/requirements.txt\n\nCOPY . \/src\nWORKDIR \/src\nENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;dvc&quot;, &quot;repro&quot;]\n<\/code><\/pre>\n<p>This is how I run the container:<\/p>\n<p><code>docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force<\/code><\/p>\n<p>Everything works great when running this locally, but it fails when run on Gitlab CI\/CD.<\/p>\n<pre><code>stages:\n  - build_image\n\nbuild_image:\n  stage: build_image\n  image: &lt;internal_docker_repo_image&gt;\n  script:\n    - echo &quot;Building Docker image...&quot;\n    - mkdir ~\/.docker\n    - cat $GOOGLE_CREDENTIALS &gt; ${CI_PROJECT_DIR}\/key.json\n    - docker build . -t &lt;image_name&gt;\n    - docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force\n  artifacts:\n        paths:\n          - &quot;.\/data\/*csv&quot;\n        expire_in: 1 week\n\n<\/code><\/pre>\n<p>This results in the following error:\n<code>ERROR: you are not inside of a DVC repository (checked up to mount point '\/src')<\/code><\/p>\n<p>Just in case you don't know what DVC is, this is a tool used in machine learning for versioning your models, datasets, metrics, and, in addition, setting up your pipelines, which I use it for in my case.<\/p>\n<p>Essentially, it requires two folders <code>.dvc<\/code> and <code>.git<\/code> in the directory from which <code>dvc repro<\/code> is executed.<\/p>\n<p>In this particular case, I have no idea why it's not able to run this command given that the contents of the folders are exactly the same and both <code>.dvc<\/code> and <code>.git<\/code> exist.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620830695847,
        "Question_score":4,
        "Question_tags":"docker|continuous-integration|gitlab-ci|dvc",
        "Question_view_count":746,
        "Owner_creation_time":1452169170337,
        "Owner_last_access_time":1663996681010,
        "Owner_location":null,
        "Owner_reputation":576,
        "Owner_up_votes":431,
        "Owner_down_votes":4,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67505910",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62529867,
        "Question_title":"Data version control (DVC) commands not working ---> TypeError: public() got an unexpected keyword argument 'SEP'",
        "Question_body":"<p>All of a sudden, dvc has stopped functioning.\nAny command typed fails and throws an exception.\nexample. dvc remote list results in -<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/home\/dev2\/.local\/bin\/dvc&quot;, line 5, in &lt;module&gt;\n    from dvc.main import main\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/main.py&quot;, line 6, in &lt;module&gt;\n    from dvc import analytics\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/analytics.py&quot;, line 16, in &lt;module&gt;\n    from dvc.lock import Lock, LockError\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/lock.py&quot;, line 8, in &lt;module&gt;\n    import flufl.lock\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/__init__.py&quot;, line 3, in &lt;module&gt;\n    from flufl.lock._lockfile import (\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/_lockfile.py&quot;, line 54, in &lt;module&gt;\n    public(SEP=SEP)\nTypeError: public() got an unexpected keyword argument 'SEP'\n \n<\/code><\/pre>\n<p>Any suggestions will be of great help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1592898526370,
        "Question_score":2,
        "Question_tags":"linux|git|dvc",
        "Question_view_count":307,
        "Owner_creation_time":1534165472313,
        "Owner_last_access_time":1663626263697,
        "Owner_location":null,
        "Owner_reputation":449,
        "Owner_up_votes":3,
        "Owner_down_votes":2,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62529867",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70478173,
        "Question_title":"How to track the big data stored in Gdrive through DVC?",
        "Question_body":"<p>I am currently working on the ML project and the data size is around 10 GB. The data I stored in google drive. Its impossible for me to download it on my local machine. So, how to use the DVC (data version control) to track that data? Thank you in advance for your time.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1640410347580,
        "Question_score":2,
        "Question_tags":"machine-learning|version-control|mlops|dvc",
        "Question_view_count":47,
        "Owner_creation_time":1567192416523,
        "Owner_last_access_time":1663279611567,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":76,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1640412287833,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70478173",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620170453717,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_time":1379685720450,
        "Owner_last_access_time":1663797728603,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1620171022370,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620171651067,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73084915,
        "Question_title":"How to fix DVC error 'FileNotFoundError: [Errno 2] No such file or directory' in Github actions",
        "Question_body":"<p>Trying to pull a folder with test data into a GitHub actions container, I get<\/p>\n<blockquote>\n<p>FileNotFoundError: [Errno 2] No such file or directory<\/p>\n<\/blockquote>\n<p>I tried running <code>dvc checkout --relink<\/code> locally, but that did not work. I am using Gdrive for the data-repository with a service account. It seems the login works. But strangely the file is not present. Maybe I can push the files again somehow and recreate the data in the repository?<\/p>\n<hr \/>\n<h2><code>dvc doctor<\/code> output:<\/h2>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.7.13 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.14.0),\n    webhdfs (fsspec = 2022.5.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sda1\nRepo: dvc, git\n<\/code><\/pre>\n<p>I was able to clone the GIT repo and pull the data from the DVC data-registry, however, it did not work from GH.<\/p>\n<hr \/>\n<h2>Full traceback<\/h2>\n<pre><code>Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiohttp-retry-2.5.1 aiosignal-1.2.0 appdirs-1.4.4 async-timeout-4.0.2 asynctest-0.13.0 atpublic-2.3 attrs-21.4.0 cached-property-1.5.2 cachetools-4.2.4 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.0.12 colorama-0.4.5 commonmark-0.9.1 configobj-5.0.6 contextvars-2.4 cryptography-37.0.4 dataclasses-0.8 decorator-4.4.2 dictdiffer-0.9.0 diskcache-5.4.0 distro-1.7.0 dpath-2.0.6 dulwich-0.20.45 dvc-2.8.1 flatten-dict-0.4.2 flufl.lock-3.2 frozenlist-1.2.0 fsspec-2022.1.0 ftfy-6.0.3 funcy-1.17 future-0.18.2 gitdb-4.0.9 gitpython-3.1.18 google-api-core-2.8.2 google-api-python-client-2.52.0 google-auth-2.9.1 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.3 grandalf-0.6 httplib2-0.20.4 idna-3.3 idna-ssl-1.1.0 immutables-0.18 importlib-metadata-4.8.3 importlib-resources-5.4.0 mailchecker-4.1.18 multidict-5.2.0 nanotime-0.5.2 networkx-2.5.1 oauth2client-4.1.3 packaging-21.3 pathspec-0.8.1 phonenumbers-8.12.52 ply-3.11 protobuf-3.19.4 psutil-5.9.1 pyOpenSSL-22.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydot-1.4.2 pydrive2-1.10.1 pygit2-1.6.1 pygments-2.12.0 pygtrie-2.5.0 pyparsing-2.4.7 python-benedict-0.25.2 python-dateutil-2.8.2 python-fsutil-0.6.1 python-slugify-6.1.2 requests-2.27.1 rich-12.5.1 rsa-4.9 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 shortuuid-1.0.9 shtab-1.5.5 six-1.16.0 smmap-5.0.0 tabulate-0.8.10 text-unidecode-1.3 toml-0.10.2 tqdm-4.64.0 typing-extensions-4.1.1 uritemplate-4.1.1 urllib3-1.26.10 voluptuous-0.13.1 wcwidth-0.2.5 xmltodict-0.13.0 yarl-1.7.2 zc.lockfile-2.0 zipp-3.6.0\nDVC version: 2.8.1 (pip)\n---------------------------------\nPlatform: Python 3.6.15 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.10.1),\n    webhdfs (fsspec = 2022.1.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sdb1\nRepo: dvc, git\n2022-07-22 19:01:08,361 DEBUG: failed to pull cache for 'tests\/data'\n2022-07-22 19:01:08,364 WARNING: No file hash info found for 'tests\/data'. It won't be created.\n1 file failed\n2022-07-22 19:01:08,365 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/pull.py&quot;, line 44, in pull\n    recursive=recursive,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/checkout.py&quot;, line 110, in checkout\n    raise CheckoutError(stats[&quot;failed&quot;], stats)\ndvc.exceptions.CheckoutError: Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\n2022-07-22 19:01:08,369 DEBUG: Analytics is enabled.\n2022-07-22 19:01:08,412 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\n2022-07-22 19:01:08,413 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\nError: The operation was canceled.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1658515971860,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":209,
        "Owner_creation_time":1444959339707,
        "Owner_last_access_time":1663889479043,
        "Owner_location":null,
        "Owner_reputation":6513,
        "Owner_up_votes":1296,
        "Owner_down_votes":60,
        "Owner_views":1057,
        "Question_last_edit_time":1658518512243,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73084915",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67421254,
        "Question_title":"How to add a file to a dvc-tracked folder without pulling the whole folder's content?",
        "Question_body":"<p>Let's say I am working inside a git\/dvc repo. There is a folder <code>data<\/code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc add data\n<\/code><\/pre>\n<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.<\/p>\n<p>I clone the repo on another workspace, and now I have the <code>data.dvc<\/code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt<\/code> to the <code>data<\/code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data<\/code> locally<\/em> ?<\/p>\n<p>What I have tried for now:<\/p>\n<ol>\n<li><p>Adding the <code>data<\/code> folder again:<\/p>\n<pre><code>mkdir data\nmv path\/to\/newfile.txt data\/newfile.txt\ndvc add data\n<\/code><\/pre>\n<p>The <code>data.dvc<\/code> file is built again from the local state of <code>data<\/code> which only contains <code>newfile.txt<\/code> so this doesn't work.<\/p>\n<\/li>\n<li><p>Adding the file as a single element in <code>data<\/code> folder:<\/p>\n<pre><code> dvc add data\/newfile.txt\n<\/code><\/pre>\n<p>I get :<\/p>\n<pre><code> Cannot add 'data\/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. \n To include 'data\/newfile.txt' in 'data', run 'dvc commit data.dvc'\n<\/code><\/pre>\n<\/li>\n<li><p>Using dvc commit as suggested<\/p>\n<pre><code> mkdir data\n mv path\/to\/newfile.txt data\/newfile.txt\n dvc commit data.dvc\n<\/code><\/pre>\n<p>Similarly as 1., the <code>data.dvc<\/code> is rebuilt again from local state of <code>data<\/code>.<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620314719220,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1781,
        "Owner_creation_time":1620311720937,
        "Owner_last_access_time":1626773929723,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1620314912033,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67421254",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_time":1578574709920,
        "Owner_last_access_time":1648636957603,
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1627513406643,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72622280,
        "Question_title":"How does one add individual files with DVC?",
        "Question_body":"<p>Suppose I run the following commands:<\/p>\n<pre><code># set up DVC\n\nmkdir foo\ncd foo &amp;&amp; git init\ndvc init\ngit add * &amp;&amp; git commit -m &quot;dvc init&quot;\n\n\n# make a data file\n\nmkdir -p bar\/biz\ntouch bar\/biz\/boz\n\n\n# add the data file\n\ndvc add bar\/biz\/boz\n<\/code><\/pre>\n<p>And DVC outputs the following:<\/p>\n<pre><code>To track the changes with git, run:\n\n  git add bar\/biz\/.gitignore bar\/biz\/boz.dvc\n<\/code><\/pre>\n<hr \/>\n<p>This last part is what I would like to avoid.  Preferably, DVC would only change the top level <code>.gitignore<\/code> (located at the project root, where <code>git init<\/code> was executed), and will change only DVC files at the top level.<\/p>\n<p><strong>And here's why:<\/strong><\/p>\n<p>I have a rather large dataset developed in an original work more or less ad-hoc. This data is not systematically organized, nor do I want to organize it as-is.<\/p>\n<p>Instead, I want to incrementally add this old, bespoke data to the DVC directory tree.  And each time I add some of the data to the tree, I want to check it in with DVC as I would if I were modifying code or mixing one project's code into another.<\/p>\n<p>However, DVC wants to create a local file and gitignore at every location I add.  This creates a mess and I have no reasonable faith that it will be easy to maintain all of these atomic and distributed datastores.<\/p>\n<hr \/>\n<p><strong>The question:<\/strong><\/p>\n<p>What is the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655234204993,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":112,
        "Owner_creation_time":1405262190020,
        "Owner_last_access_time":1663968225423,
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Assuming bar\/ is the dataset directory you're incrementally adding to, you can instead<\/p>\n<pre><code>dvc add bar\n<\/code><\/pre>\n<p>This creates a bar.dvc file and writes to .gitignore at the top level.<\/p>\n<p>When you update content in bar\/, <code>dvc add<\/code> it again or use <code>dvc commit<\/code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5<\/code> hash that identifies to the latest directory structure.<\/p>\n<p>Some docs:<br \/>\n<a href=\"https:\/\/dvc.org\/doc\/start\/data-management#making-changes\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/start\/data-management#making-changes<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/add<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1655349731160,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655350036863,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72622280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72069555,
        "Question_title":"Second dvc push on AWS Batch using IAM role gets \"Unable to locate credentials\"",
        "Question_body":"<p>I'm running a job on AWS Batch, and this job prepares some data and versions it using <code>dvc<\/code>. Secondly, the job does some transformation generating new data, and it should save this new data using <code>dvc<\/code> again. Also, in this case, i'm setting a instance-profile role to enable the AWS Batch to persist on my S3 bucket.<\/p>\n<p>The first <code>dvc push<\/code> works perfectly. But the second one generates the error <code>Unable to locate credentials<\/code><\/p>\n<p>I have also changed the script to just touch a file, add to dvc and push, and then repeat the process in with other file, and could replicate the problem.<\/p>\n<p>I have already solved, changing the command <code>dvc push<\/code> to <code>dvc push especific-file-to-push<\/code>, but I'm now trying to understand what is the problem with <code>dvc push<\/code> command without the parameter specifying the file.<\/p>\n<p>Does anybody know?<\/p>\n<p>I'm using dvc <code>dvc==2.9.5<\/code> and <code>boto3==1.21.21<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1651330586727,
        "Question_score":2,
        "Question_tags":"amazon-s3|boto3|amazon-iam|aws-batch|dvc",
        "Question_view_count":152,
        "Owner_creation_time":1379631386420,
        "Owner_last_access_time":1663244472720,
        "Owner_location":"Rio de Janeiro, Brazil",
        "Owner_reputation":101,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1651331434543,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72069555",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72333237,
        "Question_title":"Why I got an invalid bucket name error using dvc mlflow on macos",
        "Question_body":"<p>Could anyone tell what's the reason for error:<\/p>\n<p>botocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]<\/em>:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9-]{1,63}$&quot;<\/p>\n<p>I try to use mlflow with docker.\n.env file contains:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/127.0.0.1:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>Also tried to use:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/localhost:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>docker-compose contains:<\/p>\n<pre><code>... \n   mlflow:\n        restart: always\n        image: mlflow_server\n        container_name: mlflow_server\n        ports:\n          - &quot;5000:5000&quot;\n        networks:\n          - postgres\n          - s3\n        environment:\n          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n          - MLFLOW_S3_ENDPOINT_URL=http:\/\/nginx:9000\n        command: mlflow server --backend-store-uri postgresql:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@db\/${POSTGRES_DB} --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/ --host 0.0.0.0\n...\n<\/code><\/pre>\n<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1653167453863,
        "Question_score":0,
        "Question_tags":"docker|mlflow|mlops|dvc",
        "Question_view_count":117,
        "Owner_creation_time":1577631947330,
        "Owner_last_access_time":1663676491067,
        "Owner_location":"Saint Petersburg",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1653372987987,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72333237",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":53213596,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_time":1420765480790,
        "Owner_last_access_time":1663614206113,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549414531500,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56818930,
        "Question_title":"\"dvc push\" after several local commits",
        "Question_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561823734517,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":916,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1561842649417,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561847539150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66505949,
        "Question_title":"problem with dvc import-url from google spreadsheet export",
        "Question_body":"<p>I'm in the process of converting a Makefile-based data workflow to dvc. I have a Google spreadsheet that I'm using in a data workflow to make it easy to update a few things in a makeshift database. Currently this works with something like this:<\/p>\n<pre><code># Makefile\ndata.csv:\n    curl -L https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv &gt; data.csv\n<\/code><\/pre>\n<p>Of course, I can incorporate the same step into my dvc pipeline directly with <code>dvc run<\/code>, but my understanding is that something like <code>dvc import-url<\/code> would be more appropriate but I'm getting an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import-url https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv data.csv\nImporting 'https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv' -&gt; 'data.csv'\nERROR: unexpected error - 'NoneType' object has no attribute 'endswith'\n<\/code><\/pre>\n<p>My guess is that this is because the response data from the Google Spreadsheet export url doesn't have a filename suffix associated with it. Is there a way to work around this problem? Is there a better way to pull data from a google spreadsheet into a dvc workflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1615035920157,
        "Question_score":2,
        "Question_tags":"google-sheets|google-sheets-api|dvc",
        "Question_view_count":59,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66505949",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58163305,
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1569833846627,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569837069043,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61816429,
        "Question_title":"DVC dependencies for derived data without imports",
        "Question_body":"<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.<\/p>\n\n<p><strong>My question:<\/strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import<\/code> to achieve this.<\/p>\n\n<p><strong>Details:<\/strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.<\/p>\n\n<p>Currently my Idea is to <code>dvc get<\/code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.<\/p>\n\n<pre><code>hugeFileRepo\n +metaData.csv\n +dataFolder\n +-- hugeFile_1\n ...\n +-- hugeFile_n\n<\/code><\/pre>\n\n<p>in the derivedData repository I do<\/p>\n\n<pre><code> dvc import hugeFileRepo.git metaData.csv\n dvc run -f derivedData.dvc \\\n    -d metaData.csv \\\n    -d deriveData.py \\\n    -o derivedDataFolder \\\n    python deriveData.py \n<\/code><\/pre>\n\n<p>My deriveData.py does something along the line (pseudocode)<\/p>\n\n<pre><code>metaData = read(metaData.csv)\n\n#Hack because I don't know how to it right:\ngitRevision = getGitRevision(metaData.csv.dvc)          \n...\nfor metaDataForFile, file in metaData:\n   if(iWantFile(metaDataForFile) ):\n      #download specific file\n      !dvc get --rev {gitRevision} -o tempFolder\/{file} hugeFileRepo.git {file}\n\n      #do processing of huge file and store result in derivedDataFolder\n      processAndWrite(tempFolder\/file)\n<\/code><\/pre>\n\n<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589536322927,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":179,
        "Owner_creation_time":1453724533500,
        "Owner_last_access_time":1663059905923,
        "Owner_location":null,
        "Owner_reputation":171,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61816429",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60527213,
        "Question_title":"How does DVC store differences on the directory level into DVC cache?",
        "Question_body":"<p>Can someone explain how DVC stores differences on the directory level into DVC cache. <\/p>\n\n<p>I understand that the DVC-files (.dvc) are metafiles to track data, models and reproduce pipeline stages. However, it is not clear for me how the process of creating branches, commiting them and switching back to a master file is exactly saved in differences. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583328485690,
        "Question_score":4,
        "Question_tags":"version-control|dvc",
        "Question_view_count":623,
        "Owner_creation_time":1583308412843,
        "Owner_last_access_time":1587640257240,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1583332201397,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60527213",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643641422880,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_time":1457469301700,
        "Owner_last_access_time":1663223040440,
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1643651143317,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67122683,
        "Question_title":"DVC Files Incomplete",
        "Question_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618565400113,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":548,
        "Owner_creation_time":1348150034833,
        "Owner_last_access_time":1663852884933,
        "Owner_location":"Glasgow, UK",
        "Owner_reputation":2400,
        "Owner_up_votes":66,
        "Owner_down_votes":12,
        "Owner_views":263,
        "Question_last_edit_time":1618826341417,
        "Answer_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1618566517710,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65771384,
        "Question_title":"Data Version Control (dvc) cannot push to remote storage because querying cache",
        "Question_body":"<p>I am setting up a remote storage with dvc using webdavs<\/p>\n<p>I can connect to the remote storage from Finder.<\/p>\n<p>I added the new remote and I see it when I check (dvc remote list)<\/p>\n<p>But when I try to push data, I have the request for password with 0% Querying cache<\/p>\n<p>It stays 0% forever. And when I enter the password, it ends with the following error:<\/p>\n<p>ERROR: unexpected error - No connection with LINK_OF_REMOTE_STORAGE<\/p>\n<p>The only thing I am thinking about is how to check if I can connect to the server from dvc and why querying cache never ends (maybe never starts even)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1610960223987,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":436,
        "Owner_creation_time":1411637192833,
        "Owner_last_access_time":1648687432467,
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1610961648253,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65771384",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67104752,
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_body":"<p>I'm using dvc and connecting to remote S3 for data track and also setting remote dvc cache in same remote S3.\nFollowing is configure file,<\/p>\n<pre><code>[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n[\u2018remote \u201cs3remote\u201d\u2019]\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n[\u2018remote \u201cs3cache\u201d\u2019]\n    url = s3:\/\/dvc-example\/cache\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>I'm able to push and pull from remote repository to local using s3remote.<\/p>\n<p>But when I try to add external data by configuring cache(s3cache), am getting error.<\/p>\n<p>Both s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618474768363,
        "Question_score":4,
        "Question_tags":"git|amazon-web-services|machine-learning|amazon-s3|dvc",
        "Question_view_count":352,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":1618497721960,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67104752",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58511529,
        "Question_title":"Highlight.js not respecting parent regex of a sub mode",
        "Question_body":"<p>I need to write a lexer which highlights my command-line tool commands properly.<\/p>\n\n<pre><code>$ dvc add file.csv\n$ dvc pipeline list\n<\/code><\/pre>\n\n<p>So the command starts with <code>dvc<\/code> and it may have one or two subcommands - <code>add<\/code> or <code>pipeline list<\/code> respectively.<\/p>\n\n<p>Therefore, it should highlight <code>dvc add<\/code> and <code>dvc pipeline list<\/code> in first and second case respectively.<\/p>\n\n<pre><code>contains: [\n          {\n            begin: \/^\\s*\\$\\s(dvc|git) [a-z-]+\/,\n            returnBegin: true,\n            contains: [\n              {\n                begin: \/dvc [a-z-]+ ?\/,\n                lexemes: '[a-z-]+',\n                keywords: {\n                  built_in:\n                    'dvc'\n                },\n                contains: [\n                  {\n                    begin: \/\\w+(?![\\S])\/,\n                    keywords: {\n                      built_in: 'list'\n                    }\n                  }\n                ],\n                className: 'strong'\n              }\n            ]\n          }\n        ]\n<\/code><\/pre>\n\n<p>It matches <code>dvc pipeline list<\/code> even though the parent regex i.e. <code>\/^\\s*\\$\\s(dvc|git) [a-z-]+\/<\/code> should only match till <code>dvc pipeline<\/code>. How is it exactly functioning?<\/p>\n\n<p>How does <code>\/dvc [a-z-]+ ?\/<\/code> override it and continues matching the expression?<\/p>\n\n<p>Please refer to this library docs here: <a href=\"https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html\" rel=\"nofollow noreferrer\">https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1571774550387,
        "Question_score":3,
        "Question_tags":"javascript|syntax-highlighting|highlight|highlight.js|dvc",
        "Question_view_count":177,
        "Owner_creation_time":1562529879143,
        "Owner_last_access_time":1625679183400,
        "Owner_location":null,
        "Owner_reputation":209,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":1572048186933,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58511529",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71378280,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_score":1,
        "Question_tags":"git|dataset|google-colaboratory|dvc",
        "Question_view_count":707,
        "Owner_creation_time":1525227015313,
        "Owner_last_access_time":1663842020310,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1652856778060,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1647022114533,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654718868090,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_time":1562403039337,
        "Owner_last_access_time":1659463717997,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Question_last_edit_time":1655157056467,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1654875420463,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_time":1558529684193,
        "Owner_last_access_time":1642243398997,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1594640021920,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1587029761967,
        "Answer_score":6.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72645665,
        "Question_title":"How to track a folder again when used \"git rm -rf --cached folder_name\" : Error: The following paths are ignored by one of your .gitignore files",
        "Question_body":"<p>I wanted to un-track my git files so I put <code>.dvc<\/code> inside my <code>.gitignore<\/code> file, and run<\/p>\n<pre><code>git rm -rf --cached .dvc\n<\/code><\/pre>\n<p>and then committed.<\/p>\n<p>I realised my mistake soon and then wanted to add the files again . I tried deleting the <code>gitignore<\/code> file, commit, make a new <code>.gitignore<\/code> and then try adding but all is futile. <code>git add .dvc<\/code> does not track my files and using <code>git add .dvc\/*<\/code> gives me error:<\/p>\n<pre><code>The following paths are ignored by one of your .gitignore files:\n.dvc\/cache\n.dvc\/tmp\nUse -f if you really want to add them.\n<\/code><\/pre>\n<p>Running the command <code>git check-ignore -v .dvc\/*<\/code> gives me:<\/p>\n<pre><code>.dvc\/.gitignore:3:\/cache    .dvc\/cache\n.dvc\/.gitignore:2:\/tmp  .dvc\/tmp\n\n<\/code><\/pre>\n<p>What can be done now?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1655381585170,
        "Question_score":1,
        "Question_tags":"git|gitignore|dvc",
        "Question_view_count":41,
        "Owner_creation_time":1561995857583,
        "Owner_last_access_time":1664035105223,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":2716,
        "Owner_up_votes":403,
        "Owner_down_votes":26,
        "Owner_views":668,
        "Question_last_edit_time":1655382233167,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72645665",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70939675,
        "Question_title":"DVC Push KeyError fileSize",
        "Question_body":"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with<\/p>\n<pre><code>ERROR: unexpected error - KeyError('fileSize')\n<\/code><\/pre>\n<p><strong>Edit<\/strong>\nSo searching around it seem that it might help to include the verbose log with regards to the error.<\/p>\n<pre><code>T11:27:08~\/documents\/*****\/data$ dvc push -v\n2022-02-01 11:32:13,186 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/config.local' to gitignore file.\n2022-02-01 11:32:13,199 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp' to gitignore file.\n2022-02-01 11:32:13,200 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to gitignore file.\n2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to '*********'\n2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'\n2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'\n2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files\n2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse\n2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 226, in __getitem__\n    return dict.__getitem__(self, key)\nKeyError: 'fileSize'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 56, in push\n    pushed += self.cloud.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 499, in hashes_exist\n    remote_hashes = set(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 334, in _list_hashes_traverse\n    yield from itertools.chain.from_iterable(in_remote)\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 324, in list_with_update\n    return list(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 107, in find\n    yield from self.fs.find(path)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/fs\/spec.py&quot;, line 323, in find\n    &quot;size&quot;: int(item[&quot;fileSize&quot;]),\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 229, in __getitem__\n    raise KeyError(e)\nKeyError: KeyError('fileSize')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1643714210580,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":54,
        "Owner_creation_time":1333908536530,
        "Owner_last_access_time":1664025870700,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":884,
        "Owner_up_votes":185,
        "Owner_down_votes":3,
        "Owner_views":59,
        "Question_last_edit_time":1643715642690,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70939675",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73700203,
        "Question_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Question_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663059618623,
        "Question_score":1,
        "Question_tags":"git|amazon-s3|caching|pycharm|dvc",
        "Question_view_count":31,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663149911323,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69254525,
        "Question_title":"ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored",
        "Question_body":"<p>I just started with DVC. I have a git repo in which there are heavy models that i want to push to dvc. So I initialized the dvc by<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>and then configured the bucket<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>Now there is <code>\/models<\/code> folders, in which there was <code>.gitkeep<\/code> file and trained models. Following entry was in my <code>.gitignore<\/code><\/p>\n<pre><code>*.tar.gz\n<\/code><\/pre>\n<p>I ran the following command<\/p>\n<pre><code>git rm -r --cached my_server\\models\n<\/code><\/pre>\n<p>and added the following in the <code>.gitignore<\/code><\/p>\n<pre><code>models\n<\/code><\/pre>\n<p>I want to add all the <code>tar.gz<\/code> files to push on dvc<\/p>\n<p>so i tried<\/p>\n<pre><code>dvc add .\/my_server\/models\/*.tar.gz\n<\/code><\/pre>\n<p>but this is showing<\/p>\n<pre><code>ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored.\n<\/code><\/pre>\n<p>If I do\ndvc add .\/my_server\/models\/<\/p>\n<p>then this folder is added and a <code>models.dvc<\/code> file gets created. then git code shows for the changes.<\/p>\n<p>what is the correct way, do i need to mention <code>*.dvc<\/code> to <code>.gitignore<\/code> as well?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632141296913,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":760,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236886743,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69254525",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68284742,
        "Question_title":"DVC experiment is restoring deleted files",
        "Question_body":"<p>I am using DVC to run experiments in my project using<\/p>\n<pre><code>dvc exp run\n<\/code><\/pre>\n<p>Now when i make changes to a file(example train.py) and run &quot;dvc exp run&quot; everything goes well,\nbut my problem is that when making changes by <strong>deleting<\/strong> a file(example train.py or an image in the data folder) as soon as i run the &quot;dvc exp run&quot; the file is restored.\nhow to stop that from happening?<\/p>\n<p>This is my dvc.yaml:<\/p>\n<pre><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n    - train.py\n    metrics:\n    - metrics.txt:\n        cache: false\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1625655452383,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":272,
        "Owner_creation_time":1557314563687,
        "Owner_last_access_time":1645485519443,
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":113,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68284742",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861552,
        "Question_title":"Getting this weird error when trying to run DVC pull",
        "Question_body":"<p>I am new to using DVC and just exploring it. I am trying to pull data from s3 that was pushed by another person on my team. But I am getting this error:<\/p>\n\n<pre><code>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\nname: head_test_file.csv, md5: 45db668193ba44228d61115b1d0304fe\nWARNING: Cache '45db668193ba44228d61115b1d0304fe' not found. File 'head_test_file.csv' won't be created.\nNo changes.\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\nhead_test_file.csv\nDid you forget to fetch?\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585201186813,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":7063,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861552",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56456463,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1559722020690,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":326,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1568948355943,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559758177417,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68517516,
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation:",
        "Question_body":"<p>my ~\/.aws\/credentials looks like<\/p>\n<pre><code>[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<pre><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<p>I have made my .dvc\/config.local to look like<\/p>\n<pre><code>[\u2018remote \u201cmyremote\u201d\u2019]\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test\naccess_key_id = XYZ\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS\nprofile=\u2018testing\u2019\ncredentialpath = \/Users\/nyt21\/.aws\/credentials\n<\/code><\/pre>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n<\/blockquote>\n<p>** Update\nhere is the output of <code>dvc push -v<\/code><\/p>\n<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.\n2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3:\/\/bib-ds-models-testing\/data\/dvc-test'\n2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3:\/\/bib-ds-models-testing\/data\/dvc-test\n2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...\n2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     \n2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes\n2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists\n2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          \n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1057, in _info\n    out = await self._simple_info(path)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 970, in _simple_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Access Denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 50, in do_run\n    return self.run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 51, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 44, in push\n    pushed += self.cloud.push(objs, jobs, remote=remote)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 79, in push\n    return remote_obj.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 57, in wrapper\n    return f(obj, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 494, in push\n    ret = self._process(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 351, in _process\n    dir_status, file_status, dir_contents = self._status(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 195, in _status\n    self.hashes_exist(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 145, in hashes_exist\n    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 438, in hashes_exist\n    remote_hashes = self.list_hashes_exists(hashes, jobs, name)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 389, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 619, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 444, in result\n    return self.__get_result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 389, in __get_result\n    raise self._exception\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 380, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 92, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 87, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 68, in sync\n    raise result[0]\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 24, in _runner\n    result[0] = await coro\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 802, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1061, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1004, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-07-25 22:40:39,712 DEBUG: Version info for developers:\nDVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk3s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk3s1s1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.\n2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n<\/code><\/pre>\n<p>I can upload through python<\/p>\n<pre><code>import boto3\nimport os\nimport pickle\n\nbucket_name = 'bib-ds-models-testing'\nos.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;\nsession = boto3.Session()\ns3_client = boto3.client('s3')\n\ns3_client.upload_file('\/Users\/nyt21\/Devel\/DVC\/test\/data\/iris.csv',\n    'bib-ds-models-testing',\n    'data\/dvc-test\/my_iris.csv')\n<\/code><\/pre>\n<p>I don't use aws CLI but the following also gives an access deny !<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDenied) when calling the ListObjectsV2\noperation: Access Denied<\/p>\n<\/blockquote>\n<p>but it works if I add --profile=testing<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test --profile=testing\n                       \n<\/code><\/pre>\n<blockquote>\n<p>PRE dvc-test\/<\/p>\n<\/blockquote>\n<p>just you know environment variable <code>AWS_PROFILE<\/code> is already set to 'testing'<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>I have tried both <code>AWS_PROFILE='testing'<\/code> and <code>AWS_PROFILE=testing<\/code>, neither of them worked.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_time":1627207477173,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":1526,
        "Owner_creation_time":1297356892887,
        "Owner_last_access_time":1663786584013,
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":4988,
        "Owner_up_votes":350,
        "Owner_down_votes":22,
        "Owner_views":416,
        "Question_last_edit_time":1627383297973,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68517516",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73567700,
        "Question_title":"What DVC does when git merge is executed?",
        "Question_body":"<p>I have two git branches (master and develop). DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662027331777,
        "Question_score":1,
        "Question_tags":"git|dvc",
        "Question_view_count":40,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662097250203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567700",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_time":1331553057367,
        "Owner_last_access_time":1663939436043,
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Question_last_edit_time":1641199464667,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641179335767,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72641284,
        "Question_title":"Undo changes in pandas Dataframe(column drops, row drops, edits performed on a single cell)",
        "Question_body":"<p>I am currently working on developing a 'undo' operation for my interface that deals with changes performed on csv files. I want to provide an option for the user to revert the changes that he had done to the csv file, these changes include edit a cell, deleting column, deleting row, adding row, adding column etc. For this I want to know, does version control works in this scenario? If yes, which data version control should I prefer? If not, please suggest me an another alternative.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1655360134273,
        "Question_score":1,
        "Question_tags":"python|pandas|git|version-control|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1655359768507,
        "Owner_last_access_time":1658381602853,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655716541577,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72641284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":52871630,
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539857086453,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_time":1508231047660,
        "Owner_last_access_time":1663539768267,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Question_last_edit_time":1539881078543,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1540629268433,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861593,
        "Question_title":"How do I specify encryption type when using s3remote for DVC",
        "Question_body":"<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting <\/p>\n\n<p>But when I run the <code>dvc push<\/code> command, I get the generic error saying <\/p>\n\n<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>which I know for a fact that I get that error when I don't specify the encryption.<\/p>\n\n<p>It is similar to running <code>aws s3 cp<\/code> with <code>--sse<\/code> flag or specifying <code>ServerSideEncryption<\/code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585201508167,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":293,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861593",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_time":1587308895270,
        "Owner_last_access_time":1663873987327,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655524559503,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655524277717,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_time":1618255062697,
        "Owner_last_access_time":1645555346683,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1622257491983,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1622257759210,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1622503453297,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":54324146,
        "Question_title":"Azure DataLake with DVC",
        "Question_body":"<p>We are thinking to use DVC for versioning input data for DataScience project.\nmy data resides in Azure DataLake Gen1.<\/p>\n\n<p>how do i configure DVC to push data to Azure DataLake using Service Principal?\ni want DVC to store cache and data into Azure DataLake instead on local disk.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1548236331560,
        "Question_score":5,
        "Question_tags":"azure-data-lake|service-principal|dvc",
        "Question_view_count":473,
        "Owner_creation_time":1257862078920,
        "Owner_last_access_time":1662705544550,
        "Owner_location":"Pune, India",
        "Owner_reputation":6219,
        "Owner_up_votes":227,
        "Owner_down_votes":7,
        "Owner_views":587,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54324146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67215839,
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1619103654467,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1619127242127,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67215839",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67891465,
        "Question_title":"ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored",
        "Question_body":"<p>Getting the error &quot;<em>ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored.<\/em>&quot; while trying to add local files for tracking<\/p>\n<p>Python Version : 3.7<\/p>\n<p>Library used:<\/p>\n<p><code>pip install dvc  pip install dvc[gdrive]   dvc init   <\/code><\/p>\n<p><strong>dvc add -R Training_Batch_Files<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1623171603727,
        "Question_score":3,
        "Question_tags":"python|git|dvc",
        "Question_view_count":1844,
        "Owner_creation_time":1593952964623,
        "Owner_last_access_time":1664007410690,
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67891465",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73565648,
        "Question_title":"DVC shows files not tracked in source control in visual studio code",
        "Question_body":"<p>I'm using DVC extension in VScode inside a python project. The problem is that dvc shows files not tracked by dvc in the source control panel! As in the following picture.\nDVC track only data folder and not the src folder. How can I fix it? Have you also encountered these problems?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662017466370,
        "Question_score":0,
        "Question_tags":"visual-studio-code|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662021092697,
        "Answer_body":"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.\nOnce the files are tracked by one of the tools they should only show up under the appropriate tree.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662022756173,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662022916690,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73565648",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67635688,
        "Question_title":"Installation DVC on MinIO storage",
        "Question_body":"<p>Does anybody install DVC on MinIO storage?<\/p>\n<p>I have read <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">docs<\/a> but not all clear for me.<\/p>\n<p>Which command should I use for setup MinIO storage with this entrance parameters:<\/p>\n<p>storage url: <a href=\"https:\/\/minio.mysite.com\/minio\/bucket-name\/\" rel=\"nofollow noreferrer\">https:\/\/minio.mysite.com\/minio\/bucket-name\/<\/a>\nlogin: my_login\npassword: my_password<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621595380413,
        "Question_score":5,
        "Question_tags":"python|minio|dvc",
        "Question_view_count":1547,
        "Owner_creation_time":1526481416047,
        "Owner_last_access_time":1663922031783,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1621622008697,
        "Answer_body":"<p><strong>Install<\/strong><\/p>\n<p>I usually use it as a Python package, int this case you need to install:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install &quot;dvc[s3]&quot;\n<\/code><\/pre>\n<p><strong>Setup remote<\/strong><\/p>\n<p>By default DVC supports AWS S3 storages and they work fine.<br \/>\nAlso they support &quot;S3-compatible storage&quot;, but setup for this type of remotes is nod described properly. In particular case of MinIO you have <strong>bucket<\/strong> - directory on MinIO server where actual data stores (it is similar to AWS bucket), but DVC uses AWS CLI to authenticate. In case of MinIO you need to pass them explicitly.<\/p>\n<p>Then follow commands to setup your DVC remote:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)\ndvc remote add -d minio s3:\/\/bucket-name -f\n\n# add information about storage url (where &quot;https:\/\/minio.mysite.com&quot; your url)\ndvc remote modify minio endpointurl https:\/\/minio.mysite.com\n\n#  add info about login and password\ndvc remote modify minio access_key_id my_login\ndvc remote modify minio secret_access_key my_password\n<\/code><\/pre>\n<p><strong>If you move from old remote<\/strong>, use follow command to move your data:<\/p>\n<p>Before setup (download all old remote cache to local machine):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>\n<p>After setup (upload all cache to a new remote):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621599285543,
        "Answer_score":6.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67635688",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65847574,
        "Question_title":"Failed to pull existing files from SSH DVC Remote",
        "Question_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1611327752360,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1715,
        "Owner_creation_time":1355002392777,
        "Owner_last_access_time":1663450068853,
        "Owner_location":"Bolzano, Italia",
        "Owner_reputation":530,
        "Owner_up_votes":282,
        "Owner_down_votes":3,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1616082756537,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72651603,
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655411665790,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1655415427550,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655523571800,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655706111940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69725612,
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635260868803,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1635332461183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635332764020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70711015,
        "Question_title":"Is there an alternative to DVC pipelines to create a DAG which is also aware of inputs\/outputs to nodes to cache results?",
        "Question_body":"<p>I recently started to use DVC pipelines to create DAG in my application. I work on Machine Learning projects, and I need to experiment a lot with different nodes of my system. For example:<\/p>\n<p><code>Data preprocessing -&gt; feature extraction -&gt; model training -&gt; model evaluation<\/code><\/p>\n<p>Each node produces an output, and the output of each node is used in another node. What DVC allows me to do is to create a pipeline in which I can specify dependencies between nodes. I also use <code>.yaml<\/code> files to configure parameters of my application, and you can also specify these parameters as dependencies for different nodes. So, whenever a dependency changes between nodes (it can be either configuration parameters or inputs\/outputs specified), DVC is able to detect this, and run the necessary parts of the pipeline. If a dependency hasn't changed for a particular node, DVC can use its cache to skip that step. This is really useful for me, since some nodes take really long time to execute, and they don't always need to be ran (if their dependencies hasn't changed).<\/p>\n<p>I also started to use hydra to manage my config files, and to be honest, DVC doesn't work well with hydra. It expects a static config to specify parameter dependencies, and with hydra it is a bit tricky to do, and complicate things.<\/p>\n<p>My question is: is there any alternative to DVC Pipelines which also goes well with hydra?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1642165507820,
        "Question_score":1,
        "Question_tags":"pipeline|directed-acyclic-graphs|dvc|hydra-core",
        "Question_view_count":181,
        "Owner_creation_time":1548055925357,
        "Owner_last_access_time":1664039002623,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":551,
        "Owner_up_votes":9,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70711015",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64456396,
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603252093370,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_time":1446746840593,
        "Owner_last_access_time":1664083975707,
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Question_last_edit_time":1603305923907,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1603314290770,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603325349590,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56269391,
        "Question_title":"Readding missing files to DVC",
        "Question_body":"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull<\/code> I get the output<\/p>\n\n<pre><code>[##############################] 100% Analysing status.\nWARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data\/2.mp4' won't be created.\nWARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data\/3.mp4' won't be created.\n<\/code><\/pre>\n\n<p>The <code>dvc status<\/code> command gives me:<\/p>\n\n<pre><code>data\/2.mp4.dvc:\n    changed outs:\n        not in cache:       data\/2.mp4\ndata\/3.mp4.dvc:\n    changed outs:\n        not in cache:       data\/3.mp4\n<\/code><\/pre>\n\n<p>It seems that <code>2.mp4<\/code> and <code>3.mp4<\/code> where added under dvc control but <code>dvc push<\/code> command has not been executed.<\/p>\n\n<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:<\/p>\n\n<pre><code>dvc remove data\/2.mp4.dvc\ndvc remove data\/3.mp4.dvc\n\ndvc add data\/2.mp4 \ndvc add data\/3.mp4 \n<\/code><\/pre>\n\n<p>But there is no effect. How can I remove files from under dvc control and add them again?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558593719503,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1065,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558637203487,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56269391",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69265000,
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632209467140,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236873617,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1632210565513,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632211831430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57966851,
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568689927047,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_time":1383611307000,
        "Owner_last_access_time":1664061570950,
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1568693889197,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1568725966083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73126208,
        "Question_title":"Python: Ssl Certificate verify failed",
        "Question_body":"<p>I have installed <code>dvc<\/code> on my <code>ubuntu-18.04-LTS<\/code> system and while trying to download the <code>data<\/code> files from github using dvc, it fails with below error.<\/p>\n<pre><code>$ dvc get https:\/\/github.com\/iterative\/dataset-registry get-started\/data.xml -o data\/data.xml -v\n\n2022-07-22 12:55:22,260 DEBUG: Creating external repo https:\/\/github.com\/iterative\/dataset-registry@None\n2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https:\/\/github.com\/iterative\/dataset-registry' to a temporary dir\n2022-07-22 12:55:23,683 DEBUG: Removing '\/dvc\/dvc_test\/data\/.UEeAzwmJCY3q85YQuCeahx'\n2022-07-22 12:55:23,684 ERROR: failed to get 'get-started\/data.xml' from 'https:\/\/github.com\/iterative\/dataset-registry' - Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;urllib3\/connectionpool.py&quot;, line 703, in urlopen\n  File &quot;urllib3\/connectionpool.py&quot;, line 386, in _make_request\n  File &quot;urllib3\/connectionpool.py&quot;, line 1042, in _validate_conn\n  File &quot;urllib3\/connection.py&quot;, line 414, in connect\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 449, in ssl_wrap_socket\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl\n  File &quot;ssl.py&quot;, line 500, in wrap_socket\n  File &quot;ssl.py&quot;, line 1040, in _create\n  File &quot;ssl.py&quot;, line 1309, in do_handshake\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;dvc\/scm.py&quot;, line 145, in clone\n  File &quot;scmrepo\/git\/__init__.py&quot;, line 143, in clone\n  File &quot;scmrepo\/git\/backend\/dulwich\/__init__.py&quot;, line 199, in clone\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n<\/code><\/pre>\n<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com<\/code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc<\/code>the above errors are occurring, Even the below couldn't solve the issue.<\/p>\n<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;\n\nDefaultVerifyPaths(cafile=None, capath='\/usr\/lib\/ssl\/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='\/usr\/lib\/ssl\/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='\/usr\/lib\/ssl\/certs')\n<\/code><\/pre>\n<pre><code>export SSL_CERT_DIR=\/etc\/ssl\/certs\/\nexport REQUESTS_CA_BUNDLE=\/usr\/local\/lib\/python2.7\/dist-packages\/certifi\/cacert.pem\npip install --upgrade certifi\nexport PYTHONHTTPSVERIFY=0\n\nsudo apt install ca-certificates\nsudo update-ca-certificates --fresh\n<\/code><\/pre>\n<pre><code>$ python --version\nPython 2.7.17\n\n$ dvc doctor\nDVC version: 2.13.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\n<\/code><\/pre>\n<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;<\/code> Similarly do we have option in dvc?<\/p>\n<p>Further what should i update to resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658849780953,
        "Question_score":1,
        "Question_tags":"python|ssl|pip|ssl-certificate|dvc",
        "Question_view_count":157,
        "Owner_creation_time":1432810927473,
        "Owner_last_access_time":1664076190223,
        "Owner_location":null,
        "Owner_reputation":1609,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":447,
        "Question_last_edit_time":1658898077490,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73126208",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620243318287,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_time":1620132740443,
        "Owner_last_access_time":1641927840700,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620292401493,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66409283,
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1614516713937,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":1614699218993,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1614537291720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614698988013,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558708772617,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558674266680,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71663330,
        "Question_title":"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?",
        "Question_body":"<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.<\/p>\n<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.<\/p>\n<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.<\/p>\n<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.<\/p>\n<ol>\n<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?<\/p>\n<\/li>\n<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.<\/p>\n<\/li>\n<\/ol>\n<p>I understand from <a href=\"https:\/\/stackoverflow.com\/a\/35578715\/4533188\">here<\/a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.<\/p>\n<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1648561944137,
        "Question_score":0,
        "Question_tags":"git|git-lfs|dvc|git-annex",
        "Question_view_count":370,
        "Owner_creation_time":1423144219917,
        "Owner_last_access_time":1663937106050,
        "Owner_location":null,
        "Owner_reputation":11374,
        "Owner_up_votes":415,
        "Owner_down_votes":2,
        "Owner_views":845,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71663330",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72451922,
        "Question_title":"How can dvc pipeline recognize when to use encoding pipeline while new data added for the modeling?",
        "Question_body":"<p>I have created separate pipelines for feature encoding and feature scaling in DVC.\nNow, when I will input new data from my flask API, how these DVC pipelines will automatically run and encode and scale data for modelling?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1654017845533,
        "Question_score":0,
        "Question_tags":"machine-learning|pipeline|mlops|dvc",
        "Question_view_count":25,
        "Owner_creation_time":1654016718443,
        "Owner_last_access_time":1664013919797,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72451922",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617399923477,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_time":1407091594730,
        "Owner_last_access_time":1654654035420,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1617643343263,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1617404822540,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1617478114567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58952962,
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574248229420,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1574267475363,
        "Answer_score":13.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1642527991693,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68771979,
        "Question_title":"problems installing a DVC lower version [0.9.4]",
        "Question_body":"<p>I need to install an older version of DVC, namely 0.9.4, in a Python virtual environment.<\/p>\n<p>I used the command:<\/p>\n<pre><code>pip install dvc==0.9.4\n<\/code><\/pre>\n<p>Everything seemed to work fine. However, when I try to run a <code>dvc pull<\/code> command, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\Scripts\\dvc.exe\\__main__.py&quot;, line 4, in &lt;module&gt;\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\main.py&quot;, line 2, in &lt;module&gt;\n    from dvc.cli import parse_args\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cli.py&quot;, line 8, in &lt;module&gt;\n    from dvc.command.init import CmdInit\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\command\\init.py&quot;, line 1, in &lt;module&gt;\n    from dvc.project import Project\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\project.py&quot;, line 15, in &lt;module&gt;\n    from dvc.cloud.data_cloud import DataCloud\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\data_cloud.py&quot;, line 11, in &lt;module&gt;\n    from dvc.cloud.gcp import DataCloudGCP\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\gcp.py&quot;, line 4, in &lt;module&gt;\n    from google.cloud import storage as gc\nModuleNotFoundError: No module named 'google.cloud'\n<\/code><\/pre>\n<p>When I print the dvc version, I see:<\/p>\n<pre><code>0.9.4+6bb66e.mod\n<\/code><\/pre>\n<p>Can anyone please help? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628856076700,
        "Question_score":2,
        "Question_tags":"python|google-cloud-storage|dvc",
        "Question_view_count":234,
        "Owner_creation_time":1569838509623,
        "Owner_last_access_time":1663596099763,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1628882501660,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68771979",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67454531,
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620537862843,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_time":1620537484800,
        "Owner_last_access_time":1642059984007,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1620625906413,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620591578073,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62441146,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1592457436920,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592496929890,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619078487020,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_time":1580932981007,
        "Owner_last_access_time":1663744754923,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619081803750,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619084940030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73651050,
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1662650337133,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662656090387,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60365473,
        "Question_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Question_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582482701247,
        "Question_score":7,
        "Question_tags":"python|sql|r|git|dvc",
        "Question_view_count":689,
        "Owner_creation_time":1504097190907,
        "Owner_last_access_time":1647022258673,
        "Owner_location":null,
        "Owner_reputation":1365,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":193,
        "Question_last_edit_time":1582486128287,
        "Answer_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1582487867857,
        "Answer_score":12.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70752204,
        "Question_title":"How to resolve DVC Pull Error on Pycharm?",
        "Question_body":"<p>When I execute 'DVC Pull', I get the following error<\/p>\n<pre><code>&gt; dvc pull ERROR: unexpected error - invalid syntax (tz.py, line 78)    \n&gt; Traceback (most recent call last):   File\n&gt; &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/main.py&quot;,\n&gt; line 55, in main\n&gt;     ret = cmd.do_run()   File &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/command\/base.py&quot;,\n &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dateutil\/tz.py&quot;,\n&gt; line 78\n&gt;     `self._name`,\n&gt;     ^ SyntaxError: invalid syntax\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/jasma-balasangameshwara\/ml-heroku-fastapi\" rel=\"nofollow noreferrer\">How to resolve it? The link to the Github repository is <\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1642493442793,
        "Question_score":1,
        "Question_tags":"python|dvc",
        "Question_view_count":105,
        "Owner_creation_time":1579349493753,
        "Owner_last_access_time":1652115144133,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70752204",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71155959,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1645092064283,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085253,
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Question_last_edit_time":1645132430277,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1645113091963,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65824766,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_time":1493101921290,
        "Owner_last_access_time":1663968152030,
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1611228029327,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56881619,
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562220779043,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_time":1562220403123,
        "Owner_last_access_time":1563409319090,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881619",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73727709,
        "Question_title":"Can we connect oracle database with DVC ? and if yes then how?",
        "Question_body":"<p>I was trying to connect dvc with oracle database but unable to do it. So, Please can anyone help me with that.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1663229583107,
        "Question_score":1,
        "Question_tags":"oracle|dvc",
        "Question_view_count":20,
        "Owner_creation_time":1663229291407,
        "Owner_last_access_time":1663926949987,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727709",
        "Question_exclusive_tag":"DVC"
    }
]