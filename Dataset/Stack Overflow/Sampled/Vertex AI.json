[
    {
        "Question_title":"Vertex AI scheduled executor container updates",
        "Question_body":"<p>I have a Vertex AI scheduled executor which runs custom jobs. Runs are going fine and trains the model, but we often update helper classes in our project (our notebook instance has a GIT connection). These classes need to be updated in the Docker container as well. Is there a way to automatically point the schedule to the latest docker container in the artifact registry?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-16 09:46:56.923 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":10,
        "Owner_creation_date":"2020-04-21 08:20:19.48 UTC",
        "Owner_last_access_date":"2022-09-21 12:47:49.243 UTC",
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI custom prediction vs Google Kubernetes Engine",
        "Question_body":"<p>I have been exploring using Vertex AI for my machine learning workflows. Because deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI, I am considering a <a href=\"https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\">workaround<\/a>. With this workaround, I will be unable to use many Vertex AI features, like model monitoring, feature attribution etc., and it simply becomes, I think, a managed alternative to running the prediction application on, say, a GKE cluster. So, besides the cost difference, I am exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, for example, only <strong>N1<\/strong> machine types are available for prediction in Vertex AI<\/p>\n<p>There is a similar <a href=\"https:\/\/stackoverflow.com\/questions\/67930882\/google-kubernetes-engine-vs-vertex-ai-ai-platform-unified-for-serving-model-pr\">question<\/a>, but I it does not raise the specific questions I hope to have answered.<\/p>\n<ul>\n<li>I am not sure of the available disk space. In Vertex AI, one can specify the machine type, such as n1-standard-2 etc., but I am not sure what disk space will be available and if\/how one can specify it? In the custom container code, I may copy multiple model artifacts, or data from outside sources to the local directory before processing them so understanding any disk space limitations is important.<\/li>\n<li>For custom training in Vertex AI, one can use an interactive shell to inspect the container where the training code is running, as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/monitor-debug-interactive-shell\" rel=\"nofollow noreferrer\">here<\/a>. Is something like this possible for a custom prediction container? I have not found anything in the docs.<\/li>\n<li>For custom training, one can use a private IP for custom training as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-private-ip\" rel=\"nofollow noreferrer\">here<\/a>. Again, I have not found anything similar for custom prediction in the docs, is it possible?<\/li>\n<\/ul>\n<p>If you know of any other possible limitations, please post.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-15 18:11:54.017 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-kubernetes-engine|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":304,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":"<ol>\n<li>we don't specify a disk size, so default to 100GB<\/li>\n<li>I'm not aware of this right now. But if it's a custom container, you could just run it locally or on GKE for debugging purpose.<\/li>\n<li>are you looking for this? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-11-17 04:10:54.8 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Documentation on Vertex AI Feature Store online serving architecture?",
        "Question_body":"<p>There's documentation on <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/serving-online\" rel=\"nofollow noreferrer\">Vertex AI online serving<\/a>, but no mention of the underlying system being used other than <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">&quot;online serving nodes&quot;<\/a>. Is it <a href=\"https:\/\/cloud.google.com\/datastore\" rel=\"nofollow noreferrer\">Datastore<\/a>? Something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-10 03:44:33.257 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":21,
        "Owner_creation_date":"2013-12-10 22:23:13.85 UTC",
        "Owner_last_access_date":"2022-09-21 23:19:21.323 UTC",
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"I am wondering about limitations of BQML. What are some of the biggest limitations for data science projects when using BQML?",
        "Question_body":"<p>I am  dealing with an unbalanced panel dataset. So, I have a timestamp column as well as an orders column, a categories columns, product name,  office location column and selling price column and a few other columns. If I am trying to forecast how much I would be selling for both every category and product name for the next year given three years worth of data, Would BQML be a good place to start? Any recommended tutorials to get me started?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-16 14:29:05.493 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-bigquery|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training|multivariate-time-series",
        "Question_view_count":22,
        "Owner_creation_date":"2022-09-07 21:07:02.113 UTC",
        "Owner_last_access_date":"2022-09-24 19:41:56.25 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Error importing JSONL dataset into Vertex AI",
        "Question_body":"<p>I tried importing a JSONL dataset into Google's Vertex AI and get a weird and seemingly unrelated error:<\/p>\n<pre><code>Error: Could not parse the line, json is invalid or the format does not match the input schema: Cannot find field: classificationAnnotation in message google.cloud.aiplatform.master.schema.ImageBoundingBoxIoFormat. for: gs:\/\/[bucketname]\/set.jsonl line 10\n<\/code><\/pre>\n<p>It happens every 4 lines of code. All of my lines are identical except the image name changes.<\/p>\n<p>Line 10:<\/p>\n<pre><code>{&quot;imageGcsUri&quot;:&quot;gs:\/\/[mybucket]\/path\/to\/image.png&quot;,&quot;classificationAnnotation&quot;:{&quot;displayName&quot;:&quot;MyLabel&quot;,&quot;annotationResourceLabels&quot;:{&quot;aiplatform.googleapis.com\/annotation_set_name&quot;:&quot;MyLabel&quot;}},&quot;dataItemResourceLabels&quot;:{&quot;aiplatform.googleapis.com\/ml_use&quot;:&quot;training&quot;}}\n<\/code><\/pre>\n<p>Why am I getting this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-10 13:53:43.31 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":93,
        "Owner_creation_date":"2011-02-17 05:25:50.39 UTC",
        "Owner_last_access_date":"2022-09-25 02:17:50.377 UTC",
        "Owner_location":"Perth, Australia",
        "Owner_reputation":2950,
        "Owner_up_votes":237,
        "Owner_down_votes":98,
        "Owner_views":367,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI AutoML regression - batch predition error due to datatype mismatch",
        "Question_body":"<p>I trained a Vertex AI AutoML regression model (using the UI).\nI ran a Batch Prediction (alos with the UI) and it failed because of a datatype mismatch:\nThe Batch Prediction returned an error table in the export location in BigQuery.<\/p>\n<p>The Error :<\/p>\n<ul>\n<li><a href=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" alt=\"List item\" \/><\/a><\/li>\n<\/ul>\n<p><strong>In the output from the Batch Prediction, DISCOUNT_PCT is indeed a STRING<\/strong>:\n<a href=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>But in the table I loaded for the Batch Prediction, it is a NUMERIC<\/strong> (as it is in the data I used to train the model):\n<a href=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/q42iM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q42iM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the Batch Prediction process somehow changed the datatype of the table I loaded. Why is this happening and how can I solve it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-20 05:32:20.08 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"prediction|google-cloud-ml|type-mismatch|google-cloud-vertex-ai",
        "Question_view_count":65,
        "Owner_creation_date":"2018-08-23 00:42:15.633 UTC",
        "Owner_last_access_date":"2022-08-26 05:00:30.977 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-20 07:22:04.513 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"vertex ai: ResourceExhausted 429 received trailing metadata size exceeds limit",
        "Question_body":"<p>I am using google vertex AI online prediction:\nIn order to send an image it has to be in a JSON file in unit8 format which has to be less than 1.5 MB, when converting my image to uint8 it definitely exceeds 1.5MB.<\/p>\n<p>To go around this issue we can encode the unit8 file to b64, that makes the JSON file in KBs\nwhen running the prediction I get <code>Resource Exhausted: 429 received trailing metadata size exceeds limit<\/code>  Is there anyone who knows what's the problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-07-07 06:16:43.36 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-ai-platform|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":875,
        "Owner_creation_date":"2019-02-28 10:39:39.477 UTC",
        "Owner_last_access_date":"2022-09-22 11:56:41.14 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-07-07 08:27:52.767 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How do I set overcommit_memory on Google Cloud notebook?",
        "Question_body":"<p>I am using a GCP Vertex managed notebook and I get a memory error which I think can be fixed by:<\/p>\n<pre><code>echo 1 &gt; \/proc\/sys\/vm\/overcommit_memory\n<\/code><\/pre>\n<p>but when I run this from a Jupyterlab terminal I am asked for a sudo password, which I do not know. What can I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-25 13:38:16.42 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|memory-management|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":76,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Automl SDK code with file location from bigquery but having issue while predicting",
        "Question_body":"<p>I was creating a model using GCP automl sdk module in AI Platform (using bigquery table as input for training and predicting) and predicting using batch_prediction. The issue is that the code runs fine but the output table of predictions is empty and error table has all series from prediction dataframe and addition column stating error code 3 and error is &quot;The time series has no values to predict. The time series has been excluded from predictions.&quot;.<\/p>\n<p>Code which I have used for model training:<\/p>\n<pre><code>job = aiplatform.AutoMLForecastingTrainingJob(\n    display_name='train-sdk-automl_tst1',\n    optimization_objective='minimize-mae',    \n    column_transformations=[\n        {&quot;timestamp&quot;: {&quot;column_name&quot;: &quot;Date&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Price&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Grammage&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMax&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMin&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Consumer_promo&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Promo_Value&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Trade_Promotion&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Holiday&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Sales&quot;}},\n    ]\n)\n\n# This will take around an hour to run\nmy_model = job.run(\n    dataset=ds,\n    target_column='Sales',\n    time_column='Date',\n    time_series_identifier_column='SKU',\n    available_at_forecast_columns=['Date', 'Price','Grammage'\n                                   ,'apparentTemperatureMax','apparentTemperatureMin','Consumer_promo',\n                                   &quot;Promo_Value&quot;,&quot;Trade_Promotion&quot;,&quot;Holiday&quot;],\n    unavailable_at_forecast_columns=['Sales'],\n    forecast_horizon=21.0,\n    data_granularity_unit='week',\n    data_granularity_count=1,\n    weight_column=None,\n    budget_milli_node_hours=1000,\n    model_display_name='sdk_tsting_bq-forecast-model', \n    predefined_split_column_name=None\n)\n<\/code><\/pre>\n<p>Code for predictions:<\/p>\n<pre><code>BATCH_PREDICT_SOURCE = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool.test_data_sdk1'\nBATCH_PREDICT_DESTINATION_PREFIX = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool' \nmy_model.batch_predict(\n   bigquery_source=BATCH_PREDICT_SOURCE,\n   instances_format='bigquery',\n   bigquery_destination_prefix = BATCH_PREDICT_DESTINATION_PREFIX,\n   predictions_format='bigquery',\n   job_display_name='predict_sdk_tst')\n<\/code><\/pre>\n<p>Please suggest what might be going wrong here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-04 12:06:34.493 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-bigquery|sdk|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_date":"2021-08-04 11:57:28.313 UTC",
        "Owner_last_access_date":"2021-11-25 11:56:42.88 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"WebScrapping error in Google Cloud - Vertex AI Workbench (using Python3 & Selenium)",
        "Question_body":"<p>I am trying to use Workbench Managed Notebooks to schedule some Jupyter notebooks to run Selenium for webscrapping some pages.<\/p>\n<p>My code is below:<\/p>\n<pre><code>from get_gecko_driver import GetGeckoDriver\nget_driver = GetGeckoDriver()\nget_driver = GetGeckoDriver()\nget_driver.install()\n\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\n\nopts = FirefoxOptions()\nopts.add_argument(&quot;--headless&quot;)\nbrowser = webdriver.Firefox(options=opts)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nSessionNotCreatedException                Traceback (most recent call last)\n\/tmp\/ipykernel_1\/3371773802.py in &lt;module&gt;\n      4 opts = FirefoxOptions()\n      5 opts.add_argument(&quot;--headless&quot;)\n----&gt; 6 browser = webdriver.Firefox(options=opts)\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/firefox\/webdriver.py in __init__(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\n    178             command_executor=executor,\n    179             options=options,\n--&gt; 180             keep_alive=True)\n    181 \n    182         self._is_remote = False\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in __init__(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\n    275         self._authenticator_id = None\n    276         self.start_client()\n--&gt; 277         self.start_session(capabilities, browser_profile)\n    278 \n    279     def __repr__(self):\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in start_session(self, capabilities, browser_profile)\n    368         w3c_caps = _make_w3c_caps(capabilities)\n    369         parameters = {&quot;capabilities&quot;: w3c_caps}\n--&gt; 370         response = self.execute(Command.NEW_SESSION, parameters)\n    371         if 'sessionId' not in response:\n    372             response = response['value']\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in execute(self, driver_command, params)\n    433         response = self.command_executor.execute(driver_command, params)\n    434         if response:\n--&gt; 435             self.error_handler.check_response(response)\n    436             response['value'] = self._unwrap_value(\n    437                 response.get('value', None))\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/errorhandler.py in check_response(self, response)\n    245                 alert_text = value['alert'].get('text')\n    246             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 247         raise exception_class(message, screen, stacktrace)\n    248 \n    249     def _value_or_default(self, obj: Mapping[_KT, _VT], key: _KT, default: _VT) -&gt; _VT:\n\nSessionNotCreatedException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line\n<\/code><\/pre>\n<p>It seems that I need to setup somehow the location of Firefox, but I don't know if this is possible and how to do it (or if there are some alternatives that are easier to setup e.g. chromium).<\/p>\n<p><em>Please Note: this code is running perfectly fine when I run Jupyter notebooks in VM instances but I cannot schedule those notebooks to run automatically so I guess my only option is to go with the Vertex AI &gt; Workbench &gt; Managed Notebooks solution.<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 05:47:23.243 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|selenium|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":137,
        "Owner_creation_date":"2021-06-17 07:19:52.097 UTC",
        "Owner_last_access_date":"2022-07-22 04:46:15.717 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Question_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-06-30 02:07:26.45 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":122,
        "Owner_creation_date":"2021-03-03 02:45:00.313 UTC",
        "Owner_last_access_date":"2022-09-24 19:09:40.507 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-09 11:40:19.4 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-07-05 02:19:30.56 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to load images from Vertex AI managed dataset inside Python training code?",
        "Question_body":"<p>I am trying to create a <strong>custom training job<\/strong> in Vertex AI. I created a managed dataset stored in the same bucket I am exporting the training code to.\nI have a Python code that looks like this:<\/p>\n<pre><code>#Defining paths \nTRAIN_PATH = os.environ['AIP_TRAINING_DATA_URI']\nVAL_PATH = os.environ['AIP_VALIDATION_DATA_URI']\n\n#skipped model definition#\n\ntrain_datagen = image.ImageDataGenerator(rescale = 1.\/255, shear_range = 0.2,zoom_range = 0.2, horizontal_flip = True)\n\ntest_dataset = image.ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_PATH,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = 'binary')\nvalidation_generator = test_dataset.flow_from_directory(\n    VAL_PATH,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = 'binary')\n\nhist_new = model.fit(\n     train_generator, ...)\n<\/code><\/pre>\n<p>The question is, how do I load the images so the ImageDataGenerator can use them?\nThe error I get when starting the training job is:<\/p>\n<pre><code> No such file or directory: 'gs:\/\/(bucket name)\/dataset-5820440723492700160-image_classification_multi_label-2022-05-29T10:53:33.245485Z\/training-*'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-29 11:14:59.06 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|google-cloud-vertex-ai",
        "Question_view_count":98,
        "Owner_creation_date":"2019-01-04 19:50:49.567 UTC",
        "Owner_last_access_date":"2022-07-31 14:05:54.17 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-17 07:14:28.123 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"java|google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_date":"2008-11-20 08:57:51.293 UTC",
        "Owner_last_access_date":"2022-09-24 19:18:28.08 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-09-17 10:50:53.753 UTC",
        "Answer_last_edit_date":"2021-09-17 14:14:20.71 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-09-18 17:51:18.697 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex ai custom model training for pyspark ml model",
        "Question_body":"<p>Is it possible to train a spark\/pyspark ML lib model using VertexAI custom container model building? I couldn't find any reference in the vertex ai documents regarding spark model training. For distributed processing model building only options available are PyTorch or TensorFlow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-03 06:00:46.963 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|apache-spark-mllib|machine-learning-model|google-cloud-vertex-ai",
        "Question_view_count":432,
        "Owner_creation_date":"2021-09-03 05:52:21.403 UTC",
        "Owner_last_access_date":"2022-09-23 09:40:44.083 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Reading Data in Vertex AI Pipelines",
        "Question_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2022-04-22 00:40:56.41 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":892,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-22 19:57:49.577 UTC",
        "Answer_last_edit_date":"2022-04-25 07:07:54.733 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":"2022-04-25 00:09:49.253 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Use a model trained on image to detect objects in videos?",
        "Question_body":"<p>Using Google Vertex AI, I trained a model to detect some specific objects in images.<\/p>\n<p>Can i use this trained model to detect sames objects, but in videos ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-30 14:11:03.927 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":49,
        "Owner_creation_date":"2015-03-26 22:58:34.92 UTC",
        "Owner_last_access_date":"2022-03-13 18:06:48.14 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Vertex AI image AutoML classification when an important image feature is text inside the image",
        "Question_body":"<p>I'd like to do <strong>image classification<\/strong>. In my dataset, despite the fact that images features is a strong component for this classification (colors, shapes, etc), <strong>some categories<\/strong> of images will be <strong>hard to distinguish without interpreting the text inside the image<\/strong>.<\/p>\n<p>I don't think VertexAI\/AutoML will use pre-trained models in order to facilitate classification if in some case the only difference is the text. I know Google Vision\/OCR is capable of doing such extraction. But <strong>is there a way to do image classification (VertexAI\/AutoML) using Google Cloud Vision extraction as an additional image feature<\/strong>?<\/p>\n<p>Currently my project uses 3 models (no google cloud):<\/p>\n<ul>\n<li><em>model 1<\/em>: classify an image using images features<\/li>\n<li><em>model 2<\/em>: classify an image, only using OCR + regex (same categories)<\/li>\n<li><em>model 3<\/em>: combine both models and decide when to use model 1 or model 2<\/li>\n<\/ul>\n<p>I'd like to switch to Vertex AI the following will improve my project quality for the following:<\/p>\n<ul>\n<li>AutoML classification seems very good for <em>model 1<\/em><\/li>\n<li>I need to use a tool to manage my datasets (Vertex AI managed dataset)<\/li>\n<li>Vertex AI has interesting pipeline training features<\/li>\n<\/ul>\n<p>If it is confirmed that AutoML won't perform well if some images categories only differs in the text, I would recreate a similar 3-tier models using Vertex AI custom training scripts. I can easily create <em>model 1<\/em> with VertexAI\/AutoML. However I have no idea if:<\/p>\n<ul>\n<li>I can create <em>model 2<\/em> with a vertex ai custom training script using google cloud vision\/ocr to do image classification<\/li>\n<li>I can create <em>model 3<\/em> that would use <em>models 1<\/em> and <em>2<\/em> created by vertex ai.<\/li>\n<\/ul>\n<p>Could you give me recommendations on how to achieve that using Google Cloud Platform?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-07 11:48:25.613 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|ocr|google-cloud-vision|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":296,
        "Owner_creation_date":"2013-11-25 15:57:22.78 UTC",
        "Owner_last_access_date":"2022-07-22 16:56:54.153 UTC",
        "Owner_location":null,
        "Owner_reputation":197,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-11 10:22:56.593 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is it possible from a gcp vertex job to hit an http endpoint in another gcp pod in the same project?",
        "Question_body":"<p>I have a custom container vertex endpoint that is passed a url as input so that the job can call it to get a particular frame of data needed for the job. (gcs:\/\/ buckets do work) but I want to specifically use an http request to a server in the same gcp project.<\/p>\n<p>I have tried setting the endpoint up as private using the --networks param on the endpoint but then get the message:<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Making request from public OnePlatform API is not allowed on a private Endpoint peered with network (projects\/11111111111\/global\/networks\/some-dev-project-vpc).&quot;,\n    &quot;status&quot;: &quot;FAILED_PRECONDITION&quot;\n  }\n}\n<\/code><\/pre>\n<p>when I try to hit that private vertex endpoint.  I've tried curling it from within a running pod in the same project, but that didn't work either.<\/p>\n<p>Is there a way to do this?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-25 21:11:36.18 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":274,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>The error states that your request is to a public API, which may because you are using the public url schema to make your prediction. The structure of vertex endpoints differ between private and public, so double check that you are using the private endpoint url for your requests.<\/p>\n<p><strong>Public<\/strong><\/p>\n<pre><code>https:\/\/{REGION}-aiplatform.googleapis.com\/v1\/projects\/{PROJECT}\/locations\/{REGION}\/endpoints\/{ENDPOINT_ID}:predict\n<\/code><\/pre>\n<p><strong>Private<\/strong><\/p>\n<pre><code>http:\/\/{ENDPOINT_ID}.aiplatform.googleapis.com\/v1\/models\/{DEPLOYED_MODEL_ID}:predict\n<\/code><\/pre>\n<p>You can generate a private endpoint url using the following gcloud command:<\/p>\n<pre><code>gcloud beta ai endpoints describe {ENDPOINT_ID} \\\n  --region=us-central1 \\\n  --format=&quot;value(deployedModels.privateEndpoints.predictHttpUri)&quot;\n<\/code><\/pre>\n<p>More documentation on private endpoints can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#private-predict-uri-format\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-28 00:50:21.11 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2022-01-26 05:34:22.993 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Question_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-12 22:02:54.297 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":223,
        "Owner_creation_date":"2018-10-30 18:54:03.62 UTC",
        "Owner_last_access_date":"2022-09-23 17:00:36.977 UTC",
        "Owner_location":"Utah, USA",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Answer_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-13 04:16:45.167 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Custom Container deployment in vertex ai",
        "Question_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-24 13:42:01.653 UTC",
        "Question_favorite_count":2.0,
        "Question_score":3,
        "Question_tags":"flask|dockerfile|google-cloud-vertex-ai",
        "Question_view_count":629,
        "Owner_creation_date":"2021-09-08 09:22:34.063 UTC",
        "Owner_last_access_date":"2022-05-19 04:33:00.303 UTC",
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-09-28 05:16:11.387 UTC",
        "Answer_last_edit_date":"2021-09-28 06:36:09.573 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Custom Container Training Job python SDK - InvalidArgument 400 error",
        "Question_body":"<p>I'm attempting to run a Vertex AI custom training job using the python SDK, following the general instructions laid out in <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\" rel=\"nofollow noreferrer\">this readme<\/a>. My code is as follows (sensitive data removed):<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n    display_name='python_api_test',\n    container_uri='{URI FOR CUSTOM CONTAINER IN GOOGLE ARTIFACT REGISTRY}',\n    staging_bucket='{GCS BUCKET PATH IN 'gs:\/\/' FORMAT}',\n    model_serving_container_image_uri='us-docker.pkg.dev\/vertex-ai\/prediction\/tf2-cpu.2-4:latest',\n)\n\njob.run(\n    model_display_name='python_api_model',\n    args='{ARG PASSED TO CONTAINER ENTRYPOINT}',\n    replica_count=1,\n    machine_type='n1-standard-4',\n    accelerator_type='NVIDIA_TESLA_T4',\n    accelerator_count=2,\n    environment_variables={\n        {A COUPLE OF SECRETS PASSED TO CONTAINER IN DICTIONARY FORMAT}\n    }\n)\n<\/code><\/pre>\n<p>When I execute <code>job.run()<\/code>, I get the following error:<\/p>\n<pre><code>InvalidArgument: 400 Unable to parse `training_pipeline.training_task_inputs` into custom task `inputs` defined in the file: gs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml\n<\/code><\/pre>\n<p>The full traceback does not show where it is unhappy with any specific inputs. I've successfully run jobs in the same container using the Vertex CLI.I'm confident that there is nothing wrong with my <code>aiplatform.init()<\/code> (I'm running the job from a Vertex workbench machine in the same project).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 16:53:55.217 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":79,
        "Owner_creation_date":"2016-05-24 16:22:09.61 UTC",
        "Owner_last_access_date":"2022-09-13 16:37:07.037 UTC",
        "Owner_location":null,
        "Owner_reputation":457,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Sending http request Google Vertex AI end point",
        "Question_body":"<p>I've just deployed an ML model on Google vertex AI, it can make predictions using vertex AI web interface. But is it possible to send a request from a browser, for example, to this deployed model. Something like<\/p>\n<pre><code>http:\/\/myapp.cloud.google.com\/input=&quot;features of an example&quot; \n<\/code><\/pre>\n<p>and get the prediction as output.\nThanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-09 12:06:50.517 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":929,
        "Owner_creation_date":"2013-08-28 21:08:53.3 UTC",
        "Owner_last_access_date":"2022-09-24 20:18:43.383 UTC",
        "Owner_location":null,
        "Owner_reputation":349,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":"<p>Yes, you can send using endpoint URL as.<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\/v1beta1\/projects\/&lt;PROJECT_ID&gt;\/locations\/us-central1\/endpoints\/&lt;ENDPOINT_ID&gt;:predict\n<\/code><\/pre>\n<p>Data should be given as in POST parameter.<\/p>\n<pre><code>{\n  &quot;instances&quot;: \n    [1.4838871833555929,\n 1.8659883497083019,\n 2.234620276849616,\n 1.0187816540094903,\n -2.530890710602246,\n -1.6046416850441676,\n -0.4651483719733302,\n -0.4952254087173721,\n 0.774676376873553]\n}\n<\/code><\/pre>\n<p>URL should be Region Based.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-10 05:48:57.24 UTC",
        "Answer_last_edit_date":"2021-09-12 07:40:17.907 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Deploy Pretrained Model Test Error: The first dimension of paddings must be the rank of inputs[4,2]",
        "Question_body":"<p>I have a successfully trained and tested a custom instance segmentation model using pixellib Mask_RCNN model.  The model runs inferences fine locally, but when I try to serve predictions using vertex ai \/ google cloud platform I cannot get the predictions to serve correctly.<\/p>\n<h2>Model Signature:<\/h2>\n<pre><code>  inputs['input_anchors'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, 4)\n      name: serving_default_input_anchors:0\n  inputs['input_image'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, -1, 3)\n      name: serving_default_input_image:0\n  inputs['input_image_meta'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 14)\n      name: serving_default_input_image_meta:0\n<\/code><\/pre>\n<h2>JSON Requests - Snippet:<\/h2>\n<p>To test the model independent of any other code I use the test feature in google cloud console for vertex AI.  I enter JSON structured as follows.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;instances&quot;: \n    [\n    {\n     &quot;input_anchors&quot;: [[[-0.35, ...], ... ]],\n     &quot;input_image&quot;: [[[[-123.7, -116.8, -103.9], ... ]]], \n     &quot;input_image_meta&quot;: [[0.0, 240.0, ....]]\n    }\n    ]\n}\n<\/code><\/pre>\n<h2>Shapes of input<\/h2>\n<p>I can confirm that the shapes of the inputs are the correct dimensions for the model signature.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>np.array(instance['input_anchors']).shape -&gt; (1,1023,4)\nnp.array(instance['input_image']).shape -&gt; (1,64,64,3)\nnp.array(instance['input_image_meta'].shape -&gt; (1,14)\n<\/code><\/pre>\n<h2>Returns the error:<\/h2>\n<p>Testing the model returns the following error.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;The first dimension of paddings must be the rank of inputs[4,2] [1,1,64,64,3]\\\\n\\\\t [[{{node mask_rcnn\/zero_padding2d_1\/Pad}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>The message indicates that the model is looking for a 5D input <code>[1,1,64,64,3]<\/code>, but the model signature requires a 4D input <code>(-1,-1,-1,3)<\/code>.<\/p>\n<p>Using np.expand_dims to make the input_image 5D, it results in the same error message but now asking for a 6D input <code>[1,1,1,64,64,3]<\/code>.  A 6D input yields an error message asking for a 7D input...<\/p>\n<p>If I reduce the input_image to 3D (which should definitely be incorrect) <code>[64,64,3]<\/code> I get a different error:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;slice index 1 of dimension 0 out of bounds.\\\\n\\\\t [[{{node mask_rcnn\/roi_align_classifier\/strided_slice_8}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>Can someone help me understand if I'm structuring my inputs incorrectly for the model or if I'm running into a known bug?  It's strange that the error message always asks for +1 dimension than the dimension that I give it.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-30 21:27:07.497 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_date":"2016-03-08 10:05:14.813 UTC",
        "Owner_last_access_date":"2022-09-01 10:08:54.273 UTC",
        "Owner_location":null,
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Misstated quota exceed errors on managed notebooks in GCP",
        "Question_body":"<p>I am migrating some of my notebooks from the soon to be deprecated AI-Platform to the new Vertex platform in GCP. In Vertex I am using the &quot;Managed Notebooks&quot;, and all seemed to be working fine, but then suddenly I got this strange quota exceed error, when I am still far below my limits (I am currently at 4-5% of limit for the given APIs). See the error below:<\/p>\n<p>*Restarting notebook prototyping-notebook: Quota of &quot;::internal: operation &quot;projects\/1096432937575\/locations\/us-central1\/operations\/start-92b393a0-6c2e-4e11-be32-0172418d33c11643717891501967646&quot; completed with error: %!w(<em>status.Status=&amp;{{{} [] [] } 13 INTERNAL: operation name: operation-1643717891545-5d6f3e5095a3c-9aa08cdd-4bd8b9cd error code: QUOTA_EXCEEDED error message: Quota &quot; exceeded limit: 40 in region us-central1.<\/em><\/p>\n<p>I know it is still in preview, but anyone faced the same error or have any experience in how to fix this? Is there some combination of configurations that seems to be more stable than others?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-02-01 12:42:17.91 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_date":"2015-10-27 18:59:51.513 UTC",
        "Owner_last_access_date":"2022-09-22 13:11:14.613 UTC",
        "Owner_location":null,
        "Owner_reputation":181,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to run huge datasets in Vertex AI",
        "Question_body":"<p>I am working with large feature sets (20,000 rows x 20,000 columns) and Vertex AI has a hard limit of 1,000 columns. How can I import data into Google cloud efficiently so that I can run TensorFlow models or auto ML on my data? I haven't been able to find documentation for this issue.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-29 17:51:19.26 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_date":"2021-11-29 16:23:50.907 UTC",
        "Owner_last_access_date":"2022-09-07 21:59:54.27 UTC",
        "Owner_location":null,
        "Owner_reputation":353,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-31 13:31:49.61 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI deploy custom model to an endpoint",
        "Question_body":"<p>After I run<\/p>\n<pre><code>gcloud beta ai endpoints deploy-model (ENDPOINT : --region=REGION) --display-name=DISPLAY_NAME --model=MODEL [--accelerator=[count=COUNT],[type=TYPE]] [--deployed-model-id=DEPLOYED_MODEL_ID] [--disable-container-logging] [--enable-access-logging] [--machine-type=MACHINE_TYPE] [--max-replica-count=MAX_REPLICA_COUNT] [--min-replica-count=MIN_REPLICA_COUNT] [--service-account=SERVICE_ACCOUNT] [--traffic-split=[DEPLOYED_MODEL_ID=VALUE,\u2026]] [GCLOUD_WIDE_FLAG \u2026]\n<\/code><\/pre>\n<p>I got error &quot;(gcloud.beta.ai.endpoints.deploy-model) INVALID_ARGUMENT: AUTOMATIC_RESOURCES is not one of the supported deployment resources types for Model projects\/...\/locations\/us-central1\/models\/...<\/p>\n<p>What does this mean?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 00:56:05.487 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":63,
        "Owner_creation_date":"2021-04-01 22:58:50.923 UTC",
        "Owner_last_access_date":"2022-07-20 08:28:55.803 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to fix \"Kernel Unknown\" error in JupyterLab on Google Vertex AI notebooks (Python 3)",
        "Question_body":"<p>When I try to open an existing or new notebook on my Google Vertex Notebooks instance (on Python 3.7.6), no code cells run and it says &quot;kernel unknown&quot; at the top left. When I try to run a code cell, an asterisk appears as if it were running, but nothing else happens. Everything else works fine (e.g. opening the terminal, editing markdown cells in a notebook, viewing files). The issue started happening immediately after I tried to update conda to fix conflicting package errors.  What can I do to fix the issue and be able to run code cells again?<\/p>\n<p>(Please let me know if I'm leaving out important information)<\/p>\n<p>screenshot of the issue:\n<img src=\"https:\/\/i.stack.imgur.com\/7dwuB.png\" alt=\"error screenshot\" \/><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-12-13 17:43:00.063 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|conda|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":1110,
        "Owner_creation_date":"2021-12-13 17:33:05.647 UTC",
        "Owner_last_access_date":"2022-04-08 22:36:48.013 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-13 17:52:55.813 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"OSError: [WinError 123] when I create kfp component",
        "Question_body":"<p>I'm trying to create a pipeline in Vertex AI with kfp using my own components from local notebook in Spyder.<\/p>\n<p>When I run the following piece of code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;pandas&quot;])\ndef create_dataset(\n    gcs_csv_path_train: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(gcs_csv_path_train)\n    dataset = df.pop('Class')\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '&lt;ipython-input-11-b28c15ec667f&gt;'\n<\/code><\/pre>\n<p>The error is not raised if I use a Jupyter notebook online.<\/p>\n<p>What am I doing wrong? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-04 16:54:14.3 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":105,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to Access Managed Dataset in Vertex AI using Custom Container",
        "Question_body":"<p>In the google cloud documentation below:<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application<\/a><\/p>\n<p>It says that the following environment variables are sent to the training container:<\/p>\n<pre><code>AIP_DATA_FORMAT: The format that your dataset is exported in. Possible values include: jsonl, csv, or bigquery.\nAIP_TRAINING_DATA_URI: The location that your training data is stored at.\nAIP_VALIDATION_DATA_URI: The location that your validation data is stored at.\nAIP_TEST_DATA_URI: The location that your test data is stored at.\n<\/code><\/pre>\n<p>Where each of the URI values are wildcards that annotate training, validation, and test data files in <code>.jsonl<\/code> format as such:<\/p>\n<pre><code>gs:\/\/bucket_name\/path\/training-*\ngs:\/\/bucket_name\/path\/validation-*\ngs:\/\/bucket_name\/path\/test-*\n<\/code><\/pre>\n<p><strong>Now, in your custom container that contains the python code, how do you actually access the contents of each of the files?<\/strong><\/p>\n<p>I've tried splitting the URI string using the following regex to obtain the <code>bucket_name<\/code> and the <code>prefix<\/code> info, and attempted the grab it using <code>bucket.list_blobs(delimiter='\/', prefix=prefix[:-1])<\/code> but it returns nothing when the files are definitely there. Here is a minimal example of the attempted code:<\/p>\n<pre><code>import os\nimport re\nfrom google.cloud import storage\n\naip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\nmatch = re.match('gs:\/\/(.*?)\/(.*)', aip_training_data_uri)\nbucket_name, prefix = match.groups()\n\nclient = storage.Client()\nbucket = client.bucket(bucket_name)\nblobs = bucket.list_blobs(delimiter='\/', prefix=prefix[:-1]) # &quot;[:-1]&quot; to remove wildcard asterisks\n\nfor blob in blobs:\n   print(blob.download_as_string()) # This returns an empty string\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-02 18:44:38.26 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_date":"2018-01-26 07:18:10.47 UTC",
        "Owner_last_access_date":"2022-09-21 19:53:34.45 UTC",
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"BigQuery cannot find GeoPandas",
        "Question_body":"<p>I am trying to load a BigQuery table into a GeoDataFrame via:<\/p>\n<pre><code>gdf = \\\nclient.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      .result()\\\n      .to_geodataframe()\n<\/code><\/pre>\n<p>However, I get <strong><code>ValueError: The geopandas library is not installed<\/code><\/strong>:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n\/tmp\/ipykernel_782\/24642811.py in &lt;module&gt;\n      1 gdf = \\\n----&gt; 2 client.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      3       .result()\\\n      4       .to_geodataframe()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/bigquery\/table.py in to_geodataframe(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_column)\n   2098         &quot;&quot;&quot;\n   2099         if geopandas is None:\n-&gt; 2100             raise ValueError(_NO_GEOPANDAS_ERROR)\n   2101 \n   2102         geography_columns = set(\n\nValueError: The geopandas library is not installed, please install geopandas to use the to_geodataframe() function.\n<\/code><\/pre>\n<p>But <code>geopandas<\/code> is definitely installed. <code>import geopandas<\/code> works, <code>!pip3 install geopandas<\/code> results in <code>Requirement already satisfied<\/code> messages. I am on a Vertex AI workbench Jupyter notebook.<\/p>\n<p>Loading the data to a Pandas dataframe works fine:<\/p>\n<pre><code>df = \\\nclient.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;)\\\n      .result()\\\n      .to_dataframe()\n<\/code><\/pre>\n<p>I read about <code>to_geodataframe()<\/code> <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/bigquery\/latest\/google.cloud.bigquery.table.RowIterator#google_cloud_bigquery_table_RowIterator_to_geodataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p><strong>How could I fix the <code>to_geodataframe()<\/code> attribute of the <code>RowIterator<\/code> object <code>client.query(&quot;SELECT * FROM `myproject.mydataset.mytable`&quot;).result()<\/code>?<\/strong><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-31 15:05:28.033 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-bigquery|geopandas|google-cloud-vertex-ai",
        "Question_view_count":173,
        "Owner_creation_date":"2017-09-05 19:27:41.23 UTC",
        "Owner_last_access_date":"2022-09-24 21:09:42.077 UTC",
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":5748,
        "Owner_up_votes":2565,
        "Owner_down_votes":761,
        "Owner_views":969,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Authenticating model upload to VertexAI job from Cloud Scheduler",
        "Question_body":"<p>I am trying to run a custom training job on VertexAI. The goal is to train a model, save the model to cloud storage and then upload it to VertexAI as a VertexAI Model object. When I run the job from local workstation, it runs, but when I run the job from Cloud Scheduler  it fails. Details below.<\/p>\n<p><strong>Python Code for the job:<\/strong><\/p>\n<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport pickle\nfrom google.cloud import storage\nfrom google.cloud import aiplatform\n\nprint(&quot;FITTING THE MODEL&quot;)\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n# fit the model\nmodel.fit(X, y)\n\n\nprint(&quot;SAVING THE MODEL TO CLOUD STORAGE&quot;)\nif 'AIP_MODEL_DIR' not in os.environ:\n    raise KeyError(\n        'The `AIP_MODEL_DIR` environment variable has not been' +\n        'set. See https:\/\/cloud.google.com\/ai-platform-unified\/docs\/tutorials\/image-recognition-custom\/training'\n    )\n\nartifact_filename = 'model' + '.pkl'\n# Save model artifact to local filesystem (doesn't persist)\nlocal_path = artifact_filename\nwith open(local_path, 'wb') as model_file:\n    pickle.dump(model, model_file)\n\n# Upload model artifact to Cloud Storage\nmodel_directory = os.environ['AIP_MODEL_DIR']\nstorage_path = os.path.join(model_directory, artifact_filename)\nblob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\nblob.upload_from_filename(local_path)\n\n\nprint (&quot;UPLOADING MODEL TO VertexAI&quot;)\n\n# Upload the model to vertex ai\nproject=&quot;...&quot;\nlocation=&quot;...&quot;\ndisplay_name=&quot;custom_mdoel&quot;\nartifact_uri=model_directory\nserving_container_image_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/tf-cpu.2-4:latest&quot;\ndescription=&quot;test model&quot;\nsync=True\n\naiplatform.init(project=project, location=location)\nmodel = aiplatform.Model.upload(\n    display_name=display_name,\n    artifact_uri=artifact_uri,\n    serving_container_image_uri=serving_container_image_uri,\n    description=description,\n    sync=sync,\n)\nmodel.wait()\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p><strong>Running from Local Workstation:<\/strong>\nI set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the location of the Compute Engine default service account keys I have downloaded on my local workstation. I also set the AIP_MODEL_DIR environment variable to point to a cloud storage bucket. After I run the script, I can see the model.pkl file being created in the cloud storage bucket and the Model object  being created in VertexAI.<\/p>\n<p><strong>Triggering the training job from Cloud Scheduler:<\/strong>\nThis is what I ultimately want to achieve - to run the custom training job periodically from Cloud Scheduler. I have converted the python script above into a docker image and uploaded to google artifact registry. The job specification for the Cloud Scheduler is below, I can provide additional details if required. The service account email in the <code>oauth_token<\/code> is the same whose keys I  use to set the GOOGLE_APPLICATION_CREDENTIALS environment variable. When I run this, (either from local workstation or directly in a VertexAI notebook), I can see that the Cloud Scheduler job gets created which keeps triggering the custom job. The custom job is able to train the model and save it to the cloud storage. However, it is not able to upload it to VertexAI and I get the error meessages, <code>status = StatusCode.PERMISSION_DENIED<\/code> and <code>{...&quot;grpc_message&quot;:&quot;Request had insufficient authentication scopes.&quot;,&quot;grpc_status&quot;:7<\/code>}. Cannot figure out  what the authentication issue is because in both cases I am using the same service account.<\/p>\n<pre><code>job = {\n  &quot;name&quot;: f'projects\/{project_id}\/locations\/{location}\/jobs\/test_job',\n  &quot;description&quot;: &quot;Test scheduler job&quot;,\n  &quot;http_target&quot;: {\n    &quot;uri&quot;: f'https:\/\/{location}-aiplatform.googleapis.com\/v1\/projects\/{project_id}\/locations\/{location}\/customJobs',\n    &quot;http_method&quot;: &quot;POST&quot;,\n    &quot;headers&quot;: {\n      &quot;User-Agent&quot;: &quot;Google-Cloud-Scheduler&quot;,\n      &quot;Content-Type&quot;: &quot;application\/json; charset=utf-8&quot;\n    },\n    &quot;body&quot;: &quot;...&quot; \/\/ the custom training job body,\n    &quot;oauth_token&quot;: {\n      &quot;service_account_email&quot;: &quot;...&quot;,\n      &quot;scope&quot;: &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;\n    }\n  },\n  &quot;schedule&quot;: &quot;* * * * *&quot;,\n  &quot;time_zone&quot;: &quot;Africa\/Abidjan&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-13 20:56:27.137 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-iam|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":260,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Question_body":"<p>I successfully trained and deployed a Tensorflow Recommender model on Vertex AI.<\/p>\n<p>Everything is online and to predict the output. In the notebook I do:<\/p>\n<pre><code>loaded = tf.saved_model.load(path)\nscores, titles = loaded([&quot;doctor&quot;])\n<\/code><\/pre>\n<p>That returns:<\/p>\n<pre><code>Recommendations: [b'Nelly &amp; Monsieur Arnaud (1995)'\n b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\n<\/code><\/pre>\n<p>That is, the payload (input for the neural network) must be <code>[&quot;doctor&quot;]<\/code><\/p>\n<p>Then I generate the JSON for payload (the error is here):<\/p>\n<pre><code>!echo {&quot;\\&quot;&quot;instances&quot;\\&quot;&quot; : [{&quot;\\&quot;&quot;input_1&quot;\\&quot;&quot; : {[&quot;\\&quot;&quot;doctor&quot;\\&quot;&quot;]}}]} &gt; instances0.json\n<\/code><\/pre>\n<p>And submit to the endpoint:<\/p>\n<pre><code>!curl -X POST  \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\n-d @instances0.json &gt; results.json\n<\/code><\/pre>\n<p>... as seen here: <a href=\"https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd<\/a><\/p>\n<p>However, when I use this payload, I get error 400:<\/p>\n<pre><code>code: 400\nmessage: &quot;Invalid JSON payload received. Expected an object key or }. s&quot; : [{&quot;input_1&quot; : {[&quot;doctor&quot;]}}]} ^&quot;\nstatus: &quot;INVALID_ARGUMENT&quot;\n<\/code><\/pre>\n<p>This below don't work either:<\/p>\n<pre><code>!echo {&quot;inputs&quot;: {&quot;input_1&quot;: [&quot;doctor&quot;]}} &gt; instances0.json\n<\/code><\/pre>\n<p>Even with validated JSON Lint, it does not return the proper prediction.<\/p>\n<p>In another Stackoverflow question is suggested to remove the &quot; \\ &quot; in the payload, but this didn't work either.<\/p>\n<p>Running:<\/p>\n<pre><code>!saved_model_cli show --dir \/home\/jupyter\/model --all\n<\/code><\/pre>\n<p>I get:<\/p>\n<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:0\n    outputs['output_2'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:1\n  Method name is: tensorflow\/serving\/predict\n\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n<\/code><\/pre>\n<p>The point is: I'm passing an array and I'm not sure if it must be in b64 format.<\/p>\n<p>This Python code works, but returns a different result than expected:<\/p>\n<pre><code>import tensorflow as tf\nimport base64\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nimport numpy as np\nfrom google.cloud import aiplatform\nimport os\nvertex_model = tf.saved_model.load(&quot;gs:\/\/bucket\/model&quot;)\n\nserving_input = list(\n    vertex_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()\n)[0]\n\nprint(&quot;Serving input :&quot;, serving_input)\n\naip_endpoint_name = (\n    f&quot;projects\/my-project\/locations\/us-west1\/endpoints\/12345567&quot;\n)\nendpoint = aiplatform.Endpoint(aip_endpoint_name)\n\ndef encode_input(input):\n    return base64.b64encode(np.array(input)).decode(&quot;utf-8&quot;)\n\ninstances_list = [{serving_input: {&quot;b64&quot;: encode_input(np.array([&quot;doctor&quot;]))}}]\ninstances = [json_format.ParseDict(s, Value()) for s in instances_list]\n\nresults = endpoint.predict(instances=instances)\nprint(results.predictions[0][&quot;output_2&quot;])\n\n\n['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\n<\/code><\/pre>\n<p>Any ideas on how to fix \/ encode the payload ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-27 16:47:24.647 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":105,
        "Owner_creation_date":"2016-09-29 20:35:09.097 UTC",
        "Owner_last_access_date":"2022-09-25 02:50:38.173 UTC",
        "Owner_location":"Brazil",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-02 04:53:07.72 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Why do I get There are no registered serializers for type \"google.VertexEndpoint\"",
        "Question_body":"<p>I am trying to deploy a model using <code>ModelDeployOp<\/code> in a Vertex AI pipeline component. My code is:<\/p>\n<pre><code>parent = client.common_location_path(project=project, location=location)\nrequest = aiplatform_v1.ListEndpointsRequest(parent=parent)\n\npage_result = client.list_endpoints(request=request)\nfor response in page_result:\n     latest_endpoint=response\n\n\ndeploy_op=gcc_aip.ModelDeployOp(\nmodel=model_name,\nendpoint=latest_endpoint,\ndedicated_resources_min_replica_count=1,\ndedicated_resources_max_replica_count=1,\ndedicated_resources_machine_type=&quot;n1-standard-4&quot;,\n)\n<\/code><\/pre>\n<p>but i get:<\/p>\n<pre><code>TypeError: There are no registered serializers for type &quot;google.VertexEndpoint&quot;.\n<\/code><\/pre>\n<p>How can I fix this or maybe find a better way of getting the endpoint?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 12:17:30.463 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":9,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Question_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-26 13:05:04.193 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|spacy-3|google-cloud-vertex-ai|spacy-transformers",
        "Question_view_count":234,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-27 21:12:04.243 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-04-27 20:18:25.967 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Optionally use component functions added in VertexAI python SDK",
        "Question_body":"<p>I am using vertex ai's python SDK and it's built on top of Kubeflow pipelines. In it, you supposedly can do this:<\/p>\n<pre><code>train_op = (sklearn_classification_train(\n        train_data = data_op.outputs['train_out']\n    ).\n    set_cpu_limit(training_cpu_limit).\n    set_memory_limit(training_memory_limit).\n    add_node_selector_constraint(training_node_selector).\n    set_gpu_limit(training_gpu_limit)\n)\n<\/code><\/pre>\n<p>where you can add these functions (<code>set_cpu_limit<\/code>, <code>set_memory_limit<\/code>, <code>add_node_selector<\/code>, and <code>set_gpu_limit<\/code>) onto your component. I've haven't used this syntax before.<\/p>\n<p>How I can optionally use each 'sub function' only if the variables are specified each function?<\/p>\n<p>For example, if <code>training_gpu_limit<\/code> isn't set, I don't want to execute <code>set_gpu_limit<\/code> on the component.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-23 15:20:12.233 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":51,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Error while deploying a model on an endpoint- Vertex AI",
        "Question_body":"<p>I'm working on a model that I need to deploy on a Vertex AI endpoint. The model is a DNN developed in Tensorflow. I've saved the model locally, loaded to GCS and imported it in the Vertex AI Model section without problems. When I'm trying to deploy it to a new endpoint Vertex responses is the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5AztD.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>I've tried to search the error <em>List of found errors: 1.Field: deployed_model.prediction_resources; Message: Required field is not set.<\/em> but found nothing. What could be the cause?<\/p>\n<p>Version of tensorflow and python are the same of the pre-built environment given by Google, the model is saved through tf.keras.Model.save(model, model_path).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-06-28 20:26:51.533 UTC",
        "Question_favorite_count":0.0,
        "Question_score":2,
        "Question_tags":"tensorflow|google-cloud-platform|gcloud|google-cloud-vertex-ai",
        "Question_view_count":235,
        "Owner_creation_date":"2018-05-02 15:27:04.18 UTC",
        "Owner_last_access_date":"2022-09-23 14:37:08.307 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-24 18:12:06.007 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_date":"2020-08-31 11:39:36.143 UTC",
        "Owner_last_access_date":"2022-09-18 19:46:19.503 UTC",
        "Owner_location":"Versailles, France",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-25 03:21:58.69 UTC",
        "Answer_last_edit_date":"2022-01-25 03:35:51.06 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP Vertex AI Rest API for Notebook Execution",
        "Question_body":"<p>I'm a bit of a noob to Vertex AI.<\/p>\n<p>I've made a user-managed notebook with a single Python3 notebook which does the work I need it to do.<\/p>\n<p>My question is how can I call this notebook to run in Vertex AI from Python client? What would be the correct service(s) to call? The Vertex AI documentation is a little confusing. This basic use case or quickstart is difficult to locate.<\/p>\n<p>Do I need to create a pipelines, then call that?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-20 06:58:49.98 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":78,
        "Owner_creation_date":"2014-01-16 11:59:01.83 UTC",
        "Owner_last_access_date":"2022-09-24 20:50:39.103 UTC",
        "Owner_location":null,
        "Owner_reputation":766,
        "Owner_up_votes":76,
        "Owner_down_votes":7,
        "Owner_views":142,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI notebook kernel lost on PC sleep",
        "Question_body":"<p>While using Vertex AI notebook instance kernel on GCP, the notebook gets detached everytime my system sleeps.<\/p>\n<p>How can I keep my notebook running even if my system shuts down?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-12 08:04:37.41 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":551,
        "Owner_creation_date":"2012-11-17 11:31:07.41 UTC",
        "Owner_last_access_date":"2022-09-22 08:40:30.397 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-12 10:34:05.987 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is real-time collaboration supported by \"user-managed notebooks\" on google vertex ai?",
        "Question_body":"<p>I use GCP's vertex ai platform's &quot;user-managed notebook&quot; service. how do i enable real-time collaboration for the jupyter lab server? it sounds like this can be enabled by adding <code>--collaborative<\/code> when running <code>jupyter lab<\/code>. but the command is not exposed to me with vertex ai notebooks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-15 19:51:04.847 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":391,
        "Owner_creation_date":"2011-06-02 22:42:48.717 UTC",
        "Owner_last_access_date":"2022-09-23 23:11:43.113 UTC",
        "Owner_location":"florida, usa",
        "Owner_reputation":4311,
        "Owner_up_votes":878,
        "Owner_down_votes":21,
        "Owner_views":533,
        "Answer_body":"<p>You can activate the collaboration feature by doing the following steps below:<\/p>\n<ol>\n<li>Go to GCP Console &gt; Vertex AI &gt; Workbench and identify the notebook you want to use.<\/li>\n<li>Click the notebook name to open notebook information<\/li>\n<li>Click &quot;VIEW VM DETAILS&quot;<\/li>\n<li>You will be redirected to Compute Engine &gt; VM Instances and it shows your notebook details<\/li>\n<li>Click &quot;EDIT&quot; and look for the section &quot;Metadata&quot;<\/li>\n<li>Under &quot;Metadata&quot;, click &quot;+ Add Item&quot; and assign value &quot;use-collaborative&quot; at <strong>key<\/strong> and &quot;true&quot; at <strong>value<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/A3ElC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A3ElC.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<li>Click &quot;SAVE&quot;<\/li>\n<li>Restart the VM by clicking &quot;STOP&quot; and wait for the operation to finish<\/li>\n<li>Click &quot;START \/ RESUME&quot; to start the VM and you should be able to use the collaboration feature.<\/li>\n<\/ol>\n<p>To share your notebook:<\/p>\n<ol>\n<li>Make sure that you give the user the correct permission in your IAM &amp; Admin &gt; IAM. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/iam\" rel=\"nofollow noreferrer\">predefined notebook permissions<\/a>.<\/li>\n<li>Open Jupyter lab and open a Python notebook. Copy the URL of the notebook and you could share this and be able to collaborate with users that were given the correct permissions.\n<blockquote>\n<p>The URL is in this format <a href=\"https:\/\/xxxxxxx-dot-us-west1.notebooks.googleusercontent.com\/lab\/tree\/your_sharable_notebook.ipynb\" rel=\"nofollow noreferrer\">https:\/\/xxxxxxx-dot-us-west1.notebooks.googleusercontent.com\/lab\/tree\/your_sharable_notebook.ipynb<\/a><\/p>\n<\/blockquote>\n<\/li>\n<\/ol>\n<p>NOTE: I tested the steps above by giving IAM permission &quot;Notebooks Admin&quot; to a colleague of mine.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-04-18 02:07:55.783 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Cloud Platform - Vertex AI - is there a way to look at a chart of training performance over time?",
        "Question_body":"<p>I'd like to know how the training performance changes over the course of the training. Is there any way to access that via Vertex AI automl service?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-30 20:08:32.743 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":249,
        "Owner_creation_date":"2014-02-18 22:34:13.817 UTC",
        "Owner_last_access_date":"2022-09-22 15:59:48.447 UTC",
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to make Vertex AI multi-label classification AutoML not ignore texts with no labels?",
        "Question_body":"<p>I prepared a training dataset for multi-label classification in JSON Lines format as described in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#json-lines_1\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>My upload file looks like<\/p>\n<pre class=\"lang-js prettyprint-override\"><code>{\n  &quot;textContent&quot;: &quot;This text corresponds to 2 labels&quot;,\n  &quot;classificationAnnotations&quot;: [\n    {&quot;displayName&quot;: &quot;LABEL_1&quot;},\n    {&quot;displayName&quot;: &quot;LABEL_2&quot;}\n  ]\n}\n{\n  &quot;textContent&quot;: &quot;This text doesn't correspond to any labels&quot;,\n  &quot;classificationAnnotations&quot;: []\n}\n\/\/ ... and other 5,853 lines\n<\/code><\/pre>\n<p>Only 1,037 texts have non-empty list of labels.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NpmKf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NpmKf.png\" alt=\"Training dataset labels\" \/><\/a><\/p>\n<p>Other texts are considered &quot;Unlabeled&quot;. AutoML ignores unlabeled texts.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UXEEj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UXEEj.png\" alt=\"AutoML training result\" \/><\/a><\/p>\n<p>As a workaround I added an extra label to every text<\/p>\n<pre class=\"lang-js prettyprint-override\"><code>{\n  &quot;textContent&quot;: &quot;This text corresponds to 2 labels&quot;,\n  &quot;classificationAnnotations&quot;: [\n    {&quot;displayName&quot;: &quot;LABEL_1&quot;},\n    {&quot;displayName&quot;: &quot;LABEL_2&quot;},\n    {&quot;displayName&quot;: &quot;EXTRA_LABEL&quot;}\n  ]\n}\n{\n  &quot;textContent&quot;: &quot;This text doesn't correspond to any labels&quot;,\n  &quot;classificationAnnotations&quot;: [\n    {&quot;displayName&quot;: &quot;EXTRA_LABEL&quot;}\n  ]\n}\n\/\/ ... and other 5,853 texts\n<\/code><\/pre>\n<p>Is there a way to make AutoML use &quot;Unlabeled&quot; texts as texts with 0 labels?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-17 15:55:56.5 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":87,
        "Owner_creation_date":"2017-01-13 22:43:47.133 UTC",
        "Owner_last_access_date":"2022-09-24 18:14:12.057 UTC",
        "Owner_location":"Russia",
        "Owner_reputation":323,
        "Owner_up_votes":207,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-30 16:41:46.813 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-09 15:35:21.003 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-10 14:43:07.553 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Run a Vertex AI model locally",
        "Question_body":"<p>Using the Vertex AI product at GCP training was very easy, I uploaded a data set and it returned a model which is saved in a gcp bucket, I downloaded the files and the tree has these files<\/p>\n<pre><code>\u251c\u2500\u2500 environment.json\n\u251c\u2500\u2500 feature_attributions.yaml\n\u251c\u2500\u2500 final_model_structure.pb\n\u251c\u2500\u2500 instance.yaml\n\u251c\u2500\u2500 predict\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 001\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 PVC_vocab\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets.extra\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tf_serving_warmup_requests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 saved_model.pb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 variables\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 variables.index\n\u251c\u2500\u2500 prediction_schema.yaml\n\u251c\u2500\u2500 tables_server_metadata.pb\n\u2514\u2500\u2500 transformations.pb\n<\/code><\/pre>\n<p>I would like to serve this model locally from a dockerized python application, but I don't know enough TF to do this and I am very confused about which <code>.pb<\/code> file is the actual one that has the neural network I need.<\/p>\n<p>Thanks for any tips.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2022-04-10 01:40:15.26 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|docker|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":261,
        "Owner_creation_date":"2016-12-30 00:31:15.593 UTC",
        "Owner_last_access_date":"2022-09-25 04:46:59.423 UTC",
        "Owner_location":"St. Louis, MO, USA",
        "Owner_reputation":710,
        "Owner_up_votes":43,
        "Owner_down_votes":16,
        "Owner_views":175,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Authenticate Custom Training Job in Vertex AI with Service Account",
        "Question_body":"<p>I am trying to run a Custom Training Job to deploy my model in Vertex AI directly from a Jupyterlab. This Jupyterlab is instantiated from a Vertex AI Managed Notebook where I already specified the service account.<\/p>\n<p>My aim is to deploy the training script that I specify to the method <code>CustomTrainingJob<\/code> directly from the cells of my notebook. This would be equivalent to pushing an image that contains my script to <strong>container registry<\/strong> and deploying the Training Job manually from the UI of Vertex AI (in this way, by specifying the service account, I was able to corectly deploy the training job). However, I need everything to be executed from the same notebook.<\/p>\n<p>In order to specify the credentials to the <code>CustomTrainingJob<\/code> of aiplatform, I execute the following cell, where all variables are correctly set:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>import google.auth\nfrom google.cloud import aiplatform\nfrom google.auth import impersonated_credentials\n\nsource_credentials = google.auth.default()\ntarget_credentials = impersonated_credentials.Credentials(\nsource_credentials=source_credentials,\ntarget_principal='SERVICE_ACCOUNT.iam.gserviceaccount.com',\ntarget_scopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform'])\n\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n\njob = aiplatform.CustomTrainingJob(\n    display_name=JOB_NAME,\n    script_path=SCRIPT_PATH,\n    container_uri=MODEL_TRAINING_IMAGE,\n    credentials=target_credentials\n)\n<\/code><\/pre>\n<p>When after the <code>job.run()<\/code> command is executed it seems that the credentials are not correctly set. In particular, the following error is returned:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>\/opt\/conda\/lib\/python3.7\/site-packages\/google\/auth\/impersonated_credentials.py in _update_token(self, request)\n    254 \n    255         # Refresh our source credentials if it is not valid.\n--&gt; 256         if not self._source_credentials.valid:\n    257             self._source_credentials.refresh(request)\n    258 \n\nAttributeError: 'tuple' object has no attribute 'valid'\n<\/code><\/pre>\n<p>I also tried different ways to configure the credentials of my service account but none of them seem to work. In this case it looks like the tuple that contains the source credentials is missing the 'valid' attribute, even if the method <code>google.auth.default()<\/code> only returns two values.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-20 09:20:32.043 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_date":"2022-03-24 16:46:24.823 UTC",
        "Owner_last_access_date":"2022-09-24 21:22:42.907 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Set artifact name when using kfp dsl.importer",
        "Question_body":"<p>When importing an artifact using the kfp <code>dsl.importer()<\/code> function, the imported artifact gets the default (display) name <code>artifact<\/code>. I would like to give it a custom name to make the pipeline and lineage tracking more clear. I checked the <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.dsl.html#kfp.dsl.importer\" rel=\"nofollow noreferrer\">documentation<\/a>, but I can't seem to find a way to change the name of the artifact that the <code>dsl.importer()<\/code> function produces.<\/p>\n<p>Example code <code>dsl.importer()<\/code>:<\/p>\n<pre><code>    load_dataset_step = dsl.importer(\n        artifact_uri=input_data_uri,\n        artifact_class=dsl.Dataset,\n        reimport=False\n    ).set_display_name(&quot;Load Dataset&quot;)\n<\/code><\/pre>\n<p>Visualisation of the <code>dsl.importer()<\/code> step:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" alt=\"pipelines visualisation\" \/><\/a><\/p>\n<p>I'm making use of Google Cloud Vertex AI Pipelines.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-04-06 08:25:10.08 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_date":"2012-11-21 14:42:05.387 UTC",
        "Owner_last_access_date":"2022-09-22 13:53:11.917 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":383,
        "Owner_up_votes":100,
        "Owner_down_votes":8,
        "Owner_views":132,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-29 09:23:26.933 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_date":"2011-03-18 10:16:38.977 UTC",
        "Owner_last_access_date":"2022-02-21 16:31:56.407 UTC",
        "Owner_location":"Switzerland",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-03 16:29:02.093 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-09-29 11:28:26.017 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How do I undeploy a model from an endpoint without knowing its id in Vertex AI?",
        "Question_body":"<p>I have managed to undeploy a model from an endpoint using <code>UndeployModelRequest<\/code>:<\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)\n    deployed_models_info = model_info.deployed_models\n    deployed_model_id=model_info.deployed_models[0].deployed_model_id       \n    \n    undeploy_request = aiplatform_v1.types.UndeployModelRequest\n                       (endpoint=end_point, deployed_model_id=deployed_model_id)\n\n    client.undeploy_model(request=undeploy_request)\n<\/code><\/pre>\n<p>but all this depends on knowing <code>model_id<\/code>. I want to be able to just undeploy a model from an endpoint without knowing the model's id (there will only be one model per endpoint ever). Is that possible or can I get the model id from the endpoint somehow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 08:49:45.407 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":14,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Question_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2022-01-31 19:50:05.453 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter|google-cloud-vertex-ai",
        "Question_view_count":514,
        "Owner_creation_date":"2021-05-28 09:49:06.03 UTC",
        "Owner_last_access_date":"2022-03-28 19:00:47.687 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-02 06:08:15.707 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-02-19 15:30:43.35 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Batch Prediction with scikit-learn using jsonl in Vertex AI",
        "Question_body":"<p>I have a scikit-learn model successfully trained and loaded onto Vertex AI, but I can't seem to do batch prediction with jsonl. I've tried using these formats with jsonl:<\/p>\n<pre><code>{&quot;dense_input&quot;: [1, 2, 3, ...]}\n{&quot;dense_input&quot;: [4, 5, 6, ...]}\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>{&quot;val_1&quot;: 1, &quot;val_2&quot;: 2, ...}\n{&quot;val_1&quot;: 4, &quot;val_2&quot;: 5, ...}\n<\/code><\/pre>\n<p>but I get this error for both:<\/p>\n<blockquote>\n<p>('Post request fails. Cannot get predictions. Error: Predictions are not in the response. Got: {&quot;error&quot;: &quot;Prediction failed: Exception during sklearn prediction: float() argument must be a string or a number, not 'dict'&quot;}.', 2)<\/p>\n<\/blockquote>\n<p>I've tried batch prediction using a CSV file and it works fine, but I'm having difficulty with the jsonl file. Does anyone know what's the problem? Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-18 15:56:45.463 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"json|google-cloud-platform|scikit-learn|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":73,
        "Owner_creation_date":"2022-07-18 15:32:34.55 UTC",
        "Owner_last_access_date":"2022-09-18 18:25:28.147 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Question_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-16 12:19:41.04 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":220,
        "Owner_creation_date":"2019-07-09 21:04:51.28 UTC",
        "Owner_last_access_date":"2022-09-24 15:30:31.527 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-05 10:50:32.383 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Question_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-18 19:32:54.733 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":114,
        "Owner_creation_date":"2016-02-12 09:56:16.143 UTC",
        "Owner_last_access_date":"2022-09-22 18:01:16.733 UTC",
        "Owner_location":"Grand Rapids, MI, USA",
        "Owner_reputation":1269,
        "Owner_up_votes":134,
        "Owner_down_votes":0,
        "Owner_views":261,
        "Answer_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-19 01:40:51.973 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI invoke endpoint failed. ERROR: Prediction failed. Please contact cloudml-feedback@google.com",
        "Question_body":"<p>I have deployed my App on Vertex AI endpoint. The endpoint is created successfully and I am getting &quot;Active&quot; status. But when I try to invoke the endpoint, I am getting the following error:<\/p>\n<pre><code>{\n    &quot;error&quot;: {\n        &quot;code&quot;: 500,\n        &quot;message&quot;: &quot;Prediction failed. Please contact cloudml-feedback@google.com&quot;,\n        &quot;status&quot;: &quot;INTERNAL&quot;\n    }\n}\n<\/code><\/pre>\n<p>POST Request URL: <code>https:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:predict<\/code><\/p>\n<p>Header: I am passing the required header i.e., Content-Type: application\/json and Authorization token.<\/p>\n<p>Body:<\/p>\n<pre><code>{\n  &quot;instances&quot;: [{\n      &quot;image_path&quot;: &quot;https:\/\/github.com\/naqishah13\/multilabelimages\/blob\/main\/movie-poster-1.jpg?raw=true&quot;\n      }\n  ]\n}\n<\/code><\/pre>\n<p>Here is my flask code that I am using for prediction:<\/p>\n<pre><code>@app.route('\/get_movie_genres\/', methods=['POST'])\ndef main():\n    request_json = request.get_json()\n    request_instances = request_json['instances']\n    image_path = request_instances[0]['image_path']\n    predicted_genres = predict_(image_path)\n    output = {'predictions':\n               [\n                   {\n                       'predicted_genres' : prediction\n                   }\n               ]\n           }\n    return jsonify(output)\n<\/code><\/pre>\n<p>I have saved some images on github and I am using those images (via github path) in order to do the movie genre prediction.<\/p>\n<p>Am I doing something wrong? what might be causing the issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-02 21:31:19.767 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|flask|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_date":"2022-05-02 21:09:40.543 UTC",
        "Owner_last_access_date":"2022-06-21 11:13:56.757 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-03 04:40:01.557 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"When running custom container in Kubeflow, how to pass arguments to the container?",
        "Question_body":"<p>I created a container image, and want to run it as part of my Kubeflow pipeline.<\/p>\n<p>I tested the image locally and have no problem running it:<\/p>\n<pre><code>docker run $IMAGE_URI --my_argument 28\n<\/code><\/pre>\n<p>I tried running it in Kubeflow using pre-built component, [CustomContainerTrainingJobRunOp][1], but failed. It seems that the problem is with the arguments, since it had no problem when hard-coded the argument value in the Python code.  How do I correctly pass the argument in this component? Thanks.<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root='gs:\/\/a-gcs-bucket')\ndef pipeline():\n    \n    subclass_training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n        project=project_id, \n        display_name=model_display_name,\n        container_uri=subclass_container_uri,\n        args=[&quot;--my_argument&quot;, 28],\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_uri,\n        #dataset=dataset_create_op.outputs[&quot;dataset&quot;]\n    )\n\nThank you!\n\n\n  [1]: https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-17 02:46:37.897 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"containers|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":316,
        "Owner_creation_date":"2017-10-27 01:07:11.177 UTC",
        "Owner_last_access_date":"2022-09-05 21:01:42.29 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-17 03:01:40.167 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How can I change the security setting and enable terminal for a Vertex AI managed notebook?",
        "Question_body":"<p>I created a notebook using Vertex AI without enabling terminal first, but I want to enable terminal now so that I can run a Python file from a terminal. Is there any way I can change the setting retrospectively?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-04 18:44:02.433 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":110,
        "Owner_creation_date":"2022-06-30 04:56:56.23 UTC",
        "Owner_last_access_date":"2022-09-18 19:07:37.86 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>As of now, when you create a Notebook instance with unchecked <em>&quot;Enable terminal&quot;<\/em> like the below screenshot, you <strong>cannot re-enable this option once the Notebook instance is already created<\/strong>.\n<a href=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The only workaround is to <strong>recreate the Notebook instance<\/strong> and then enable it.<\/p>\n<p>Right now, there is already a <a href=\"https:\/\/issuetracker.google.com\/222694899\" rel=\"nofollow noreferrer\">Feature Request<\/a> for this. You can <strong>star<\/strong> the public issue tracker feature request and add <strong>\u2018Me too\u2019<\/strong> in the thread. This will bring more attention to the request as more users request support for it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-08 03:33:52.893 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-07-06 04:37:07.957 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Unable to view Vertex AI pipeline node logs",
        "Question_body":"<p>I created a Vertex AI pipeline to perform a simple ML flow of creating a dataset, training a model on it and then predicting on the test set. There is a python function based component (train-logistic-model) where I train the model. However, in the component I specify an invalid package and hence the step in the pipeline fails. I know this because when I corrected the package name the step worked fine. However, for the failed pipeline I am unable to see any logs. When I click on the &quot;VIEW JOB&quot; under &quot;Execution Info&quot; on the pipeline Runtime Graph (pic attached) it takes me to the &quot;CUSTOM JOB&quot; page which the pipeline ran. There is a message:<\/p>\n<blockquote>\n<p>Custom job failed with error message: The replica workerpool0-0 exited\nwith a non-zero status of 1 ...<\/p>\n<\/blockquote>\n<p>When I click the VIEW LOGS button, it takes me to the Logs Explorer where there are NO logs. Why are there no logs? Do I need to enable logging somewhere in the pipeline for this? Or could it be a permission issue (it does not mention anything about it though, just this message on the Logs Explorer and 0 logs below it.<\/p>\n<blockquote>\n<p>Showing logs for time specified in query. To view more results update\nyour query<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/btVwP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/btVwP.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-12-14 23:18:47.93 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":216,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to view and Interprete Vertex AI Logs",
        "Question_body":"<p>We have deployed Models in the Vertex AI endpoint.\nNow we want to know and interpret logs regarding events\nof Node creation, POD creation, user API call matric etc.<\/p>\n<p>Is there any way or key by which we can filter the logs for Analysis?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-09-13 04:45:36.94 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":309,
        "Owner_creation_date":"2017-01-03 19:47:17.57 UTC",
        "Owner_last_access_date":"2022-02-22 06:30:56.963 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":72,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-22 13:19:31.87 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Error when trying to use CustomPythonPackageTrainingJobRunOp in VertexAI pipeline",
        "Question_body":"<p>I am using the google cloud pipeline component CustomPythonPackageTrainingJobRunOp in a VertexAI pipeline . I have been able to run this package successfully as a CustomTrainingJob before. I can see multiple (11) error messages in the logs but the only one that seems to make sense to me is, &quot;ValueError: too many values to unpack (expected 2) &quot; but I am unable to figure out the solution. I can add all the other error messages too if required. I am logging some messages at the start of the training code so I know the errors happen before the training code is executed. I am completely stuck on this. Links to samples where someone has used CustomPythonPackageTrainingJobRunOp in a pipeline would very helpful as well. Below is the pipeline code that I am trying to execute:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name)\ndef pipeline(\n    project: str = &quot;adsfafs-321118&quot;,\n    location: str = &quot;us-central1&quot;,\n    display_name: str = &quot;vertex_pipeline&quot;,\n    python_package_gcs_uri: str = &quot;gs:\/\/vertex\/training\/training-package-3.0.tar.gz&quot;,\n    python_module_name: str = &quot;trainer.task&quot;,\n    container_uri: str = &quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n    staging_bucket: str = &quot;vertex_bucket&quot;,\n    base_output_dir: str = &quot;gs:\/\/vertex_artifacts\/custom_training\/&quot;\n):\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=python_module_name,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=package_path\n)\n\napi_client = AIPlatformClient(project_id=project_id, region=region)\n\nresponse = api_client.create_run_from_job_spec(\n    package_path,\n    pipeline_root=pipeline_root_path\n)\n\n<\/code><\/pre>\n<p>In the documentation for CustomPythonPackageTrainingJobRunOp, the type of the argument &quot;python_module&quot; seems to be &quot;google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob&quot; instead of string, which seems odd. However, I tried to re-define the pipeline, where I have replaced argument python_module in CustomPythonPackageTrainingJobRunOp with a CustomPythonPackageTrainingJob object instead of a string, as below but still getting the same error:<\/p>\n<pre><code>def pipeline(\n    project: str = &quot;...&quot;,\n    location: str = &quot;...&quot;,\n    display_name: str = &quot;...&quot;,\n    python_package_gcs_uri: str = &quot;...&quot;,\n    python_module_name: str = &quot;...&quot;,\n    container_uri: str = &quot;...&quot;,\n    staging_bucket: str = &quot;...&quot;,\n    base_output_dir: str = &quot;...&quot;,\n):\n\n    job = aiplatform.CustomPythonPackageTrainingJob(\n        display_name= display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module_name=python_module_name,\n        container_uri=container_uri,\n        staging_bucket=staging_bucket\n    )\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=job,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>Added the args that I was passing and had forgotten to add here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-15 20:22:26.03 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-16 22:09:40.473 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Cloud Vertex AI Notebook Scheduled Runs Aren't Running Code?",
        "Question_body":"<p>I've followed their <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-schedule-execution-console\" rel=\"nofollow noreferrer\">instructions<\/a> to how to set up a managed Jupyter notebook and schedule a run, and I tossed in some pretty standard parameters and my bucket.<\/p>\n<p>After setting up the schedule, however, the run just comes out as &quot;Failed&quot;, and when I get &quot;view results&quot;, I just get my code back (with no output indication). For some reason it's just not running. Ideas?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/r0vra.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r0vra.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>[2]<a href=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-06 22:44:38.66 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":457,
        "Owner_creation_date":"2018-04-09 01:06:01.717 UTC",
        "Owner_last_access_date":"2022-01-21 07:39:34.45 UTC",
        "Owner_location":null,
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-06 22:48:04.197 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Batch Prediction Job non-blocking",
        "Question_body":"<p>I am running a Vertex AI batch prediction using the python API.\nThe function I am using is from the google cloud docs:<\/p>\n<pre><code>def create_batch_prediction_job_dedicated_resources_sample(\n    key_path,\n    project: str,\n    location: str,\n    model_display_name: str,\n    job_display_name: str,\n    gcs_source: Union[str, Sequence[str]],\n    gcs_destination: str,\n    machine_type: str = &quot;n1-standard-2&quot;,\n    sync: bool = True,\n):\n    credentials = service_account.Credentials.from_service_account_file(\n    key_path)\n\n# Initilaize an aiplatfrom object\n aiplatform.init(project=project, location=location, credentials=credentials)\n\n# Get a list of Models by Model name\n models = aiplatform.Model.list(filter=f'display_name=&quot;{model_display_name}&quot;')\n model_resource_name = models[0].resource_name\n\n# Get the model\n my_model = aiplatform.Model(model_resource_name)\n\n batch_prediction_job = my_model.batch_predict(\n    job_display_name=job_display_name,\n    gcs_source=gcs_source,\n    gcs_destination_prefix=gcs_destination,\n    machine_type=machine_type,\n    sync=sync,\n)\n\n #batch_prediction_job.wait_for_resource_creation()\n batch_prediction_job.wait()\n\n print(batch_prediction_job.display_name)\n print(batch_prediction_job.resource_name)\n print(batch_prediction_job.state)\n return batch_prediction_job\n\ndatetime_today = datetime.datetime.now()\nmodel_display_name = 'test_model'\nkey_path = 'vertex_key.json'\nproject = 'my_project'\nlocation = 'asia-south1'\njob_display_name = 'batch_prediction_' + str(datetime_today)\nmodel_name = '1234'\ngcs_source = 'gs:\/\/my_bucket\/Cleaned_Data\/user_item_pairs.jsonl'\ngcs_destination = 'gs:\/\/my_bucket\/prediction'\n\ncreate_batch_prediction_job_dedicated_resources_sample(key_path,project,location,model_display_name,job_display_name,\n                                                      gcs_source,gcs_destination)\n<\/code><\/pre>\n<p>OUTPUT:<\/p>\n<pre><code>92 current state:\nJobState.JOB_STATE_RUNNING\nINFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects\/my_project\/locations\/asia-south1\/batchPredictionJobs\/37737350127597649\n<\/code><\/pre>\n<p>The above output is being printed on the terminal over and over after every few seconds.<\/p>\n<p>The issue that I have is that the python program calling this function keeps on running until it is force stopped. I have tried both <code>batch_prediction_job.wait()<\/code> &amp; <code>batch_prediction_job.wait_for_resource_creation()<\/code> with the same results.<\/p>\n<p>How do I start a batch_prediction_job without waiting for it to complete and terminating the program just after the job has be created?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-01-24 09:36:50.77 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":270,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-27 05:35:34.943 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI custom job: cannot launch it via python",
        "Question_body":"<p>Trying to follow the example <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#create_custom_job-python\" rel=\"nofollow noreferrer\">here<\/a> to launch a custom job in vertex ai.<\/p>\n<p>This is the code I am using<\/p>\n<pre><code>from google.cloud import aiplatform\n\n\ndef create_custom_job_sample(\n    project: str,\n    display_name: str,\n    container_image_uri: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n    custom_job = {\n        &quot;display_name&quot;: display_name,\n        &quot;job_spec&quot;: {\n            &quot;worker_pool_specs&quot;: [\n                {\n                    &quot;machine_spec&quot;: {\n                        &quot;machine_type&quot;: &quot;n1-standard-4&quot;\n                    },\n                    &quot;replica_count&quot;: 1,\n                    &quot;container_spec&quot;: {\n                        &quot;image_uri&quot;: container_image_uri,\n                        &quot;command&quot;: [],\n                        &quot;args&quot;: [],\n                    },\n                }\n            ]\n        },\n    }\n    parent = f&quot;projects\/{project}\/locations\/{location}&quot;\n    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n    print(&quot;response:&quot;, response)\n\ncreate_custom_job_sample(\n    &quot;MY_PROJECT&quot;,\n    &quot;job-123&quot;,\n    &quot;europe-west1-docker.pkg.dev\/&lt;MYPROJECT&gt;\/&lt;MY_IMAGE_URI&gt;&quot;,\n    &quot;europe-west1&quot;,\n    &quot;eu-west1-aiplatform.googleapis.com&quot;\n)\n<\/code><\/pre>\n<p>However I get an error that start with<\/p>\n<pre><code>E0728 15:00:53.742356000 4417760704 hpack_parser.cc:1234]              Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8\n<\/code><\/pre>\n<p>and ends with\n<code>google.api_core.exceptions.Unknown: None Stream removed <\/code><\/p>\n<p>Don't understand what the problem is. The job starts correctly from the terminal with<\/p>\n<pre><code>gcloud ai custom-jobs create \\\n  --region=europe-west1 \\\n  --display-name=test-job-1 \\\n  --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,executor-image-uri=&lt;MY_IMAGE_URI&gt;,local-package-path=.,script=myfolder\/myscript.py\n<\/code><\/pre>\n<p>Could someone help me?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-28 14:26:29.777 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":160,
        "Owner_creation_date":"2017-01-27 15:07:04.88 UTC",
        "Owner_last_access_date":"2022-09-22 15:19:15.02 UTC",
        "Owner_location":null,
        "Owner_reputation":1991,
        "Owner_up_votes":468,
        "Owner_down_votes":27,
        "Owner_views":107,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to login as human labeler on GCP Vertex AI",
        "Question_body":"<p>I set up a Labeling Task in Vertex-AI, and assigned a team.\nThe manager of that team received an email to manage the <a href=\"https:\/\/datacompute.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/datacompute.google.com\/<\/a> console.\nNone of the human labelers received such an email.\nWhat do they have to do to start labeling? Is there a console for them?<\/p>\n<p>Any advice would be amazing!\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 16:02:04.327 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":8,
        "Owner_creation_date":"2022-09-22 15:59:05.683 UTC",
        "Owner_last_access_date":"2022-09-22 16:39:31.053 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Question_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-22 17:36:32.977 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":312,
        "Owner_creation_date":"2016-06-06 14:08:12.253 UTC",
        "Owner_last_access_date":"2022-09-22 14:59:43.617 UTC",
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Answer_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-18 19:26:06.353 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Managing data drift when using w2vec embeddings on VertexAI",
        "Question_body":"<p>So I am looking into moving my models from GCP's AI Platform to Vertex AI, my main motivation for it being the fact that Vertex AI has automatic email notifications when your data skews or drifts (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring<\/a>).<\/p>\n<p>So if you start receiving dodgy data that doesn't resemble the training set, they send you an email telling you which features (columns) of the data you are trying to predict are drifting away from your training data.<\/p>\n<p>However, I am unsure how this would work in my case since my data is text data that has been encoded using word2vec embeddings. Therefore, my dataset has 300 columns but I don't know what feature each of the columns refers to.<\/p>\n<p>Is this sort of data drift analysis still useful in my particular case?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-20 06:47:31.863 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|google-cloud-platform|word2vec|google-cloud-vertex-ai",
        "Question_view_count":133,
        "Owner_creation_date":"2021-02-03 09:38:25.2 UTC",
        "Owner_last_access_date":"2022-09-22 13:24:14.067 UTC",
        "Owner_location":"London, Reino Unido",
        "Owner_reputation":57,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Question_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-20 06:48:34.453 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|johnsnowlabs-spark-nlp",
        "Question_view_count":238,
        "Owner_creation_date":"2022-05-03 20:17:35.81 UTC",
        "Owner_last_access_date":"2022-09-18 18:37:06.877 UTC",
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-21 06:20:07.35 UTC",
        "Answer_last_edit_date":"2022-07-21 13:49:12.227 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-07-21 12:34:49.243 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Cloud Platform - Vertex AI training with custom data format",
        "Question_body":"<p>I need to train a custom OCR in vertex AI. My data with have folder of cropped image, each image is a line, and a csv file with 2 columns: image name and text in image.\nBut when I tried to import it into a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets\" rel=\"nofollow noreferrer\">dataset<\/a> in vertex AI, I see that image dataset only support for classification, segmentation, object detection. All of dataset have fixed number of label, but my data have a infinite number of labels(if we view text in image as label), so all types doesn't match with my requirement. Can I use vertex AI for training, and how to do that ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-20 09:11:51.14 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|ocr|google-cloud-vertex-ai",
        "Question_view_count":303,
        "Owner_creation_date":"2014-10-09 13:12:23.897 UTC",
        "Owner_last_access_date":"2022-09-24 04:54:07.693 UTC",
        "Owner_location":"Hanoi, Vietnam",
        "Owner_reputation":803,
        "Owner_up_votes":76,
        "Owner_down_votes":8,
        "Owner_views":114,
        "Answer_body":"<p>Since Vertex AI managed datasets do not support OCR applications, you can train and deploy a custom model using Vertex AI\u2019s training and prediction services.<\/p>\n<p>I found a good <a href=\"https:\/\/medium.com\/geekculture\/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b\" rel=\"nofollow noreferrer\">article<\/a> on building an OCR system from scratch. This OCR system is implemented in 2 steps<\/p>\n<ol>\n<li>Text detection<\/li>\n<li>Text recognition<\/li>\n<\/ol>\n<p>Please note that this article is not officially supported by Google Cloud.<\/p>\n<p>Once you have tested the model locally, you can train the same on Vertex AI using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/custom-training\" rel=\"nofollow noreferrer\">custom model training service<\/a>. Please follow this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for step-by-step instructions on training and deploying a custom model.<\/p>\n<p>Once the training is complete, the model can be deployed for inference using a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers\" rel=\"nofollow noreferrer\">pre-built container<\/a> offered by Vertex AI or a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">custom container<\/a> based on your requirements. You can also choose between batch predictions for synchronous requests and online predictions for asynchronous requests.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-09 10:17:21.187 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to structure container logs in Vertex AI?",
        "Question_body":"<p>I have a model in Vertex AI, from the logs it seems that Vertex AI has ingested the log into <code>message<\/code> field within <code>jsonPayload<\/code> field, but i would like to structure the <code>jsonPayload<\/code> field such that every key in <code>message<\/code> will be a field within <code>jsonPayload<\/code>, i.e: flatten\/extract <code>message<\/code> <a href=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-23 12:26:44.81 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":342,
        "Owner_creation_date":"2020-12-23 08:27:36.58 UTC",
        "Owner_last_access_date":"2022-09-22 12:31:57.143 UTC",
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"VertexAI Batch Inference Failing for Custom Container Model",
        "Question_body":"<p>I'm having trouble executing VertexAI's batch inference, despite endpoint deployment and inference working perfectly. My TensorFlow model has been trained in a custom Docker container with the following arguments:<\/p>\n<pre><code>aiplatform.CustomContainerTrainingJob(\n        display_name=display_name,\n        command=[&quot;python3&quot;, &quot;train.py&quot;],\n        container_uri=container_uri,\n        model_serving_container_image_uri=container_uri,\n        model_serving_container_environment_variables=env_vars,\n        model_serving_container_predict_route='\/predict',\n        model_serving_container_health_route='\/health',\n        model_serving_container_command=[\n            &quot;gunicorn&quot;,\n            &quot;src.inference:app&quot;,\n            &quot;--bind&quot;,\n            &quot;0.0.0.0:5000&quot;,\n            &quot;-k&quot;,\n            &quot;uvicorn.workers.UvicornWorker&quot;,\n            &quot;-t&quot;,\n            &quot;6000&quot;,\n        ],\n        model_serving_container_ports=[5000],\n)\n<\/code><\/pre>\n<p>I have a Flask endpoint defined for predict and health essentially defined below:<\/p>\n<pre><code>@app.get(f&quot;\/health&quot;)\ndef health_check_batch():\n    return 200\n\n@app.post(f&quot;\/predict&quot;)\ndef predict_batch(request_body: dict):\n    pred_df = pd.DataFrame(request_body['instances'],\n                           columns = request_body['parameters']['columns'])\n    # do some model inference things\n    return {&quot;predictions&quot;: predictions.tolist()}\n<\/code><\/pre>\n<p>As described, when training a model and deploying to an endpoint, I can successfully hit the API with JSON schema like:<\/p>\n<pre><code>{&quot;instances&quot;:[[1,2], [1,3]], &quot;parameters&quot;:{&quot;columns&quot;:[&quot;first&quot;, &quot;second&quot;]}}\n<\/code><\/pre>\n<p>This also works when using the endpoint Python SDK and feeding in instances\/parameters as functional arguments.<\/p>\n<p>However, I've tried performing batch inference with a CSV file and a JSONL file, and every time it fails with an Error Code 3. I can't find logs on why it failed in Logs Explorer either. I've read through all the documentation I could find and have seen other's successfully invoke batch inference, but haven't been able to find a guide. Does anyone have recommendations on batch file structure or the structure of my APIs? Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-24 07:29:05.693 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":111,
        "Owner_creation_date":"2016-06-28 17:55:10.36 UTC",
        "Owner_last_access_date":"2022-05-23 21:53:23.47 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How can I access to text data (value) of feature importance from a traiend model on Vertex AI with Python",
        "Question_body":"<p>I am working on Predictive Modeling with AutoML on Vertex AI and got the trained model. I checked its feature importance Graphically at Model Tab on Vertex AI, and then I want to have its text data of feature importance with the below code, but can only see it and can not get each of items as value.<\/p>\n<p>------------------------ Python Code<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform2\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nclient_model = aiplatform2.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'this is my project id'\nlocation = 'us-central1'\nmodel_id = 'my trained id'\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n\nlist_eval_request = aiplatform2.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\nlist_eval.model_evaluations\n<\/code><\/pre>\n<p>------------------------ Get this just visually on Notebook<\/p>\n<pre><code>[name: &quot;projects\/*********\/locations\/us-central1\/models\/*********\/evaluations\/*********&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/regression_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;meanAbsoluteError&quot;\n      value {\n        number_value: 2863.7043\n      }\n    }\n    fields {\n      key: &quot;meanAbsolutePercentageError&quot;\n      value {\n        number_value: 197.63817\n      }\n<\/code><\/pre>\n<p>------------------------ Question<\/p>\n<p>How can I access to the &quot;key&quot; and its &quot;value&quot;.\nExample key: &quot;meanAbsoluteError&quot; \/ value : number_value: 2863.7043<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-08 08:39:32.76 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_date":"2021-10-08 08:15:59.657 UTC",
        "Owner_last_access_date":"2021-12-22 23:44:00.18 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-08 08:46:10.727 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-07 15:42:30.64 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-07 20:47:51.21 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to use kfp Artifact with sklearn?",
        "Question_body":"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:<\/p>\n<ol>\n<li>read data from a big query table<\/li>\n<li>create a pandas <code>DataFrame<\/code><\/li>\n<li>use the <code>DataFrame<\/code> to train a K-Means model<\/li>\n<li>deploy the model to an endpoint<\/li>\n<\/ol>\n<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]<\/code> as output because <code>pd.DataFrame<\/code> type that I found <a href=\"https:\/\/stackoverflow.com\/questions\/43890844\/pythonic-type-hints-with-pandas\">here<\/a> did not work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    df: Output[Artifact],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    df = client.list_rows(table).to_dataframe()\n<\/code><\/pre>\n<p>Here the code of the step 3:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])\ndef kmeans_training(\n        dataset: Input[Artifact],\n        model: Output[Model],\n        num_clusters: int,\n):\n    from sklearn.cluster import KMeans\n    model = KMeans(num_clusters, random_state=220417)\n    model.fit(dataset)\n<\/code><\/pre>\n<p>The run of the pipeline is stopped due to the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: float() argument must be a string or a number, not 'Artifact'\n<\/code><\/pre>\n<p>Is it possible to convert Artifact to <code>numpy array<\/code> or <code>Dataframe<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-15 16:10:03.783 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp",
        "Question_view_count":339,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"AutoML Vertex AI - How to set continuous value for labels and default values?",
        "Question_body":"<p>I have an image dataset where it needs multi-labeling support. Additionally, there is a task that needs a count of how many of a specific item there is in each photo. Therefore, I need an input where the user can specify the number (from 0-100 for example).<\/p>\n<p>Is there anyway to do this? Additionally, is there a way to set default values to the labels?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-25 16:49:11.527 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":134,
        "Owner_creation_date":"2014-02-18 22:34:13.817 UTC",
        "Owner_last_access_date":"2022-09-22 15:59:48.447 UTC",
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"creating custom model on Google vertex ai",
        "Question_body":"<p>I should use Google\u2019s managed ML platform Vertex AI to build an end-to-end machine learning workflow for an internship. Although I completely follow the tutorial, when I run a training job, I see this error message:<\/p>\n<pre><code>Training pipeline failed with error message: There are no files under &quot;gs:\/\/dps-fuel-bucket\/mpg\/model&quot; to copy.\n<\/code><\/pre>\n<p>based on the tutorial, we should not have a \/model directory in the bucket. And the model should create this directory and save the final result there.<\/p>\n<pre><code># Export model and save to GCS\nmodel.save(BUCKET + '\/mpg\/model')\n<\/code><\/pre>\n<p>I added this directory but still face this error.\nDoes anybody have any idea, thanks in advance :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 08:56:13.937 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-bucket",
        "Question_view_count":132,
        "Owner_creation_date":"2021-11-17 08:37:31.163 UTC",
        "Owner_last_access_date":"2022-03-17 11:22:27.09 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex API - Failed to build Pipeline Internal Error",
        "Question_body":"<p>I built a dataset using 30 books in .txt format. Loaded them up started training and pipeline build and it kept failing with an &quot;Internal Error&quot;. Does anyone out there have an idea of what is the root cause of this error? I am using the full Vertex AI AutoML preconfigured models as this is a quick demo. Please Advise..<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 13:51:29.087 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":8,
        "Owner_creation_date":"2022-08-05 13:45:00.253 UTC",
        "Owner_last_access_date":"2022-09-12 19:30:38.87 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"The feature statistics under datasets in Google Vertex AI shows inconsistent results. Has anyone had a similar problem?",
        "Question_body":"<p>I created a dataset in Google Vertex AI that contains numerous features and after clicking on &quot;Generate Statistics&quot; inside the dataset, I can see some basic stats about each feature, and when I click on each feature a pie chart of % distribution of each value and a histogram show up.<\/p>\n<p>Here is my question. For one of my numerical features, I have lots of zeros, specifically 652 zeros and my whole dataset contains 81K distinct values of that feature. The pie chart shows me that my dataset contains %83 percent zero values.<\/p>\n<p>How is it possible? When I calculate the percentages I get that the dataset has only %0.80 percent zeros. (652\/81K)*100=0.80%<\/p>\n<p>Is it a reporting problem, a formatting problem? Has anyone had any issues with the stats in Vertex AI datasets?<\/p>\n<p>Note: I don't have such problems with my other numerical features, I have the problem with only one feature containing a large number of zeros.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-04 08:58:28.463 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":63,
        "Owner_creation_date":"2017-09-20 13:24:49.35 UTC",
        "Owner_last_access_date":"2022-09-23 08:25:17.93 UTC",
        "Owner_location":"\u0130stanbul, Turkey",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"AlphaFold on VertexAI - Stuck in setting up notebook for 2 hours",
        "Question_body":"<p>I am trying to run AlphaFold on VertexAI as explained <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/running-alphafold-on-vertexai\" rel=\"nofollow noreferrer\">here<\/a>. However, my instance creation is stuck in this state for roughly two hours now. There is no error message either. I am wondering if something has gone wrong or this is just the expected time it will take to setup a new instance?<\/p>\n<p>I actually tried with two different notebooks. One is the <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/raw\/main\/community-content\/alphafold_on_workbench\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">default one<\/a> linked in the above article and the other is <a href=\"https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb<\/a><\/p>\n<p>Both are in the same state for roughly the same time.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-30 15:43:01.82 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_date":"2012-11-30 09:46:45.153 UTC",
        "Owner_last_access_date":"2022-09-23 18:26:35.997 UTC",
        "Owner_location":"Islamabad Capital Territory, Pakistan",
        "Owner_reputation":388,
        "Owner_up_votes":411,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Question_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2022-09-15 15:04:19.473 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":39,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-16 16:15:04.267 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_date":"2022-07-25 17:37:33.85 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|hyperparameters|google-cloud-vertex-ai|spacy-3",
        "Question_view_count":133,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2022-08-16 17:10:10.153 UTC",
        "Answer_last_edit_date":"2022-08-18 19:17:08.733 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-08-17 13:12:47.557 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI executor with .env file",
        "Question_body":"<p>Normally when using environment variables in Docker you would specify them with ENV command in the Dockerfile or give the .env file with the --env-file option in a docker run command.<\/p>\n<p>When creating a Vertex AI notebook executor this latter option is not available while the first option requires maintenance of the Dockerfile when an env variable changes.<br \/>\nWe can specify the env variables as parameters in a .yaml when creating the executor, but that requires again an extra step editing in the process when a env variable changes.<\/p>\n<p>How can we pass the content of the .env file to the container at runtime with no extra effort in Vertex AI?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-15 14:30:40.647 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"docker|environment-variables|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_date":"2020-04-21 08:20:19.48 UTC",
        "Owner_last_access_date":"2022-09-21 12:47:49.243 UTC",
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Pipelines (Kubeflow) skip step with dependent outputs on later step",
        "Question_body":"<p>I\u2019m trying to run a Vertex AI Pipelines job where I skip a certain pipeline step if the value of a certain pipeline parameter (in this case <code>do_task1<\/code>) is <code>False<\/code>. But because there is another step that runs unconditionally and expects the output of the first potentially skipped step, I get the following error, independently of do_task1 being <code>True<\/code> or <code>False<\/code>:<\/p>\n<pre><code>AssertionError: component_input_artifact: pipelineparam--task1-output_path not found. All inputs: parameters {\n  key: &quot;do_task1&quot;\n  value {\n    type: STRING\n  }\n}\nparameters {\n  key: &quot;task1_name&quot;\n  value {\n    type: STRING\n  }\n}\n<\/code><\/pre>\n<p>It seems like the compiler just cannot find the output <code>output_path<\/code> from <code>task1<\/code>. So I wonder if there is any way to have some sort of placeholders for the outputs of those steps that are under a <code>dsl.Condition<\/code> , and thus they get filled with default values unless the actual steps run and fill them with the non-default values.\nThe code below represents the problem and is easily reproducible.<\/p>\n<p>I'm using <code>google-cloud-aiplatform==1.14.0<\/code> and <code>kfp==1.8.11<\/code><\/p>\n<pre><code>from typing import NamedTuple\n\nfrom kfp import dsl\nfrom kfp.v2.dsl import Dataset, Input, OutputPath, component\nfrom kfp.v2 import compiler\n\nfrom google.cloud.aiplatform import pipeline_jobs\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task1(\n    # inputs\n    task1_name: str,\n    # outputs\n    output_path: OutputPath(&quot;Dataset&quot;),\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;output_1&quot;, str), (&quot;output_2&quot;, int)]):\n\n    import pandas as pd\n    \n    output_1 = task1_name + &quot;-processed&quot;\n    output_2 = 2\n\n    df_output_1 = pd.DataFrame({&quot;output_1&quot;: [output_1]})\n    df_output_1.to_csv(output_path, index=False)\n\n    return (output_1, output_2)\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task2(\n    # inputs\n    task1_output: Input[Dataset],\n) -&gt; str:\n\n    import pandas as pd\n\n    task1_input = pd.read_csv(task1_output.path).values[0][0]\n\n    return task1_input\n\n@dsl.pipeline(\n    pipeline_root='pipeline_root',\n    name='pipelinename',\n)\ndef pipeline(\n    do_task1: bool,\n    task1_name: str,\n):\n\n    with dsl.Condition(do_task1 == True):\n\n        task1_op = (\n            task1(\n                task1_name=task1_name,\n            )\n        )\n\n    task2_op = (\n        task2(\n            task1_output=task1_op.outputs[&quot;output_path&quot;],\n        )\n    )\n\n\nif __name__ == '__main__':\n    \n    do_task1 = True # &lt;------------ The variable to modify ---------------\n\n    # compile pipeline\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path='pipeline.json')\n\n    # create pipeline run\n    pipeline_run = pipeline_jobs.PipelineJob(\n        display_name='pipeline-display-name',\n        pipeline_root='pipelineroot',\n        job_id='pipeline-job-id',\n        template_path='pipelinename.json',\n        parameter_values={\n            'do_task1': do_task1, # pipeline compilation fails with either True or False values\n            'task1_name': 'Task 1',\n        },\n        enable_caching=False\n    )\n    \n    # execute pipeline run\n    pipeline_run.run()\n<\/code><\/pre>\n<p>Any help is much appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-07 15:45:03.98 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":39,
        "Owner_creation_date":"2018-02-10 21:10:15.053 UTC",
        "Owner_last_access_date":"2022-09-23 15:42:30.4 UTC",
        "Owner_location":"Spain",
        "Owner_reputation":21,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Got \"400 Request contains an invalid argument.\" error from Vertex AI",
        "Question_body":"<p>I uploaded my custom model on Vertex AI and used it for custom inference until last week.<\/p>\n<p>But today when I tried to inference with the same code(actually, it was the same code from the official <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">example code<\/a>), I received this error message.<\/p>\n<pre><code>InvalidArgument: 400 Request contains an invalid argument.\n<\/code><\/pre>\n<p>The detailed error message from grpc is below.<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.INVALID_ARGUMENT\ndetails = &quot;Request contains an invalid argument.&quot;\ndebug_error_string = &quot;{&quot;created&quot;:&quot;@1636358161.014643000&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:&lt;some_ip_address&gt;:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1070,&quot;grpc_message&quot;:&quot;Request contains an invalid argument.&quot;,&quot;grpc_status&quot;:3}&quot;&gt;\n<\/code><\/pre>\n<p>I've searched a couple of hours with this error message but I couldn't find any hints for solving this problem.<\/p>\n<p>I ran my code on the GCP VM instance, set the API endpoint and region with the model endpoint.\nAre there any recent changes from Vertex AI API?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-08 08:10:49.39 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":338,
        "Owner_creation_date":"2021-08-27 00:34:21.567 UTC",
        "Owner_last_access_date":"2022-08-11 02:53:07.69 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-08 23:23:20.607 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Workbench notebooks unresponsive",
        "Question_body":"<p>Having various problems accessing with GCP Vertex AI Workbench managed notebooks.  Could really use some suggestions about recovering, and avoiding further failure.<\/p>\n<p>The original behavior (two days ago) was<\/p>\n<ul>\n<li>After working in the JupyterLab instance for a bit over an hour (creating a handful of notebooks within the instance), some kind of connectivity is lost.\n<ul>\n<li>Inside the JupyterLab interface:  cells won't run, the notebook is unable to save to disk or export, and restarting the kernel doesn't work.<\/li>\n<li>On-screen error pop-up:  502, with message mentioning &quot;bad gateway&quot; or something like that<\/li>\n<li>Back out in the console screen for managing my Workbench instances, I was able to use the Reset command to get the instance back to a working state.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>**Note: **  This instance was provisioned with a setting to suspend after one hour of idle time.  It's not obviously relevant; the failure was a little more than an hour after creation, but there certainly wasn't an hour of idle time before things went to heck.<\/p>\n<p>Today, I came back and was again able to work in the same instance for a bit over an hour, but then the same symptoms locked in.  Couldn't execute code, couldn't save the notebook.<\/p>\n<p>However, things are worse now, because hitting Reset has led to an endless period of spinning cursor.  The instance won't complete its reset and can't start.  When I hover over the spinning cursor where the OPEN JUPYTERLAB button ought to be, a hover box says &quot;Setting up proxy to JupyterLab&quot;.<\/p>\n<p>The hover text for the Instance status says:  &quot;Provisioning&quot;.<\/p>\n<p>More:  I also tried creating a new notebook instance from the Workbench console screen, and it's stuck in the same condition -- just spinning, never reaching running state.  If I try to Reset it, a minor little pop-up appears at the bottom of the screen like so:\n<a href=\"https:\/\/i.stack.imgur.com\/GA5fl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GA5fl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Subsequently, the hover text raised by the Reset button is:\n<a href=\"https:\/\/i.stack.imgur.com\/7cbZI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7cbZI.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>At the least, I'm hoping to regain access to the initial instance at least once to recover some code in the notebooks (and go run it in a less flaky cloud service).  At best, you could help me manage this so that this GCP service is actually viable over time for me.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-06-16 19:27:31.767 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"jupyter-notebook|google-cloud-ml|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":622,
        "Owner_creation_date":"2015-11-23 03:15:36.167 UTC",
        "Owner_last_access_date":"2022-09-23 21:05:58.28 UTC",
        "Owner_location":"Austin, TX, United States",
        "Owner_reputation":919,
        "Owner_up_votes":400,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-02 08:08:31.85 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Hugging face code throwing error when running using vertex-ai in gcp",
        "Question_body":"<p>I am training a NLP Hugging face model in vertex-ai with custom image.<\/p>\n<p>The same code works in local machine.<\/p>\n<p>Here is my code and the error.<\/p>\n<pre><code>import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport transformers as tr\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM\nfrom transformers import DataCollatorForLanguageModeling\nfrom scipy.special import softmax\nimport scipy\nimport random\nimport pickle\nimport os\n\nprint(&quot;package imported completed&quot;)\n\nos.environ['TRANSFORMERS_OFFLINE']='1'\nos.environ['HF_MLFLOW_LOG_ARTIFACTS']='TRUE'\n\nprint(&quot;env setup completed&quot;)\nprint( tr.__version__)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(&quot;Using&quot;, device)\ntorch.backends.cudnn.deterministic = True  \n\ntr.trainer_utils.set_seed(0)\n\nprint(&quot;here&quot;)\n\ntokenizer = tr.XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-large&quot;,local_files_only=True)\nmodel = tr.XLMRobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-large&quot;, return_dict=True,local_files_only=True)\n\nmodel.to(device)\nprint(&quot;Model loaded successfully&quot;)\n\ndf=pd.read_csv(&quot;gs:\/\/****bucket***\/data.csv&quot;) \nprint(&quot;read csv&quot;)\n# ,engine='openpyxl',sheet_name=&quot;master_data&quot;\ntrain_df=df.text.tolist()\nprint(len(train_df))\n\ntrain_df=list(set(train_df))\ntrain_df = [x for x in train_df if str(x) != 'nan']\ntrain_df=train_df[:50]\n\nprint(&quot;Length of training data is \\n &quot;,len(train_df))\nprint(&quot;DATA LOADED successfully&quot;)\n\n\ntrain_encodings = tokenizer(train_df, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nprint(&quot;encoding done&quot;)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\nprint(&quot;data collector done&quot;)\n\nclass SEDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings[&quot;attention_mask&quot;])\n\ntrain_data = SEDataset(train_encodings)\n\nprint(&quot;train data created&quot;)\n\ntraining_args = tr.TrainingArguments(\n    output_dir='gs:\/\/****bucket***\/results_mlm_exp1', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n#     ,save_strategy=&quot;epoch&quot;\n#     ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n#     logging_dir='gs:\/\/****bucket***\/logs_mlm_exp1',            # directory for storing logs\n#     logging_steps=32000,\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n)\nprint(&quot;training to start&quot;)\ntrainer.train()\nprint(&quot;model training finished&quot;)\ntrainer.save_model(&quot;gs:\/\/****bucket***\/model_mlm_exp1&quot;)\n\nprint(&quot;training finished&quot;)\n<\/code><\/pre>\n<p>The error that I get is:<\/p>\n<pre><code>None    INFO    train data created\nNone    INFO    training to start\nNone    ERROR   0%| | 0\/8 [00:00&lt;?, ?it\/s]train.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nNone    ERROR     item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\nNone    ERROR   \/opt\/conda\/lib\/python3.7\/site-packages\/torch\/nn\/parallel\/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\nNone    ERROR     warnings.warn('Was asked to gather along dimension 0, but all '\n\/var\/sitecustomize\/sitecustomize.py INFO    None\nNone    ERROR   0%| | 0\/8 [00:09&lt;?, ?it\/s]\n<\/code><\/pre>\n<p>Most of them are warning but still my code stops with error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-15 10:19:39.24 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":153,
        "Owner_creation_date":"2018-06-07 08:44:46.053 UTC",
        "Owner_last_access_date":"2022-09-23 09:15:48.837 UTC",
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"When can one find logs for Vertex AI Batch Prediction jobs?",
        "Question_body":"<p>I couldn't find relevant information in the Documentation. I have tried all options and links in the batch transform pages.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-14 06:08:05.233 UTC",
        "Question_favorite_count":2.0,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":492,
        "Owner_creation_date":"2021-10-14 05:56:24.71 UTC",
        "Owner_last_access_date":"2022-09-23 07:04:34.437 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-05-01 03:22:15.713 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":358,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-17 15:43:32.843 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Executor gives NoSuchKernel",
        "Question_body":"<p>I have a simple hello-world ipynb file in a Vertex AI notebook instance that looks like this:<\/p>\n<pre><code>print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>When setting up an executor for this as shown below I receive the following error in the executor logs: <em>jupyter_client.kernelspec.NoSuchKernel: No such kernel named local-python3\nerror<\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The notebook has the following metadata<\/p>\n<pre><code>{\n    &quot;kernelspec&quot;: {\n        &quot;display_name&quot;: &quot;Python 3 (Local)&quot;,\n        &quot;language&quot;: &quot;python&quot;,\n        &quot;name&quot;: &quot;local-python3&quot;\n    },\n    &quot;language_info&quot;: {\n        &quot;codemirror_mode&quot;: {\n            &quot;name&quot;: &quot;ipython&quot;,\n            &quot;version&quot;: 3\n        },\n        &quot;file_extension&quot;: &quot;.py&quot;,\n        &quot;mimetype&quot;: &quot;text\/x-python&quot;,\n        &quot;name&quot;: &quot;python&quot;,\n        &quot;nbconvert_exporter&quot;: &quot;python&quot;,\n        &quot;pygments_lexer&quot;: &quot;ipython3&quot;,\n        &quot;version&quot;: &quot;3.7.12&quot;\n    }\n}\n<\/code><\/pre>\n<p>What would require to run this notebook successfully? I looked into the possibility of customer containers but that should be to much of a complex solution for such.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-02 15:51:43.42 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_date":"2020-04-21 08:20:19.48 UTC",
        "Owner_last_access_date":"2022-09-21 12:47:49.243 UTC",
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Cost of deploying a TensorFlow model in GCP?",
        "Question_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 05:32:59.287 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2019-09-26 00:32:01.527 UTC",
        "Owner_last_access_date":"2022-09-24 06:51:48.563 UTC",
        "Owner_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Owner_reputation":41,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-08 12:27:10.41 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-07-08 05:39:36.587 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Question_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-09 19:49:36.953 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":894,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-11-11 04:01:38.293 UTC",
        "Answer_last_edit_date":"2021-11-12 01:15:17.047 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2021-11-11 04:06:18.603 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to extract model file(s) from Vertex AI?",
        "Question_body":"<p>I have been trying to import model file(s) from Vertex AI to Workbench environment. My pipeline consists of preprocessing, training and batch predictions. Sometimes the training fails due to unknown reasons yet I want the batch predictions from the latest model in that case. Is there a way to access the trained models using Python?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 08:05:44.86 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":34,
        "Owner_creation_date":"2022-04-14 11:12:57.04 UTC",
        "Owner_last_access_date":"2022-09-24 05:03:36.517 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-19 09:03:36.54 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Question_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 10:49:48.007 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"docker|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":248,
        "Owner_creation_date":"2022-03-24 16:46:24.823 UTC",
        "Owner_last_access_date":"2022-09-24 21:22:42.907 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-24 14:20:18.333 UTC",
        "Answer_last_edit_date":"2022-08-24 14:25:20.347 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-04-28 13:15:19.75 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to solve \"Encountered unresolved custom op: edgetpu-custom-op\" exception",
        "Question_body":"<p>Im trying to implement my custom ML model in a kotlin app.<\/p>\n<p>I first make and train my model in GCP Vertex AI.\nAfter my model was ready i've export it as tensor flow model to the edge and then upload to my Firebase Machine Learning proyect.<\/p>\n<p>After that i follow the <a href=\"https:\/\/firebase.google.com\/docs\/ml\/android\/use-custom-models?hl=es&amp;authuser=0\" rel=\"nofollow noreferrer\">guide to implement a custom model of tensor flow lite on android.<\/a><\/p>\n<p>Then when i execute my app it crash on this part of the code:<\/p>\n<pre><code>val conditions = CustomModelDownloadConditions.Builder()\n        .requireWifi()  \/\/ Also possible: .requireCharging() and .requireDeviceIdle()\n        .build()\nFirebaseModelDownloader.getInstance()\n        .getModel(&quot;your_model&quot;, DownloadType.LOCAL_MODEL_UPDATE_IN_BACKGROUND,\n            conditions)\n        .addOnSuccessListener { model: CustomModel? -&gt;\n            \/\/ Download complete. Depending on your app, you could enable the ML\n            \/\/ feature, or switch from the local model to the remote model, etc.\n\n            \/\/ The CustomModel object contains the local path of the model file,\n            \/\/ which you can use to instantiate a TensorFlow Lite interpreter.\n            val modelFile = model?.file\n            if (modelFile != null) {\n                interpreter = Interpreter(modelFile) \/\/ this line crash\n            }\n        }\n<\/code><\/pre>\n<p>More specific at the line &quot;interpreter = Interpreter(modelFile)&quot;.\nI get the following exception:<\/p>\n<blockquote>\n<p>java.lang.IllegalStateException: Internal error: Unexpected failure\nwhen preparing tensor allocations: Encountered unresolved custom op:\nedgetpu-custom-op. See instructions:\n<a href=\"https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom<\/a>  Node number 0\n(edgetpu-custom-op) failed to prepare.<\/p>\n<\/blockquote>\n<p>What is the meaning of this error?  How can i solve it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 18:55:49.29 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"kotlin|machine-learning|google-cloud-vertex-ai|firebase-machine-learning",
        "Question_view_count":21,
        "Owner_creation_date":"2012-08-22 16:10:22.773 UTC",
        "Owner_last_access_date":"2022-09-25 02:05:58.237 UTC",
        "Owner_location":null,
        "Owner_reputation":109,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Question_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-08 12:40:04.323 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":85,
        "Owner_creation_date":"2016-03-30 15:15:05.81 UTC",
        "Owner_last_access_date":"2022-09-22 14:32:04.693 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-09-12 07:31:28.203 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Cannot use `gcloud auth print-identity-token` from within Vertex AI Custom Job",
        "Question_body":"<h3>Background<\/h3>\n<ul>\n<li>I want to use a service account, with a vertex AI custom job, and create an identity token.<\/li>\n<li>I am attempting to notify a cloud run API once a vertex-AI custom job has finished.<\/li>\n<li>I have created a service account with a <code>roles\/run.invoker<\/code> role, and have confirmed that the service account can access the cloud run API.\n<ul>\n<li>I used credentials from the service account and created an identity token with audience set to the cloud run API URL<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h3>Problem<\/h3>\n<ul>\n<li>When a vertex AI custom job is created using <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/create\" rel=\"nofollow noreferrer\"><code>gcloud ai custom-jobs create<\/code><\/a> or through the <a href=\"https:\/\/pkg.go.dev\/cloud.google.com\/go\/aiplatform\" rel=\"nofollow noreferrer\">golang client library<\/a>, an identity token cannot be obtained for a custom service account.\n<ul>\n<li><code>gcloud auth print-identity-token<\/code> results in an error:\n<ul>\n<li><code>(gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Using the <a href=\"https:\/\/cloud.google.com\/run\/docs\/authenticating\/service-to-service#use_the_metadata_server\" rel=\"nofollow noreferrer\">metadata server URL<\/a> directly gives a <code>Not Found<\/code> response.\n<ul>\n<li>Command:\n<pre><code>curl \\\n  -s \\\n  --get \\\n  --data-urlencode &quot;audience=$CLOUD_RUN_API_URL&quot; \\\n  --data-urlencode &quot;format=full&quot; \\\n  -H &quot;Metadata-Flavor: Google&quot; \\\n  http:\/\/metadata\/computeMetadata\/v1\/instance\/service-accounts\/default\/identity\n<\/code><\/pre>\n<\/li>\n<li>Response: <code>Not Found<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>I need some guidance on what settings \/ way of invoking the custom job will allow me to access the service account's identity token.<\/li>\n<\/ul>\n<hr \/>\n<h3>Details<\/h3>\n<ul>\n<li>For the purposes of testing creating an identity token, the custom container is specified using the docker file below:<\/li>\n<\/ul>\n<pre><code>FROM alpine:3.6\n\nRUN apk add --update python curl which bash\nRUN curl -sSL https:\/\/sdk.cloud.google.com | bash -s -- --disable-prompts\nENV PATH $PATH:\/root\/google-cloud-sdk\/bin\n\nCMD [&quot;gcloud&quot;, &quot;auth&quot;, &quot;print-identity-token&quot;, &quot;--verbosity&quot;, &quot;debug&quot;]\n<\/code><\/pre>\n<ul>\n<li><p>The following produce a working identity token:<\/p>\n<ul>\n<li>\u2705 Running this image locally<\/li>\n<li>\u2705 Running this image using the &quot;Create&quot; button on the vertex AI GUI and setting the service account to my custom one, created for this use case.<\/li>\n<li>\u2705 Running this image using <code>gcloud<\/code> with NO service account set (using vertex-ai default service account for the project)<\/li>\n<\/ul>\n<\/li>\n<li><p>The following results in an error (the problem which prompted me to ask this question):<\/p>\n<ul>\n<li><p>\u274c Running this image using <code>gcloud<\/code> with the following settings, I get an <strong>error<\/strong>:<\/p>\n<ul>\n<li>Command (image name\/service account redacted):\n<pre><code>gcloud ai custom-jobs create \\\n  --region=asia-northeast1 \\\n  --display-name=cli_identity_test \\\n  --worker-pool-spec=machine-type=n2-standard-4,replica-count=1,container-image-uri=&lt;IMAGE NAME&gt; \\\n  --service-account=&lt;SERVICE ACCOUNT EMAIL&gt;\n<\/code><\/pre>\n<\/li>\n<li>Error:\n<pre><code>DEBUG: (gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.                                                                \nTraceback (most recent call last):                                                                                                                                       \n  File &quot;\/root\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/cli.py&quot;, line 987, in Execute                                                                                 \n    resources = calliope_command.Run(cli=self, args=args)                                                                                                                \n  File &quot;\/root\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/backend.py&quot;, line 809, in Run                                                                                 \n    resources = command_instance.Run(args)                                                                                                                               \n  File &quot;\/root\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/exceptions.py&quot;, line 129, in TryFunc                                                                          \n    return func(*args, **kwargs)                                                                                                                                         \n  File &quot;\/root\/google-cloud-sdk\/lib\/surface\/auth\/print_identity_token.py&quot;, line 130, in Run                                                                               \n    credential = _Run(args)                                                                                                                                              \n  File &quot;\/root\/google-cloud-sdk\/lib\/surface\/auth\/print_identity_token.py&quot;, line 78, in _Run                                                                               \n    'No identity token can be obtained from the current credentials.')                                                                                                   \nInvalidIdentityTokenError: No identity token can be obtained from the current credentials.                                                                               \nERROR: (gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.\n<\/code><\/pre>\n<\/li>\n<\/ul>\n<\/li>\n<li><p>\u274c Running the image with a custom service account, and using the vertex ai golang API client library to start the job<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<hr \/>\n<h3>A note on roles<\/h3>\n<ul>\n<li>I have also tried extending the roles associated with my service account, but these do not fix the error:\n<ul>\n<li>to match the default vertex-ai service account (<code>roles\/aiplatform.customCodeServiceAgent<\/code>)<\/li>\n<li>Service Account Token Creator (<code>roles\/iam.serviceAccountTokenCreator<\/code>)<\/li>\n<li>Service Account User (<code>roles\/iam.serviceAccountUser<\/code>)<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-04 08:02:27.75 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcloud|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":243,
        "Owner_creation_date":"2010-07-31 06:08:12.487 UTC",
        "Owner_last_access_date":"2022-09-25 03:32:37.283 UTC",
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":8173,
        "Owner_up_votes":150,
        "Owner_down_votes":10,
        "Owner_views":394,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to query \/ flatten from vertex ml results saved to bigquery",
        "Question_body":"<p>This is driving me bonkers so any help greatly appreciated.<\/p>\n<p>I am using Google's Vertex ML. I have exported a batch prediction to BigQuery.<\/p>\n<p>The schema is I believe a record with repeat fields.<\/p>\n<p>So I think it would like this in JSON:<\/p>\n<pre><code>[{&quot;category&quot;:true,&quot;score&quot;:.9999},{&quot;category&quot;:false,&quot;score&quot;,.05}]\n<\/code><\/pre>\n<p>I can not figure out how to either unnest or narrow a search where a category is true.<\/p>\n<p>I need to have a flat select that has the correct category column and score value<\/p>\n<pre><code>123 | true | .9999\n123 | false | .05\n<\/code><\/pre>\n<p>or a select with a where clause to only get true values<\/p>\n<pre><code>123 | .9999\n<\/code><\/pre>\n<p>The following unnests everything but it creates four rows joining both the true and false to both the scores.<\/p>\n<pre><code>SELECT\n  row_id,\n  classes,\n  scores\nFROM\n  `database`\ncross JOIN\n  UNNEST(exported.classes) AS classes,\n  UNNEST(exported.scores) AS scores\nLIMIT\n  10\n<\/code><\/pre>\n<p>creates rows like:<\/p>\n<pre><code>123 | true | .9999\n123 | false | .9999\n123 | true | .05\n123 | false | .05\n<\/code><\/pre>\n<p>This does select the values I need but it's still a nested field...<\/p>\n<pre><code>select\nrow_id,\nclasses.classes,\nclasses.scores\nfrom (\nSELECT\n  voter_id,\n  ARRAY_CONCAT([predicted_results]) as the_results\nFROM\n  `data`\nLIMIT\n  10\n),\nunnest(the_results) as classes\n<\/code><\/pre>\n<p>creates rows like<\/p>\n<pre><code>123 | [true:.9999,false:.05]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-26 22:59:27.83 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"sql|google-bigquery|flatten|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_date":"2010-08-24 00:45:37.523 UTC",
        "Owner_last_access_date":"2022-09-24 01:12:22.42 UTC",
        "Owner_location":null,
        "Owner_reputation":185,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<pre><code>select \n primary_key, \n  predicted_supports.classes[SAFE_OFFSET(index)] as class,\n  predicted_supports.scores[SAFE_OFFSET(index)] as score,\nFROM `database`,\nunnest(generate_array(0,array_length(predicted_supports.classes)-1)) as index\n<\/code><\/pre>\n<p>Here is the ouput:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-08-02 04:48:32.443 UTC",
        "Answer_last_edit_date":"2022-08-03 00:22:16.643 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-08-01 05:26:29.37 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to solve VertexAI prediction endpoint error?",
        "Question_body":"<p>I am trying to get predictions from an endpoint that is already created in VertexAI through UI.<\/p>\n<p>I am getting an error when I run the following code<\/p>\n<pre><code>from typing import Dict\nimport pandas as pd\nfrom google.cloud import aiplatform\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nfrom google.oauth2 import service_account\n\nkey_path = '..\/golden-tempest-xxxxx.json'\ncredentials = service_account.Credentials.from_service_account_file(key_path, scopes=[\n    &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;], )\n\n\naiplatform.init(\n    project='golden-tempest-xxxxx',\n    location='us-central1',\n    credentials=credentials,\n)\n\ndef predict_tabular_classification_sample(\n    project: str,\n    endpoint_id: str,\n    instance_dict: Dict,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n    instance = json_format.ParseDict(instance_dict, Value())\n    instances = [instance]\n    parameters_dict = {}\n    parameters = json_format.ParseDict(parameters_dict, Value())\n    endpoint = client.endpoint_path(\n        project=project, location=location, endpoint=endpoint_id\n    )\n    response = client.predict(\n        endpoint=endpoint, instances=instances, parameters=parameters\n    )\n    print(&quot;response&quot;)\n    print(&quot; deployed_model_id:&quot;, response.deployed_model_id)\n    # See gs:\/\/google-cloud-aiplatform\/schema\/predict\/prediction\/tabular_classification_1.0.0.yaml for the format of the predictions.\n    predictions = response.predictions\n    for prediction in predictions:\n        print(&quot; prediction:&quot;, dict(prediction))\n\n\ndf = pd.read_csv('..\/btc_test_classification_2.csv')\n\ndf_dict = df.to_dict('index')\n\npredict_tabular_classification_sample(\n    project=&quot;xxxxx&quot;,\n    endpoint_id=&quot;886215168080478208&quot;,\n    location=&quot;us-central1&quot;,\n    instance_dict={'instances': [df_dict[4]]}\n)\n<\/code><\/pre>\n<p><strong>Error<\/strong>:<\/p>\n<pre><code>InvalidArgument: 400 {&quot;error&quot;: &quot;Column prefix: . Error: Missing struct property: tick_count.&quot;}\n<\/code><\/pre>\n<p>My training data contains the same columns I have in test. <a href=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" alt=\"columns\" \/><\/a><\/p>\n<p>I am not sure why I am getting this error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-08-30 12:46:34.927 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-api|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_date":"2015-03-14 11:20:42.323 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:39.243 UTC",
        "Owner_location":null,
        "Owner_reputation":1026,
        "Owner_up_votes":236,
        "Owner_down_votes":1,
        "Owner_views":256,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Working link for Google Cloud pipeline components docs?",
        "Question_body":"<p>Does anyone have a working link for the docs for Google Cloud pipeline components. The link in the <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/google-cloud\" rel=\"nofollow noreferrer\">github page<\/a> under &quot;ReadTheDocs page&quot; is broken. Tried some other tutorial notebooks, such as <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/ai-platform-samples\/blob\/master\/ai-platform-unified\/notebooks\/official\/pipelines\/google-cloud-pipeline-components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this one<\/a>, the link under &quot;The components are documented here.&quot; seems to be broken too.<\/p>\n<p>Edit:<\/p>\n<p>The <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/\" rel=\"nofollow noreferrer\">link<\/a> is up now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-13 15:36:37.68 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-15 13:17:59.457 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI updating dataset and train model",
        "Question_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-30 19:19:40.96 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 22:30:59.89 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI returns a different result from the local tflite model",
        "Question_body":"<p>I uploaded my tflite model on Vertex AI and made an endpoint, and I requested inference with some input value, but it returns a different result from my local tflite model's inference result.<\/p>\n<p>The input value is float32 array(actually sampled audio data) and I used <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this code<\/a> for the request. Although there was the same input array, the local tflite model and the model which is uploaded on Vertex AI returns quite big different result.<\/p>\n<p>Is there any possibility of distortion on the value while it transfers to the Vertex AI instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 11:09:27.18 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":175,
        "Owner_creation_date":"2021-08-27 00:34:21.567 UTC",
        "Owner_last_access_date":"2022-08-11 02:53:07.69 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-17 22:29:31.083 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Specify signature name on Vertex AI Predict",
        "Question_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2021-11-05 18:47:59.96 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|vertex|google-ai-platform|tfx|google-cloud-vertex-ai",
        "Question_view_count":508,
        "Owner_creation_date":"2020-11-28 23:13:00.56 UTC",
        "Owner_last_access_date":"2022-09-25 01:03:03.153 UTC",
        "Owner_location":"Brazil",
        "Owner_reputation":98,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-09 06:24:48.55 UTC",
        "Answer_last_edit_date":"2021-11-09 10:18:28.543 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-11-06 12:50:38.13 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Retrieving the cpu, gpu specs on a deployed Deep Learning VM",
        "Question_body":"<p>I already deployed a Deep Learning VM by configuring cpu and gpu. I would like to retrieve that information.<\/p>\n<p>Is there a way to find out what gpu and cpu specification my deployed DeepLearning VM has?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-05 05:13:13.507 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|deep-learning|google-cloud-vertex-ai|google-ai-platform|google-dl-platform",
        "Question_view_count":61,
        "Owner_creation_date":"2013-05-26 03:45:19.747 UTC",
        "Owner_last_access_date":"2022-09-22 01:29:51.95 UTC",
        "Owner_location":null,
        "Owner_reputation":311,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-06 00:47:02.413 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Feasibility of calling vertex AI endpoints from dataflow streaming pipeline",
        "Question_body":"<p>I have an application where a streaming dataflow pipeline does inference on an incoming stream of images. It does so by loading a tensorflow CNN model saved in a GCS location as h5 file and using that model loaded in a dataflow user defined PTransform to do the inference.<\/p>\n<p>I have been going through GCP documentations but the following is still not clear to me.\nInstead of having to load the tensorflow model from a GCS bucket, is it possible to deploy them model on vertex AI endpoint and call the endpoint to do inference on an image from a dataflow Ptransform? Is it feasible?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 13:06:36.257 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-dataflow|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_date":"2018-06-21 09:40:34.03 UTC",
        "Owner_last_access_date":"2022-09-21 02:44:50.143 UTC",
        "Owner_location":null,
        "Owner_reputation":361,
        "Owner_up_votes":14,
        "Owner_down_votes":4,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to make a prediction to a private Vertex AI endpoint with Node.js client libraries?",
        "Question_body":"<p>Documentation on this is a bit vague at the time of posting <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#sending-prediction-to-private-endpoint\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#sending-prediction-to-private-endpoint<\/a> , they only mention how to do it with curl.<\/p>\n<p>I would like to use the node.js client library if possible, but I've only managed to find examples that don't use a private endpoint ie: <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-custom-trained-model.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-custom-trained-model.js<\/a> .<\/p>\n<p>I've read through the type definitions of <code>PredictionServiceClient<\/code> imported from <code>@google-cloud\/aiplatform<\/code> and didn't find a way to plug in my private endpoint. I've tried making the request anyway by simply specifying the resource name by doing <code>const endpoint = projects\/${project}\/locations\/${location}\/endpoints\/${endpointId}<\/code> but this leads to the following error:<\/p>\n<pre><code>Error: 13 INTERNAL: Received RST_STREAM with code 0\n    at Object.callErrorFromStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/call.ts:81:24)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client.ts:343:36)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client-interceptors.ts:462:34)\n    at Object.onReceiveStatus (\/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/client-interceptors.ts:424:48)\n    at \/home\/vitor\/vertexai\/node_modules\/@grpc\/grpc-js\/src\/call-stream.ts:323:24\n    at processTicksAndRejections (node:internal\/process\/task_queues:78:11) {\n  code: 13,\n  details: 'Received RST_STREAM with code 0',\n  metadata: Metadata { internalRepr: Map(0) {}, options: {} }\n}\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>(async () =&gt; {\n        const client = new v1beta1.PredictionServiceClient();\n        const location = &quot;****&quot;;\n        const project = &quot;****&quot;;\n        const endpointId = &quot;****&quot;\n        const endpoint = `projects\/${project}\/locations\/${location}\/endpoints\/${endpointId}`;\n\n        const parameters = {\n            structValue: {\n                fields: {},\n            },\n        };\n\n        const toInstance = (obj: any) =&gt; (\n            {\n                structValue: {\n                    fields: {\n                        ****\n                    }\n                }\n            });\n\n        const instance = toInstance(****);\n        const instances = [instance];\n\n        const res = await client.predict({\n            instances,\n            endpoint,\n            parameters\n        });\n        console.log(res);\n    })();\n<\/code><\/pre>\n<p>Is it possible to make this kind of request atm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":10,
        "Question_creation_date":"2022-03-14 13:05:35.233 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"node.js|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":676,
        "Owner_creation_date":"2020-02-15 20:36:26.27 UTC",
        "Owner_last_access_date":"2022-09-08 19:19:18.53 UTC",
        "Owner_location":null,
        "Owner_reputation":185,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is it possible to import custom source files into Kubeflow components?",
        "Question_body":"<p>I know that Kubeflow only modifies the container with the specified libraries to be installed. But I want to use my custom module in the training Component section of the pipeline.<\/p>\n<p>So let me clarify my case; I'm deploying a GCP Vertex AI pipeline which exists of preprocessing and training  steps. And there is also custom library that I created using some libraries like scikit. My main issue is that I want to re-use that library objects within my training step which looks like;<\/p>\n<pre><code>    packages_to_install = [\n        &quot;pandas&quot;,\n        &quot;sklearn&quot;,\n        &quot;mycustomlibrary?&quot;\n    ],\n)\ndef train_xgb_model(\n    dataset: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \n    from MyCustomLibrary import XGBClassifier\n    import pandas as pd\n    \n    data = pd.read_csv(dataset.path)\n\n    model = XGBClassifier(\n        objective=&quot;binary:logistic&quot;\n    )\n    model.fit(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    score = model.score(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    model_artifact.metadata[&quot;train_score&quot;] = float(score)\n    model_artifact.metadata[&quot;framework&quot;] = &quot;XGBoost&quot;\n    \n    model.save_model(model_artifact.path)``` \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 14:24:14.327 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":31,
        "Owner_creation_date":"2018-06-25 06:49:27.513 UTC",
        "Owner_last_access_date":"2022-09-12 08:12:55.84 UTC",
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to create MLOps vertex ai pipeline with custom sklearn code?",
        "Question_body":"<p>I'm trying to build MLOps pipeline using vertex ai but failing to deploy it<\/p>\n<pre><code>@dsl.pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=&quot;pipeline-test-1&quot;,\n)\ndef pipeline(\nserving_container_image_uri: str = &quot;us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest&quot;\n):\n    dataset_op = get_data()\n    train_op = train_xgb_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    train_knn = knn_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    \n    eval_op = eval_model(\n        test_set=dataset_op.outputs[&quot;dataset_test&quot;],\n        xgb_model=train_op.outputs[&quot;model_artifact&quot;],\n        knn_model=train_knn.outputs['best_model_artifact']\n    )\n    \n    endpoint_op = gcc_aip.ModelDeployOp(\n    project=PROJECT_ID,\n    model=eval_op.outputs[&quot;model_artifacts&quot;],\n    machine_type=&quot;n1-standard-4&quot;,\n    )\n    \n    #endpoint_op.after(eval_op)\n    \ncompiler.Compiler().compile(pipeline_func=pipeline,\n        package_path='xgb_pipe.json')\n<\/code><\/pre>\n<p>gcc_aip.ModelDeployOp is throwing error that correct model id or name should be pass<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-01 22:23:54.787 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"mlops|google-cloud-vertex-ai",
        "Question_view_count":154,
        "Owner_creation_date":"2015-11-25 08:05:38.667 UTC",
        "Owner_last_access_date":"2022-06-15 12:39:30.893 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI custom container batch prediction",
        "Question_body":"<p>I have created a custom container for prediction and successfully uploaded the model to Vertex AI. I was also able to deploy the model to an endpoint and successfully request predictions from the endpoint. Within the custom container code, I use the <code>parameters<\/code> field as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">here<\/a>, which I then supply later on when making an online prediction request.\nMy questions are regarding requesting batch predictions from a custom container for prediction.<\/p>\n<ol>\n<li><p>I cannot find any documentation that describes what happens when I request a batch prediction. Say, for example, I use the <code>my_model.batch_predict<\/code> function from the Python SDK and set the <code>instances_format<\/code> to &quot;csv&quot; and provide the <code>gcs_source<\/code>. Now, I have setup my custom container to expect prediction requests at <code>\/predict<\/code> as described in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">documentation<\/a>. Does Vertex AI make a POST request to this path, converting the cvs data into the appropriate POST body?<\/p>\n<\/li>\n<li><p>How do I specify the <code>parameters<\/code> field for batch prediction as I did for online prediction?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-11 23:44:50.16 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":808,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Tensorboard trough user interface",
        "Question_body":"<p>I have been using the Vertex AI training service with a custom container for my own machine learning pipeline. I would like to get tensorboard logs into the experiments tab to see in real-time the metrics while the model is training.<\/p>\n<p>I was wondering if it is possible to set a custom training job in the user interface setting a <code>TENSORBOARD_INSTANCE_NAME<\/code>. It seems that this is only possible through a json-post-request.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-30 11:18:07.87 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_date":"2021-08-10 14:58:20.49 UTC",
        "Owner_last_access_date":"2022-09-14 19:08:59.14 UTC",
        "Owner_location":"Colombia",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Vertex AI fails AutoML training due to large BigQuery dataset being too large",
        "Question_body":"<p>I am currently training some models via Googles AutoML feature contained within their Vertex AI products.<\/p>\n<p>The normal pipeline is creating a dataset, which I do by creating a table in Bigquery, and then starting the training process.<\/p>\n<p>This has normally worked before but for my latest dataset I get the following error message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: The size of source BigQuery table is larger than 107374182400 bytes.<\/p>\n<\/blockquote>\n<p>While it seemed unlikely to me that the table is actually too large for AutoML, I tried re-training on a new dataset that's a 50% sample of the original table but the same error occured.<\/p>\n<p>Is my dataset really to large for AutoML to handle or is there another issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-25 17:05:43.937 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_date":"2016-02-16 11:11:55.903 UTC",
        "Owner_last_access_date":"2022-09-20 09:07:15.447 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":1179,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Answer_body":"<p>There are some perspectives of limits for AutoML Tables -- not only size in bytes (100GB as maximum supported size), but also number of rows (~200bi lines) and number of columns (up to 1000 columns).<\/p>\n<p>You can find more details on <a href=\"https:\/\/cloud.google.com\/automl-tables\/docs\/quotas#limits\" rel=\"nofollow noreferrer\">AutoML Tables limits<\/a> documentation.<\/p>\n<p>Is your source data within those limits?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-07-25 18:19:54.927 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is there anyway to train a classification model with Google's AutoML, with mixed (language and tabular) data?",
        "Question_body":"<p>I want to train a NLP classification model with Google's AutoML. The inputs of the model are tabular data and a text field which is the main field for the classification task. Without the tabular data the classification error gets huge. I'm aware that it can be done with a custom model using Keras or PyTorch. Could it be done using Google's AutoML?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-08 15:42:38.097 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"keras|nlp|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":46,
        "Owner_creation_date":"2017-07-16 21:33:23.887 UTC",
        "Owner_last_access_date":"2022-02-23 03:13:14.337 UTC",
        "Owner_location":"Medell\u00edn, Medellin, Antioquia, Colombia",
        "Owner_reputation":38,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-09 16:14:47.533 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Object Tracking with only one label",
        "Question_body":"<p>I want to train an object tracking model in Vertex AI for one type of object. The &quot;Train New Model&quot; button says &quot;To train a model, you must have at least two labels and each label included in training must have at least 15 videos assigned to it.&quot; I do not find any explanation of this requirement in the documentation. Does anyone know why I must have two labels?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 22:06:40.823 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai|object-tracking",
        "Question_view_count":88,
        "Owner_creation_date":"2021-10-08 15:45:59.98 UTC",
        "Owner_last_access_date":"2022-09-23 19:31:28.02 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-08 23:34:38.527 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI model version using Python SDK",
        "Question_body":"<p>Vertex AI offers a very interesting Model Registry that allows you to store all trained models and track all their versions.<\/p>\n<p>However, I don't manage to create new versions of the same model using the Python SDK. In particular, I have a Vertex AI Pipeline that performs: 1) data preprocessing, 2) feature engineering, 3) feature store creation, and in the end, 4) train a model with AutoML Tabular.<\/p>\n<p>The code of the Pipeline component dedicated to the point 4 is:<\/p>\n<pre><code> automl_training_electric_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n    project=project_bq,\n    model_display_name=&quot;pred-model&quot;,\n    display_name=&quot;pred-model&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-prc&quot;,\n    budget_milli_node_hours=1000,\n    dataset=comp5a.outputs[&quot;dataset&quot;],\n    target_column=&quot;fault&quot;,\n    location=location\n)\n<\/code><\/pre>\n<p>In the Google documentation I didn't find anything that could help me in creating new versions of the &quot;pred-model&quot;, in fact, any time I run the pipeline, Vertex AI creates a new model with the same name.<\/p>\n<p>I would like that at each training, AutoML creates a new version of the same model. E.g., v1, v2, v3.<\/p>\n<p>Here, the current situation, in which the same model is replicated and not versioned:\n<a href=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-01 17:40:58.413 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":58,
        "Owner_creation_date":"2014-11-06 09:41:52.943 UTC",
        "Owner_last_access_date":"2022-09-24 19:13:38.297 UTC",
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google AutoML Video Tracking Architecture",
        "Question_body":"<p>I'm developing an object tracking system using Google's Vertex AI AutoML Video Tracking. We currently have an accurate model that identifies objects per frame (as a picture) and I'm exploring models that may be able to gain further insight and accuracy by using a collection of frames (video) for the classification and tracking purposes. I want to learn more about the architecture used in the AutoML Object Tracking, but all I can find is articles hyping up the dynamic nature of the architecture. Mainly, I'm trying to answer the following 3 questions:<\/p>\n<ol>\n<li>What methods does the AutoML Object Tracking use to classify the objects and track them? Are the classifications done frame to frame, with a Euclidean distance tracker mapping objects together? Or are the objects identified and classified across multiple frames a recurrent network in space (image) and time (frame to frame). Something like a LSTM.<\/li>\n<li>What performance can object tracking in AutoML achieve that is better than their image object identification models?<\/li>\n<li>Where can I go to learn more about the model architectures on Vertex AI? It's hard to know which google publications are associated with their current platform.<\/li>\n<\/ol>\n<p>Any feedback is greatly appreciated!!!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-02 20:00:34.79 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|google-cloud-automl|google-cloud-vertex-ai|object-tracking",
        "Question_view_count":27,
        "Owner_creation_date":"2022-06-02 19:50:27.997 UTC",
        "Owner_last_access_date":"2022-06-10 01:45:17.947 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Vertex AI AutoML - cannot specify schema for CSV Dataset",
        "Question_body":"<p>I have created Tabular datasets in Vertex AI \/ Datasets based on some CSV files. However when I try to use these datasets in AutoML for training and prediction, there is no way to specify the data types of the fields. In <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#csv\" rel=\"nofollow noreferrer\">the docs<\/a> I could not find how to do the &quot;transformations&quot;. In theory it supports the following types:<\/p>\n<ul>\n<li>Text<\/li>\n<li>Categorical<\/li>\n<li>Numeric<\/li>\n<li>Timestamp<\/li>\n<\/ul>\n<p>In case of BigQuery tables it is pretty obvious to get the data types as it is explicitely specified by the schema of the table. However in case of a CSV file sometimes it is not obvious to find out the type of a field and indeed in my case sometimes AutoML guesses incorrectly. Any ideas how to specify the data types explicitely for CSV files?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-28 02:08:08.91 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":443,
        "Owner_creation_date":"2016-03-03 15:37:41.14 UTC",
        "Owner_last_access_date":"2022-05-27 00:45:31.473 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-06-29 04:28:42.617 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-14 20:57:29.023 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-15 04:13:37.373 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2021-10-14 21:57:40.77 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to set environment variables for a User-managed notebook in Vertex AI",
        "Question_body":"<p>I am trying to set some environment variables for a user managed notebook in google cloud Vertex AI. I don't want to set this from a jupyter notebook itself because I want these environment variables to be available to anyone who opens a jupyter notebook from this notebook instance. This is what I have tried so far but nothing has worked:<\/p>\n<ol>\n<li>I have an existing user managed notebook. I ssh'd into the notebook vm and then set a environment variable, <code>export TEST_VAR=TEST_VARIABLE_WAS_SET<\/code> there. However, when I open a jupyter notebook from the console and do <code>os.environ[&quot;TEST_VAR&quot;]<\/code>, it gives a key error. So, I am assuming that this has something to do with the fact that the jupyter lab session that Vertex AI starts is in a different shell session or something similar. I also tried to add the following two metadata keys to the vm, and then restarted the vm, but it did not work:<\/li>\n<\/ol>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata startup-script-url=$GCS_BUCKET_NAME\/script.sh<\/code><\/p>\n<p>where script.sh is:<\/p>\n<pre><code>#!\/bin\/bash\n\nexport TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<p>AND<\/p>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata container-env-file=$GCS_BUCKET_NAME\/notebook-env.txt<\/code><\/p>\n<p>where notebook-env.txt is<\/p>\n<pre><code>TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<ol start=\"2\">\n<li>I also tried to create a new instance of a user managed notebook from the cloud console. In that I tried to provide a script in the &quot;Select a script to run after creation&quot; and also through the &quot;Metadata&quot; option by providing, the key as <code>startup-script-url<\/code> and the value as the script location on google cloud storage. The script was the same startup script earlier.<\/li>\n<\/ol>\n<p>So, how do I achieve this, for existing user managed notebooks and when I create new ones?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-21 15:48:01.007 UTC",
        "Question_favorite_count":0.0,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-ai-platform|google-cloud-vertex-ai|google-notebook",
        "Question_view_count":851,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-23 22:07:38.733 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Creating a Vertex AI Workbench with a Non Organization Account and problems with constraints\/compute.vmExternalIpAccess",
        "Question_body":"<p>I'm trying to create a Vertex AI Workbench on GCP, but every time I try I get the following error:<\/p>\n<p><em>&lt;Workbench Name&gt; Constraint constraints\/compute.vmExternalIpAccess violated for project &lt;Project ID&gt;. Add instance &lt;Workbench ID&gt; to the constraint to use external IP with it.<\/em><\/p>\n<p>I went to the Organization Policies page to edit the constraint: <em>constraints\/compute.vmExternalIpAccess<\/em> and saw that is denied for all (which is odd because in the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints\" rel=\"nofollow noreferrer\">constraints documentation<\/a> it says that is should be enabled for all by default). Now, the problem is that when I go to edit the constraint, it says that it requires this set of permissions:<\/p>\n<ul>\n<li><em>orgpolicy.policies.create<\/em><\/li>\n<li><em>orgpolicy.policies.delete<\/em><\/li>\n<li><em>orgpolicy.policies.update<\/em><\/li>\n<li><em>orgpolicy.policy.get<\/em><\/li>\n<\/ul>\n<p>which are all part of the role: <em>roles\/orgpolicy.policyAdmin<\/em> that can only be granted at an organization level, and well, I have a Non Organization Account.<\/p>\n<p>Am I missing something?<\/p>\n<p>Thank for your time!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2022-01-18 10:28:24.547 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|workbench|google-cloud-vertex-ai",
        "Question_view_count":333,
        "Owner_creation_date":"2012-11-20 02:51:44.267 UTC",
        "Owner_last_access_date":"2022-05-25 19:01:53.253 UTC",
        "Owner_location":null,
        "Owner_reputation":215,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to update Python on Vertex AI notebooks?",
        "Question_body":"<p>I am working in notebooks provided in the Workbench section of Vertex AI. I need an updated version of Python, but I only have access to Python 3.7 in these notebooks. I have successfully followed <a href=\"https:\/\/stackoverflow.com\/a\/62831268\/8565438\">these steps<\/a> and if I run <code>python3.8 --version<\/code> in terminal, I get <code>Python 3.8.2<\/code>, which is good, but <code>python --version<\/code> still returns <code>Python 3.7.12<\/code>. If, following <a href=\"https:\/\/stackoverflow.com\/questions\/40694528\/how-to-know-which-python-is-running-in-jupyter-notebook\">this answer<\/a> and restarting notebook's kernel, I run<\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>in a notebook, and I get <code>3.7.12<\/code>.<\/p>\n<p><strong>How do I get a notebook in Vertex AI supporting an up-to-date Python version?<\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-30 18:08:51.833 UTC",
        "Question_favorite_count":2.0,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1736,
        "Owner_creation_date":"2017-09-05 19:27:41.23 UTC",
        "Owner_last_access_date":"2022-09-24 21:09:42.077 UTC",
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":5748,
        "Owner_up_votes":2565,
        "Owner_down_votes":761,
        "Owner_views":969,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-27 13:17:11.667 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"VertexAI Pipeline: How to use an output from a custom kfp component as input for google_cloud_pipeline_components?",
        "Question_body":"<p>I'm trying to write the Python code for a pipeline in VertexAI using kfp components. I have a step where i create a <code> system.Dataset<\/code>  object that is the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;,&quot;fsspec&quot;,&quot;gcsfs&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    dataset: Output[Dataset],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    train = client.list_rows(table).to_dataframe()\n    train.drop(&quot;&lt;list_of_columns&gt;&quot;, axis=1, inplace=True)\n    train['class'] = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n    \n    train.to_csv(dataset.uri)\n<\/code><\/pre>\n<p>Then I use the dataset as input for <code>AutoMLTabularTrainingJobRunOp<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = create_dataframe(project=project,\n                      region=region,\n                      destination_dataset=destination_dataset,\n                      destination_table_name=destination_table_name,\n)\n    \n# Training with AutoML\ntraining_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n            project=project,\n            display_name=&quot;train-automl-task&quot;,\n            optimization_prediction_type=&quot;classification&quot;,\n            column_transformations=[\n                &quot;&lt;nested_dict&gt;&quot;,\n            ],\n            dataset=df.outputs[&quot;dataset&quot;],\n            target_column=&quot;class&quot;,\n            budget_milli_node_hours=1000,\n)\n<\/code><\/pre>\n<p>Looking at the logs, I found this error:<\/p>\n<pre><code>&quot;Traceback (most recent call last): &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/runpy.py&quot;, line 193, in _run_module_as_main &quot;\n\n&quot; &quot;__main__&quot;, mod_spec) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/runpy.py&quot;, line 85, in _run_code &quot;\n\n&quot; exec(code, run_globals) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google_cloud_pipeline_components\/remote\/aiplatform\/remote_runner.py&quot;, line 284, in &lt;module&gt; &quot;\n\n&quot; main() &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google_cloud_pipeline_components\/remote\/aiplatform\/remote_runner.py&quot;, line 280, in main &quot;\n\n&quot; print(runner(args.cls_name, args.method_name, executor_input, kwargs)) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google_cloud_pipeline_components\/remote\/aiplatform\/remote_runner.py&quot;, line 236, in runner &quot;\n\n&quot; prepare_parameters(serialized_args[METHOD_KEY], method, is_init=False) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google_cloud_pipeline_components\/remote\/aiplatform\/remote_runner.py&quot;, line 205, in prepare_parameters &quot;\n\n&quot; value = cast(value, param_type) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google_cloud_pipeline_components\/remote\/aiplatform\/remote_runner.py&quot;, line 176, in cast &quot;\n\n&quot; return annotation_type(value) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/datasets\/dataset.py&quot;, line 81, in __init__ &quot;\n\n&quot; self._gca_resource = self._get_gca_resource(resource_name=dataset_name) &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/base.py&quot;, line 532, in _get_gca_resource &quot;\n\n&quot; location=self.location, &quot;\n\n&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/utils\/__init__.py&quot;, line 192, in full_resource_name &quot;\n\n&quot; raise ValueError(f&quot;Please provide a valid {resource_noun[:-1]} name or ID&quot;) &quot;\n\n&quot;ValueError: Please provide a valid dataset name or ID &quot;\n<\/code><\/pre>\n<p>So, I looked at source code in <code>google\/cloud\/aiplatform\/utils\/__init__.py<\/code> at line 192 and I found that the resource name should be like: <code>&quot;projects\/...\/locations\/...\/datasets\/12345&quot;<\/code> or <code>&quot;projects\/...\/locations\/...\/metadataStores\/...\/contexts\/12345&quot; <\/code>.<\/p>\n<p>Opening the <code>executor_output.json<\/code> file that is created in my bucket after running <code>create_dataframe<\/code> I discovered that the file name seems to be in the right format:<\/p>\n<p><code>{&quot;artifacts&quot;: {&quot;dataset&quot;: {&quot;artifacts&quot;: [{&quot;name&quot;: &quot;projects\/my_project\/locations\/my_region\/metadataStores\/default\/artifacts\/1299...&quot;, &quot;uri&quot;: &quot;my_bucket\/object_folder&quot;, &quot;metadata&quot;: {&quot;name&quot;: &quot;reshaped-training-dataset&quot;}}]}}}<\/code><\/p>\n<p>I tried also to set a human readable name for dataset in metadata, but I did not work.\nAny suggestion would be really helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 14:51:01.623 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp",
        "Question_view_count":322,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to schedule repeated runs of a custom training job in Vertex AI",
        "Question_body":"<p>I have packaged my training code as a python package and then am able to run it as a custom training job on Vertex AI. Now, I wanted to be able to schedule this job to run, say every 2 weeks, and re-train the model. The Scheduling settings in the CustomJoBSpec allow only 2 fields, &quot;timeout&quot; and &quot;restartJobOnWorkerRestart&quot; so it's not possible using the scheduling settings in the CustomJobSpec. One way to achieve this I could think of was to create a Vertex AI pipeline with a single step using the &quot;CustomPythonPackageTrainingJobRunOp&quot; Google Cloud Pipeline Component and then scheduling the pipeline to run as I see fit. Are there better alternatives to achieve this?<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>I was able to schedule the custom training job using Cloud Scheduler, but I found using the create_schedule_from_job_spec method in the AIPlatformClient very easy to use in the Vertex AI pipeline. The steps I took to schedule the custom job using Cloud Scheduler in gcp are as follows, <a href=\"https:\/\/cloud.google.com\/scheduler\/docs\/http-target-auth#setting_up_the_service_account\" rel=\"nofollow noreferrer\">link<\/a> to google docs:<\/p>\n<ol>\n<li>Set target type to HTTP<\/li>\n<li>For the url to specify the custom job, I followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#curl\" rel=\"nofollow noreferrer\">this<\/a> link to get the url<\/li>\n<li>For the authentication, under Auth header, I selected the &quot;Add OAauth token&quot;<\/li>\n<\/ol>\n<p>You also need to have a &quot;Cloud Scheduler service account&quot; with  a &quot;Cloud Scheduler Service Agent role granted to it&quot; in your project. Although the docs ay this should have been set up automatically if you enabled the Cloud Scheduler API after March 19, 2019, this was not the case for me and had to add the service account with the role manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2021-08-15 16:12:43.683 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":2669,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-16 21:06:01.557 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Pipelines: APIs from v1 namespace not supported in v2 compiler",
        "Question_body":"<p>I started taking <a href=\"https:\/\/stackoverflow.com\/a\/71973994\/16706763\">my first steps<\/a> in Vertex AI on past week. As you can see, that basic code works, however it displays the following warning:<\/p>\n<pre><code>\/home\/jupyter\/.local\/lib\/python3.7\/site-packages\/kfp\/v2\/compiler\/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n  category=FutureWarning,\n<\/code><\/pre>\n<p>Plus, if I run the same pipeline again, &quot;the warning disappears&quot;. What am I missing in my code, which could make that warning disappear at all times (i.e., solve it)?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-04-26 01:24:50.497 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":199,
        "Owner_creation_date":"2021-08-19 14:58:58.957 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.4 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-26 17:06:58.687 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Tracking resources used by VertexAI pipeline",
        "Question_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-06 03:07:12.657 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-vertex-ai",
        "Question_view_count":272,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-06 14:42:50.907 UTC",
        "Answer_last_edit_date":"2021-08-11 14:27:16.933 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP's Vertex AI(AI Platform) PipelineServiceClient gives unimplemented error",
        "Question_body":"<p>When trying to list pipelines with <code>PipelineServiceClient<\/code> <code>list_pipeline_jobs<\/code> method as given <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.services.pipeline_service.PipelineServiceClient#google_cloud_aiplatform_v1_services_pipeline_service_PipelineServiceClient_list_pipeline_jobs\" rel=\"nofollow noreferrer\">here<\/a>, I get the following error:<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNIMPLEMENTED\ndetails = &quot;Received http2 header with status: 404&quot;\n...\n<\/code><\/pre>\n<p>How is the API unimplemented, how do I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-16 05:06:48.807 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_date":"2019-02-17 05:09:55.09 UTC",
        "Owner_last_access_date":"2022-09-21 08:01:13.577 UTC",
        "Owner_location":null,
        "Owner_reputation":434,
        "Owner_up_votes":106,
        "Owner_down_votes":34,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI Predictions using a .NET Core webapi Custom Container",
        "Question_body":"<p>I'm getting some strange behavior out of Vertex AI when calling Predictions on a custom container that is a .NET core based application.<\/p>\n<p>A little background.  I'm doing a small proof of concept to test the viability of using .NET core based Docker images as custom containers in Vertex AI.<\/p>\n<p>I have created a simple .NET core webapi that mocks out the prediction endpoint by returning what vertex-ai expects as a prediction result from the endpoint.  The Controller code is below:<\/p>\n<pre><code>[ApiController]\n[Route(&quot;api\/[controller]&quot;)]\n[Produces(&quot;application\/json&quot;)]\npublic class PredictionController : ControllerBase\n{\n    private readonly ILogger&lt;PredictionController&gt; logger;\n    public PredictionController(ILogger&lt;PredictionController&gt; logger)\n    {\n        this.logger = logger;\n    }\n\n    [HttpPost]\n    public Prediction PostPrediction(dynamic data) \n    {\n        Prediction prediction = new Prediction();\n\n        List&lt;double&gt; predictions = new List&lt;double&gt;();\n\n        predictions.Add(0.82);\n\n        prediction.Predictions = predictions;\n\n        return prediction;\n         \n    }\n}\n<\/code><\/pre>\n<p>The Dockerfile used to host the .NET core application is:<\/p>\n<pre><code>FROM mcr.microsoft.com\/dotnet\/sdk:6.0-alpine as build\nWORKDIR \/app\nCOPY . .\nRUN dotnet restore\nRUN dotnet publish -o \/app\/published-app\n\nFROM mcr.microsoft.com\/dotnet\/aspnet:6.0-alpine as runtime\nWORKDIR \/app\nCOPY --from=build \/app\/published-app \/app\nENTRYPOINT [ &quot;dotnet&quot;, &quot;\/app\/dotnet-poc.dll&quot; ]\n<\/code><\/pre>\n<p>Here is an image of hitting the dockerized .NET core app endpoint with the expected vertex-ai input and the output of the endpoint.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" alt=\"docker prediction\" \/><\/a><\/p>\n<p>Following the standard documentation for deploying a model using a custom container for Vertex AI predictions, I have uploaded my docker image to Artifact Registry and imported the model using the custom container in Vertex AI.  I then deploy the model to an endpoint and run a prediction test...<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you can see Vertex is returning a 502 without much detail.  I cannot see any issue with the return from the prediction endpoint that should cause any error.<\/p>\n<p>Thanks for reading and looking forward to any answers that might shed light on the issue.<\/p>\n<p>Other things to note:<\/p>\n<p>Health endpoint is returning 200<\/p>\n<p>Vertex AI Endpoint is healthy<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-22 15:51:39.573 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":".net-core|google-cloud-vertex-ai",
        "Question_view_count":72,
        "Owner_creation_date":"2011-11-15 04:07:02.043 UTC",
        "Owner_last_access_date":"2022-09-23 19:05:45.94 UTC",
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP AI Platform API - Object Detection Metrics at Class Level (Python)",
        "Question_body":"<p>I have trained a AutoML Object Detection model in Vertex AI (a service under AI Platform in GCP). I am trying to access model evaluation metrics for each label (precision, recall, accuracy etc.) for varying Confidence Score Threshold and IoU Threshold.<\/p>\n<p>However, I am stuck at step one, even to get model's aggerate performance metric much less to the performance metric at granular levels. I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models?authuser=1#aggregate\" rel=\"nofollow noreferrer\">this instruction<\/a> But I cannot seem to figure out what is <code>evaluation_id<\/code> (also see the official sample code snippet <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/model_service\/get_model_evaluation_image_object_detection_sample.py\" rel=\"nofollow noreferrer\">here<\/a>), which is:<\/p>\n<pre><code>def get_model_evaluation_image_object_detection_sample(\n    project: str,\n    model_id: str,\n    evaluation_id: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n    name = client.model_evaluation_path(\n        project=project, location=location, model=model_id, evaluation=evaluation_id\n    )\n    response = client.get_model_evaluation(name=name)\n    print(&quot;response:&quot;, response)\n<\/code><\/pre>\n<p>After sometime I have figured out that for model trained in EU,  and <code>api_endpoint<\/code> shall be passed as:<\/p>\n<pre><code>location: str = &quot;europe-west4&quot;\napi_endpoint: str = &quot;europe-west4-aiplatform.googleapis.com&quot;\n<\/code><\/pre>\n<p>But whatever I try for <code>evaluation_id<\/code> leads to the following errors:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: name; Message: Invalid ModelEvaluation resource name.\n<\/code><\/pre>\n<p>There in the documentation it says (which is seems it contains what I need):<\/p>\n<blockquote>\n<p>For the bounding box metric, Vertex AI returns an array of metric\nvalues at different IoU threshold values (between 0 and 1) and\nconfidence threshold values (between 0 and 1). For example, you can\nnarrow in on evaluation metrics at an IoU threshold of 0.85 and a\nconfidence threshold of 0.8228. By viewing these different threshold\nvalues, you can see how they affect other metrics such as precision\nand recall.<\/p>\n<\/blockquote>\n<p>Without knowing that is contained in the output array, how would that work for each class? Basically I need for each class the model metrics for varying IoU threshold values and confidence threshold.<\/p>\n<p>Also I have tried to query from AutoML API instead, like:<\/p>\n<pre><code>client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n\nclient = automl.AutoMlClient(client_options=client_options)\n# Get the full path of the model.\nmodel_full_id = client.model_path(project_id, &quot;europe-west4&quot;, model_id)\n\nprint(&quot;List of model evaluations:&quot;)\nfor evaluation in client.list_model_evaluations(parent=model_full_id, filter=&quot;&quot;):\n    print(&quot;Model evaluation name: {}&quot;.format(evaluation.name))\n    print(&quot;Model annotation spec id: {}&quot;.format(evaluation.annotation_spec_id))\n    print(&quot;Create Time: {}&quot;.format(evaluation.create_time))\n    print(&quot;Evaluation example count: {}&quot;.format(evaluation.evaluated_example_count))\n    print(\n        &quot;Classification model evaluation metrics: {}&quot;.format(\n            evaluation.classification_evaluation_metrics\n        )\n    )\n<\/code><\/pre>\n<p>No surprise, also this doesn't work, and leads to:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: parent; Message: The provided location ID doesn't match the endpoint. For automl.googleapis.com, the valid location ID is `us-central1`. For eu-automl.googleapis.com, the valid location ID is `eu`.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-22 11:37:21.63 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_date":"2017-03-29 13:05:41.113 UTC",
        "Owner_last_access_date":"2022-09-23 09:40:26.75 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":423,
        "Owner_up_votes":897,
        "Owner_down_votes":4,
        "Owner_views":84,
        "Answer_body":"<p>I was able to get the response of the model evaluation using <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">aiplatform_v1<\/a> which is well documented and this is the reference linked from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/client-libraries?authuser=1#client_libraries\" rel=\"nofollow noreferrer\">Vertex AI reference page<\/a>.<\/p>\n<p>On this script I ran <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluations\" rel=\"nofollow noreferrer\">list_model_evaluations()<\/a> to get the evaluation name and used it as input for <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation\" rel=\"nofollow noreferrer\">get_model_evaluation()<\/a> which will return the evaluation details for Confidence Score Threshold, IoU Threshold, etc.<\/p>\n<p>NOTE: I don't have a trained model in <code>europe-west4<\/code> so I used <code>us-central1<\/code> instead. But if you have trained in <code>europe-west4<\/code> you should use <code>https:\/\/europe-west4-aiplatform.googleapis.com<\/code> as <code>api_endpoint<\/code> as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations#specifying_the_location_using_the\" rel=\"nofollow noreferrer\">location document<\/a>.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nget_eval_request = aiplatform.types.GetModelEvaluationRequest(name=eval_name)\nget_eval = client_model.get_model_evaluation(request=get_eval_request)\nprint(get_eval)\n<\/code><\/pre>\n<p>See response snippet:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999999\/evaluations\/1234567890&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.20201288\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.15670435\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.09326923\n                          }\n                        }\n                        fields {\n                          key: &quot;recall&quot;\n                          value {\n                            number_value: 0.48989898\n                          }\n                        }\n                      }\n                    }\n                    values {\n                      struct_value {\n....\n<\/code><\/pre>\n<p><strong>EDIT 1: Get response per class<\/strong><\/p>\n<p>To get metrics per class, you can use <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluation_slices\" rel=\"nofollow noreferrer\">list_model_evaluation_slices()<\/a> to get the name for each class, then use the name to <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation_slice\" rel=\"nofollow noreferrer\">get_model_evaluation_slice()<\/a>. In this code I pushed the names to a list since I have multiple classes. Then just use the values stored in the array to get the metric per class.<\/p>\n<p>In my code I used <code>label[0]<\/code> to get a single response from this class.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nlabel=[]\nslice_eval_request = aiplatform.types.ListModelEvaluationSlicesRequest(parent=eval_name)\nslice_eval = client_model.list_model_evaluation_slices(request=slice_eval_request)\nfor data in slice_eval:\n    label.append(data.name)\n\nget_eval_slice_request = aiplatform.types.GetModelEvaluationSliceRequest(name=label[0])\nget_eval_slice = client_model.get_model_evaluation_slice(request=get_eval_slice_request)\nprint(get_eval_slice)\n<\/code><\/pre>\n<p>Print all classes:\n<a href=\"https:\/\/i.stack.imgur.com\/pinWU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pinWU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Classes in UI:\n<a href=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Response snippet for a class:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999\/evaluations\/0000000000\/slices\/777777777&quot;\nslice_ {\n  dimension: &quot;annotationSpec&quot;\n  value: &quot;Cheese&quot;\n}\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.14256561\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.10344828\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.06198347\n                          }\n                        }\n....\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-10-25 02:40:16.533 UTC",
        "Answer_last_edit_date":"2021-10-25 08:38:55.017 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"AI\/ML Assisted labeling in Vertex AI",
        "Question_body":"<p>Is there a feature in Vertex AI which will allow AI\/ML to assist in labeling data? This usually works by providing a small set of labeled data, followed by a model creation which assists in labeling more data. As more and more data is labeled the model keeps getting better.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 19:48:45.143 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":58,
        "Owner_creation_date":"2012-05-30 05:48:34.657 UTC",
        "Owner_last_access_date":"2022-09-24 23:10:38.117 UTC",
        "Owner_location":null,
        "Owner_reputation":840,
        "Owner_up_votes":1070,
        "Owner_down_votes":1,
        "Owner_views":196,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"how to configure proxy in .Net application",
        "Question_body":"<p>I am facing problem while calling <strong>gcp api's (Vertex Ai &amp; Bigquery)<\/strong>. we have been using these api's from last few months. The behavior is unpredictable, sometimes we get api response successfully and sometimes its failing.<\/p>\n<p>what we noticed that the same host <strong>us-central1-aiplatform.googleapis.com<\/strong> communication is disrupted.<\/p>\n<p>Currently we are connecting with <strong>GOOGLE_APPLICATION_CREDENTIALS<\/strong>.<\/p>\n<p>I want to go through proxy. any suggestions ? or any other way I can solve this ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 15:19:49.337 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"c#|.net|google-cloud-platform|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to view Vertex AI Matching Engine Deployed Index logs",
        "Question_body":"<p>I have deployed an index in Vertex AI IndexEndpoint. According to the docs for <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1beta1.types.DeployedIndex\" rel=\"nofollow noreferrer\">DeployedIndex<\/a>, I have set the attribute <code>enable_access_logging<\/code> to <code>True<\/code> to enable private endpoints access logs.<\/p>\n<blockquote>\n<p><strong>enable_access_logging<\/strong><br \/>\nOptional. If true, private endpoint's access logs are sent to StackDriver Logging. These logs are like standard server access logs, containing information like timestamp and latency for each MatchRequest. Note that Stackdriver logs may incur a cost, especially if the deployed index receives a high queries per second rate (QPS). Estimate your costs before enabling this option.<\/p>\n<\/blockquote>\n<p>However, in cloud logging I only see Vertex AI audit logs and no access logs. Where can I find logs that contain information for timestamp and latency for each request?<\/p>\n<p>Deployed Index Configuration<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-12-13T16:37:27.030230Z'\n  deploymentGroup: default\n  displayName: glove_brute_force_deployed_V1\n  enableAccessLogging: true\n  id: glove_brute_force_deployed_V1\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXXXXXXXXX\n  indexSyncTime: '2021-12-13T20:19:00.874937Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.0.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yNMD_AR3V6LIrGln9Ye5PuWWYAOoJwxgSHs2T2Xt8iwAPv1mLOZTfaDMLFTAaBC\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/NETWORK_ID\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Cloud Logging<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-13 21:56:15.87 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":290,
        "Owner_creation_date":"2016-05-18 21:46:27.53 UTC",
        "Owner_last_access_date":"2022-05-01 15:16:30.167 UTC",
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-14 12:26:57.827 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-04-23 10:54:46.3 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_date":"2015-09-22 13:41:55.877 UTC",
        "Owner_last_access_date":"2022-09-24 13:00:43.763 UTC",
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-01 10:46:51.487 UTC",
        "Answer_last_edit_date":"2022-05-01 10:52:08.353 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-04-23 11:39:30.783 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Cloud Vertex AI with .Net",
        "Question_body":"<p>I am new to google cloud service VERTEX AI.<\/p>\n<p>I am looking to Create, train, and deploy an AutoML text classification model through .Net application. I did not find anything for .Net with Vertex AI. If someone can please guide my to the location or any .Net code samples, will be really helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-22 14:15:47.627 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":".net|machine-learning|google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>You can check <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AutoML.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AutoML.V1<\/a> NuGet package for .NET. Additionally, check the <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/tree\/main\/apis\/Google.Cloud.AutoML.V1\" rel=\"nofollow noreferrer\">Github of Google.Cloud.AutoML.V1<\/a> NuGet Package where you can see the sample codes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-22 22:27:24.36 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-03-22 20:28:39.087 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"400 Invalid image \"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest\" for deployment. Please use a Model with a valid image",
        "Question_body":"<p>What could be the point I am missing here. All datasets and training is done on gcp and my setup is basic.<\/p>\n<ol>\n<li>Training, validation and test done on JupyterLab<\/li>\n<li>Model pushed to a gcp storage bucket<\/li>\n<li>Create and endpoint<\/li>\n<li>Deploy model to endpoint.<\/li>\n<\/ol>\n<p>All steps looked fine until the last (4). Tried other pre-built pytorch images recommended by google but the error is persisting. The error long is as shown below:<\/p>\n<pre><code>---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     65         try:\n---&gt; 66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = &quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1652032269.328842405&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:142.250.148.95:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:903,&quot;grpc_message&quot;:&quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;,&quot;grpc_status&quot;:3}&quot;\n&gt;\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2924\/2180059764.py in &lt;module&gt;\n      5     machine_type = DEPLOY_COMPUTE,\n      6     min_replica_count = 1,\n----&gt; 7     max_replica_count = 1\n      8 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    697             explanation_parameters=explanation_parameters,\n    698             metadata=metadata,\n--&gt; 699             sync=sync,\n    700         )\n    701 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/base.py in wrapper(*args, **kwargs)\n    728                 if self:\n    729                     VertexAiResourceNounWithFutureManager.wait(self)\n--&gt; 730                 return method(*args, **kwargs)\n    731 \n    732             # callbacks to call within the Future (in same Thread)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    812             explanation_metadata=explanation_metadata,\n    813             explanation_parameters=explanation_parameters,\n--&gt; 814             metadata=metadata,\n    815         )\n    816 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy_call(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\n    979             deployed_model=deployed_model,\n    980             traffic_split=traffic_split,\n--&gt; 981             metadata=metadata,\n    982         )\n    983 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/endpoint_service\/client.py in deploy_model(self, request, endpoint, deployed_model, traffic_split, retry, timeout, metadata)\n   1155 \n   1156         # Send the request.\n-&gt; 1157         response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n   1158 \n   1159         # Wrap the response in an operation future.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[&quot;metadata&quot;] = metadata\n    153 \n--&gt; 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n---&gt; 68             raise exceptions.from_grpc_error(exc) from exc\n     69 \n     70     return error_remapped_callable\n\nInvalidArgument: 400 Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.\n<\/code><\/pre>\n<p>Below are some details where I create model, endpoint and deploy to endpoint.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>DEPLOY_COMPUTE = 'n1-standard-4'\nDEPLOY_IMAGE='us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest'\n\nmodel = aip.Model.upload(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    serving_container_image_uri = DEPLOY_IMAGE,\n    artifact_uri = URI,\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint = aip.Endpoint.create(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint.deploy(\n    model = model,\n    deployed_model_display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    traffic_percentage = 100,\n    machine_type = DEPLOY_COMPUTE,\n    min_replica_count = 1,\n    max_replica_count = 1\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-08 18:14:28.977 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":208,
        "Owner_creation_date":"2015-03-21 06:55:29.353 UTC",
        "Owner_last_access_date":"2022-09-02 13:58:42.187 UTC",
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>You are using <code>us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest<\/code> <strong>container image<\/strong> for importing models. However, models trained in <code>pytorch<\/code> cannot use pre-built containers when importing models since as mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#container-type\" rel=\"nofollow noreferrer\">documentation<\/a>,<\/p>\n<blockquote>\n<p>You can use a pre-built container if your model meets the following\nrequirements:<\/p>\n<ul>\n<li>Trained in Python 3.7 or later<\/li>\n<li>Trained using TensorFlow, scikit-learn, or XGBoost<\/li>\n<li>Exported to meet framework-specific requirements for one of the pre-built prediction containers<\/li>\n<\/ul>\n<\/blockquote>\n<p>I suggest 2 workaround options for your use case:<\/p>\n<ol>\n<li><p>You can create a custom prediction container image for your <code>pytorch<\/code> trained model by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-container\" rel=\"nofollow noreferrer\">documentation<\/a> .<\/p>\n<p>or<\/p>\n<\/li>\n<li><p>Re-train your model that meets the above requirements so that you can use the pre-bult container.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-05-11 07:02:40.713 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-05-11 03:55:18.217 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Kubeflow vs Vertex AI Pipelines",
        "Question_body":"<p>I was exploring kubeflow pipelines and Vertex AI pipelines. From what I understand, Vertex AI pipelines is a managed version of kubeflow pipelines so one doesn't need to deploy a full fledged kubeflow instance. In that respect, pricing aside, Vertex AI pipelines is a better choice. But then, in kubeflow, one can create <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/concepts\/experiment\/\" rel=\"noreferrer\">experiments<\/a>, an equivalent for which I have not found in Vertex AI pipelines. The only kubeflow features that Vertex AI does not support that I have been able to spot in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/migrate-kfp#features_not_supported_in\" rel=\"noreferrer\">documentation<\/a> are &quot;Cache expiration&quot; and &quot;Recursion&quot; but they do not mention anything about experiments. Makes me wonder if there are other differences that are worth considering when deciding between the two.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-15 04:21:00.217 UTC",
        "Question_favorite_count":null,
        "Question_score":9,
        "Question_tags":"google-cloud-platform|google-ai-platform|kubeflow-pipelines|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":3371,
        "Owner_creation_date":"2016-08-15 20:29:46.79 UTC",
        "Owner_last_access_date":"2022-09-24 22:22:50.413 UTC",
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Google Auto ML taking huge time for forcasting",
        "Question_body":"<p>I have around 500 time series dataset for a period of 2.5 yrs with a granularity of 1 day for each series. This amounts to roughly 1 million data points.\nI want to forecast for 2 weeks in 1 day granularity for each of the time series. There might be correlation among these 500 time series.\nAfter ensuring that I have data for each timestamp, we are feeding these (500) time series to autoML where each time series is identified by \u201cseries identifier\u201d.\nSo, our input to the autoML (Forecasting) is timestamp, series identifier, features, and target value. I have 30 feature which are combination of categorical and numerical.\nWith this setup, if I feed to autoML, it is taking more than 20 hrs for training which is not cost effective for me.<\/p>\n<p>Please help me to optimized this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-25 01:22:19.447 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"time-series|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_date":"2017-06-19 14:28:31.323 UTC",
        "Owner_last_access_date":"2022-04-04 01:07:49.423 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI: Unknown Custom Training Job Error",
        "Question_body":"<p>While attempting to use the Vertex AI via Google Console to create a custom training as follows:<\/p>\n<ol>\n<li>Select Dataset<\/li>\n<li>Select Annotation set<\/li>\n<li>Select &quot;Custom training (advanced)&quot;<\/li>\n<li>Select &quot;Train new model&quot;<\/li>\n<li>Provide Name<\/li>\n<li>Select training options<\/li>\n<li>Select &quot;Custom container&quot;<\/li>\n<li>Browse and select Docker container in Artifact Registry<\/li>\n<li>Select &quot;Dataset export directory&quot; to GCS location<\/li>\n<li>No hyperparameters<\/li>\n<li>Select &quot;Region&quot; us-central1<\/li>\n<li>Select &quot;Machine type&quot; a2-highgpu-1g<\/li>\n<li>Select &quot;Accelerator type&quot; NVIDIA_TESLA_A100<\/li>\n<li>Select &quot;Accelerator count&quot; 1<\/li>\n<li>No &quot;Prediction container&quot;<\/li>\n<\/ol>\n<p>It results in the following error message:<\/p>\n<blockquote>\n<p>Unable to start training due to the following error: Unable to parse\n`training_pipeline.training_task_inputs` into custom task `inputs`\ndefined in the file:\ngs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml<\/p>\n<\/blockquote>\n<p>This error message does not provide enough information as to exactly what is the issue, and was wondering if anyone else know of a solution for this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-31 15:02:34.07 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_date":"2018-01-26 07:18:10.47 UTC",
        "Owner_last_access_date":"2022-09-21 19:53:34.45 UTC",
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-31 17:41:08.82 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"batch predictions in GCP Vertex AI",
        "Question_body":"<p>While trying out batch predictions in GCP Vertex AI for an AutoML model, the batch prediction results span over several files(which is not convenient from a user perspective). If it would have been a single batch prediction result file i.e. covering all the records in a single file, it would make the procedure much more simple.<\/p>\n<p>For instance, I had 5585 records in my input dataset file. The batch prediction results comprise of 21 files wherein each file has records in the range of 200-300, thus, covering 5585 records altogether.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-23 15:58:50.777 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1708,
        "Owner_creation_date":"2017-04-14 13:36:11.557 UTC",
        "Owner_last_access_date":"2022-07-08 13:50:03.94 UTC",
        "Owner_location":"India",
        "Owner_reputation":105,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Batch predictions on an image, text,video,tabular AutoML model, runs the jobs using distributed processing which means the data is distributed among an arbitrary cluster of virtual machines and is processed in an unpredictable order because of which you will get the prediction results stored across various files in Cloud Storage. Since the batch prediction output files are not generated with the same order as an input file, a feature request has been raised and you can track the update on this request from this <a href=\"https:\/\/issuetracker.google.com\/issues\/202080076\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>We cannot provide an ETA at this moment but you can follow the progress in the issue tracker and you can \u2018STAR\u2019 the issue to receive automatic updates and give it traction by referring to this <a href=\"https:\/\/developers.google.com\/issue-tracker\/guides\/subscribe#starring_an_issue\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>However, if you are doing batch prediction for a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#batch_request_input\" rel=\"nofollow noreferrer\">tabular AutoML model<\/a>, there you have the option to choose the BigQuery as storage where all the prediction output will be stored in a single table and then you can export the table data to a single CSV file.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-24 14:18:00.55 UTC",
        "Answer_last_edit_date":"2021-10-24 16:20:57.4 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2021-10-25 19:54:43.16 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Question_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-25 23:09:26.993 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_date":"2019-07-13 14:14:21.237 UTC",
        "Owner_last_access_date":"2022-09-24 22:32:14.17 UTC",
        "Owner_location":null,
        "Owner_reputation":315,
        "Owner_up_votes":44,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-27 03:09:01.597 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-22 18:04:35.697 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-vertex-ai",
        "Question_view_count":319,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-23 06:05:01.513 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-11-23 16:51:56.647 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Why does my dbt container hang in Vertex AI?",
        "Question_body":"<p>I am trying to follow <a href=\"https:\/\/datatonic.com\/insights\/dbt-vertex-ai-pipelines-google-cloud\/\" rel=\"nofollow noreferrer\">this<\/a> tutorial to run a dbt docker image as a Vertex AI component. When the pipeline runs the component just seems to sit there for ever. Is there any way of debugging the component?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-06 09:30:39.053 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|dbt|google-cloud-vertex-ai|kfp",
        "Question_view_count":73,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>You see the pipeline logs to get an idea regarding what is going on in the pipeline.\nFrom the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/logging\" rel=\"nofollow noreferrer\">doc<\/a><\/p>\n<blockquote>\n<p>After you define and build a pipeline, you can use Cloud Logging to create log entries to help you monitor events such as pipeline failures. You can create custom log-based metrics that send notifications when the rate of pipeline failures reaches a given threshold.<\/p>\n<\/blockquote>\n<p>You can also select a component inside Pipeline's runtime graph and then view detailed info and logs of that particular component.\n<a href=\"https:\/\/i.stack.imgur.com\/QXm02.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QXm02.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also you can hover your cursor on the component status area(green check or grey disabled icon) to check the current status of that component.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-07 06:26:36.14 UTC",
        "Answer_last_edit_date":"2022-07-07 06:31:56.073 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"I am not able to create a feature store in vertexAI using labels",
        "Question_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-03 22:20:13.553 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":83,
        "Owner_creation_date":"2018-07-01 14:59:34.833 UTC",
        "Owner_last_access_date":"2022-09-24 15:38:47.317 UTC",
        "Owner_location":null,
        "Owner_reputation":1043,
        "Owner_up_votes":17,
        "Owner_down_votes":6,
        "Owner_views":212,
        "Answer_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-05-05 00:16:53.3 UTC",
        "Answer_last_edit_date":"2022-05-05 00:25:05.26 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-05-04 00:16:51.107 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Cannot use tensorboard with Vertex AI Custom job",
        "Question_body":"<p>I'm trying to launch a custom training job using Vertex AI through <a href=\"https:\/\/github.com\/deepmind\/xmanager\" rel=\"nofollow noreferrer\">XManager<\/a>. When running Custom jobs with tensorboard enabled I get a tensorboard instance in <code>experiments -&gt; tensorboard instances<\/code> and a button on the custom job page that says <code>OPEN TENSORBOARD<\/code>. However, this leads to an empty page that says <code>Not found: TensorboardExperiment<\/code>.<\/p>\n<ul>\n<li>I observed this behaviour when running my own custom job and when running XManager's example <a href=\"https:\/\/github.com\/deepmind\/xmanager\/tree\/main\/examples\/cifar10_tensorflow\" rel=\"nofollow noreferrer\">cifar10_tensorflow<\/a>. Note that in both cases the job runs to completion without problems.<\/li>\n<li>I can visualise the logs locally via the standard tensorboard package and passing as <code>log_dir<\/code> the cloud storage directory containing the experiments logs.<\/li>\n<li>I can upload experiment logs to Vertex AI tensorboard manually using<\/li>\n<\/ul>\n<pre><code>tb-gcp-uploader --tensorboard_resource_name \\\n  TENSORBOARD_INSTANCE_NAME \\\n  --logdir=LOG_DIR \\\n  --experiment_name=TB_EXPERIMENT_NAME --one_shot=True\n<\/code><\/pre>\n<ul>\n<li>For more details check out the discussion: <a href=\"https:\/\/github.com\/deepmind\/xmanager\/issues\/15\" rel=\"nofollow noreferrer\">https:\/\/github.com\/deepmind\/xmanager\/issues\/15<\/a><\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-05 17:04:41.173 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_date":"2016-01-04 14:55:18.69 UTC",
        "Owner_last_access_date":"2022-08-25 11:51:00.77 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-05 17:39:30.237 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Use a model trained by Google Cloud Vertex AI accelerated with TRT on Jetson Nano",
        "Question_body":"<p>I am trying to standardize our deployment workflow for machine vision systems. So we were thinking of the following workflow.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lZiQs.png\" rel=\"nofollow noreferrer\">Deployment workflow<\/a><\/p>\n<p>So, we want to create the prototype for the same, so we followed the workflow. So, there is no problem with GCP operation whatsoever but when we try to export models, which we train on the <code>vertexAI<\/code> it will give three models as mentioned in the workflow which is:<\/p>\n<ol>\n<li>SaveModel<\/li>\n<li>TFLite<\/li>\n<li>TFJS<\/li>\n<\/ol>\n<p>and we try these models to convert into the ONNX model but we failed due to different errors.<\/p>\n<ol>\n<li>SaveModel - Always getting the same error with any parameter which is as follows\n<a href=\"https:\/\/i.stack.imgur.com\/HsBoa.png\" rel=\"nofollow noreferrer\">Error in savemodel<\/a>\nI tried to track the error and I identified that the model is not loading inside the TensorFlow only which is wired since it is exported from the <code>GCP vertexAI<\/code> which leverages the power of TensorFlow.<\/li>\n<li>TFLite - Successfully converted but again the problem with the <code>opset<\/code> of ONNX but with 15 <code>opset<\/code> it gets successfully converted but then NVIDIA tensorRT ONNXparser doesn't recognize the model during ONNX to TRT conversion.<\/li>\n<li>TFJS - yet not tried.<\/li>\n<\/ol>\n<p>So we are blocked here due to these problems.<\/p>\n<p>We can run these models exported directly from the <code>vertexAI<\/code> on the Jetson Nano device but the problem is <code>TF-TRT<\/code> and TensorFlow is not memory-optimized on the GPU so the system gets frozen after 3 to 4 hours of running.<\/p>\n<p>We try this workflow with google teachable machine once and it workout well all steps are working perfectly fine so I am really confused How I conclude this full workflow since it's working on a teachable machine which is created by Google and not working on vertexAI model which is again developed by same Company.<\/p>\n<p>Or am I doing Something wrong in this workflow?\nFor the background we are developing this workflow inside C++ framework for the realtime application in industrial environment.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-06 12:16:25.767 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|onnx|nvidia-jetson-nano|google-cloud-vertex-ai",
        "Question_view_count":189,
        "Owner_creation_date":"2022-01-06 11:21:46.99 UTC",
        "Owner_last_access_date":"2022-09-23 06:29:45.187 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-12 04:39:09.42 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"google's notebook on vertex ai throwing following error: type name google.VertexModel is different from expected: Model",
        "Question_body":"<p>I got this error, when compiling my pipeline:<\/p>\n<blockquote>\n<p>type name google.VertexModel is different from expected: Model<\/p>\n<\/blockquote>\n<p>when running the following notebook by google: <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/pipelines\/automl_tabular_classification_beans.ipynb\" rel=\"nofollow noreferrer\">automl_tabular_classification_beans<\/a><\/p>\n<p>I suppose that kubeflow v2 is not able to handle (yet) google.vertexmodel as type for component input. However, I've been browsing a bit and did not find any good clue, or refs (kfp documentation for v2 is not up to date..) to solve this issue. Hopefully someone here can give me a good pointer? I look forward to all of your ideas.<\/p>\n<p>Cheers<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-03 10:34:49.273 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|kfp",
        "Question_view_count":527,
        "Owner_creation_date":"2021-11-03 10:25:15.277 UTC",
        "Owner_last_access_date":"2022-09-22 11:34:07.727 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-03 10:42:44.13 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Annotation specs - AutoML (VertexAi)",
        "Question_body":"<p>We're trying to build an imaged based product search for our webshop using the vertex ai image classification model (single label).<\/p>\n<p>Currently we have around 20k products with xx images per product.\nSo our dataset containing 20k of labels (one for each product - product number), but on import we receive the following error message:<\/p>\n<p><code>There are too many AnnotationSpecs in the dataset. Up to 5000 AnnotationSpecs are allowed in one Dataset. Check your csv\/jsonl format with our public documentation.<\/code><\/p>\n<p>Looks like not more than 5000 labels are allowed per Dataset... This quota is not really visible in the documentation - or we didn't find it.<\/p>\n<p>Anyway, any ideas how we can make it work? Does we have to build 5 Datasets with 5 different Endpoints and than query every Enpoint for matching?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-20 07:39:36.407 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":38,
        "Owner_creation_date":"2022-07-20 07:24:37.233 UTC",
        "Owner_last_access_date":"2022-09-19 15:58:45.803 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How does Google Vertex AI Matching engine deny list work?",
        "Question_body":"<p>How does <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/filtering#denylist\" rel=\"nofollow noreferrer\">Vertex matching engine deny list<\/a> work?<\/p>\n<p>Let's say I have a class fruit which will ONLY have deny list tokens (no allow) such as &quot;apple&quot;, &quot;mango&quot;, etc. How do I filter out &quot;mango&quot; in the query (search all fruits except mango)? I have tried the following method but it does not work as expected:<\/p>\n<p>json:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{&quot;id&quot;: &quot;1&quot;, &quot;embedding&quot;:[0.002792,0.000492], &quot;restricts&quot;: [{&quot;namespace&quot;: &quot;fruit&quot;, &quot;deny&quot;: [&quot;mango&quot;]}]}\n<\/code><\/pre>\n<p>code to query:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>deny_namespace = match_service_pb2.Namespace()\ndeny_namespace.name = &quot;fruit&quot;\ndeny_namespace.deny_tokens.append(&quot;mango&quot;)\nrequest.restricts.append(deny_namespace)\n<\/code><\/pre>\n<p>I have coded this similar to allow list which has worked for me but with deny tokens it does not seem to skip deny tokens even after completely overwriting the index.<\/p>\n<p>Barely changing the field &quot;deny&quot; to &quot;allow&quot; works but &quot;deny&quot; fails to work as expected (it does not throw any error though).<\/p>\n<p>Full code<\/p>\n<p>query<\/p>\n<p><a href=\"https:\/\/gist.github.com\/niladridutt\/673d4aa2a6225fa47d8aad7398b4cbd1\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/673d4aa2a6225fa47d8aad7398b4cbd1<\/a><\/p>\n<p>Proto files-<\/p>\n<p><a href=\"https:\/\/gist.github.com\/niladridutt\/746833b8d61ec366c8c61de57c784ac4\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/746833b8d61ec366c8c61de57c784ac4<\/a>\n<a href=\"https:\/\/gist.github.com\/niladridutt\/31e9dc3432e206589729989acddf1225\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/niladridutt\/31e9dc3432e206589729989acddf1225<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-01 07:41:20.61 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_date":"2022-09-01 07:36:06.697 UTC",
        "Owner_last_access_date":"2022-09-23 12:07:06.403 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-01 10:37:31.937 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Can the list of custom jobs in vertex AI custom seen in the UI?",
        "Question_body":"<p>I have created a custom job with<\/p>\n<pre><code>gcloud ai custom-jobs create --region=us-west1 --display-name=test-job --config=trainjob.yaml\n<\/code><\/pre>\n<p>where <code>trainjob.yaml<\/code> is<\/p>\n<pre><code>workerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-4\n  replicaCount: 1\n  containerSpec:\n    imageUri: eu.gcr.io\/myproject\/myimage\n<\/code><\/pre>\n<p>I can see the list of the job via<\/p>\n<pre><code>gcloud ai custom-jobs list --region=us-west1\n<\/code><\/pre>\n<p>. Can this list seen in the UI? For AI Platform product there is jobs but I don't see anything like this in Vertex AI<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 15:50:55.003 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|google-ai-platform|gcp-ai-platform-training",
        "Question_view_count":68,
        "Owner_creation_date":"2017-01-27 15:07:04.88 UTC",
        "Owner_last_access_date":"2022-09-22 15:19:15.02 UTC",
        "Owner_location":null,
        "Owner_reputation":1991,
        "Owner_up_votes":468,
        "Owner_down_votes":27,
        "Owner_views":107,
        "Answer_body":"<p>I don't know if it is exactly what you are looking for, but you can see the custom training jobs details using the UI at <code>Console<\/code> &gt; <code>Vertex AI<\/code> &gt; <code>Training<\/code> &gt; <code>Custom Jobs<\/code> or following the next <a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/training\/custom-jobs\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-18 14:27:13.09 UTC",
        "Answer_last_edit_date":"2022-07-18 16:05:12.117 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI - No module named 'google_cloud_pipeline_components.remote on ModelDeployOp(...)",
        "Question_body":"<p>I have created a simple pipeline that trains a model and deploys it to a Vertex AI endpoint. I have noticed that while attempting to deploy the model using the  <code>google_cloud_pipeline_components.aiplatform.ModelDeployOp()<\/code> component, it returns an error.<\/p>\n<p>If we look at the documentation of <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.5\/google_cloud_pipeline_components.aiplatform.html\" rel=\"nofollow noreferrer\">google_cloud_pipeline_components.aiplatform<\/a>, we can find two entries for ModelDeployOp(). One illustrates how they <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.5\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.utils.convert_method_to_component\" rel=\"nofollow noreferrer\">converted the original Methods into components<\/a>, the other is the documentation on <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.5\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelDeployOp\" rel=\"nofollow noreferrer\">how to use the ModelDeployOp()<\/a>method.<\/p>\n<p>If we look at how they convert the Method, we find the following information:\n...<\/p>\n<pre><code>Generates and invokes the following Component:\nname: Model-deploy inputs: - {name: project, type: String} - {name: endpoint, type: Artifact} - {name: model, type: Model} outputs: - {name: endpoint, type: Artifact} implementation:\n\ncontainer:\nimage: gcr.io\/sashaproject-1\/mb_sdk_component:latest command: - python3 - remote_runner.py - \u2013cls_name=Model - \u2013method_name=deploy - \u2013method.deployed_model_display_name=my-deployed-model - \u2013method.machine_type=n1-standard-4 args: - \u2013resource_name_output_artifact_path - {outputPath: endpoint} - \u2013init.project - {inputValue: project} - \u2013method.endpoint - {inputPath: endpoint} - \u2013init.model_name - {inputPath: model}\n<\/code><\/pre>\n<p>While looking at my error that gcp logging retruned:<\/p>\n<pre><code>\/usr\/local\/bin\/python3: Error while finding module specification for 'google_cloud_pipeline_components.remote.aiplatform.remote_runner' (ModuleNotFoundError: No module named 'google_cloud_pipeline_components.remote')\n<\/code><\/pre>\n<p>It seems that its an issue from inside the container itself.<\/p>\n<p>So... I suppose my question is if I am correct in assuming this is a bug in the library? Are there any workarounds?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-21 09:21:22.48 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":821,
        "Owner_creation_date":"2021-10-21 08:47:59.07 UTC",
        "Owner_last_access_date":"2022-08-03 13:58:55.67 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI 504 Errors in batch job - How to fix\/troubleshoot",
        "Question_body":"<p>We have a Vertex AI model that takes a relatively long time to return a prediction.<\/p>\n<p>When hitting the model endpoint with one instance, things work fine.  But batch jobs of size say 1000 instances end up with around 150 504 errors (upstream request timeout). (We actually need to send batches of 65K but I'm troubleshooting with 1000).<\/p>\n<p>I tried increasing the number of replicas assuming that the # of instances handed to the model would be (1000\/# of replicas) but that doesn't seem to be the case.<\/p>\n<p>I then read that the default batch size is 64 and so tried decreasing the batch size to 4 like this from the python code that creates the batch job:<\/p>\n<p>model_parameters = dict(batch_size=4)<\/p>\n<pre><code>def run_batch_prediction_job(vertex_config):\n\n    aiplatform.init(\n        project=vertex_config.vertex_project, location=vertex_config.location\n    )\n\n    model = aiplatform.Model(vertex_config.model_resource_name)\n\n    model_params = dict(batch_size=4)\n    batch_params = dict(\n        job_display_name=vertex_config.job_display_name,\n        gcs_source=vertex_config.gcs_source,\n        gcs_destination_prefix=vertex_config.gcs_destination,\n        machine_type=vertex_config.machine_type,\n        accelerator_count=vertex_config.accelerator_count,\n        accelerator_type=vertex_config.accelerator_type,\n        starting_replica_count=replica_count,\n        max_replica_count=replica_count,\n        sync=vertex_config.sync,\n        model_parameters=model_params\n    )\n\n    batch_prediction_job = model.batch_predict(**batch_params)\n\n    batch_prediction_job.wait()\n\n    return batch_prediction_job\n<\/code><\/pre>\n<p>I've also tried increasing the machine type to n1-high-cpu-16 and that helped somewhat but I'm not sure I understand how batches are sent to replicas?<\/p>\n<p>Is there another way to decrease the number of instances sent to the model?\nOr is there a way to increase the timeout?\nIs there log output I can use to help figure this out?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-22 01:15:43.853 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":194,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP Vertex AI Workbench custom image - persistence with gcs don't work",
        "Question_body":"<p>While creating a workbench with a custom jupyter image I choose backup\/ persistence with GCS (google cloud storage) and it doesn't work.\nFor now, I tried to test with:<\/p>\n<ul>\n<li><code>jupyter\/base-notebook:python-3.8.8<\/code> <a href=\"https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/base-v2\/Dockerfile\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/base-v2\/Dockerfile<\/a><\/li>\n<li><code>gcr.io\/deeplearning-platform-release\/base-cpu:latest<\/code> <a href=\"https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/vertex\/Dockerfile\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Santhin\/jupyter-images\/blob\/main\/vertex\/Dockerfile<\/a><\/li>\n<\/ul>\n<p>I thought it was a problem with permissions on the folder named <code>jupyter<\/code> which is created always while creating a workbench on every jupyter image (custom \/ pre-built by gcp).\nIs there any method to accomplish persistence with gcs with a custom docker image?\n<a href=\"https:\/\/i.stack.imgur.com\/srXbE.png\" rel=\"nofollow noreferrer\">Screen showing option with backup on gcs<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-06 07:49:45.307 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|docker-image|google-cloud-vertex-ai|gcs",
        "Question_view_count":249,
        "Owner_creation_date":"2018-09-17 15:05:44.723 UTC",
        "Owner_last_access_date":"2022-09-15 15:25:15.08 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-22 20:32:35.95 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":128,
        "Owner_creation_date":"2016-05-05 17:32:36.837 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583 UTC",
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-23 02:54:51.847 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to invoke custom prediction logic in Vertex AI?",
        "Question_body":"<p>Goal: serve prediction requests from a Vertex AI Endpoint by executing custom prediction logic.<\/p>\n<p>Detailed steps:\nFor example, we may already uploaded have an image_quality.pb model (developed in a non-vertex-ai pythonic environment) in a GCS bucket<\/p>\n<p>Next, we want to create a custom image inference logic by deserializing the deployed  model and serving the inference functionality in a vertex AI endpoint<\/p>\n<p>Finally, we want to pass a list of images (stored in another GCS bucket) to that endpoint.<\/p>\n<p>We also want to see the logs and metrics in tensorboard.<\/p>\n<p>Existing Vertex AI code samples provide examples for invoking model.batch_predict \/ endpoint. predict, but don't mention how to execute custom prediction code.<\/p>\n<p>It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-25 05:13:57.68 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":133,
        "Owner_creation_date":"2015-02-08 00:42:09.317 UTC",
        "Owner_last_access_date":"2021-11-29 04:25:35.947 UTC",
        "Owner_location":null,
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-26 05:56:41.977 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Batch predictions Vertext AI",
        "Question_body":"<p>How do I create <code>JSONL<\/code> file which contains list of files in Google Cloud Bucket for Batch prediction in Vertex AI?\nWhat I've tried so far.<\/p>\n<ol>\n<li>Get list of file from bucket and write it to a txt file\n<code>gsutil ls gs:\/\/bucket\/dir &gt; list.txt<\/code><\/li>\n<li>Convert <code>list.txt<\/code> to <code>list.jsonl<\/code> following <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions?_ga=2.136818160.-1986884001.1629344951&amp;_gac=1.159283784.1630027944.Cj0KCQiAv8PyBRDMARIsAFo4wK2vd8OO5X9HFkii9knl6nSzFOK_srkD484THsck1a6r5rqonXafFkkaAgBgEALw_wcB#batch_request_input\" rel=\"nofollow noreferrer\">Vertext AI docs<\/a>:<\/li>\n<\/ol>\n<pre><code>{&quot;content&quot;: &quot;gs:\/\/sourcebucket\/datasets\/images\/source_image1.jpg&quot;, &quot;mimeType&quot;: &quot;image\/jpeg&quot;}\n{&quot;content&quot;: &quot;gs:\/\/sourcebucket\/datasets\/images\/source_image2.jpg&quot;, &quot;mimeType&quot;: &quot;image\/jpeg&quot;}\n<\/code><\/pre>\n<p>After create batch prediction, I got this error: <code>cannot be parsed as JSONL.<\/code>\nHow do I correct the format of this <code>JSONL<\/code> file?\nAlso, is there anyway to directly export list files in bucket to <code>JSONL<\/code> file format?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-14 09:01:41.267 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|gsutil|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_date":"2021-06-17 10:11:32.18 UTC",
        "Owner_last_access_date":"2021-10-16 12:25:40.05 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-14 09:12:49.777 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Model evaluation predictions from VertexAI AutoMLTabularTrainingJob",
        "Question_body":"<p>I am following python api documentation to create a AutoMLTabularTrainingJob : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular<\/a>. It trains successfully with a dataset that consists of train\/valid\/test splits. I can also get the model evaluation metrics after following this documentation : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models<\/a><\/p>\n<p>However, I am unable to find a way to get the raw predictions that are used to generate the model evaluation metrics. Any pointers would be much appreciated !<\/p>\n<p>Do I have to request for batch predictions again ? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2021-08-11 06:57:16.657 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":53,
        "Owner_creation_date":"2011-06-19 17:47:04.643 UTC",
        "Owner_last_access_date":"2022-09-23 00:45:08.543 UTC",
        "Owner_location":null,
        "Owner_reputation":1876,
        "Owner_up_votes":165,
        "Owner_down_votes":2,
        "Owner_views":201,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-11 07:23:38.203 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI quota limit error when current usage percentage is zero",
        "Question_body":"<p>In the context of a personal project, Im trying to use Vertex AI to run a TFX pipeline to train a model using custom training, based on <a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/tfx\/gcp\/vertex_pipelines_vertex_training\" rel=\"nofollow noreferrer\">this guide<\/a>. When I run the pipeline I get the error:<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus<\/p>\n<\/blockquote>\n<p>On the IAM quotas I have limit &quot;1&quot; for the resource &quot;Custom model training CPUs for N1\/E2 machine types per region&quot;, for all regions, and 0% current usage for each one of them. I even tried multiple regions and multiple types of machines (n1, e2, ...) and I alway get that quota limit error.<\/p>\n<p>Can anyone explain why Im getting this quota error?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-19 16:52:14.727 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":28,
        "Owner_creation_date":"2016-08-09 10:03:39.32 UTC",
        "Owner_last_access_date":"2022-09-24 14:10:31.477 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":223,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex Ai issue when deploying a model using Java",
        "Question_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2021-12-25 04:28:22.527 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":244,
        "Owner_creation_date":"2013-05-29 00:23:37.73 UTC",
        "Owner_last_access_date":"2022-09-23 15:05:41.287 UTC",
        "Owner_location":"Atlanta, Georgia",
        "Owner_reputation":55,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-29 23:30:03.63 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-10 06:58:21.177 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_date":"2020-01-23 17:50:31.103 UTC",
        "Owner_last_access_date":"2022-09-15 02:08:52.26 UTC",
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-10 10:36:28.04 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-01-11 10:27:33.597 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"`systemd` error in Workbench instances created using custom container",
        "Question_body":"<p>I am creating a Vertex AI Workbench instance from a custom container. Below is my <code>Dockerfile<\/code>:<\/p>\n<pre><code>FROM gcr.io\/deeplearning-platform-release\/base-cu113\nRUN pip install ml_collections\n<\/code><\/pre>\n<p>Note that I have intentionally kept the Dockerfile short here because it will be enough to reproduce the issue.<\/p>\n<p>The Workbench instance gets created but when I run <code>sudo shutdown now<\/code> from a terminal within the instance, it leads to:<\/p>\n<pre><code>System has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nFailed to talk to init daemon.\n<\/code><\/pre>\n<p>Anything to mitigate this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-20 05:44:05.91 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":80,
        "Owner_creation_date":"2017-02-28 15:43:38.107 UTC",
        "Owner_last_access_date":"2022-09-20 03:33:53.823 UTC",
        "Owner_location":null,
        "Owner_reputation":326,
        "Owner_up_votes":153,
        "Owner_down_votes":1,
        "Owner_views":231,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Run a Vertex AI two tower model locally",
        "Question_body":"<p>I have successfully trained a Two Tower model on Google Vertex AI as per the guide here.<\/p>\n<p>I now would like to download the model and try some inference locally on my own machine, I have been battling with various errors for a while and now am stuck at the following:<\/p>\n<p>Code:<\/p>\n<pre><code>import tensorflow as tf\nimport tensorflow_text\n\n\nload_options = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\ntf.saved_model.load('model_path', options=load_options)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/ops.py in _get_op_def(self, type)\n   3957     try:\n-&gt; 3958       return self._op_def_cache[type]\n   3959     except KeyError:\n\nKeyError: 'IO&gt;DecodeJSON'\n\nDuring handling of the above exception, another exception occurred:\n\nNotFoundError                             Traceback (most recent call last)\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/saved_model\/load.py in load_internal(export_dir, tags, options, loader_cls, filters)\n    905         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n--&gt; 906                             ckpt_options, filters)\n    907       except errors.NotFoundError as err:\n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/saved_model\/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\n    133         function_deserialization.load_function_def_library(\n--&gt; 134             meta_graph.graph_def.library))\n    135     self._checkpoint_options = ckpt_options\n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/saved_model\/function_deserialization.py in load_function_def_library(library, load_shared_name_suffix)\n    357     with graph.as_default():\n--&gt; 358       func_graph = function_def_lib.function_def_to_graph(copy)\n    359     _restore_gradient_functions(func_graph, renamed_functions)\n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/function_def_to_graph.py in function_def_to_graph(fdef, input_shapes)\n     63   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(\n---&gt; 64       fdef, input_shapes)\n     65 \n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/function_def_to_graph.py in function_def_to_graph_def(fdef, input_shapes)\n    227     else:\n--&gt; 228       op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\n    229 \n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/ops.py in _get_op_def(self, type)\n   3962         pywrap_tf_session.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type),\n-&gt; 3963                                            buf)\n   3964         # pylint: enable=protected-access\n\nNotFoundError: Op type not registered 'IO&gt;DecodeJSON' in binary running on 192.168.1.105. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n\nDuring handling of the above exception, another exception occurred:\n\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-2-39fe5910a28b&gt; in &lt;module&gt;\n      5 \n      6 load_options = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n----&gt; 7 tf.saved_model.load('query_model\/20220219125209', options=load_options)\n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/saved_model\/load.py in load(export_dir, tags, options)\n    867     ValueError: If `tags` don't match a MetaGraph in the SavedModel.\n    868   &quot;&quot;&quot;\n--&gt; 869   return load_internal(export_dir, tags, options)[&quot;root&quot;]\n    870 \n    871 \n\n~\/.pyenv\/versions\/3.7.8\/lib\/python3.7\/site-packages\/tensorflow\/python\/saved_model\/load.py in load_internal(export_dir, tags, options, loader_cls, filters)\n    907       except errors.NotFoundError as err:\n    908         raise FileNotFoundError(\n--&gt; 909             str(err) + &quot;\\n If trying to load on a different device from the &quot;\n    910             &quot;computational device, consider using setting the &quot;\n    911             &quot;`experimental_io_device` option on tf.saved_model.LoadOptions &quot;\n\nFileNotFoundError: Op type not registered 'IO&gt;DecodeJSON' in binary running on 192.168.1.105. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '\/job:localhost'.\n<\/code><\/pre>\n<p>The issue seems to be the fact that the model was trained user <code>albert-base<\/code> and there are some extra ops and packages needed for it to run, that is why I <code>import tensorflow_text<\/code> I have also tried to <code>import tensorflow_io<\/code> but I receive an error just trying to load the package, stating an S3 filesystem has already been registered.<\/p>\n<p>Any help would be greatly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-02-19 13:48:47.237 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":255,
        "Owner_creation_date":"2015-05-13 15:21:55.873 UTC",
        "Owner_last_access_date":"2022-09-22 08:59:46.38 UTC",
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":2763,
        "Owner_up_votes":203,
        "Owner_down_votes":35,
        "Owner_views":264,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Running custom Docker container with GPU using Vertex AI Pipelines - how to install NVIDIA driver?",
        "Question_body":"<p>I need to run a custom Docker container with GPU support using Vertex AI Pipelines, and I'm not seeing a clear way to do that. This requires several components:<\/p>\n<ol>\n<li>Applications (pytorch in my case)<\/li>\n<li>CUDA toolkit<\/li>\n<li>CUDA driver<\/li>\n<li>NVIDIA GPUs<\/li>\n<\/ol>\n<p>I can use a <a href=\"https:\/\/github.com\/NVIDIA\/nvidia-docker\" rel=\"nofollow noreferrer\">NVIDIA Docker<\/a> base image for #1 and #2, and a GCP accelerator for #4, but how do I install the CUDA driver in a Vertex AI pipeline? There's documentation on how to install NVIDIA drivers <a href=\"https:\/\/cloud.google.com\/compute\/docs\/gpus\/install-drivers-gpu#no-secure-boot\" rel=\"nofollow noreferrer\">on GCE instances<\/a> and <a href=\"https:\/\/cloud.google.com\/kubernetes-engine\/docs\/how-to\/gpus#installing_drivers\" rel=\"nofollow noreferrer\">GKE nodes<\/a>, but nothing for Vertex AI.<\/p>\n<p>One option could be to <a href=\"https:\/\/cloud.google.com\/deep-learning-containers\/docs\/derivative-container\" rel=\"nofollow noreferrer\">create a derivative container based on a GCP Deep Learning Container<\/a>, but then I have to use a GCP container and don't have as much control over the environment.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-07 23:22:40.08 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"nvidia|google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_date":"2013-12-10 22:23:13.85 UTC",
        "Owner_last_access_date":"2022-09-21 23:19:21.323 UTC",
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Question_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-21 08:27:28.29 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":330,
        "Owner_creation_date":"2021-03-24 12:34:53.617 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103 UTC",
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-11-09 07:41:34.643 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-10-26 10:54:23.99 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI batch prediction location",
        "Question_body":"<p>When I initiate a batch prediction job on Vertex AI of google cloud, I have to specify a cloud storage bucket location. Suppose I provided the bucket location, <code>'my_bucket\/prediction\/'<\/code>, then the prediction files are stored in something like: <code>gs:\/\/my_bucket\/prediction\/prediction-test_model-2022_01_17T01_46_39_898Z<\/code>, which is a subdirectory within the bucket location I provided. The prediction files are stored within that subdirectory and are named:<\/p>\n<pre><code>prediction.results-00000-of-00002\nprediction.results-00001-of-00002\n<\/code><\/pre>\n<p>Is there any way to programmatically get the final export location from the batch prediction name, id or any other parameter as shown below in the details of the batch prediction job?\n<a href=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-20 07:23:48.603 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":554,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI model showing failure but 0 bytes in prediction.errors",
        "Question_body":"<p>I'm running a vertex batch job on a custom model with 676 instances in my jsonl.<\/p>\n<p>I get results for all 676 instances but the job shows &quot;Due to one or more errors, this batch prediction job was canceled on Aug 22, 2022 at 09:04PM&quot;<\/p>\n<p>The error file prediction.errors_stats-00000-of-00001 has 0 bytes and there is a directory:<\/p>\n<pre><code>beam-temp-prediction.errors_stats-e289aa8c228c11eda06642010a800fdc \n<\/code><\/pre>\n<p>with 4 files with 0 bytes\u00a0in them.<\/p>\n<pre><code>34e19e3d-e717-4c6f-860c-5c2a177f1e93.prediction.errors_stats\n7c9374c8-65f7-4a97-8a95-6068ba9cd936.prediction.errors_stats\n981a634b-d27c-4285-b871-648b25ca87b8.prediction.errors_stats\nf14dbc29-42d7-480e-8b79-95e799e132bf.prediction.errors_stats\n<\/code><\/pre>\n<p>Any ideas on why the job is showing failure but the results seem ok?<\/p>\n<p>Job params:<\/p>\n<pre><code>{&quot;displayName&quot;: &lt;MY_DISPLAY_NAME&gt;,\n &quot;model&quot;: &lt;MY_MODEL&gt;,\n &quot;inputConfig&quot;: {&quot;instancesFormat&quot;: &quot;jsonl&quot;,\n                 &quot;gcsSource&quot;: {&quot;uris&quot;: [&quot;gs:\/\/&lt;MY_BUCKET&gt;\/MY_INSTANCES.jsonl&quot;]}},\n &quot;outputConfig&quot;: {&quot;predictionsFormat&quot;: &quot;jsonl&quot;,\n                  &quot;gcsDestination&quot;: {&quot;outputUriPrefix&quot;: &quot;gs:\/\/&lt;MY_OUTPUTS&gt;\/outputs\/2022-08-23&quot;}},\n&quot;dedicatedResources&quot;: {&quot;machineSpec&quot;: {&quot;machineType&quot;: &quot;n1-standard-8&quot;, \n&quot;acceleratorType&quot;: null, &quot;acceleratorCount&quot;: null},\n&quot;startingReplicaCount&quot;: 2},\n&quot;manualBatchTuningParameters&quot;: {&quot;batch_size&quot;: 1}}\n<\/code><\/pre>\n<p>Note: I've also tried leaving out the 'startingReplicaCount and manualBatchTuningParameters.<\/p>\n<p>I'm using a curl command to initiate the batch job:<\/p>\n<pre><code>    curl -X POST \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n-H &quot;Content-Type: application\/json; charset=utf-8&quot; \\\n-d @$INPUT_JSON \\\n&quot;https:\/\/$LOCATION-aiplatform.googleapis.com\/v1\/projects\/$PROJECT\/locations\/$LOCATION\/batchPredictionJobs&quot;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-23 18:14:26.123 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":42,
        "Owner_creation_date":"2016-05-07 00:35:30.17 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807 UTC",
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-23 18:23:57.73 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Simplest GUI frontend to demo GCP Vertex AutomML Image Classification",
        "Question_body":"<p>I've built a GCP Vertex AutoML Image Classification model and deployed to endpoint. It works great from the Deploy and Test tab. What's the simplest way to let others without access to the project try it via a GUI? The required functionality is to upload an image from your computer and let the model output the predicted class.<\/p>\n<p>Is there an existing tool I can use (has to be a GUI, not command line)? If not what's the simplest way to build such frontend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-08 18:49:05.48 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"vertex|google-cloud-automl|automl|google-cloud-vertex-ai|gpc",
        "Question_view_count":79,
        "Owner_creation_date":"2016-01-09 01:34:07.89 UTC",
        "Owner_last_access_date":"2022-02-26 19:24:23.073 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-03 18:13:40.147 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_date":"2018-03-01 19:41:46.78 UTC",
        "Owner_last_access_date":"2022-09-25 03:26:23.16 UTC",
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-02-04 02:35:20.973 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How can I set hidden_units to a list in Vertex AI?",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/02-experimentation.ipynb\" rel=\"nofollow noreferrer\">this<\/a> notebook '02 ML Experimentation with Custom Model'.<\/p>\n<p>When i try <code>vertex_ai.log_params(hyperparams)<\/code>, I get:<\/p>\n<pre><code>TypeError: Value for key hidden_units is of type list but must be one of float, int, str\n<\/code><\/pre>\n<p>but the next step <code>classifier = trainer.train<\/code> needs <code>hidden_units<\/code> to be a list.<\/p>\n<p>(My version of <code>google-cloud-aiplatform<\/code> is <code>1.16.0<\/code>.)\nAny help is appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-03 10:05:25.98 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-vertex-ai|google-cloud-aiplatform",
        "Question_view_count":49,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"GCP vertex - a direct way to get deployed model ID",
        "Question_body":"<p>Is there a way to directly acquire the model ID from the <code>gcloud ai models upload<\/code> command?<\/p>\n<p>Either using JSON output or value output, need to manipulate by splitting and extracting. If there is a way to directly get the model ID without manipulation, please advise.<\/p>\n<pre><code>output = !gcloud ai models upload \\\n  --region=$REGION \\\n  --display-name=$JOB_NAME \\\n  --container-image-uri=us-docker.pkg.dev\/vertex-ai\/prediction\/tf2-cpu.2-8:latest \\\n  --artifact-uri=$GCS_URL_FOR_SAVED_MODEL \\\n  --format=&quot;value(model)&quot;\n\noutput\n-----\n['Using endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]',\n 'projects\/xxxxxxxx\/locations\/us-central1\/models\/1961937762277916672',\n 'Waiting for operation [8951184153827606528]...',\n '...................................done.']\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-10 05:09:55.347 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcloud|google-cloud-vertex-ai",
        "Question_view_count":431,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-11 06:01:13.717 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Question_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-12 14:15:36.247 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_date":"2012-10-25 08:48:34.717 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-13 05:46:49.75 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Batch prediction Input",
        "Question_body":"<p>I have a tensorflow model deployed on Vertex AI of Google Cloud. The model definition is:<\/p>\n<pre><code>item_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=item_vocab, mask_token=None),\n  tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dim)\n])\n\nuser_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=user_vocab, mask_token=None),\n  # We add an additional embedding to account for unknown tokens.\n  tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dim)\n])\n\n\nclass NCF_model(tf.keras.Model):\n    def __init__(self,user_model, item_model):\n        super(NCF_model, self).__init__()\n        # define all layers in init\n        \n        self.user_model = user_model\n        self.item_model  = item_model\n        self.concat_layer   = tf.keras.layers.Concatenate()\n        self.feed_forward_1 = tf.keras.layers.Dense(32,activation= 'relu')\n        self.feed_forward_2 = tf.keras.layers.Dense(64,activation= 'relu')\n        self.final = tf.keras.layers.Dense(1,activation= 'sigmoid')\n\n\n    def call(self, inputs ,training=False):\n        user_id , item_id = inputs[:,0], inputs[:,1]\n        x = self.user_model(user_id)\n        y = self.item_model(item_id)\n\n        x = self.concat_layer([x,y])\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = self.final(x)\n\n\n        return x\n<\/code><\/pre>\n<p>The model has two string inputs and it outputs a probability value.\nWhen I use the following input in the batch prediction file, I get an empty prediction file.\nSample of csv input file:<\/p>\n<pre><code>userid,itemid\nyuu,190767\nyuu,364\nyuu,154828\nyuu,72998\nyuu,130618\nyuu,183979\nyuu,588\n<\/code><\/pre>\n<p>When I use a jsonl file with the following input.<\/p>\n<pre><code>{&quot;input&quot;:[&quot;yuu&quot;, &quot;190767&quot;]}\n<\/code><\/pre>\n<p>I get the following error.<\/p>\n<pre><code>('Post request fails. Cannot get predictions. Error: Exceeded retries: Non-OK result 400 ({\\n    &quot;error&quot;: &quot;Failed to process element: 0 key: input of \\'instances\\' list. Error: INVALID_ARGUMENT: JSON object: does not have named input: input&quot;\\n}) from server, retry=3.', 1)\n<\/code><\/pre>\n<p>What seems to be going wrong with these inputs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-16 11:25:44.157 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":323,
        "Owner_creation_date":"2015-04-20 12:22:18.823 UTC",
        "Owner_last_access_date":"2022-09-24 14:16:22.667 UTC",
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Custom Model for Batch Prediction on Vertex.ai",
        "Question_body":"<p>I want to run batch predictions inside Google Cloud's vertex.ai using a custom trained model.  I was able to find documentation to get online prediction working with a custom built docker image by setting up an endpoint, but I can't seem to find any documentation on what the Dockerfile should be for batch prediction.  Specifically how does my custom code get fed the input and where does it put the output?<\/p>\n<p>The documentation I've found is <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions?_ga=2.262616524.-1738078585.1553812508&amp;_gac=1.83826788.1630620723.Cj0KCQjw7MGJBhD-ARIsAMZ0eesi3e2_fghoLZcWRKqw_ZbncT3LeZvgYPu929bJELdeiX3RSNHPApcaAo8dEALw_wcB#custom-trained_3\" rel=\"nofollow noreferrer\">here<\/a>, it certainly looks possible to use a custom model and when I tried it didn't complain, but eventually it did throw an error.  According to the documentation no endpoint is required for running batch jobs.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":10,
        "Question_creation_date":"2021-09-20 15:55:16.607 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":480,
        "Owner_creation_date":"2018-10-30 18:54:03.62 UTC",
        "Owner_last_access_date":"2022-09-23 17:00:36.977 UTC",
        "Owner_location":"Utah, USA",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI - Deployment failed",
        "Question_body":"<p>I'm trying to deploy my custom-trained model using a custom-container, i.e. create an endpoint from a model that I created.\nI'm doing the same thing with AI Platform (same model &amp; container) and it works fine there.<\/p>\n<p>At the first try I deployed the model successfully, but ever since whenever I try to create an endpoint it says &quot;deploying&quot; for 1+ hours and then it fails with the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at (link)\n<\/code><\/pre>\n<p>The log shows the following:<\/p>\n<pre><code>* Running on all addresses (0.0.0.0)\n WARNING: This is a development server. Do not use it in a production deployment.\n* Running on http:\/\/127.0.0.1:8080\n[05\/Jul\/2022 12:00:37] &quot;[33mGET \/v1\/endpoints\/1\/deployedModels\/2025850174177280000 HTTP\/1.1[0m&quot; 404 -\n[05\/Jul\/2022 12:00:38] &quot;[33mGET \/v1\/endpoints\/1\/deployedModels\/2025850174177280000 HTTP\/1.1[0m&quot; 404 -\n<\/code><\/pre>\n<p>Where the last line is being spammed until it ultimately fails.<\/p>\n<p>My flask app is as follows:<\/p>\n<pre><code>import base64\nimport os.path\nimport pickle\nfrom typing import Dict, Any\nfrom flask import Flask, request, jsonify\nfrom streamliner.models.general_model import GeneralModel\n\nclass Predictor:\n    def __init__(self, model: GeneralModel):\n        self._model = model\n\n    def predict(self, instance: str) -&gt; Dict[str, Any]:\n        decoded_pickle = base64.b64decode(instance)\n        features_df = pickle.loads(decoded_pickle)\n        prediction = self._model.predict(features_df).tolist()\n        return {&quot;prediction&quot;: prediction}\n\napp = Flask(__name__)\nwith open('.\/model.pkl', 'rb') as model_file:\n    model = pickle.load(model_file)\n    predictor = Predictor(model=model)\n\n\n@app.route(&quot;\/predict&quot;, methods=['POST'])\ndef predict() -&gt; Any:\n    if request.method == &quot;POST&quot;:\n        instance = request.get_json()\n        instance = instance['instances'][0]\n        predictions = predictor.predict(instance)\n        return jsonify(predictions)\n\n\n@app.route(&quot;\/health&quot;)\ndef health() -&gt; str:\n    return &quot;ok&quot;\n\n\nif __name__ == '__main__':\n    port = int(os.environ.get(&quot;PORT&quot;, 8080))\n    app.run(host='0.0.0.0', port=port)\n<\/code><\/pre>\n<p>The deployment code which I do through Python is irrelevant because the problem persists when I deploy through GCP's UI.<\/p>\n<p>The model creation code is as follows:<\/p>\n<pre><code>def upload_model(self):\n    model = {\n        &quot;name&quot;: self.model_name_on_platform,\n        &quot;display_name&quot;: self.model_name_on_platform,\n        &quot;version_aliases&quot;: [&quot;default&quot;, self.run_id],\n        &quot;container_spec&quot;: {\n            &quot;image_uri&quot;: f'{REGION}-docker.pkg.dev\/{GCP_PROJECT_ID}\/{self.repository_name}\/{self.run_id}',\n            &quot;predict_route&quot;: &quot;\/predict&quot;,\n            &quot;health_route&quot;: &quot;\/health&quot;,\n        },\n    }\n    parent = self.model_service_client.common_location_path(project=GCP_PROJECT_ID, location=REGION)\n    model_path = self.model_service_client.model_path(project=GCP_PROJECT_ID,\n                                                      location=REGION,\n                                                      model=self.model_name_on_platform)\n    upload_model_request_specifications = {'parent': parent, 'model': model,\n                                           'model_id': self.model_name_on_platform}\n    try:\n        print(&quot;trying to get model&quot;)\n        self.get_model(model_path=model_path)\n    except NotFound:\n        print(&quot;didn't find model, creating a new one&quot;)\n    else:\n        print(&quot;found an existing model, creating a new version under it&quot;)\n        upload_model_request_specifications['parent_model'] = model_path\n    upload_model_request = model_service.UploadModelRequest(upload_model_request_specifications)\n    response = self.model_service_client.upload_model(request=upload_model_request, timeout=1800)\n    print(&quot;Long running operation:&quot;, response.operation.name)\n    upload_model_response = response.result(timeout=1800)\n    print(&quot;upload_model_response:&quot;, upload_model_response)\n<\/code><\/pre>\n<p>My problem is very close to <a href=\"https:\/\/stackoverflow.com\/questions\/69316032\/custom-container-deployment-in-vertex-ai\">this one<\/a> with the difference that I do have a health check.<\/p>\n<p>Why would it work on the first deployment and fail ever since? Why would it work on AI Platform but fail on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-05 14:34:45.97 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"docker|flask|google-cloud-vertex-ai",
        "Question_view_count":299,
        "Owner_creation_date":"2021-10-19 12:36:59.98 UTC",
        "Owner_last_access_date":"2022-09-19 13:44:39.633 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-14 19:56:43.173 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to call a Google Vertex AI endpoint with an Api Key with c# and curl",
        "Question_body":"<p>from a c# program i want to call my <strong>vertex ai endpoint<\/strong> for prediction with an <strong>api key<\/strong> (via <strong>&quot;Google Cloud&quot;\/Credentials\/API Keys&quot;<\/strong> )). I gave the api key access to Vertex AI and as a test everything else too.<\/p>\n<p>calling it with curl or c# i get the error that the service expects an &quot;OAuth 2 access token &quot; or something else.<\/p>\n<p><strong>CURL:<\/strong>\ncurl -X POST\n-H &quot;apikey=..mykey...&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/.....googleapis.com\/v1\/projects\/[PROJECT_ID]\/locations\/europe-west4\/endpoints\/[ENDPOINT_ID]:predict\n-d @image.json<\/p>\n<p><strong>ERROR:<\/strong>\n&quot;error&quot;: {\n&quot;code&quot;: 401,\n&quot;message&quot;: &quot;Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. ...\n}<\/p>\n<p><strong>MY QUESTION:<\/strong>\nIs there a way to use the Apikey for authentication to vertex ai?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-24 15:38:37.85 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"c#|authentication|curl|api-key|google-cloud-vertex-ai",
        "Question_view_count":229,
        "Owner_creation_date":"2022-06-24 15:13:06.047 UTC",
        "Owner_last_access_date":"2022-09-08 18:11:55.73 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Is it possible to run Vertex AI Workbench on Spot machines?",
        "Question_body":"<p>I'm trying to save budget on jupyter notebooks on Google Cloud but couldn't find a way to run Vertex AI Workbench (Notebooks) on spot machines.\nWhat are my alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-28 09:25:32.77 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-notebook|jupyter|google-cloud-vertex-ai",
        "Question_view_count":176,
        "Owner_creation_date":"2011-11-25 20:39:39.12 UTC",
        "Owner_last_access_date":"2022-08-22 07:33:27.01 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Question_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-09-23 15:03:34.673 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"docker|google-cloud-platform|gcloud|google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":1033,
        "Owner_creation_date":"2016-05-24 16:22:09.61 UTC",
        "Owner_last_access_date":"2022-09-13 16:37:07.037 UTC",
        "Owner_location":null,
        "Owner_reputation":457,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-09-30 07:43:34.7 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":"2021-09-23 15:22:11.833 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Add decorator to component decorator in KFP v2 in Vertex AI",
        "Question_body":"<p>Usually, KFP v2 supports adding a component decorator like this:<\/p>\n<pre><code>@component\ndef test():\n  print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>I would like to add an additional decorator to add new functionality like this:<\/p>\n<pre><code>@component\n@added_functionality\ndef test():\n  print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>Where <code>added_functionality<\/code> is imported and looks like this:<\/p>\n<pre><code>from functools import wraps\n\ndef added_functionality(func):\n  print(&quot;starting added functionality&quot;)\n\n  @wraps(func)\n  def wrapper(*args, **kwargs):\n    print(&quot;starting wrapper&quot;)\n    return func(*args, **kwargs)\n\n  return wrapper\n<\/code><\/pre>\n<p>The issue is that when I compile the pipeline, I see 'starting added functionality' printed to the console, but &quot;starting wrapper&quot; doesn't show up in the log in Vertex AI. Am I doing something wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-21 18:08:57.37 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|google-cloud-vertex-ai|kfp",
        "Question_view_count":48,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-21 18:57:48.923 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"quotas are exceeded error while deploying Vertex AI Matching Engine Index",
        "Question_body":"<p>I am following this example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> and used custom word embeddings of my own.<\/p>\n<p>i am getting the following error while deploying the index at this step<\/p>\n<pre><code>r = index_endpoint_client.deploy_index(\n    index_endpoint=INDEX_ENDPOINT_NAME, deployed_index=deploy_ann_index\n)\n<\/code><\/pre>\n<blockquote>\n<p>ResourceExhausted: 429 The following quotas are exceeded:\nMatchingEngineDeployedIndexNodes<\/p>\n<\/blockquote>\n<p>i have list endpoint deployed that was done in previous step.<\/p>\n<pre><code>ListIndexEndpointsPager&lt;index_endpoints {\n  name: &quot;projects\/xxxxxxx\/locations\/us-central1\/indexEndpoints\/xxxxxxxx&quot;\n  display_name: &quot;index_endpoint_for_demo&quot;\n  etag: &quot;AMEw9yPT2hoRkYEEApekieucfedTYyEb5prjgG60WL1pVUfxxxxxxxxxxx&quot;\n  create_time {\n    seconds: 1645125657\n    nanos: 225990000\n  }\n  update_time {\n    seconds: 1645125657\n    nanos: 794785000\n  }\n  network: &quot;projects\/xxxxx\/global\/networks\/ucaip-haystack-vpc-network&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-17 19:34:39.877 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":198,
        "Owner_creation_date":"2017-02-12 07:08:02.32 UTC",
        "Owner_last_access_date":"2022-04-27 06:13:44.55 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-17 20:07:46.177 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"setup early stopping for Vertex automl for text classification",
        "Question_body":"<p>I'm running an automl job using google cloud SDK like this:<\/p>\n<p>I couldn't see any parameters to set up early stopping. Is this a missing feature or I am missing something here?<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(\n    display_name=training_job_display_name,\n    prediction_type=&quot;classification&quot;,\n    multi_label=False,\n)\n\nmodel = job.run(\n    dataset=text_dataset,\n    model_display_name=model_display_name,\n    training_fraction_split=0.1,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    sync=True,\n)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-14 14:15:10.023 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":13,
        "Owner_creation_date":"2016-06-24 15:48:30.737 UTC",
        "Owner_last_access_date":"2022-09-23 18:59:59.153 UTC",
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":2701,
        "Owner_up_votes":598,
        "Owner_down_votes":6,
        "Owner_views":260,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Vertex AI forecasting AutoML giving different answers for same input data",
        "Question_body":"<p>I trained Vertex AI forecasting AutoML model one with target column as String and other numeric input features as String then I trained another AutoML model with target column as float and other input features as Integer.<\/p>\n<p>The predictions are different for both the models. The data is same only the datatypes\/schema changed.<\/p>\n<p>Google <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#forecasting\" rel=\"nofollow noreferrer\">documentation<\/a> says:<\/p>\n<blockquote>\n<p>When you train a model with a feature with a numeric transformation,\nVertex AI applies the following data transformations to the feature,\nand uses any that provide signal for training:<\/p>\n<ul>\n<li>The value converted to float32.<\/li>\n<\/ul>\n<\/blockquote>\n<p>So both the data should be same even after transformation.\nWhy would results be different? Is it possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-08 11:40:16.577 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|forecasting|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":197,
        "Owner_creation_date":"2015-12-16 17:49:09.287 UTC",
        "Owner_last_access_date":"2022-08-22 11:39:25.35 UTC",
        "Owner_location":null,
        "Owner_reputation":500,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-11 11:51:40.387 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-16 06:09:14.497 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_date":"2015-12-04 05:40:05.093 UTC",
        "Owner_last_access_date":"2022-09-22 02:24:42.53 UTC",
        "Owner_location":"Manila, NCR, Philippines",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-20 02:13:08.633 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-25 13:25:10.52 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":606,
        "Owner_creation_date":"2017-04-04 08:33:44.967 UTC",
        "Owner_last_access_date":"2022-09-23 09:42:54.4 UTC",
        "Owner_location":"Herefordshire, UK",
        "Owner_reputation":3107,
        "Owner_up_votes":442,
        "Owner_down_votes":19,
        "Owner_views":434,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-06-28 14:08:58.667 UTC",
        "Answer_last_edit_date":"2021-06-29 09:56:38.497 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-06-29 13:04:34.477 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Constructing a Vertex AI Pipeline with a custom training container and a model serving container",
        "Question_body":"<p>I'd like to be able to train a model with a training app container that I've made and saved to my artifact registry. I want to be able to deploy a model with a flask app and with a \/predict route that can handle some logic -- not necessarily just predicting an input json. It'll also need a \/healthz route I understand. So basically I want a pipeline that performs a training job on a model training container that I make, and deploys the model with a flask app with a model serving container that I make. Looking around on Overflow, I wonder if <a href=\"https:\/\/stackoverflow.com\/questions\/68075940\/vertex-pipeline-custompythonpackagetrainingjobrunop-not-supplying-workerpoolspe\">this<\/a> question's pipeline has the correct layout I'll eventually want to have. So, something like this:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline():\n        training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n            project=project_id,\n            display_name=training_job_name,\n            model_display_name=model_display_name,\n            python_package_gcs_uri=python_package_gcs_uri,\n            python_module=python_module,\n            container_uri=container_uri,\n            staging_bucket=staging_bucket,\n            model_serving_container_image_uri=model_serving_container_image_uri)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project_id,\n            display_name=model_display_name,\n            artifact_uri=output_dir,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n        model_deploy_op = gcc_aip.ModelDeployOp(\n            project=project_id,\n            model=model_upload_op.outputs[&quot;model&quot;],\n            endpoint=aiplatform.Endpoint(\n                endpoint_name='0000000000').resource_name,\n            deployed_model_display_name=model_display_name,\n            machine_type=&quot;n1-standard-2&quot;,\n            traffic_percentage=100)\n\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_spec_path)\n<\/code><\/pre>\n<p>I'm hoping that <code>model_serving_container_image_uri<\/code> and <code>serving_container_image_uri<\/code> both refer to the URI for the model serving container I'm going to make. I've already made a training container that trains a model and saves <code>saved_model.pb<\/code> to Google Cloud Storage. Other than having a flask app that handles the prediction and health check routes and a Dockerfile that exposes a port for the flask app, what else will I need to do to ensure the model serving container works in this pipeline? Where in the code do I install the model from GCS? In the Dockerfile? How is the model serving container meant to work so that everything will go swimmingly in the construction of the pipeline? I'm having trouble finding any tutorials or examples of precisely what I'm trying to do anywhere even though this seems like a pretty common scenario.<\/p>\n<p>To that end, I attempted this with the following pipeline:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline(\n        project: str = [redacted project ID],\n        display_name: str = &quot;custom-pipe&quot;,\n        model_display_name: str = &quot;test_model&quot;,\n        training_container_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-training-test&quot;,\n        model_serving_container_image_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-model-serving-test&quot;,\n        model_serving_container_predict_route: str = &quot;\/predict&quot;,\n        model_serving_container_health_route: str = &quot;\/healthz&quot;,\n        model_serving_container_ports: str = &quot;8080&quot;\n):\n        training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n            display_name = display_name,\n            container_uri=training_container_uri,\n            model_serving_container_image_uri=model_serving_container_image_uri,\n            model_serving_container_predict_route = model_serving_container_predict_route,\n            model_serving_container_health_route = model_serving_container_health_route,\n            model_serving_container_ports = model_serving_container_ports)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project,\n            display_name=model_display_name,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n#        model_deploy_op = gcc_aip.ModelDeployOp(\n#            project=project,\n#            model=model_upload_op.outputs[&quot;model&quot;],\n#            endpoint=aiplatform.Endpoint(\n#                endpoint_name='0000000000').resource_name,\n#            deployed_model_display_name=model_display_name,\n#            machine_type=&quot;n1-standard-2&quot;,\n#            traffic_percentage=100)\n<\/code><\/pre>\n<p>Which is failing with<\/p>\n<pre><code>google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.trainingPipelines.create' denied on resource '\/\/aiplatform.googleapis.com\/projects\/u15c36a5b7a72fabfp-tp\/locations\/us-central1' (or it may not exist).\n<\/code><\/pre>\n<p>Despite the fact that my service account has the Viewer and Kubernetes Engine Admin roles needed to work AI Platform pipelines. My training container uploads my model to Google Cloud Storage and my model serving container I've made downloads it and uses it for serving at <code>\/predict<\/code>.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-15 14:28:20.06 UTC",
        "Question_favorite_count":2.0,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":421,
        "Owner_creation_date":"2022-04-13 14:58:44.49 UTC",
        "Owner_last_access_date":"2022-09-21 14:23:56.59 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-19 17:06:27.397 UTC",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-08-21 11:30:10.723 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_date":"2021-12-12 12:36:30.127 UTC",
        "Owner_last_access_date":"2022-09-19 20:00:07.14 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-23 15:00:06.083 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"How to train MLM model XLM Roberta large on google machine specs fastly with less memory",
        "Question_body":"<p>I am fine tuning masked language model from <code>XLM Roberta large<\/code> on google machine specs.\nI made couple of experiments and was strange to see few results.<\/p>\n<pre><code>&quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 4 Running ( 4 data*4 GPU=16 data points)\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4 , accelerator_type=&quot;NVIDIA_TESLA_A100&quot;on 4,12,672 data batch size 8 failed\n &quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 16 failed\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4.,accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 32 failed\n<\/code><\/pre>\n<p>I was not able to train model with <code>batch size <\/code> more than 4 on # of GPU's. It stopped in mid-way.<\/p>\n<p>Here is the code I am using.<\/p>\n<pre><code>training_args = tr.TrainingArguments(\n#     disable_tqdm=True,\n    output_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/results_mlm_exp2', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n    ,save_strategy=&quot;no&quot;\n    ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n    ,logging_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/logs_mlm_exp1'        # directory for storing logs\n    ,logging_steps=40000\n    ,logging_strategy='no'\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data\n    \n)\n<\/code><\/pre>\n<p><strong>My Questions<\/strong><\/p>\n<p>How can I train with larger batch size on <code>a2-highgpu-4g<\/code> machine?<\/p>\n<p>Which parameters can I include in <code>TrainingArguments<\/code> so that training is fast and occupies small memory?<\/p>\n<p>Thanks in advance.<\/p>\n<h3>Versions<\/h3>\n<pre><code>torch==1.11.0+cu113 \n\ntorchvision==0.12.0+cu113  \n\ntorchaudio==0.11.0+cu113 \n\ntransformers==4.17.0\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-16 15:51:24.443 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python-3.x|google-cloud-platform|pytorch|huggingface-transformers|google-cloud-vertex-ai",
        "Question_view_count":210,
        "Owner_creation_date":"2018-06-07 08:44:46.053 UTC",
        "Owner_last_access_date":"2022-09-23 09:15:48.837 UTC",
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Why do I occasionally get 'Post request fails. Cannot decode the prediction response' when running Vertex AI batch predictions?",
        "Question_body":"<p>I'm currently running batch predictions on Vertex AI with a custom FastAPI container and <code>manualBatchTuningParameters<\/code> set to <code>{&quot;batch_size&quot;: 2}<\/code>. My JSONL file contains 646 predictions that mostly succeed except for the few that result in the following error:<\/p>\n<pre><code>('Post request fails. Cannot decode the prediction response \n...&lt;long and seemingly valid json&gt;... \nError: Unterminated string starting at: line 1 column 97148 (char 97147)', 2)\n<\/code><\/pre>\n<p>Based on the common positioning (char 97147) of the character in the error, it seems like the response is being truncated before the stream is completely received by the batch &quot;airflow worker.&quot; Given that TCP is a streaming protocol, I believe the batch interface is only receiving a portion of the buffers.<\/p>\n<p>I've attempted to reproduce the error by deploying the same model as a vertex endpoint and requesting the same predictions that errored in batch mode.<\/p>\n<p>Why am I occasionally getting this error?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-05-10 23:43:49.8 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":77,
        "Owner_creation_date":"2016-01-08 20:33:44.53 UTC",
        "Owner_last_access_date":"2022-09-24 04:11:04.503 UTC",
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":870,
        "Owner_up_votes":18,
        "Owner_down_votes":2,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_title":"Internal error on batch prediction job. Reproduction conditions are unidentified",
        "Question_body":"<p>I trained some tabular forecasting model and executed batch prediction jobs with Vertex AI, then sometimes the following error occurred 30-60 minutes after the start of the jobs.<\/p>\n<pre><code>Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.\n<\/code><\/pre>\n<p>As the message mentioned, I retried the next day with same data and then I got the prediction job succeed.\nHowever, I have seen this error from time to time with Vertex AI.\nI cannot identify the reproduction conditions.\nI cannot find the logs of the job execution.<\/p>\n<p>What causes this error in Vertex AI?\nHow can I avoid this errror?<\/p>\n<p>Resource name:\nprojects\/832409671062\/locations\/us-central1\/batchPredictionJobs\/3813678620929425408<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-24 06:38:13.343 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":124,
        "Owner_creation_date":"2022-01-05 14:50:29.917 UTC",
        "Owner_last_access_date":"2022-04-24 12:20:45.457 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Vertex AI"
    }
]