[
    {
        "Question_title":"What DVC does when git merge is executed?",
        "Question_body":"<p>I have two git branches (master and develop). DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-01 10:15:31.777 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"git|dvc",
        "Question_view_count":40,
        "Owner_creation_date":"2020-02-02 18:40:04.397 UTC",
        "Owner_last_access_date":"2022-09-24 14:57:14.117 UTC",
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-02 05:40:50.203 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Can we connect oracle database with DVC ? and if yes then how?",
        "Question_body":"<p>I was trying to connect dvc with oracle database but unable to do it. So, Please can anyone help me with that.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-15 08:13:03.107 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"oracle|dvc",
        "Question_view_count":20,
        "Owner_creation_date":"2022-09-15 08:08:11.407 UTC",
        "Owner_last_access_date":"2022-09-23 09:55:49.987 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-04-22 15:00:54.467 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_date":"2018-11-18 10:45:00.087 UTC",
        "Owner_last_access_date":"2022-09-23 13:41:40.713 UTC",
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-04-22 21:34:02.127 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC(Data Version Control) keeps stuck at \"dvc add xxx\" with \"Collecting stages from the workspace\" in the terminal?",
        "Question_body":"<p>I used : <code>dvc[webhdfs]==2.9.3<\/code>, installed by <code>pip install dvc[webhdfs]<\/code><\/p>\n<p>Then the repo is already cloned by git.<\/p>\n<p>I have also typed : <code>dvc remote add -d storage webhdfs:\/\/xxx\/dvc<\/code> and <code>git add .dvc\/config<\/code><\/p>\n<p>But the command <code>dvc add .\/assets\/xxx\/*<\/code> was still stuck...<\/p>\n<p>The command line window keeps showing : <code>Collecting stages from the workspace<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-13 08:24:36.767 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|deployment|continuous-integration|dvc",
        "Question_view_count":115,
        "Owner_creation_date":"2019-03-24 03:57:13.283 UTC",
        "Owner_last_access_date":"2022-07-29 12:48:42.787 UTC",
        "Owner_location":"Beijing",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-13 10:11:54.663 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored",
        "Question_body":"<p>Getting the error &quot;<em>ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored.<\/em>&quot; while trying to add local files for tracking<\/p>\n<p>Python Version : 3.7<\/p>\n<p>Library used:<\/p>\n<p><code>pip install dvc  pip install dvc[gdrive]   dvc init   <\/code><\/p>\n<p><strong>dvc add -R Training_Batch_Files<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2021-06-08 17:00:03.727 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|git|dvc",
        "Question_view_count":1844,
        "Owner_creation_date":"2020-07-05 12:42:44.623 UTC",
        "Owner_last_access_date":"2022-09-24 08:16:50.69 UTC",
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-08 20:07:48.09 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_date":"2019-07-06 08:50:39.337 UTC",
        "Owner_last_access_date":"2022-08-02 18:08:37.997 UTC",
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-06-10 15:37:00.463 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-06-13 21:50:56.467 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-05-09 05:24:22.843 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_date":"2021-05-09 05:18:04.8 UTC",
        "Owner_last_access_date":"2022-01-13 07:46:24.007 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-09 20:19:38.073 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":"2021-05-10 05:51:46.413 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-04-02 21:45:23.477 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_date":"2014-08-03 18:46:34.73 UTC",
        "Owner_last_access_date":"2022-06-08 02:07:15.42 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-04-02 23:07:02.54 UTC",
        "Answer_last_edit_date":"2021-04-03 19:28:34.567 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2021-04-05 17:22:23.263 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-21 07:31:07.14 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_date":"2013-03-15 04:43:52.587 UTC",
        "Owner_last_access_date":"2022-09-25 05:38:43.07 UTC",
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-09-21 07:49:25.513 UTC",
        "Answer_last_edit_date":"2021-09-21 08:10:31.43 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-09-21 15:07:53.617 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-20 11:10:29.42 UTC",
        "Question_favorite_count":1.0,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-20 16:31:15.363 UTC",
        "Answer_last_edit_date":"2022-01-18 17:46:31.693 UTC",
        "Answer_score":13.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Second dvc push on AWS Batch using IAM role gets \"Unable to locate credentials\"",
        "Question_body":"<p>I'm running a job on AWS Batch, and this job prepares some data and versions it using <code>dvc<\/code>. Secondly, the job does some transformation generating new data, and it should save this new data using <code>dvc<\/code> again. Also, in this case, i'm setting a instance-profile role to enable the AWS Batch to persist on my S3 bucket.<\/p>\n<p>The first <code>dvc push<\/code> works perfectly. But the second one generates the error <code>Unable to locate credentials<\/code><\/p>\n<p>I have also changed the script to just touch a file, add to dvc and push, and then repeat the process in with other file, and could replicate the problem.<\/p>\n<p>I have already solved, changing the command <code>dvc push<\/code> to <code>dvc push especific-file-to-push<\/code>, but I'm now trying to understand what is the problem with <code>dvc push<\/code> command without the parameter specifying the file.<\/p>\n<p>Does anybody know?<\/p>\n<p>I'm using dvc <code>dvc==2.9.5<\/code> and <code>boto3==1.21.21<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-04-30 14:56:26.727 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-s3|boto3|amazon-iam|aws-batch|dvc",
        "Question_view_count":152,
        "Owner_creation_date":"2013-09-19 22:56:26.42 UTC",
        "Owner_last_access_date":"2022-09-15 12:21:12.72 UTC",
        "Owner_location":"Rio de Janeiro, Brazil",
        "Owner_reputation":101,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-30 15:10:34.543 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-22 11:40:30.233 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":493,
        "Owner_creation_date":"2018-11-18 10:45:00.087 UTC",
        "Owner_last_access_date":"2022-09-23 13:41:40.713 UTC",
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2021-06-22 20:24:47.733 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"problem with dvc import-url from google spreadsheet export",
        "Question_body":"<p>I'm in the process of converting a Makefile-based data workflow to dvc. I have a Google spreadsheet that I'm using in a data workflow to make it easy to update a few things in a makeshift database. Currently this works with something like this:<\/p>\n<pre><code># Makefile\ndata.csv:\n    curl -L https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv &gt; data.csv\n<\/code><\/pre>\n<p>Of course, I can incorporate the same step into my dvc pipeline directly with <code>dvc run<\/code>, but my understanding is that something like <code>dvc import-url<\/code> would be more appropriate but I'm getting an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import-url https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv data.csv\nImporting 'https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv' -&gt; 'data.csv'\nERROR: unexpected error - 'NoneType' object has no attribute 'endswith'\n<\/code><\/pre>\n<p>My guess is that this is because the response data from the Google Spreadsheet export url doesn't have a filename suffix associated with it. Is there a way to work around this problem? Is there a better way to pull data from a google spreadsheet into a dvc workflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-03-06 13:05:20.157 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"google-sheets|google-sheets-api|dvc",
        "Question_view_count":59,
        "Owner_creation_date":"2011-01-05 23:08:56.687 UTC",
        "Owner_last_access_date":"2022-08-27 16:39:52.827 UTC",
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-21 03:48:13.37 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_date":"2015-11-05 18:07:20.593 UTC",
        "Owner_last_access_date":"2022-09-25 05:32:55.707 UTC",
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-10-21 21:04:50.77 UTC",
        "Answer_last_edit_date":"2020-10-22 00:09:09.59 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2020-10-21 18:45:23.907 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Python: Ssl Certificate verify failed",
        "Question_body":"<p>I have installed <code>dvc<\/code> on my <code>ubuntu-18.04-LTS<\/code> system and while trying to download the <code>data<\/code> files from github using dvc, it fails with below error.<\/p>\n<pre><code>$ dvc get https:\/\/github.com\/iterative\/dataset-registry get-started\/data.xml -o data\/data.xml -v\n\n2022-07-22 12:55:22,260 DEBUG: Creating external repo https:\/\/github.com\/iterative\/dataset-registry@None\n2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https:\/\/github.com\/iterative\/dataset-registry' to a temporary dir\n2022-07-22 12:55:23,683 DEBUG: Removing '\/dvc\/dvc_test\/data\/.UEeAzwmJCY3q85YQuCeahx'\n2022-07-22 12:55:23,684 ERROR: failed to get 'get-started\/data.xml' from 'https:\/\/github.com\/iterative\/dataset-registry' - Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;urllib3\/connectionpool.py&quot;, line 703, in urlopen\n  File &quot;urllib3\/connectionpool.py&quot;, line 386, in _make_request\n  File &quot;urllib3\/connectionpool.py&quot;, line 1042, in _validate_conn\n  File &quot;urllib3\/connection.py&quot;, line 414, in connect\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 449, in ssl_wrap_socket\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl\n  File &quot;ssl.py&quot;, line 500, in wrap_socket\n  File &quot;ssl.py&quot;, line 1040, in _create\n  File &quot;ssl.py&quot;, line 1309, in do_handshake\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;dvc\/scm.py&quot;, line 145, in clone\n  File &quot;scmrepo\/git\/__init__.py&quot;, line 143, in clone\n  File &quot;scmrepo\/git\/backend\/dulwich\/__init__.py&quot;, line 199, in clone\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n<\/code><\/pre>\n<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com<\/code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc<\/code>the above errors are occurring, Even the below couldn't solve the issue.<\/p>\n<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;\n\nDefaultVerifyPaths(cafile=None, capath='\/usr\/lib\/ssl\/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='\/usr\/lib\/ssl\/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='\/usr\/lib\/ssl\/certs')\n<\/code><\/pre>\n<pre><code>export SSL_CERT_DIR=\/etc\/ssl\/certs\/\nexport REQUESTS_CA_BUNDLE=\/usr\/local\/lib\/python2.7\/dist-packages\/certifi\/cacert.pem\npip install --upgrade certifi\nexport PYTHONHTTPSVERIFY=0\n\nsudo apt install ca-certificates\nsudo update-ca-certificates --fresh\n<\/code><\/pre>\n<pre><code>$ python --version\nPython 2.7.17\n\n$ dvc doctor\nDVC version: 2.13.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\n<\/code><\/pre>\n<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;<\/code> Similarly do we have option in dvc?<\/p>\n<p>Further what should i update to resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-26 15:36:20.953 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|ssl|pip|ssl-certificate|dvc",
        "Question_view_count":157,
        "Owner_creation_date":"2015-05-28 11:02:07.473 UTC",
        "Owner_last_access_date":"2022-09-25 03:23:10.223 UTC",
        "Owner_location":null,
        "Owner_reputation":1609,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":447,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-27 05:01:17.49 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC Push KeyError fileSize",
        "Question_body":"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with<\/p>\n<pre><code>ERROR: unexpected error - KeyError('fileSize')\n<\/code><\/pre>\n<p><strong>Edit<\/strong>\nSo searching around it seem that it might help to include the verbose log with regards to the error.<\/p>\n<pre><code>T11:27:08~\/documents\/*****\/data$ dvc push -v\n2022-02-01 11:32:13,186 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/config.local' to gitignore file.\n2022-02-01 11:32:13,199 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp' to gitignore file.\n2022-02-01 11:32:13,200 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to gitignore file.\n2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to '*********'\n2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'\n2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'\n2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files\n2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse\n2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 226, in __getitem__\n    return dict.__getitem__(self, key)\nKeyError: 'fileSize'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 56, in push\n    pushed += self.cloud.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 499, in hashes_exist\n    remote_hashes = set(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 334, in _list_hashes_traverse\n    yield from itertools.chain.from_iterable(in_remote)\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 324, in list_with_update\n    return list(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 107, in find\n    yield from self.fs.find(path)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/fs\/spec.py&quot;, line 323, in find\n    &quot;size&quot;: int(item[&quot;fileSize&quot;]),\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 229, in __getitem__\n    raise KeyError(e)\nKeyError: KeyError('fileSize')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2022-02-01 11:16:50.58 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":54,
        "Owner_creation_date":"2012-04-08 18:08:56.53 UTC",
        "Owner_last_access_date":"2022-09-24 13:24:30.7 UTC",
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":884,
        "Owner_up_votes":185,
        "Owner_down_votes":3,
        "Owner_views":59,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-01 11:40:42.69 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Why I got an invalid bucket name error using dvc mlflow on macos",
        "Question_body":"<p>Could anyone tell what's the reason for error:<\/p>\n<p>botocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]<\/em>:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9-]{1,63}$&quot;<\/p>\n<p>I try to use mlflow with docker.\n.env file contains:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/127.0.0.1:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>Also tried to use:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/localhost:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>docker-compose contains:<\/p>\n<pre><code>... \n   mlflow:\n        restart: always\n        image: mlflow_server\n        container_name: mlflow_server\n        ports:\n          - &quot;5000:5000&quot;\n        networks:\n          - postgres\n          - s3\n        environment:\n          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n          - MLFLOW_S3_ENDPOINT_URL=http:\/\/nginx:9000\n        command: mlflow server --backend-store-uri postgresql:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@db\/${POSTGRES_DB} --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/ --host 0.0.0.0\n...\n<\/code><\/pre>\n<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2022-05-21 21:10:53.863 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|mlflow|mlops|dvc",
        "Question_view_count":117,
        "Owner_creation_date":"2019-12-29 15:05:47.33 UTC",
        "Owner_last_access_date":"2022-09-20 12:21:31.067 UTC",
        "Owner_location":"Saint Petersburg",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-24 06:16:27.987 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Expanding environment variables in the command part of a dvc run",
        "Question_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2019-07-21 10:03:46.437 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":329,
        "Owner_creation_date":"2011-03-22 10:28:37.227 UTC",
        "Owner_last_access_date":"2022-09-23 13:51:41.56 UTC",
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Answer_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-07-24 17:21:12.483 UTC",
        "Answer_last_edit_date":"2019-07-28 13:13:50.557 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-07-28 10:52:37.323 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_date":"2020-01-09 12:58:29.92 UTC",
        "Owner_last_access_date":"2022-03-30 10:42:37.603 UTC",
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2021-07-28 23:03:26.643 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Installation DVC on MinIO storage",
        "Question_body":"<p>Does anybody install DVC on MinIO storage?<\/p>\n<p>I have read <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">docs<\/a> but not all clear for me.<\/p>\n<p>Which command should I use for setup MinIO storage with this entrance parameters:<\/p>\n<p>storage url: <a href=\"https:\/\/minio.mysite.com\/minio\/bucket-name\/\" rel=\"nofollow noreferrer\">https:\/\/minio.mysite.com\/minio\/bucket-name\/<\/a>\nlogin: my_login\npassword: my_password<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-21 11:09:40.413 UTC",
        "Question_favorite_count":1.0,
        "Question_score":5,
        "Question_tags":"python|minio|dvc",
        "Question_view_count":1547,
        "Owner_creation_date":"2018-05-16 14:36:56.047 UTC",
        "Owner_last_access_date":"2022-09-23 08:33:51.783 UTC",
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p><strong>Install<\/strong><\/p>\n<p>I usually use it as a Python package, int this case you need to install:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install &quot;dvc[s3]&quot;\n<\/code><\/pre>\n<p><strong>Setup remote<\/strong><\/p>\n<p>By default DVC supports AWS S3 storages and they work fine.<br \/>\nAlso they support &quot;S3-compatible storage&quot;, but setup for this type of remotes is nod described properly. In particular case of MinIO you have <strong>bucket<\/strong> - directory on MinIO server where actual data stores (it is similar to AWS bucket), but DVC uses AWS CLI to authenticate. In case of MinIO you need to pass them explicitly.<\/p>\n<p>Then follow commands to setup your DVC remote:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)\ndvc remote add -d minio s3:\/\/bucket-name -f\n\n# add information about storage url (where &quot;https:\/\/minio.mysite.com&quot; your url)\ndvc remote modify minio endpointurl https:\/\/minio.mysite.com\n\n#  add info about login and password\ndvc remote modify minio access_key_id my_login\ndvc remote modify minio secret_access_key my_password\n<\/code><\/pre>\n<p><strong>If you move from old remote<\/strong>, use follow command to move your data:<\/p>\n<p>Before setup (download all old remote cache to local machine):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>\n<p>After setup (upload all cache to a new remote):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-21 12:14:45.543 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":6.0,
        "Question_last_edit_date":"2021-05-21 18:33:28.697 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?",
        "Question_body":"<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.<\/p>\n<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.<\/p>\n<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.<\/p>\n<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.<\/p>\n<ol>\n<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?<\/p>\n<\/li>\n<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.<\/p>\n<\/li>\n<\/ol>\n<p>I understand from <a href=\"https:\/\/stackoverflow.com\/a\/35578715\/4533188\">here<\/a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.<\/p>\n<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-03-29 13:52:24.137 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"git|git-lfs|dvc|git-annex",
        "Question_view_count":370,
        "Owner_creation_date":"2015-02-05 13:50:19.917 UTC",
        "Owner_last_access_date":"2022-09-23 12:45:06.05 UTC",
        "Owner_location":null,
        "Owner_reputation":11374,
        "Owner_up_votes":415,
        "Owner_down_votes":2,
        "Owner_views":845,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC experiment is restoring deleted files",
        "Question_body":"<p>I am using DVC to run experiments in my project using<\/p>\n<pre><code>dvc exp run\n<\/code><\/pre>\n<p>Now when i make changes to a file(example train.py) and run &quot;dvc exp run&quot; everything goes well,\nbut my problem is that when making changes by <strong>deleting<\/strong> a file(example train.py or an image in the data folder) as soon as i run the &quot;dvc exp run&quot; the file is restored.\nhow to stop that from happening?<\/p>\n<p>This is my dvc.yaml:<\/p>\n<pre><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n    - train.py\n    metrics:\n    - metrics.txt:\n        cache: false\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2021-07-07 10:57:32.383 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":272,
        "Owner_creation_date":"2019-05-08 11:22:43.687 UTC",
        "Owner_last_access_date":"2022-02-21 23:18:39.443 UTC",
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":113,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-18 02:00:22.65 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_date":"2016-07-08 02:05:15.393 UTC",
        "Owner_last_access_date":"2022-09-25 05:18:34.54 UTC",
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-06-18 05:17:16.92 UTC",
        "Answer_last_edit_date":"2020-06-18 16:15:29.89 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-31 15:03:42.88 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_date":"2016-03-08 20:35:01.7 UTC",
        "Owner_last_access_date":"2022-09-15 06:24:00.44 UTC",
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-01-31 17:45:43.317 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How do I specify encryption type when using s3remote for DVC",
        "Question_body":"<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting <\/p>\n\n<p>But when I run the <code>dvc push<\/code> command, I get the generic error saying <\/p>\n\n<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>which I know for a fact that I get that error when I don't specify the encryption.<\/p>\n\n<p>It is similar to running <code>aws s3 cp<\/code> with <code>--sse<\/code> flag or specifying <code>ServerSideEncryption<\/code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-26 05:45:08.167 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":293,
        "Owner_creation_date":"2017-04-11 13:31:59.307 UTC",
        "Owner_last_access_date":"2022-03-29 20:52:48.753 UTC",
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-18 10:04:46.453 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_date":"2017-10-17 09:04:07.66 UTC",
        "Owner_last_access_date":"2022-09-18 22:22:48.267 UTC",
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-10-27 08:34:28.433 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":"2018-10-18 16:44:38.543 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 08:01:27.02 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_date":"2020-02-05 20:03:01.007 UTC",
        "Owner_last_access_date":"2022-09-21 07:19:14.923 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 08:56:43.75 UTC",
        "Answer_last_edit_date":"2021-04-22 09:49:00.03 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Question_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-22 21:32:10.887 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"git|path|dvc",
        "Question_view_count":448,
        "Owner_creation_date":"2014-07-13 14:36:30.02 UTC",
        "Owner_last_access_date":"2022-09-23 21:23:45.423 UTC",
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Answer_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-23 01:27:49.86 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Highlight.js not respecting parent regex of a sub mode",
        "Question_body":"<p>I need to write a lexer which highlights my command-line tool commands properly.<\/p>\n\n<pre><code>$ dvc add file.csv\n$ dvc pipeline list\n<\/code><\/pre>\n\n<p>So the command starts with <code>dvc<\/code> and it may have one or two subcommands - <code>add<\/code> or <code>pipeline list<\/code> respectively.<\/p>\n\n<p>Therefore, it should highlight <code>dvc add<\/code> and <code>dvc pipeline list<\/code> in first and second case respectively.<\/p>\n\n<pre><code>contains: [\n          {\n            begin: \/^\\s*\\$\\s(dvc|git) [a-z-]+\/,\n            returnBegin: true,\n            contains: [\n              {\n                begin: \/dvc [a-z-]+ ?\/,\n                lexemes: '[a-z-]+',\n                keywords: {\n                  built_in:\n                    'dvc'\n                },\n                contains: [\n                  {\n                    begin: \/\\w+(?![\\S])\/,\n                    keywords: {\n                      built_in: 'list'\n                    }\n                  }\n                ],\n                className: 'strong'\n              }\n            ]\n          }\n        ]\n<\/code><\/pre>\n\n<p>It matches <code>dvc pipeline list<\/code> even though the parent regex i.e. <code>\/^\\s*\\$\\s(dvc|git) [a-z-]+\/<\/code> should only match till <code>dvc pipeline<\/code>. How is it exactly functioning?<\/p>\n\n<p>How does <code>\/dvc [a-z-]+ ?\/<\/code> override it and continues matching the expression?<\/p>\n\n<p>Please refer to this library docs here: <a href=\"https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html\" rel=\"nofollow noreferrer\">https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2019-10-22 20:02:30.387 UTC",
        "Question_favorite_count":2.0,
        "Question_score":3,
        "Question_tags":"javascript|syntax-highlighting|highlight|highlight.js|dvc",
        "Question_view_count":177,
        "Owner_creation_date":"2019-07-07 20:04:39.143 UTC",
        "Owner_last_access_date":"2021-07-07 17:33:03.4 UTC",
        "Owner_location":null,
        "Owner_reputation":209,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-10-26 00:03:06.933 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Initializing a DVC repository throws an error",
        "Question_body":"<p>I'm trying to use <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> and I'm following this kaggle tutorial as explained in this <a href=\"https:\/\/www.kaggle.com\/kurianbenoy\/introduction-to-data-version-control-dvc\" rel=\"nofollow noreferrer\">notebook<\/a> . Whenever I try to use the command <code>! dvc init<\/code>, I get the following error:<\/p>\n<pre><code>'dvc' is not recognized as an internal or external command,\noperable program or batch file.\n<\/code><\/pre>\n<p>I've installed and reinstalled dvc. I'm using python 3.6 and windows 8.1.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-08-12 10:43:58.987 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|python-3.x|version-control|dvc",
        "Question_view_count":1361,
        "Owner_creation_date":"2018-03-31 13:15:46.213 UTC",
        "Owner_last_access_date":"2022-09-07 02:28:21.417 UTC",
        "Owner_location":"Manipal, Karnataka, India",
        "Owner_reputation":149,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-02-28 12:51:53.937 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_date":"2011-01-05 23:08:56.687 UTC",
        "Owner_last_access_date":"2022-08-27 16:39:52.827 UTC",
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-02-28 18:34:51.72 UTC",
        "Answer_last_edit_date":"2021-03-02 15:29:48.013 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-03-02 15:33:38.993 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-26 15:07:48.803 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_date":"2009-08-13 10:15:52.417 UTC",
        "Owner_last_access_date":"2022-09-22 11:46:38.323 UTC",
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-27 11:01:01.183 UTC",
        "Answer_last_edit_date":"2021-10-27 11:06:04.02 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to fix DVC error 'FileNotFoundError: [Errno 2] No such file or directory' in Github actions",
        "Question_body":"<p>Trying to pull a folder with test data into a GitHub actions container, I get<\/p>\n<blockquote>\n<p>FileNotFoundError: [Errno 2] No such file or directory<\/p>\n<\/blockquote>\n<p>I tried running <code>dvc checkout --relink<\/code> locally, but that did not work. I am using Gdrive for the data-repository with a service account. It seems the login works. But strangely the file is not present. Maybe I can push the files again somehow and recreate the data in the repository?<\/p>\n<hr \/>\n<h2><code>dvc doctor<\/code> output:<\/h2>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.7.13 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.14.0),\n    webhdfs (fsspec = 2022.5.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sda1\nRepo: dvc, git\n<\/code><\/pre>\n<p>I was able to clone the GIT repo and pull the data from the DVC data-registry, however, it did not work from GH.<\/p>\n<hr \/>\n<h2>Full traceback<\/h2>\n<pre><code>Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiohttp-retry-2.5.1 aiosignal-1.2.0 appdirs-1.4.4 async-timeout-4.0.2 asynctest-0.13.0 atpublic-2.3 attrs-21.4.0 cached-property-1.5.2 cachetools-4.2.4 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.0.12 colorama-0.4.5 commonmark-0.9.1 configobj-5.0.6 contextvars-2.4 cryptography-37.0.4 dataclasses-0.8 decorator-4.4.2 dictdiffer-0.9.0 diskcache-5.4.0 distro-1.7.0 dpath-2.0.6 dulwich-0.20.45 dvc-2.8.1 flatten-dict-0.4.2 flufl.lock-3.2 frozenlist-1.2.0 fsspec-2022.1.0 ftfy-6.0.3 funcy-1.17 future-0.18.2 gitdb-4.0.9 gitpython-3.1.18 google-api-core-2.8.2 google-api-python-client-2.52.0 google-auth-2.9.1 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.3 grandalf-0.6 httplib2-0.20.4 idna-3.3 idna-ssl-1.1.0 immutables-0.18 importlib-metadata-4.8.3 importlib-resources-5.4.0 mailchecker-4.1.18 multidict-5.2.0 nanotime-0.5.2 networkx-2.5.1 oauth2client-4.1.3 packaging-21.3 pathspec-0.8.1 phonenumbers-8.12.52 ply-3.11 protobuf-3.19.4 psutil-5.9.1 pyOpenSSL-22.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydot-1.4.2 pydrive2-1.10.1 pygit2-1.6.1 pygments-2.12.0 pygtrie-2.5.0 pyparsing-2.4.7 python-benedict-0.25.2 python-dateutil-2.8.2 python-fsutil-0.6.1 python-slugify-6.1.2 requests-2.27.1 rich-12.5.1 rsa-4.9 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 shortuuid-1.0.9 shtab-1.5.5 six-1.16.0 smmap-5.0.0 tabulate-0.8.10 text-unidecode-1.3 toml-0.10.2 tqdm-4.64.0 typing-extensions-4.1.1 uritemplate-4.1.1 urllib3-1.26.10 voluptuous-0.13.1 wcwidth-0.2.5 xmltodict-0.13.0 yarl-1.7.2 zc.lockfile-2.0 zipp-3.6.0\nDVC version: 2.8.1 (pip)\n---------------------------------\nPlatform: Python 3.6.15 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.10.1),\n    webhdfs (fsspec = 2022.1.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sdb1\nRepo: dvc, git\n2022-07-22 19:01:08,361 DEBUG: failed to pull cache for 'tests\/data'\n2022-07-22 19:01:08,364 WARNING: No file hash info found for 'tests\/data'. It won't be created.\n1 file failed\n2022-07-22 19:01:08,365 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/pull.py&quot;, line 44, in pull\n    recursive=recursive,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/checkout.py&quot;, line 110, in checkout\n    raise CheckoutError(stats[&quot;failed&quot;], stats)\ndvc.exceptions.CheckoutError: Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\n2022-07-22 19:01:08,369 DEBUG: Analytics is enabled.\n2022-07-22 19:01:08,412 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\n2022-07-22 19:01:08,413 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\nError: The operation was canceled.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-07-22 18:52:51.86 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":209,
        "Owner_creation_date":"2015-10-16 01:35:39.707 UTC",
        "Owner_last_access_date":"2022-09-22 23:31:19.043 UTC",
        "Owner_location":null,
        "Owner_reputation":6513,
        "Owner_up_votes":1296,
        "Owner_down_votes":60,
        "Owner_views":1057,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-22 19:35:12.243 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-02 22:41:03.1 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_date":"2012-03-12 11:50:57.367 UTC",
        "Owner_last_access_date":"2022-09-23 13:23:56.043 UTC",
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-03 03:08:55.767 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2022-01-03 08:44:24.667 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Difference between git-lfs and dvc",
        "Question_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-24 12:19:46.097 UTC",
        "Question_favorite_count":5.0,
        "Question_score":27,
        "Question_tags":"git|git-lfs|dvc",
        "Question_view_count":6255,
        "Owner_creation_date":"2013-07-12 12:04:03.25 UTC",
        "Owner_last_access_date":"2022-09-24 13:14:35.13 UTC",
        "Owner_location":null,
        "Owner_reputation":382,
        "Owner_up_votes":185,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Answer_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-10-24 13:54:37.763 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":10.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Data Version Control (dvc) cannot push to remote storage because querying cache",
        "Question_body":"<p>I am setting up a remote storage with dvc using webdavs<\/p>\n<p>I can connect to the remote storage from Finder.<\/p>\n<p>I added the new remote and I see it when I check (dvc remote list)<\/p>\n<p>But when I try to push data, I have the request for password with 0% Querying cache<\/p>\n<p>It stays 0% forever. And when I enter the password, it ends with the following error:<\/p>\n<p>ERROR: unexpected error - No connection with LINK_OF_REMOTE_STORAGE<\/p>\n<p>The only thing I am thinking about is how to check if I can connect to the server from dvc and why querying cache never ends (maybe never starts even)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-01-18 08:57:03.987 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":436,
        "Owner_creation_date":"2014-09-25 09:26:32.833 UTC",
        "Owner_last_access_date":"2022-03-31 00:43:52.467 UTC",
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-01-18 09:20:48.253 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2018-11-08 17:59:32.677 UTC",
        "Question_favorite_count":1.0,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_date":"2015-01-09 01:04:40.79 UTC",
        "Owner_last_access_date":"2022-09-19 19:03:26.113 UTC",
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-02-06 00:55:31.5 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-08 14:48:49.56 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_date":"2009-07-24 16:26:11.43 UTC",
        "Owner_last_access_date":"2022-09-24 08:02:55.38 UTC",
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-09-08 16:54:50.387 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-09-08 15:18:57.133 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-21 09:44:15.303 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_date":"2017-04-25 06:32:01.29 UTC",
        "Owner_last_access_date":"2022-09-23 21:22:32.03 UTC",
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-01-21 11:20:29.327 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Question_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-16 14:19:55.94 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"directed-acyclic-graphs|data-pipeline|dvc",
        "Question_view_count":1101,
        "Owner_creation_date":"2009-05-27 17:42:14.993 UTC",
        "Owner_last_access_date":"2022-09-23 06:17:58.327 UTC",
        "Owner_location":"Gothenburg, Sweden",
        "Owner_reputation":1547,
        "Owner_up_votes":28,
        "Owner_down_votes":9,
        "Owner_views":212,
        "Answer_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2021-06-16 16:00:05.94 UTC",
        "Answer_last_edit_date":"2021-06-16 20:39:37.407 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":"2021-06-16 19:58:05.37 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to add a file to a dvc-tracked folder without pulling the whole folder's content?",
        "Question_body":"<p>Let's say I am working inside a git\/dvc repo. There is a folder <code>data<\/code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc add data\n<\/code><\/pre>\n<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.<\/p>\n<p>I clone the repo on another workspace, and now I have the <code>data.dvc<\/code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt<\/code> to the <code>data<\/code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data<\/code> locally<\/em> ?<\/p>\n<p>What I have tried for now:<\/p>\n<ol>\n<li><p>Adding the <code>data<\/code> folder again:<\/p>\n<pre><code>mkdir data\nmv path\/to\/newfile.txt data\/newfile.txt\ndvc add data\n<\/code><\/pre>\n<p>The <code>data.dvc<\/code> file is built again from the local state of <code>data<\/code> which only contains <code>newfile.txt<\/code> so this doesn't work.<\/p>\n<\/li>\n<li><p>Adding the file as a single element in <code>data<\/code> folder:<\/p>\n<pre><code> dvc add data\/newfile.txt\n<\/code><\/pre>\n<p>I get :<\/p>\n<pre><code> Cannot add 'data\/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. \n To include 'data\/newfile.txt' in 'data', run 'dvc commit data.dvc'\n<\/code><\/pre>\n<\/li>\n<li><p>Using dvc commit as suggested<\/p>\n<pre><code> mkdir data\n mv path\/to\/newfile.txt data\/newfile.txt\n dvc commit data.dvc\n<\/code><\/pre>\n<p>Similarly as 1., the <code>data.dvc<\/code> is rebuilt again from local state of <code>data<\/code>.<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-05-06 15:25:19.22 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1781,
        "Owner_creation_date":"2021-05-06 14:35:20.937 UTC",
        "Owner_last_access_date":"2021-07-20 09:38:49.723 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-05-06 15:28:32.033 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-16 07:56:22.283 UTC",
        "Question_favorite_count":0.0,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_date":"2019-05-22 12:54:44.193 UTC",
        "Owner_last_access_date":"2022-01-15 10:43:18.997 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2020-04-16 09:36:01.967 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":6.0,
        "Question_last_edit_date":"2020-07-13 11:33:41.92 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-21 14:01:34.277 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_date":"2020-10-22 15:00:31.587 UTC",
        "Owner_last_access_date":"2022-09-19 00:28:34.893 UTC",
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-22 12:58:00.163 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How does DVC store differences on the directory level into DVC cache?",
        "Question_body":"<p>Can someone explain how DVC stores differences on the directory level into DVC cache. <\/p>\n\n<p>I understand that the DVC-files (.dvc) are metafiles to track data, models and reproduce pipeline stages. However, it is not clear for me how the process of creating branches, commiting them and switching back to a master file is exactly saved in differences. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-04 13:28:05.69 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"version-control|dvc",
        "Question_view_count":623,
        "Owner_creation_date":"2020-03-04 07:53:32.843 UTC",
        "Owner_last_access_date":"2020-04-23 11:10:57.24 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-03-04 14:30:01.397 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"dvc.api.read() raises an \"UnicodeDecodeError\"",
        "Question_body":"<p>I am trying to acess a DICOM file [image saved in the Digital Imaging and Communications in Medicine (DICOM) format]:<\/p>\n<pre><code>import dvc.api\n\npath = 'dir\/image.dcm'\nremote = 'remote_name'\nrepo = 'git_repo'\nmode = 'r'\n\ndata = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode)\n<\/code><\/pre>\n<p>When I run the previous code, and after the &quot;downloading progress bar&quot; is complete, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r') File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\api.py&quot;, line 91, in read return fd.read() File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\encodings\\cp1252.py&quot;, line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 764: character maps to &lt;undefined&gt;\n<\/code><\/pre>\n<p>I tried to overcome this issue by using the encoding argument:<\/p>\n<pre><code>data = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode, encoding='ANSI')\n<\/code><\/pre>\n<p>Since, when I open a DICOM file using for example Notepad++, this is the encoding specified. However, it raises the error:<\/p>\n<pre><code>Exception ignored in: &lt;bound method Pool.__del__ of &lt;dvc.fs.pool.Pool object at 0x0000021D1347A160&gt;&gt; Traceback (most recent call last): File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\pool.py&quot;, line 42, in __del__ File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\pool.py&quot;, line 46, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\ssh\\connection.py&quot;, line 71, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp_client.py&quot;, line 194, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp_client.py&quot;, line 185, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp.py&quot;, line 158, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1372, in log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1441, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1411, in makeRecord TypeError: 'NoneType' object is not callable\n<\/code><\/pre>\n<p>I also tried <code>encoding = 'utf-8'<\/code>, but the &quot;UnicodeDecodeError&quot; continues to appear:<\/p>\n<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r', encoding='utf-8') File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\ccab_env_dev\\lib\\site-packages\\dvc\\api.py&quot;, line 91, in read return fd.read() File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\ccab_env_dev\\lib\\codecs.py&quot;, line 321, in decode (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 140: invalid continuation byte\n<\/code><\/pre>\n<p>Can anyone please help? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2021-08-11 13:32:25.447 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|encoding|dvc",
        "Question_view_count":186,
        "Owner_creation_date":"2019-09-30 10:15:09.623 UTC",
        "Owner_last_access_date":"2022-09-19 14:01:39.763 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-11 15:09:10.723 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-17 21:48:47.92 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_date":"2020-04-19 15:08:15.27 UTC",
        "Owner_last_access_date":"2022-09-22 19:13:07.327 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-18 03:51:17.717 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2022-06-18 03:55:59.503 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-02 20:42:34.297 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"version-control|dvc",
        "Question_view_count":139,
        "Owner_creation_date":"2013-06-07 18:26:33.7 UTC",
        "Owner_last_access_date":"2022-09-14 18:00:36.65 UTC",
        "Owner_location":"Colorado Springs, CO",
        "Owner_reputation":11685,
        "Owner_up_votes":2855,
        "Owner_down_votes":47,
        "Owner_views":1329,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-11-02 21:12:41.433 UTC",
        "Answer_last_edit_date":"2020-11-03 00:16:03.343 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2020-11-02 23:50:23.337 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc",
        "Question_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-06 20:35:30.497 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|gitlab-ci-runner|mlops|dvc|cml",
        "Question_view_count":23,
        "Owner_creation_date":"2019-12-12 05:07:25.14 UTC",
        "Owner_last_access_date":"2022-09-24 21:10:52.373 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to track a folder again when used \"git rm -rf --cached folder_name\" : Error: The following paths are ignored by one of your .gitignore files",
        "Question_body":"<p>I wanted to un-track my git files so I put <code>.dvc<\/code> inside my <code>.gitignore<\/code> file, and run<\/p>\n<pre><code>git rm -rf --cached .dvc\n<\/code><\/pre>\n<p>and then committed.<\/p>\n<p>I realised my mistake soon and then wanted to add the files again . I tried deleting the <code>gitignore<\/code> file, commit, make a new <code>.gitignore<\/code> and then try adding but all is futile. <code>git add .dvc<\/code> does not track my files and using <code>git add .dvc\/*<\/code> gives me error:<\/p>\n<pre><code>The following paths are ignored by one of your .gitignore files:\n.dvc\/cache\n.dvc\/tmp\nUse -f if you really want to add them.\n<\/code><\/pre>\n<p>Running the command <code>git check-ignore -v .dvc\/*<\/code> gives me:<\/p>\n<pre><code>.dvc\/.gitignore:3:\/cache    .dvc\/cache\n.dvc\/.gitignore:2:\/tmp  .dvc\/tmp\n\n<\/code><\/pre>\n<p>What can be done now?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2022-06-16 12:13:05.17 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"git|gitignore|dvc",
        "Question_view_count":41,
        "Owner_creation_date":"2019-07-01 15:44:17.583 UTC",
        "Owner_last_access_date":"2022-09-24 15:58:25.223 UTC",
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":2716,
        "Owner_up_votes":403,
        "Owner_down_votes":26,
        "Owner_views":668,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-06-16 12:23:53.167 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-04 23:20:53.717 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_date":"2013-09-20 14:02:00.45 UTC",
        "Owner_last_access_date":"2022-09-21 22:02:08.603 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-04 23:40:51.067 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_last_edit_date":"2021-05-04 23:30:22.37 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-28 20:10:29.793 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_date":"2021-04-12 19:17:42.697 UTC",
        "Owner_last_access_date":"2022-02-22 18:42:26.683 UTC",
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-05-29 03:09:19.21 UTC",
        "Answer_last_edit_date":"2021-05-31 23:24:13.297 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-05-29 03:04:51.983 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC Experiment management workflow",
        "Question_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-03 13:38:53.127 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"git|dvcs|dvc",
        "Question_view_count":152,
        "Owner_creation_date":"2022-01-28 12:42:49.633 UTC",
        "Owner_last_access_date":"2022-09-21 13:14:46.48 UTC",
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2022-03-03 15:05:59.073 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"problems installing a DVC lower version [0.9.4]",
        "Question_body":"<p>I need to install an older version of DVC, namely 0.9.4, in a Python virtual environment.<\/p>\n<p>I used the command:<\/p>\n<pre><code>pip install dvc==0.9.4\n<\/code><\/pre>\n<p>Everything seemed to work fine. However, when I try to run a <code>dvc pull<\/code> command, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\Scripts\\dvc.exe\\__main__.py&quot;, line 4, in &lt;module&gt;\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\main.py&quot;, line 2, in &lt;module&gt;\n    from dvc.cli import parse_args\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cli.py&quot;, line 8, in &lt;module&gt;\n    from dvc.command.init import CmdInit\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\command\\init.py&quot;, line 1, in &lt;module&gt;\n    from dvc.project import Project\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\project.py&quot;, line 15, in &lt;module&gt;\n    from dvc.cloud.data_cloud import DataCloud\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\data_cloud.py&quot;, line 11, in &lt;module&gt;\n    from dvc.cloud.gcp import DataCloudGCP\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\gcp.py&quot;, line 4, in &lt;module&gt;\n    from google.cloud import storage as gc\nModuleNotFoundError: No module named 'google.cloud'\n<\/code><\/pre>\n<p>When I print the dvc version, I see:<\/p>\n<pre><code>0.9.4+6bb66e.mod\n<\/code><\/pre>\n<p>Can anyone please help? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-13 12:01:16.7 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|google-cloud-storage|dvc",
        "Question_view_count":234,
        "Owner_creation_date":"2019-09-30 10:15:09.623 UTC",
        "Owner_last_access_date":"2022-09-19 14:01:39.763 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-13 19:21:41.66 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Getting this weird error when trying to run DVC pull",
        "Question_body":"<p>I am new to using DVC and just exploring it. I am trying to pull data from s3 that was pushed by another person on my team. But I am getting this error:<\/p>\n\n<pre><code>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\nname: head_test_file.csv, md5: 45db668193ba44228d61115b1d0304fe\nWARNING: Cache '45db668193ba44228d61115b1d0304fe' not found. File 'head_test_file.csv' won't be created.\nNo changes.\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\nhead_test_file.csv\nDid you forget to fetch?\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-26 05:39:46.813 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":7063,
        "Owner_creation_date":"2017-04-11 13:31:59.307 UTC",
        "Owner_last_access_date":"2022-03-29 20:52:48.753 UTC",
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Undo changes in pandas Dataframe(column drops, row drops, edits performed on a single cell)",
        "Question_body":"<p>I am currently working on developing a 'undo' operation for my interface that deals with changes performed on csv files. I want to provide an option for the user to revert the changes that he had done to the csv file, these changes include edit a cell, deleting column, deleting row, adding row, adding column etc. For this I want to know, does version control works in this scenario? If yes, which data version control should I prefer? If not, please suggest me an another alternative.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-16 06:15:34.273 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|pandas|git|version-control|dvc",
        "Question_view_count":39,
        "Owner_creation_date":"2022-06-16 06:09:28.507 UTC",
        "Owner_last_access_date":"2022-07-21 05:33:22.853 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-06-20 09:15:41.577 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Question_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-23 18:31:41.247 UTC",
        "Question_favorite_count":3.0,
        "Question_score":7,
        "Question_tags":"python|sql|r|git|dvc",
        "Question_view_count":689,
        "Owner_creation_date":"2017-08-30 12:46:30.907 UTC",
        "Owner_last_access_date":"2022-03-11 18:10:58.673 UTC",
        "Owner_location":null,
        "Owner_reputation":1365,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":193,
        "Answer_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-02-23 19:57:47.857 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":12.0,
        "Question_last_edit_date":"2020-02-23 19:28:48.287 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Data version control (DVC) edit files in place results in cyclic dependency",
        "Question_body":"<p>we have a larger dataset and have several preprocessing scripts.\nThese scripts alter data in place.\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).\nI would assume this is a very common use case.<\/p>\n\n<p>What is the best practice here ?<\/p>\n\n<p>Tried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2020-04-23 12:04:31.83 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":45,
        "Owner_creation_date":"2018-06-26 18:15:36.153 UTC",
        "Owner_last_access_date":"2022-06-24 08:39:18.773 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":121,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How do launch experiments in DVC?",
        "Question_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-06-02 09:10:42.84 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":167,
        "Owner_creation_date":"2021-06-12 15:13:14.51 UTC",
        "Owner_last_access_date":"2022-09-23 11:35:31.3 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-03 08:28:54.587 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-16 20:34:25.79 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_date":"2011-07-22 10:25:49.88 UTC",
        "Owner_last_access_date":"2022-09-21 15:11:42.327 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-18 03:39:31.8 UTC",
        "Answer_last_edit_date":"2022-06-20 06:21:51.94 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-06-16 21:37:07.55 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC pull returns ERROR: configuration error - Failed to authenticate GDrive remote: name: drive version: v2",
        "Question_body":"<p>I ran this github actions workflow with several variations, but I cannot pull the data from DVC.<\/p>\n<pre><code>name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: iterative\/setup-dvc@v1\n      - name: Get data\n        run: |\n          echo '---'\n          echo GDRIVE_CREDENTIALS_DATA: $GDRIVE_CREDENTIALS_DATA\n          echo '---'\n          #pip list\n          dvc remote default storage\n          #dvc remote modify storage --local gdrive_use_service_account true\n          #dvc remote modify storage --local gdrive_service_account_json_file_path .dvc\/gdrive-access.json\n          dvc pull\n        env:\n          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          #GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}   \n      - name: Install requirements\n        run: |\n          pip install -r requirements.txt\n      - name: Run tests\n        run: python src\/test.py  \n<\/code><\/pre>\n<p>GDRIVE_CREDENTIALS_DATA is populated with the contents of the json file that I donwloaded from google and it was tested on two other computers. I tried using the environment variable as well as adding the json file to the repo to see if it would work. But no. I am getting this error message:<\/p>\n<blockquote>\n<p>ERROR: configuration error - Failed to authenticate GDrive remote: name: drive  version: v2\nERROR: Failed to authenticate GDrive remote\nLearn more about configuration settings at <a href=\"https:\/\/man.dvc.org\/remote\/modify\" rel=\"nofollow noreferrer\">https:\/\/man.dvc.org\/remote\/modify<\/a>.\nError: Process completed with exit code 251.<\/p>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-18 15:41:49.77 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":129,
        "Owner_creation_date":"2015-10-16 01:35:39.707 UTC",
        "Owner_last_access_date":"2022-09-22 23:31:19.043 UTC",
        "Owner_location":null,
        "Owner_reputation":6513,
        "Owner_up_votes":1296,
        "Owner_down_votes":60,
        "Owner_views":1057,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Version control for machine learning data set with large amount of images?",
        "Question_body":"<p>We starting to use <a href=\"https:\/\/dvc.org\/\" rel=\"noreferrer\">dvc<\/a>  with git to control versioning of machine learning projects.\nFor dvc remote storage  we use google cloud storage.<\/p>\n\n<p>Our data set is OCR data set with more than 100000 small images, total size is about 200 MB.\nUsing  dvc to track this data set we encountered with next  problems:<\/p>\n\n<ol>\n<li>It took a lot of time to add data set for tracking.<\/li>\n<li>Very slow upload.<\/li>\n<li>Very slow download.<\/li>\n<li>Update\/delete\/add just one image in data set cause dvc to recompute\na lot of things : hashes etc....<\/li>\n<\/ol>\n\n<p>From another way if we zipping our data set and track it as single file  dvc work fast enough.But the problem is in this way we can't track changes for particular file.<\/p>\n\n<p>The goal is to have version control for data set with large amount of files with next functionality.<\/p>\n\n<ol>\n<li>Tracking for each single file.<\/li>\n<li>Committing only changes and not whole data set.<\/li>\n<li>Fast checkout\/pull<\/li>\n<\/ol>\n\n<p>Any suggestion for better solution acceptable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2019-05-08 07:33:24.753 UTC",
        "Question_favorite_count":5.0,
        "Question_score":10,
        "Question_tags":"git|machine-learning|google-cloud-storage|dvc",
        "Question_view_count":856,
        "Owner_creation_date":"2014-07-09 13:28:02.227 UTC",
        "Owner_last_access_date":"2020-11-15 16:01:42.947 UTC",
        "Owner_location":null,
        "Owner_reputation":321,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored",
        "Question_body":"<p>I just started with DVC. I have a git repo in which there are heavy models that i want to push to dvc. So I initialized the dvc by<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>and then configured the bucket<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>Now there is <code>\/models<\/code> folders, in which there was <code>.gitkeep<\/code> file and trained models. Following entry was in my <code>.gitignore<\/code><\/p>\n<pre><code>*.tar.gz\n<\/code><\/pre>\n<p>I ran the following command<\/p>\n<pre><code>git rm -r --cached my_server\\models\n<\/code><\/pre>\n<p>and added the following in the <code>.gitignore<\/code><\/p>\n<pre><code>models\n<\/code><\/pre>\n<p>I want to add all the <code>tar.gz<\/code> files to push on dvc<\/p>\n<p>so i tried<\/p>\n<pre><code>dvc add .\/my_server\/models\/*.tar.gz\n<\/code><\/pre>\n<p>but this is showing<\/p>\n<pre><code>ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored.\n<\/code><\/pre>\n<p>If I do\ndvc add .\/my_server\/models\/<\/p>\n<p>then this folder is added and a <code>models.dvc<\/code> file gets created. then git code shows for the changes.<\/p>\n<p>what is the correct way, do i need to mention <code>*.dvc<\/code> to <code>.gitignore<\/code> as well?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2021-09-20 12:34:56.913 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":760,
        "Owner_creation_date":"2013-03-15 04:43:52.587 UTC",
        "Owner_last_access_date":"2022-09-25 05:38:43.07 UTC",
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-21 15:08:06.743 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-07-04 06:12:59.043 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_date":"2019-07-04 06:06:43.123 UTC",
        "Owner_last_access_date":"2019-07-18 00:21:59.09 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Readding missing files to DVC",
        "Question_body":"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull<\/code> I get the output<\/p>\n\n<pre><code>[##############################] 100% Analysing status.\nWARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data\/2.mp4' won't be created.\nWARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data\/3.mp4' won't be created.\n<\/code><\/pre>\n\n<p>The <code>dvc status<\/code> command gives me:<\/p>\n\n<pre><code>data\/2.mp4.dvc:\n    changed outs:\n        not in cache:       data\/2.mp4\ndata\/3.mp4.dvc:\n    changed outs:\n        not in cache:       data\/3.mp4\n<\/code><\/pre>\n\n<p>It seems that <code>2.mp4<\/code> and <code>3.mp4<\/code> where added under dvc control but <code>dvc push<\/code> command has not been executed.<\/p>\n\n<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:<\/p>\n\n<pre><code>dvc remove data\/2.mp4.dvc\ndvc remove data\/3.mp4.dvc\n\ndvc add data\/2.mp4 \ndvc add data\/3.mp4 \n<\/code><\/pre>\n\n<p>But there is no effect. How can I remove files from under dvc control and add them again?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-23 06:41:59.503 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1065,
        "Owner_creation_date":"2018-03-28 16:31:38.71 UTC",
        "Owner_last_access_date":"2022-09-23 10:08:33.687 UTC",
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-05-23 18:46:43.487 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Question_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-04 14:51:05.907 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"google-client|dvc",
        "Question_view_count":979,
        "Owner_creation_date":"2020-06-24 13:54:59.21 UTC",
        "Owner_last_access_date":"2022-09-25 01:13:01.88 UTC",
        "Owner_location":null,
        "Owner_reputation":704,
        "Owner_up_votes":53,
        "Owner_down_votes":6,
        "Owner_views":33,
        "Answer_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-03-04 15:58:09.453 UTC",
        "Answer_last_edit_date":"2021-03-05 06:56:47.287 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":"2021-03-04 16:12:18.737 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-17 03:12:07.047 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_date":"2013-11-05 00:28:27 UTC",
        "Owner_last_access_date":"2022-09-24 23:19:30.95 UTC",
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2019-09-17 04:18:09.197 UTC",
        "Answer_last_edit_date":"2019-09-17 13:12:46.083 UTC",
        "Answer_score":7.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-30 07:27:18.393 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_date":"2018-03-28 16:31:38.71 UTC",
        "Owner_last_access_date":"2022-09-23 10:08:33.687 UTC",
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-30 08:57:26.627 UTC",
        "Answer_last_edit_date":"2019-09-30 09:51:09.043 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation:",
        "Question_body":"<p>my ~\/.aws\/credentials looks like<\/p>\n<pre><code>[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<pre><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<p>I have made my .dvc\/config.local to look like<\/p>\n<pre><code>[\u2018remote \u201cmyremote\u201d\u2019]\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test\naccess_key_id = XYZ\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS\nprofile=\u2018testing\u2019\ncredentialpath = \/Users\/nyt21\/.aws\/credentials\n<\/code><\/pre>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n<\/blockquote>\n<p>** Update\nhere is the output of <code>dvc push -v<\/code><\/p>\n<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.\n2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3:\/\/bib-ds-models-testing\/data\/dvc-test'\n2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3:\/\/bib-ds-models-testing\/data\/dvc-test\n2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...\n2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     \n2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes\n2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists\n2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          \n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1057, in _info\n    out = await self._simple_info(path)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 970, in _simple_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Access Denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 50, in do_run\n    return self.run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 51, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 44, in push\n    pushed += self.cloud.push(objs, jobs, remote=remote)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 79, in push\n    return remote_obj.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 57, in wrapper\n    return f(obj, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 494, in push\n    ret = self._process(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 351, in _process\n    dir_status, file_status, dir_contents = self._status(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 195, in _status\n    self.hashes_exist(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 145, in hashes_exist\n    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 438, in hashes_exist\n    remote_hashes = self.list_hashes_exists(hashes, jobs, name)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 389, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 619, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 444, in result\n    return self.__get_result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 389, in __get_result\n    raise self._exception\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 380, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 92, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 87, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 68, in sync\n    raise result[0]\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 24, in _runner\n    result[0] = await coro\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 802, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1061, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1004, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-07-25 22:40:39,712 DEBUG: Version info for developers:\nDVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk3s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk3s1s1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.\n2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n<\/code><\/pre>\n<p>I can upload through python<\/p>\n<pre><code>import boto3\nimport os\nimport pickle\n\nbucket_name = 'bib-ds-models-testing'\nos.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;\nsession = boto3.Session()\ns3_client = boto3.client('s3')\n\ns3_client.upload_file('\/Users\/nyt21\/Devel\/DVC\/test\/data\/iris.csv',\n    'bib-ds-models-testing',\n    'data\/dvc-test\/my_iris.csv')\n<\/code><\/pre>\n<p>I don't use aws CLI but the following also gives an access deny !<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDenied) when calling the ListObjectsV2\noperation: Access Denied<\/p>\n<\/blockquote>\n<p>but it works if I add --profile=testing<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test --profile=testing\n                       \n<\/code><\/pre>\n<blockquote>\n<p>PRE dvc-test\/<\/p>\n<\/blockquote>\n<p>just you know environment variable <code>AWS_PROFILE<\/code> is already set to 'testing'<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>I have tried both <code>AWS_PROFILE='testing'<\/code> and <code>AWS_PROFILE=testing<\/code>, neither of them worked.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_date":"2021-07-25 10:04:37.173 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":1526,
        "Owner_creation_date":"2011-02-10 16:54:52.887 UTC",
        "Owner_last_access_date":"2022-09-21 18:56:24.013 UTC",
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":4988,
        "Owner_up_votes":350,
        "Owner_down_votes":22,
        "Owner_views":416,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-07-27 10:54:57.973 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-05-05 19:35:18.287 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_date":"2021-05-04 12:52:20.443 UTC",
        "Owner_last_access_date":"2022-01-11 19:04:00.7 UTC",
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-06 09:13:21.493 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-02-17 10:01:04.283 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_date":"2010-02-09 19:11:11.2 UTC",
        "Owner_last_access_date":"2022-07-26 11:14:45.253 UTC",
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-02-17 15:51:31.963 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2022-02-17 21:13:50.277 UTC",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Problem running a Docker container in Gitlab CI\/CD",
        "Question_body":"<p>I am trying to build and run my Docker image using Gitlab CI\/CD, but there is one issue I can't fix even though locally everything works well.<\/p>\n<p>Here's my Dockerfile:<\/p>\n<pre><code>FROM &lt;internal_docker_repo_image&gt;\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc\nCOPY requirements.txt \/requirements.txt\n\nRUN pip install --no-cache-dir --user -r \/requirements.txt\n\nCOPY . \/src\nWORKDIR \/src\nENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;dvc&quot;, &quot;repro&quot;]\n<\/code><\/pre>\n<p>This is how I run the container:<\/p>\n<p><code>docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force<\/code><\/p>\n<p>Everything works great when running this locally, but it fails when run on Gitlab CI\/CD.<\/p>\n<pre><code>stages:\n  - build_image\n\nbuild_image:\n  stage: build_image\n  image: &lt;internal_docker_repo_image&gt;\n  script:\n    - echo &quot;Building Docker image...&quot;\n    - mkdir ~\/.docker\n    - cat $GOOGLE_CREDENTIALS &gt; ${CI_PROJECT_DIR}\/key.json\n    - docker build . -t &lt;image_name&gt;\n    - docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force\n  artifacts:\n        paths:\n          - &quot;.\/data\/*csv&quot;\n        expire_in: 1 week\n\n<\/code><\/pre>\n<p>This results in the following error:\n<code>ERROR: you are not inside of a DVC repository (checked up to mount point '\/src')<\/code><\/p>\n<p>Just in case you don't know what DVC is, this is a tool used in machine learning for versioning your models, datasets, metrics, and, in addition, setting up your pipelines, which I use it for in my case.<\/p>\n<p>Essentially, it requires two folders <code>.dvc<\/code> and <code>.git<\/code> in the directory from which <code>dvc repro<\/code> is executed.<\/p>\n<p>In this particular case, I have no idea why it's not able to run this command given that the contents of the folders are exactly the same and both <code>.dvc<\/code> and <code>.git<\/code> exist.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-12 14:44:55.847 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"docker|continuous-integration|gitlab-ci|dvc",
        "Question_view_count":746,
        "Owner_creation_date":"2016-01-07 12:19:30.337 UTC",
        "Owner_last_access_date":"2022-09-24 05:18:01.01 UTC",
        "Owner_location":null,
        "Owner_reputation":576,
        "Owner_up_votes":431,
        "Owner_down_votes":4,
        "Owner_views":68,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Azure DataLake with DVC",
        "Question_body":"<p>We are thinking to use DVC for versioning input data for DataScience project.\nmy data resides in Azure DataLake Gen1.<\/p>\n\n<p>how do i configure DVC to push data to Azure DataLake using Service Principal?\ni want DVC to store cache and data into Azure DataLake instead on local disk.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2019-01-23 09:38:51.56 UTC",
        "Question_favorite_count":1.0,
        "Question_score":5,
        "Question_tags":"azure-data-lake|service-principal|dvc",
        "Question_view_count":473,
        "Owner_creation_date":"2009-11-10 14:07:58.92 UTC",
        "Owner_last_access_date":"2022-09-09 06:39:04.55 UTC",
        "Owner_location":"Pune, India",
        "Owner_reputation":6219,
        "Owner_up_votes":227,
        "Owner_down_votes":7,
        "Owner_views":587,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"Data version control (DVC) commands not working ---> TypeError: public() got an unexpected keyword argument 'SEP'",
        "Question_body":"<p>All of a sudden, dvc has stopped functioning.\nAny command typed fails and throws an exception.\nexample. dvc remote list results in -<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/home\/dev2\/.local\/bin\/dvc&quot;, line 5, in &lt;module&gt;\n    from dvc.main import main\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/main.py&quot;, line 6, in &lt;module&gt;\n    from dvc import analytics\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/analytics.py&quot;, line 16, in &lt;module&gt;\n    from dvc.lock import Lock, LockError\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/lock.py&quot;, line 8, in &lt;module&gt;\n    import flufl.lock\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/__init__.py&quot;, line 3, in &lt;module&gt;\n    from flufl.lock._lockfile import (\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/_lockfile.py&quot;, line 54, in &lt;module&gt;\n    public(SEP=SEP)\nTypeError: public() got an unexpected keyword argument 'SEP'\n \n<\/code><\/pre>\n<p>Any suggestions will be of great help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2020-06-23 07:48:46.37 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"linux|git|dvc",
        "Question_view_count":307,
        "Owner_creation_date":"2018-08-13 13:04:32.313 UTC",
        "Owner_last_access_date":"2022-09-19 22:24:23.697 UTC",
        "Owner_location":null,
        "Owner_reputation":449,
        "Owner_up_votes":3,
        "Owner_down_votes":2,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_title":"DVC dependencies for derived data without imports",
        "Question_body":"<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.<\/p>\n\n<p><strong>My question:<\/strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import<\/code> to achieve this.<\/p>\n\n<p><strong>Details:<\/strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.<\/p>\n\n<p>Currently my Idea is to <code>dvc get<\/code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.<\/p>\n\n<pre><code>hugeFileRepo\n +metaData.csv\n +dataFolder\n +-- hugeFile_1\n ...\n +-- hugeFile_n\n<\/code><\/pre>\n\n<p>in the derivedData repository I do<\/p>\n\n<pre><code> dvc import hugeFileRepo.git metaData.csv\n dvc run -f derivedData.dvc \\\n    -d metaData.csv \\\n    -d deriveData.py \\\n    -o derivedDataFolder \\\n    python deriveData.py \n<\/code><\/pre>\n\n<p>My deriveData.py does something along the line (pseudocode)<\/p>\n\n<pre><code>metaData = read(metaData.csv)\n\n#Hack because I don't know how to it right:\ngitRevision = getGitRevision(metaData.csv.dvc)          \n...\nfor metaDataForFile, file in metaData:\n   if(iWantFile(metaDataForFile) ):\n      #download specific file\n      !dvc get --rev {gitRevision} -o tempFolder\/{file} hugeFileRepo.git {file}\n\n      #do processing of huge file and store result in derivedDataFolder\n      processAndWrite(tempFolder\/file)\n<\/code><\/pre>\n\n<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-15 09:52:02.927 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":179,
        "Owner_creation_date":"2016-01-25 12:22:13.5 UTC",
        "Owner_last_access_date":"2022-09-13 09:05:05.923 UTC",
        "Owner_location":null,
        "Owner_reputation":171,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"DVC"
    }
]