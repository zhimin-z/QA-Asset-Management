[
    {
        "Question_title":"How to safely shutdown mlflow ui?",
        "Question_body":"<p>After running <code>mlflow ui<\/code> on a remote server, I'm unable to reopen the <code>mlflow ui<\/code> again.<br>\nA workaround is to kill all my processes in the server using <code>pkill -u MyUserName<\/code>.<br>\nOtherwise I get the following error:  <\/p>\n\n<pre><code>[INFO] Starting gunicorn 20.0.4  \n[ERROR] Connection in use: ('127.0.0.1', 5000)\n[ERROR] Retrying in 1 second.  \n...\nRunning the mlflow server failed. Please see ther logs above for details.\n<\/code><\/pre>\n\n<p>I understand the error but I don't understand:<br>\n1. What is the correct way to shutdown <code>mlflow ui<\/code><br>\n2. How can I identify the <code>mlflow ui<\/code> process in order to only kill that process and not use the <code>pkill<\/code>  <\/p>\n\n<p>Currently I close the browser or use ctrl+C <\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-04 17:00:47.02 UTC",
        "Question_favorite_count":2.0,
        "Question_score":6,
        "Question_tags":"python|r|machine-learning|mlflow",
        "Question_view_count":9850,
        "Owner_creation_date":"2016-01-20 20:19:09.903 UTC",
        "Owner_last_access_date":"2022-09-11 19:40:57.297 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":1153,
        "Owner_up_votes":113,
        "Owner_down_votes":14,
        "Owner_views":168,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-31 18:18:00.327 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 14:13:41.443 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_date":"2020-04-10 11:23:52.39 UTC",
        "Owner_last_access_date":"2022-08-12 18:19:53.33 UTC",
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-17 14:26:33.837 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-11-17 14:19:26.987 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Can't edit the cluster created by mlflow model serving",
        "Question_body":"<p>I'm trying to deploy  Machine learning model into databricks production using mlflow. while in that process, I have registered the model to mlflow models. After that it created the cluster but then it was in pending state forever. when I checked the model events, I see a problem with https proxy, we have global init scripts which contain proxy information.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qF9MT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qF9MT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Ref: <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/model-serving.html\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/applications\/mlflow\/model-serving.html<\/a><\/p>\n<p>so the only way for us to edit the cluster and add them but in that process we are getting an error &quot;error: Cannot edit cluster created by ModelServing&quot;.<\/p>\n<pre><code>[Errno 101] Network is unreachable',)': \/simple\/mlflow\/ WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f258247f710&gt;: Failed to establish a new connection:\n<\/code><\/pre>\n<p>In the &quot;Model Events page&quot;, I see the above logs,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-10 23:07:09.1 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":125,
        "Owner_creation_date":"2016-11-15 06:12:07.737 UTC",
        "Owner_last_access_date":"2022-08-15 17:25:10.4 UTC",
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow UI can't show artifacts",
        "Question_body":"<p>I have mlflow running on an azure VM and connected to Azure Blob as the artifact storage.<\/p>\n<p>After uploading artifacts to the storage from the Client.<\/p>\n<p>I tried the MLflow UI and successfully was able to show the uploaded file.<\/p>\n<p>The problem happens when I try to run MLFLOW with Docker, I get the error:\n<strong>Unable to list artifacts stored under <code>{artifactUri}<\/code> for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory<\/strong><\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM python:3.7-slim-buster\n# Install python packages\nRUN pip install mlflow pymysql\n\nRUN pip install azure-storage-blob\n\nENV AZURE_STORAGE_ACCESS_KEY=&quot;#########&quot;\nENV AZURE_STORAGE_CONNECTION_STRING=&quot;#######&quot;\n<\/code><\/pre>\n<p>docker-compose.yml<\/p>\n<pre><code>web:\n        restart: always\n        build: .\/mlflow_server\n        image: mlflow_server\n        container_name: mlflow_server\n        expose:\n            - &quot;5000&quot;\n        networks:\n            - frontend\n            - backend\n        environment:\n            - AZURE_STORAGE_ACCESS_KEY=&quot;#####&quot;\n            - AZURE_STORAGE_CONNECTION_STRING=&quot;#####&quot;\n        command: mlflow server --backend-store-uri mysql+pymysql:\/\/mlflow_user:123456@db:3306\/mlflow --default-artifact-root wasbs:\/\/etc..\n<\/code><\/pre>\n<p>I tried multiple solutions:<\/p>\n<ol>\n<li>Making sure that boto3 is installed (Didn't do anything)<\/li>\n<li>Adding Environment Variables in the Dockerfile so the command runs after they're set<\/li>\n<li>I double checked the url of the storage blob<\/li>\n<\/ol>\n<p>And MLFLOW doesn't show any logs it just kills the process and restarts again.<\/p>\n<p>Anyone got any idea what might be the solution or how can i access the logs<\/p>\n<p>here're the docker logs of the container:<\/p>\n<pre><code>[2022-07-28 12:23:33 +0000] [10] [INFO] Starting gunicorn 20.1.0\n[2022-07-28 12:23:33 +0000] [10] [INFO] Listening at: http:\/\/0.0.0.0:5000 (10)\n[2022-07-28 12:23:33 +0000] [10] [INFO] Using worker: sync\n[2022-07-28 12:23:33 +0000] [13] [INFO] Booting worker with pid: 13\n[2022-07-28 12:23:33 +0000] [14] [INFO] Booting worker with pid: 14\n[2022-07-28 12:23:33 +0000] [15] [INFO] Booting worker with pid: 15\n[2022-07-28 12:23:33 +0000] [16] [INFO] Booting worker with pid: 16\n[2022-07-28 12:24:24 +0000] [10] [CRITICAL] WORKER TIMEOUT (pid:14)\n[2022-07-28 12:24:24 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2022-07-28 12:24:24 +0000] [21] [INFO] Booting worker with pid: 21\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-28 12:43:31.5 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|docker-compose|azure-blob-storage|mlflow",
        "Question_view_count":97,
        "Owner_creation_date":"2022-07-28 12:16:55.837 UTC",
        "Owner_last_access_date":"2022-09-19 22:11:22.013 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Migrating Runs to MLFlow 0.9",
        "Question_body":"<p>we have been using MLFlow 0.8.2 (with a local file store) for a while, and I was happy to see the release of MLFlow 0.9. After upgrading to the new version, I realized that pointing the MLFLow server to the old file store leads to a non-working web UI (I just see some image of a waterfall).<\/p>\n\n<p>Is there a recommendation for proper migration of data when upgrading?<\/p>\n\n<p>Thanks a lot in advance,<\/p>\n\n<p>Da<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-16 19:29:35.623 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":49,
        "Owner_creation_date":"2014-06-23 05:48:30.47 UTC",
        "Owner_last_access_date":"2019-05-09 18:45:09.163 UTC",
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to create a custom pyfunc to make predictions using a model that requires an input shape with more than two dimensions using MLflow?",
        "Question_body":"<p>I'm new to TensorFlow and MLFlow and I have a similar problem to the one asked in <a href=\"https:\/\/stackoverflow.com\/questions\/58917918\/how-to-make-predictions-using-a-model-that-requires-an-input-shape-with-more-tha\/60416642#60416642\">here<\/a>. I am implementing a TensorFlow model to predict timeseries values. With that aim, I used MLFlow's mlflow.tensorflow.autolog() to track and serve the models in my instance. Nevertheless, since my input shape has more than 2 dimensions, I was not able to use that method. <\/p>\n\n<p>As previously <a href=\"https:\/\/stackoverflow.com\/a\/60129726\/8338272\">suggested<\/a>, I've tried to encode\/decode the input data in prediction by using a custom pyfunc to do it. <\/p>\n\n<p>In this way, I have a function <em>model_test.py<\/em> with a predict method that decode its input data that is:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport os\nimport json\nimport mlflow\nimport numpy as np\nimport pandas as pd\nfrom mlflow.pyfunc import PythonModel\nimport tensorflow as tf\nimport base64\n\n\n\nclass ModelTest(PythonModel):\n\n    def __init__(self, estimator=None,window_size = 64,batch_size = 256,shuffle_buffer_size = 100):\n        # CODE TO CREATE THE EXPERIMENT\n        self.window_size = window_size\n        self.batch_size = batch_size\n        self.shuffle_buffer_size = shuffle_buffer_size\n\n    def windowed_dataset(self,series, window_size, batch_size, shuffle_buffer):\n            series = tf.expand_dims(series, axis=-1)\n            ds = tf.data.Dataset.from_tensor_slices(series)\n            ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n            ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n            ds = ds.shuffle(shuffle_buffer)\n            ds = ds.map(lambda w: (w[:-1], w[1:]))\n            self.windowed_ds = ds.batch(batch_size).prefetch(1)\n            return self\n\n    def train(self, train_set, y = None, epochs = 500):\n\n        model = tf.keras.models.Sequential([\n                  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                                      strides=1, padding=\"causal\",\n                                      activation=\"relu\",\n                                      input_shape=[None, 1]),\n                  tf.keras.layers.LSTM(60, return_sequences=True),\n                  tf.keras.layers.Dense(10, activation=\"relu\"),\n                  tf.keras.layers.Dense(1),\n                  tf.keras.layers.Lambda(lambda x: x * 400)\n                ])\n        optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n        model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=[\"mae\"])\n        model.fit(x=train_set, y=y,epochs=5)\n        self.modelo = model\n\n        return self\n\n    def predict(self, series_encoded):\n        # Decode the data that arrives to the method\n        def decode_ts(x):\n            return pd.Series(np.frombuffer(base64.b64decode(x)))\n        series_decode = decode_ts(series_encoded)\n        # Preprocess data\n        series = np.expand_dims(series_decode, axis=1)\n        ds = tf.data.Dataset.from_tensor_slices(series)\n        # Replace the number by a variable window_size\n        ds = ds.window(60, shift=1, drop_remainder=True)\n        # Replace the number by a variable window_size\n        ds = ds.flat_map(lambda w: w.batch(60))\n        ds = ds.batch(32).prefetch(1)\n        # Prediction\n        forecast = self.modelo.predict(ds)\n\n        return forecast\n<\/code><\/pre>\n\n<p>And a <em>run.py<\/em> file to train and save the model:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport mlflow.pyfunc\nimport ModelTest as model_test\nimport sys\nimport json\nimport mlflow\nimport numpy as np\nfrom pymongo import MongoClient\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport Preprocessing as pre\n\n#@click(...) # define the click options according to MLproject file\ndef run():\n    # Code to load time series data from MongoDB and preprocess it\n\n    window_size = 64\n    batch_size = 256\n    shuffle_buffer_size = 100\n    split_time = 400\n\n    series = np.array(data_df['sensor_ts'])\n    time = np.array(data_df['time'])\n    time_train = time[:split_time]\n    x_train = series[:split_time]\n    time_valid = time[split_time:]\n    x_valid = series[split_time:]\n\n\n    modelo = modelo_tercero.ModelTest()\n    modelo.windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\n    with mlflow.start_run() as run:\n        model = modelo.train(modelo.windowed_ds)\n        model_path = os.path.join('models', run.info.run_id)\n\n        # Save model\n        mlflow.pyfunc.save_model(\n            path=model_path,\n            python_model= modelo.train(modelo.windowed_ds),\n            code_path=['Modelthird.py'],\n            conda_env={\n                'channels': ['defaults', 'conda-forge'],\n                'dependencies': [\n                    'mlflow=1.6.0',\n                    'numpy=1.18.1',\n                    'tensorflow=2.1.0',\n                    'pandas=0.25.3',\n                    'python=3.7.6',\n                    'cloudpickle==0.5.8'\n                ],\n                'name': 'mlflow-env'\n            }\n        )\n\n\nif __name__ == \"__main__\":\n    run()\n\n<\/code><\/pre>\n\n<p>When I execute the run.py I get the next errors when the model is going to be saved:<\/p>\n\n<pre><code> Traceback (most recent call last):\n\nFile \"run.py\", line 116, in &lt;module&gt;\n    run()\n  File \"run.py\", line 110, in run\n    'name': 'mlflow-env'\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py\", line 596, in save_model\n    code_paths=code_path, mlflow_model=mlflow_model)\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/model.py\", line 141, in _save_model_with_class_artifacts_params\n    cloudpickle.dump(python_model, out)\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 1109, in dump\n    CloudPickler(file, protocol=protocol).dump(obj)\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 482, in dump\n    return Pickler.dump(self, obj)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"\/opt\/conda\/lib\/python3.7\/pickle.py\", line 524, in save\n    rv = reduce(self.proto)\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow_core\/python\/framework\/ops.py\", line 873, in __reduce__\n    return convert_to_tensor, (self._numpy(),)\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow_core\/python\/framework\/ops.py\", line 910, in _numpy\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\n  File \"&lt;string&gt;\", line 3, in raise_from\n<\/code><\/pre>\n\n<p>I have looked at different documentation related to save and serialize TensorFlow models, but there is not so much documentation about TensorFlow models and custom pyfunc functions in MLFlow. Could anyone help me or give me any hint?<\/p>\n\n<p>Thanks in advance!! :D<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-02-26 17:00:05.89 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|keras|tensorflow2.0|tensorflow-serving|mlflow",
        "Question_view_count":1669,
        "Owner_creation_date":"2017-07-20 12:31:30.863 UTC",
        "Owner_last_access_date":"2020-04-23 12:05:27.61 UTC",
        "Owner_location":"Madrid, Espa\u00f1a",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Bug when I try to download an artifact from the mlflow UI",
        "Question_body":"<h3>Sistem information<\/h3>\n<p>OS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.20.2\n**Python version: Python 3.9.7 **<\/p>\n<h3>Problem<\/h3>\n<p>I have saved an .h5 keras model, and when i tryed to execute mlflow.keras.load_model(&quot;run:\/id_run\/model&quot;) i have been waiting almost an hour but it doesn't finish. So i stopped the execution and I got the next error:<\/p>\n<pre><code>ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\IPython\\core\\interactiveshell.py&quot;, line 3441, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File &quot;&lt;ipython-input-3-277d37cc6084&gt;&quot;, line 1, in &lt;module&gt;\n    keras_model = mlflow.keras.load_model(&quot;runs:\/483745e28a864eceb738c852cf062774\/model&quot;)\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\keras.py&quot;, line 585, in load_model\n    local_model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\tracking\\artifact_utils.py&quot;, line 83, in _download_artifact_from_uri\n    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\runs_artifact_repo.py&quot;, line 125, in download_artifacts\n    return self.repo.download_artifacts(artifact_path, dst_path)\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py&quot;, line 180, in download_artifacts\n    return download_artifact_dir(\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py&quot;, line 147, in download_artifact_dir\n    download_artifact_dir(\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py&quot;, line 152, in download_artifact_dir\n    download_artifact(\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py&quot;, line 129, in download_artifact\n    self._download_file(\n  File &quot;~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mlflow\\store\\artifact\\azure_blob_artifact_repo.py&quot;, line 136, in _download_file\n    container_client.download_blob(remote_full_path).readinto(file)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\azure\\storage\\blob\\_download.py&quot;, line 617, in readinto\n    downloader.process_chunk(chunk)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\azure\\storage\\blob\\_download.py&quot;, line 129, in process_chunk\n    chunk_data = self._download_chunk(chunk_start, chunk_end - 1)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\azure\\storage\\blob\\_download.py&quot;, line 211, in _download_chunk\n    chunk_data = process_content(response, offset[0], offset[1], self.encryption_options)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\azure\\storage\\blob\\_download.py&quot;, line 52, in process_content\n    content = b&quot;&quot;.join(list(data))\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py&quot;, line 158, in __next__\n    chunk = next(self.iter_content_func)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\requests\\models.py&quot;, line 758, in generate\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\urllib3\\response.py&quot;, line 576, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\urllib3\\response.py&quot;, line 519, in read\n    data = self._fp.read(amt) if not fp_closed else b&quot;&quot;\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\http\\client.py&quot;, line 459, in read\n    n = self.readinto(b)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\http\\client.py&quot;, line 503, in readinto\n    n = self.fp.readinto(b)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\socket.py&quot;, line 669, in readinto\n    return self._sock.recv_into(b)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\ssl.py&quot;, line 1241, in recv_into\n    return self.read(nbytes, buffer)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\ssl.py&quot;, line 1099, in read\n    return self._sslobj.read(len, buffer)\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\IPython\\core\\interactiveshell.py&quot;, line 2061, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\IPython\\core\\ultratb.py&quot;, line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\IPython\\core\\ultratb.py&quot;, line 248, in wrapped\n    return f(*args, **kwargs)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\site-packages\\IPython\\core\\ultratb.py&quot;, line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\inspect.py&quot;, line 1515, in getinnerframes\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\inspect.py&quot;, line 1473, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\inspect.py&quot;, line 708, in getsourcefile\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\inspect.py&quot;, line 754, in getmodule\n    os.path.realpath(f)] = module.__name__\n  File &quot;~\\anaconda3\\envs\\python_38\\lib\\ntpath.py&quot;, line 647, in realpath\n    path = _getfinalpathname(path)\nKeyboardInterrupt\n<\/code><\/pre>\n<p>My artifact sotrage is an Azure Blob Storage and my MLflow Server is running in axeternal server<\/p>\n<p>I checked the model in the UI and it was there so i tryed to dowload it using the downloading button. But the download stops as you can see in the images and then restart over and over again. I noticed that it weight 355MB, soy i logged the model dicrectory compressed as an artifact with more or less the same weight and it gives the same problem when you try to download it.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qKnZB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qKnZB.png\" alt=\"introducir la descripci\u00f3n de la imagen aqu\u00ed\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fwkwc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fwkwc.png\" alt=\"introducir la descripci\u00f3n de la imagen aqu\u00ed\" \/><\/a><\/p>\n<h3>To reproduce this issue<\/h3>\n<p>Just log a file with the same weight and try to recover it<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-25 05:46:52.107 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|azure|tensorflow|mlflow",
        "Question_view_count":264,
        "Owner_creation_date":"2020-02-04 18:43:25.373 UTC",
        "Owner_last_access_date":"2022-09-21 20:36:53.347 UTC",
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow 1.2.0 define MLproject file",
        "Question_body":"<p>Trying to run mlflow run by specifying MLproject and code which lives in a different location as MLproject file.<\/p>\n\n<p>I have the following directory structure:<\/p>\n\n<pre><code>\/root\/mflow_test\n.\n\u251c\u2500\u2500 conda\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 conda.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MLproject\n\u251c\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Dockerfile\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MLproject\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 trainer\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 task.py\n    \u2514\u2500\u2500 utils.py\n<\/code><\/pre>\n\n<p>When I'm run from: <code>\/root\/<\/code><\/p>\n\n<pre><code>mlflow run mlflow_test\/docker\n<\/code><\/pre>\n\n<p>I get:<\/p>\n\n<pre><code>\/root\/miniconda3\/bin\/python: Error while finding module specification for 'trainer.task' (ImportError: No module named 'trainer')\n<\/code><\/pre>\n\n<p>Since my <code>MLproject<\/code> file can't find the Python code.\nI moved MLproject to <code>mflow_test<\/code> and this works fine.<\/p>\n\n<p>This is my MLproject entry point:<\/p>\n\n<pre><code>name: mlflow_sample\ndocker_env:\n  image: mlflow-docker-sample\nentry_points:\n  main:\n    parameters:\n      job_dir:\n        type: string\n        default: '\/tmp\/'\n    command: |\n        python -m trainer.task --job-dir {job_dir}\n<\/code><\/pre>\n\n<p>How can I run <code>mlflow run<\/code> and pass the MLproject and ask it to look in a different folder?<\/p>\n\n<p>I tried:<\/p>\n\n<pre><code>\"cd .. &amp;&amp; python -m trainer.task --job-dir {job_dir}\" \n<\/code><\/pre>\n\n<p>and I get:<\/p>\n\n<p><code>\/entrypoint.sh: line 5: exec: cd: not found<\/code><\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code># docker build -t mlflow-gcp-example -f Dockerfile .\nFROM gcr.io\/deeplearning-platform-release\/tf-cpu \nRUN git clone github.com\/GoogleCloudPlatform\/ml-on-gcp.git \nWORKDIR ml-on-gcp\/tutorials\/tensorflow\/mlflow_gcp \nRUN pip install -r requirements.txt \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_date":"2019-08-19 04:02:46.53 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|databricks|mlflow",
        "Question_view_count":660,
        "Owner_creation_date":"2010-01-28 09:42:15.677 UTC",
        "Owner_last_access_date":"2022-09-25 05:06:35.287 UTC",
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":8619,
        "Owner_up_votes":1916,
        "Owner_down_votes":102,
        "Owner_views":1286,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-08-19 05:51:16.957 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow registed model version increment in python",
        "Question_body":"<p>I want to keep versions of the model in mlflow but not as version[1,2,3,...]\ninstead, i want to increment model's versions like 1.1 1.2 and when I feel that there is some major change I want increment to 2.0<\/p>\n<p>please let me know how this can be done.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 05:03:50.677 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|model|version-control|mlflow",
        "Question_view_count":24,
        "Owner_creation_date":"2018-01-15 08:49:21.92 UTC",
        "Owner_last_access_date":"2022-02-17 14:09:19.933 UTC",
        "Owner_location":"Arunachal Pradesh, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Specify host and port in mlflow.yml and run \"kedro mlflow ui\", but host and port still default (localhost:5000) not change",
        "Question_body":"<p>I build sample kedro project refer to <a href=\"https:\/\/kedro-mlflow.readthedocs.io\/en\/0.6.0\/source\/03_getting_started\/01_example_project.html\" rel=\"nofollow noreferrer\">this page<\/a>,\nand specify host as my global ip address in mlflow.yml.\nbut when I hit &quot;kedro mlflow ui&quot; command, it still listen to local.\neven I only specify port to 5001 (not default) in mlflow.yml, it does not work.\nCan anyone help me.<\/p>\n<p>python version: 3.6.8 (anaconda)\nkedro version: 0.17.0\nkedro mlflow version: 0.6.0<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-02 09:20:58.347 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"mlflow|kedro",
        "Question_view_count":2006,
        "Owner_creation_date":"2021-04-02 08:53:11.86 UTC",
        "Owner_last_access_date":"2021-05-25 05:18:22.55 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to log a tensorflow model with mlflow.tensorflow.log_model (error module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants')",
        "Question_body":"<p>I am trying to log a trained model with MLFlow using mlflow.tensorflow.log_model.<\/p>\n<p>After training a simple sequential tf model<\/p>\n<pre><code>history = binary_model.fit(train_ds, validation_data=val_ds, epochs=num_epochs)\n<\/code><\/pre>\n<p>I am trying to log it:<\/p>\n<pre><code>    from tensorflow.python.saved_model import signature_constants\n    tag=[tf.saved_model.tag_constants.SERVING]\n    key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n                                tf_meta_graph_tags=tag,\n                                tf_signature_def_key=key,\n                                artifact_path=&quot;tf-models&quot;,\n                                registered_model_name=model_name)\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>    AttributeError                            Traceback (most recent call last)\n    \/var\/folders\/2k\/g7p7j2gx6v54vkwv3v401h2m0000gn\/T\/ipykernel_73638\/562549064.py in &lt;module&gt;\n          1 from tensorflow.python.saved_model import signature_constants\n    ----&gt; 2 tag=[tf.saved_model.tag_constants.SERVING]\n          3 key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n          4 \n          5 mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n\n    AttributeError: module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'\n<\/code><\/pre>\n<p>Any idea how to get the tags and keys correctly from the model to log it in MLFlow?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-29 17:36:29.63 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|tensorflow|mlflow",
        "Question_view_count":319,
        "Owner_creation_date":"2013-05-22 19:51:34.28 UTC",
        "Owner_last_access_date":"2022-09-21 19:50:10.223 UTC",
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":"<p>The <code>tag_constants<\/code> is in <code>tf.compat.v1.saved_model<\/code>.<\/p>\n<p>To resolve the error replace this line<\/p>\n<pre><code>tag=[tf.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>with this<\/p>\n<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>Please refer <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/saved_model\/tag_constants\" rel=\"nofollow noreferrer\">this<\/a> for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-13 04:29:29.433 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow-got error no host supplied,provided uri tracking,help me to resolve it",
        "Question_body":"<p>In below image can see i mention tracking uri and trying to load model but facing error in host supplied. <a href=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-16 14:34:29.023 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow|mlops",
        "Question_view_count":12,
        "Owner_creation_date":"2019-12-12 05:07:25.14 UTC",
        "Owner_last_access_date":"2022-09-24 21:10:52.373 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow model artifacts are not getting stored, while running the airflow dag. for that reason unable to fetch experiment details?",
        "Question_body":"<pre><code>**mlflow training code:**\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\n\n&quot; training the model and saving the model artificats&quot;\nmlflow.set_registry_uri('postgresql:\/\/postgres:postgres@localhost\/mlflow')\nmlflow.set_experiment('testing_mlflow_with_airflow')\nwith mlflow.start_run():\n    # creating the training dataframe\n    train_x = self.train_data[0]\n    train_y = self.train_data[1]\n\n    # training the given model\n    model.fit(train_x, train_y)\n                \n    mlflow.sklearn.log_model(model, &quot;model&quot;)\n\n\n&quot; getting the experiment details by experiment name&quot;\nexperiment_id = client.get_experiment_by_name('testing_mlflow_with_airflow').experiment_id\nexperiment_results = mlflow.search_runs(experiment_ids=experiment_id) '''\n\n\n**airflow task code:**\n\ntraining = BashOperator(\n              task_id = 'mlflow_training',\n              bash_command='python3 \/home\/vasanth\/airflow\/scripts\/mlproject\/src\/models\/train_mlflow.py',\n              do_xcom_push=False\n               )\n\n\n**airflow error:**\n\n[2022-04-29, 13:01:08 UTC] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'python3 \/home\/vasanth\/airflow\/scripts\/mlproject\/src\/models\/train_mlflow.py']\n\n[2022-04-29, 13:01:08 UTC] {subprocess.py:85} INFO - Output:\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - WARNING:root:Malformed experiment '2'. Detailed error Yaml file '\/tmp\/airflowtmpzjvuldm6\/mlruns\/2\/meta.yaml' does not exist.\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - Traceback (most recent call last):\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -   File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 262, in list_experiments\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -     experiment = self._get_experiment(exp_id, view_type)\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -   File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 341, in _get_experiment\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -     meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -   File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/utils\/file_utils.py&quot;, line 179, in read_yaml\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO -     raise MissingConfigException(&quot;Yaml file '%s' does not exist.&quot; % file_path)\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - mlflow.exceptions.MissingConfigException: Yaml file '\/tmp\/airflowtmpzjvuldm6\/mlruns\/2\/meta.yaml' does not exist.\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - tracking uri ***ql:\/\/***:***@localhost\/mlflow\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - Traceback (most recent call last):\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - File &quot;\/home\/vasanth\/airflow\/scripts\/mlproject\/src\/models\/train_mlflow.py&quot;, line 44, in &lt;module&gt;\n\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - experiment_id = client.get_experiment_by_name('testing_mlflow_with_airflow').experiment_id\n\n[2022-04-29, 13:01:28 UTC] {subprocess.py:89} INFO - AttributeError: 'NoneType' object has no attribute 'experiment_id'\n\n[2022-04-29, 13:01:29 UTC] {subprocess.py:93} INFO - Command exited with return code 1\n\n[2022-04-29, 13:01:29 UTC] {taskinstance.py:1774} ERROR - Task failed with exception\n<\/code><\/pre>\n<p>how i can set a directory where all my experiment runs artifacts will be stored?\nwhere the mlflow artifacts are getting stored now?\nhow i can find all runs details by the mlflow client as per the above code?<\/p>\n<p>i have tried with different approaches, None of them is worked<\/p>\n<ol>\n<li><p>setting the tracking server as  below\nmlflow.set_tracking_uri('postgresql:\/\/postgres:postgres@localhost\/mlflow')\nmlflow.set_tracking_uri('file:\/\/\/tmp\/mlruns')<\/p>\n<\/li>\n<li><p>mlflow.set_tracking_uri('http:\/\/localhost:5000')<\/p>\n<\/li>\n<li><p>mlflow.set_registry_uri('postgresql:\/\/postgres:postgres@localhost\/mlflow')\nmlflow.set_tracking_uri('\/home\/vasanth\/airflow\/scripts\/mlproject\/src\/models')<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-29 11:26:55.817 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"airflow|mlflow",
        "Question_view_count":359,
        "Owner_creation_date":"2019-11-29 14:29:20.81 UTC",
        "Owner_last_access_date":"2022-08-26 05:52:38.283 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow Azure Blob Storage Artifact upload times out",
        "Question_body":"<p>I am trying to upload mlflow artifacts to an azure blob storage instance.\nThe backend server is able to connect to the blob storage and is working fine.\nThe client is also working fine.\nThe only issue on the client-side is a time-out while uploading artifacts to the blob storage.\nThe following exception is thrown:<\/p>\n<pre><code>\/venv\/lib\/python3.9\/site-packages\/azure\/core\/pipeline\/transport\/_requests_basic.py&quot;, line 361, in send\nraise error\nazure.core.exceptions.ServiceResponseError: ('Connection aborted.', timeout('The write operation timed out'))\n<\/code><\/pre>\n<p>Usually, I would just increase the timeout, but I don't know how to do that for mlflow. I searched for a possible solution <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#mlflow-tracking-servers\" rel=\"nofollow noreferrer\">here<\/a> as well as checking their GitHub for possible open and\/or closed issues regarding the same problem, but I have yet to find a solution. Is it possible to adjust the timeout for the artifact logging in mlflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-22 18:22:45.417 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":163,
        "Owner_creation_date":"2017-11-06 09:28:56.47 UTC",
        "Owner_last_access_date":"2022-09-22 05:02:21.253 UTC",
        "Owner_location":"Portland, Oregon, USA",
        "Owner_reputation":524,
        "Owner_up_votes":177,
        "Owner_down_votes":51,
        "Owner_views":95,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Sagemaker Train Job can't connect to ec2 instance",
        "Question_body":"<p>I have MLFlow server running on ec2 instance, port 5000.<\/p>\n<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.<\/p>\n<p>ec2 instance inbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/VXwid.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VXwid.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>SageMaker outbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>These 2 security groups are in the same VPC<\/p>\n<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=\"https:\/\/stackoverflow.com\/questions\/45416882\/aws-security-group-include-another-security-group\">here<\/a>), but connection fails<\/p>\n<p>SageMaker job init:<\/p>\n<pre><code>   role = &quot;ml_sagemaker&quot;\n   security_group_ids = ['sg-04868acca16e81183']\n   bucket = sagemaker_session.default_bucket()  \n   out_path = f&quot;s3:\/\/{bucket}\/{project_name}&quot;\n\n   estimator = PyTorch(entry_point='run_train.py',\n                       source_dir='.',\n                       sagemaker_session=sagemaker_session,\n                       instance_type=instance_type,\n                       instance_count=1,\n                       framework_version='1.5.0',\n                       py_version='py3',\n                       role=role,\n                       security_group_ids=security_group_ids,\n                       hyperparameters={},\n                       )\n   ....\n\n<\/code><\/pre>\n<p>Inside <code>run_train.py<\/code>:<\/p>\n<pre><code>import mlflow\ntracking_uri = &quot;http:\/\/172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP\nmlflow.set_tracking_uri(tracking_uri)\nmlflow.log_param(&quot;test_param&quot;, 3)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n<\/code><\/pre>\n<p><strong>However<\/strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here is SageMaker Notebook configurations:<\/p>\n<img src=\"https:\/\/i.stack.imgur.com\/bslu8.png\" width=\"300\" \/>\n<p>How can I connect to ec2 instance from SageMaker Training Job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-19 17:18:49.957 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow",
        "Question_view_count":608,
        "Owner_creation_date":"2015-07-28 15:46:05.74 UTC",
        "Owner_last_access_date":"2022-09-22 13:24:29.297 UTC",
        "Owner_location":null,
        "Owner_reputation":653,
        "Owner_up_votes":216,
        "Owner_down_votes":1,
        "Owner_views":76,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Filter mlflow runs by commit ID",
        "Question_body":"<p>When using the UI of MlFlow, is it possible to filter\/search the runs using the (git) commit ID? I manage to search by parameters but it doesn't seem like there's a way to filter by the commit ID.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/npkFO.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/npkFO.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-17 10:22:40.897 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"machine-learning|version-control|mlflow",
        "Question_view_count":982,
        "Owner_creation_date":"2011-03-22 10:28:37.227 UTC",
        "Owner_last_access_date":"2022-09-23 13:51:41.56 UTC",
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-01-18 07:23:50.323 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-12 16:29:16.103 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":131,
        "Owner_creation_date":"2017-01-19 15:07:44.573 UTC",
        "Owner_last_access_date":"2022-09-22 14:55:11.743 UTC",
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-17 13:21:05.5 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow Artifacts Storing artifacts(google cloud storage) but not displaying them in MLFlow UI",
        "Question_body":"<p>I am working on a docker environment(docker-compose) with a jupyter notebook docker image and a postgres docker image for running ML models and using google cloud storage to store the model artifacts. Storing the models on the cloud storage works fine but i can't get to show them within the MLFlow UI. I have seen similar problems but non of the solutions used google cloud storage as the storage location for artifacts. The error message says the following <code>Unable to list artifacts stored under &lt;gs-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.<\/code>What could possibly be causing this problem?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2020-09-03 15:47:21.45 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker-compose|google-cloud-storage|artifacts|mlflow",
        "Question_view_count":1087,
        "Owner_creation_date":"2016-10-17 21:17:27.223 UTC",
        "Owner_last_access_date":"2022-07-05 17:50:06.73 UTC",
        "Owner_location":"Cape Town, South Africa",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"'waitress-serve' is not recognized as an internal or external command,",
        "Question_body":"<p>I try run this mlflow models serve --model-uri runs:\/f3393a61d01d4289b16707ed718f23be\/log_reg_model -p 1235 script but i got this error<\/p>\n<pre><code>'waitress-serve' is not recognized as an internal or external command,\noperable program or batch file.\nTraceback (most recent call last):\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\Scripts\\mlflow-script.py&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 1053, in main\n    rv = self.invoke(ctx)\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 1659, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 1659, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\click\\core.py&quot;, line 754, in invoke\n    return __callback(*args, **kwargs)\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\models\\cli.py&quot;, line 59, in serve\n    ).serve(model_uri=model_uri, port=port, host=host, enable_mlserver=enable_mlserver)\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\pyfunc\\backend.py&quot;, line 79, in serve\n    conda_env_path, command, self._install_mlflow, command_env=command_env\n  File &quot;C:\\Users\\ahmad\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\pyfunc\\backend.py&quot;, line 168, in _execute_in_conda_env\n    &quot;Command '{0}' returned non zero return code. Return code = {1}&quot;.format(command, rc)\nException: Command 'conda activate mlflow-270591f5d4ece78a187f6457a571ae1ce1e4d11f &amp; waitress-serve --host=127.0.0.1 --port=1235 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-19 17:51:39.403 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"conda|mlflow|waitress",
        "Question_view_count":232,
        "Owner_creation_date":"2021-11-10 13:02:12.24 UTC",
        "Owner_last_access_date":"2022-09-19 15:50:37.843 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to update a previous run into MLFlow?",
        "Question_body":"<p>I would like to update previous runs done with MLFlow, ie. changing\/updating a parameter value to accommodate a change in the implementation. Typical uses cases:<\/p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.<\/li>\n<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.<\/li>\n<li>Correct a wrong parameter value loggued in the previous runs.<\/li>\n<\/ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.<\/p>\n<p>What is the best way to do this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-10-05 13:04:07.533 UTC",
        "Question_favorite_count":2.0,
        "Question_score":6,
        "Question_tags":"logging|data-science|mlflow",
        "Question_view_count":2834,
        "Owner_creation_date":"2012-09-10 21:25:47.147 UTC",
        "Owner_last_access_date":"2022-09-24 18:14:33.217 UTC",
        "Owner_location":null,
        "Owner_reputation":1022,
        "Owner_up_votes":1127,
        "Owner_down_votes":19,
        "Owner_views":66,
        "Answer_body":"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function<\/p>\n<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:\n    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)\n    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics\n    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file\n<\/code><\/pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs<\/a>.<\/p>\n<p>Source: <a href=\"https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-12-02 14:45:49.24 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":10.0,
        "Question_last_edit_date":"2020-12-12 16:01:03.85 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to use a @pandas_udf function inside a class with pyspark?",
        "Question_body":"<p>I am trying to use one of the Hugging Face models with ML flow. My input is a pyspark DataFrame.\nThe issue is Mlflow doesn't support directly HuggingFace models, so need to use the flavor pyfunc to save it. So I need create a Python class that inherits from PythonModel and then place everything needed there.\nHow can I use a pandas_udf function inside this PythonModel? It keeps failing because I haven't specified the hint type for all the parameters inside my pandas_udf.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class RobertaClassifier(PythonModel):\n\n    def load_context(self, context: PythonModelContext):\n        import os\n        from transformers.models.auto import AutoConfig,   AutoModelForSequenceClassification\n        from transformers.models.auto.tokenization_auto import AutoTokenizer\n        \n        config_file = os.path.dirname(context.artifacts[&quot;config&quot;])\n        self.config = AutoConfig.from_pretrained(config_file)\n        self.tokenizer = AutoTokenizer.from_pretrained(config_file)\n        self.model = AutoModelForSequenceClassification.from_pretrained(config_file, config=self.config)\n        \n        if torch.cuda.is_available():\n            print('[INFO] Model is being sent to CUDA device as GPU is available')\n            self.model = self.model.cuda()\n        else:\n            print('[INFO] Model will use CPU runtime')\n        \n        _ = self.model.eval()\n    \n    \n    @pandas_udf(&quot;label string, score float&quot;)\n    def predict_batch_udf(self, data: pd.Series) -&gt; pd.Series:\n        import torch\n        import pandas as pd\n        \n        with torch.no_grad():\n            inputs = preprocessing(data['content'])\n            inputs = self.tokenizer(inputs, padding=True, return_tensors='pt', max_length=512, truncation=True)\n        \n            if self.model.device.index != None:\n                torch.cuda.empty_cache()\n                for key in inputs.keys():\n                    inputs[key] = inputs[key].to(self.model.device.index)\n\n            predictions = self.model(**inputs)\n            probs = torch.nn.Softmax(dim=1)(predictions.logits)\n            probs = probs.detach().cpu().numpy()\n\n            labels = probs.argmax(axis=1)\n            scores = probs.max(axis=1)\n\n            return labels, scores\n        \n    def predict(self, context: PythonModelContext, data: pd.Series) -&gt; pd.Series:\n        import math\n        import numpy as np\n        \n        batch_size = 64\n        sample_size = len(data)\n        \n        labels = np.zeros(sample_size)\n        scores = np.zeros(sample_size)\n\n        for batch_idx in range(0, math.ceil(sample_size \/ batch_size)):\n            bfrom = batch_idx * batch_size\n            bto = bfrom + batch_size\n            \n            l, s = self._predict_batch(data.iloc[bfrom:bto])\n            labels[bfrom:bto] = l\n            scores[bfrom:bto] = s\n            \n        return pd.DataFrame({'label': [self.config.id2label[l] for l in labels], \n                             'score': scores })\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-21 21:09:55.057 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"dataframe|class|pyspark|mlflow|pandas-udf",
        "Question_view_count":18,
        "Owner_creation_date":"2022-09-21 20:46:50.607 UTC",
        "Owner_last_access_date":"2022-09-22 21:49:26.877 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-22 06:18:47.23 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Can't get model inference using mlflow.pytorch.log_model, but could get it with mlflow.pyfunc.log_model",
        "Question_body":"<p>I've used <code>mlflow.pyfunc.log_model<\/code> and I was able to get model inference with this, but not with<code>mlflow.pytorch.log_model<\/code>. The error was Verify that the serialized input Dataframe is compatible with the model for inference.<\/p>\n<pre><code>    data = torch.randn(10, 3, 224, 224)  # shape: [bs, channel, size, size]\n    model_input = {\n                &quot;inputs&quot;: { \n                    &quot;x&quot;: data.tolist() }\n                }\n    request = json.dumps(model_input)\n    headers = {&quot;content-type&quot;: &quot;application\/json&quot;}\n    response = requests.post(URL, data=request, headers=headers) # to mlflow\n    response = response.json() \n    print(response)\n<\/code><\/pre>\n<p>The very same input to the model, but I could get inference on one but not the other? Am I missing something here? I would like to use <code>mlflow.pytorch.log_model<\/code> so I don't have to do a model wrapper for generalisation with <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n<p>Can anyone help me with this please.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 02:12:45.467 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"rest|deployment|pytorch|mlflow|serving",
        "Question_view_count":14,
        "Owner_creation_date":"2017-02-23 06:31:44.143 UTC",
        "Owner_last_access_date":"2022-09-24 07:48:55.5 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-22 02:15:10.113 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Combine MLflow projects with docker-compose",
        "Question_body":"<p>I face the following situation:<\/p>\n<p>We train our models within docker container, which is build by running a docker-compose file. I have implemented MLflow to work with docker-compose (by doing something similar to e.g. this post: <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039<\/a>), creating two more containers (one for the server and one for the postgresql backend).<\/p>\n<p>However, the story doesn't end here. Our goal is to implement a full ML pipeline, which includes data creation, preprocessing steps and so on. I know, that ML projects is something which helps to create such pipeline. I have seen that it is designed to work with docker images (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/projects.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/projects.html<\/a>), but I don't get it, how one could use it with docker-compose.<\/p>\n<p>Could you help me in that by giving any tipps, guidelines, documentations, etc?<\/p>\n<p>Or in general, any advice, how a full machine learning pipeline could be implemented using mlflow?<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-12 07:58:26.943 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|docker-compose|pipeline|mlflow",
        "Question_view_count":1007,
        "Owner_creation_date":"2017-12-06 14:53:51.753 UTC",
        "Owner_last_access_date":"2022-06-23 07:16:10.68 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-12 13:31:50.07 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"pip install fails when installing mlflow",
        "Question_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>Traceback (most recent call last):\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 360, in _error_catcher\nyield\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in read\ndata = self._fp.read(amt)\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\", line 447, in read\nn = self.readinto(b)\n File \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\", line 491, in readinto\nn = self.fp.readinto(b)\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\", line 589, in readinto\nreturn self._sock.recv_into(b)\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\", line 1052, in recv_into\nreturn self.read(nbytes, buffer)\nFile \"C:\\Users\\username\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\", line 911, in read\nreturn self._sslobj.read(len, buffer)\nsocket.timeout: The read operation timed out\n....\n\nDuring handling of the above exception, another exception occurred:\n<\/code><\/pre>\n\n<p>What's this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-17 19:29:32.213 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pip|mlflow",
        "Question_view_count":175,
        "Owner_creation_date":"2012-12-12 20:12:11.933 UTC",
        "Owner_last_access_date":"2022-04-05 02:26:00.75 UTC",
        "Owner_location":null,
        "Owner_reputation":5655,
        "Owner_up_votes":73,
        "Owner_down_votes":3,
        "Owner_views":629,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How should I mount docker volumes in mlflow project?",
        "Question_body":"<p>I use <code>mlflow<\/code> in a docker environment as described in this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">example<\/a> and I start my runs with <code>mlflow run .<\/code>.<\/p>\n\n<p>I get output like this<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>2019\/07\/17 16:08:16 INFO mlflow.projects: === Building docker image mlflow-myproject-ab8e0e4 ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Created directory \/var\/folders\/93\/xt2vz36s7jd1fh9bkhkk9sgc0000gn\/T\/tmp1lxyqqw9 for downloading remote URIs passed to arguments of type 'path' ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Running command 'docker run \n--rm -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e \nMLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py' in run with ID 'ef21de61d8a6436b97b643e5cee64ae1' ===\n<\/code><\/pre>\n\n<p>I would like to mount a docker volume named <code>my_docker_volume<\/code> to the container\n at \nthe path <code>\/data<\/code>. So instead of the <code>docker run<\/code> shown above, I would like to\n use<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>docker run --rm --mount source=my_docker_volume,target=\/data -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e MLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py\n<\/code><\/pre>\n\n<p>I see that I could in principle run it once without mounted volume and then \ncopy the <code>docker run ...<\/code> and add <code>--mount source=my_volume,target=\/data<\/code> but\n I'd rather use something like<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>mlflow run --mount source=my_docker_volume,target=\/data .\n<\/code><\/pre>\n\n<p>but this obviously doesn't work because --mount is not a parameter for \n<code>mlflow run<\/code>.\nWhat's the recommened way of mounting a docker volume then?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-17 14:22:29.4 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1301,
        "Owner_creation_date":"2013-08-08 09:55:24.343 UTC",
        "Owner_last_access_date":"2022-09-24 13:14:05.257 UTC",
        "Owner_location":"Freiburg im Breisgau, Germany",
        "Owner_reputation":191,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-08-02 14:08:39.9 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to restore code of pyfunc class in MlFlow?",
        "Question_body":"<p>I have lost a code of my pyfunc class which I uploaded to MlFlow. Model is working on a production but via MLFlow UI I can't get code view of model class. For some R models I can get code class using load_model. After call this method I get the whole code view of class which is uploaded to MlFlow. Unfortunately, it doesnt work the same way in python. All files are inside hdfs directory of MLFlow.<\/p>\n<pre><code>import mlflow\nlogged_model = '&quot;file:\/\/\/var\/mm\/mlflow\/artifacts\/***\/*******\/artifacts\/my_model_name'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n<\/code><\/pre>\n<p>After invoking to loaded_model I get only some not important infos about model:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n  artifact_path: my_model_name\n  flavor: mlflow.pyfunc.model\n  run_id: *****\n<\/code><\/pre>\n<p>How can I check the code of the class which was uploaded to MLFlow earlier?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-15 21:07:53.73 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":31,
        "Owner_creation_date":"2021-04-03 20:09:07.677 UTC",
        "Owner_last_access_date":"2022-09-23 14:47:28.92 UTC",
        "Owner_location":null,
        "Owner_reputation":7,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-16 07:14:09.49 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Serving models from mlflow registry to sagemaker",
        "Question_body":"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local<\/code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:<\/p>\n<pre><code>export MODEL_PATH=models:\/churn-lgb-test\/2\nexport LOCAL_PORT=8000\nmlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine\/mlflow-pyfunc:1.6.0\n<\/code><\/pre>\n<p>it starts the container and I immediately get this error:<\/p>\n<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)\n[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)\n[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)\n[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 209, in run\n    self.sleep()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 357, in sleep\n    ready = select.select([self.PIPE[0]], [], [], 1.0)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n    sys.exit(run())\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n    super().run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n    Arbiter(self).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n    self.stop()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n    time.sleep(0.1)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\ncreating and activating custom environment\nGot sigterm signal, exiting.\n[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)\n[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)\n[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-27 13:26:19.06 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":429,
        "Owner_creation_date":"2020-06-05 19:27:43.857 UTC",
        "Owner_last_access_date":"2020-11-19 23:45:52.31 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to get url of mlflow logged artifacts?",
        "Question_body":"<p>I am running an ML pipeline, at the end of which I am logging certain information using mlflow. I was mostly going through Databricks' official mlflow tracking tutorial.<\/p>\n<pre><code>import mlflow\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nwith mlflow.start_run():\n  n_estimators = 100\n  max_depth = 6\n  max_features = 3\n  # Create and train model\n  rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)\n  rf.fit(X_train, y_train)\n  # Make predictions\n  predictions = rf.predict(X_test)\n  \n  # Log parameters\n  mlflow.log_param(&quot;num_trees&quot;, n_estimators)\n  mlflow.log_param(&quot;maxdepth&quot;, max_depth)\n  mlflow.log_param(&quot;max_feat&quot;, max_features)\n  \n  # Log model\n  mlflow.sklearn.log_model(rf, &quot;random-forest-model&quot;)\n  \n  # Create metrics\n  mse = mean_squared_error(y_test, predictions)\n    \n  # Log metrics\n  mlflow.log_metric(&quot;mse&quot;, mse)\n<\/code><\/pre>\n<p>When I run the above block of code in Databricks notebook, the below status message shows:<\/p>\n<pre><code>(1) MLflow run\nLogged 1 run to an experiment in MLflow. Learn more\n<\/code><\/pre>\n<p>And I can view the logged information by clicking on &quot;1 run.&quot;<\/p>\n<p>However, I would like to automatically retrieve this link. In particular, I need the link to the mlflow uri where the artifacts are stored. This link is in the following format:<\/p>\n<pre><code>https:\/\/mycompany-dev.cloud.databricks.com\/?o=&lt;ID_1&gt;#mlflow\/experiments\/&lt;ID_2&gt;\/runs\/&lt;ID_3&gt;\n<\/code><\/pre>\n<p>I tried investigating the url and finding the various id codes that are present in it by printing the following information:<\/p>\n<pre><code>print(&quot;Tracking URI: &quot;, mlflow.get_tracking_uri())\nprint(&quot;Run id:&quot;, run.info.run_id)\nprint(&quot;Experiment:&quot;, run.info.experiment_id)\n<\/code><\/pre>\n<p>I figured out that <code>&lt;ID_2&gt;<\/code> in the link above is the <code>experiment_id<\/code> and <code>&lt;ID_3&gt;<\/code> is the <code>run_id<\/code>. But I have no idea what <code>&lt;ID_1&gt;<\/code> stands for. Also, I believe there should be a built-in functionality to retrieve the link of saved artifacts, instead of manually having to build up the link from sections. However, I haven't found such a funcitonality in the documentation so far.<\/p>\n<p><strong>Edit:<\/strong> Now I discovered that <code>&lt;ID_1&gt;<\/code> is the Databricks workplace id. But it is still a question how I can access it programatically.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 13:46:21.563 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|databricks|azure-databricks|mlflow",
        "Question_view_count":27,
        "Owner_creation_date":"2015-07-16 10:20:07.427 UTC",
        "Owner_last_access_date":"2022-09-24 16:13:50.543 UTC",
        "Owner_location":null,
        "Owner_reputation":968,
        "Owner_up_votes":512,
        "Owner_down_votes":10,
        "Owner_views":241,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-22 14:03:46.853 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to store artifacts on a server running MLflow",
        "Question_body":"<p>I define the following docker image:<\/p>\n\n<pre><code>FROM python:3.6\n\nRUN pip install --upgrade pip\nRUN pip install --upgrade mlflow\n\nENTRYPOINT mlflow server --host 0.0.0.0 --file-store \/mnt\/mlruns\/\n<\/code><\/pre>\n\n<p>and build an image called <code>mlflow-server<\/code>. Next, I start this server from a local machine:<\/p>\n\n<pre><code>docker run --rm -it -p 5000:5000 -v ${PWD}\/mlruns\/:\/mnt\/mlruns mlflow-server\n<\/code><\/pre>\n\n<p>Next, I define the following function:<\/p>\n\n<pre><code>def foo(x, with_af=False):\n    mlflow.start_run()\n    mlflow.log_param(\"x\", x)\n    print(x)\n    if with_af:\n        with open(str(x), 'wb') as fout:\n            fout.write(os.urandom(1024))\n        mlflow.log_artifact(str(x))\n        mlflow.log_artifact('.\/foo.data')\n    mlflow.end_run()\n<\/code><\/pre>\n\n<p>From the same directory I run <code>foo(10)<\/code> and the parameter is logged correctly. However, <code>foo(10, True)<\/code> yields the following error: <code>PermissionError: [Errno 13] Permission denied: '\/mnt'<\/code>. Seems like <code>log_artifact<\/code> tries to save the file on the local file system directly.<\/p>\n\n<p>Any idea what am I doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-09-14 11:41:42.767 UTC",
        "Question_favorite_count":3.0,
        "Question_score":20,
        "Question_tags":"python|docker|mlflow",
        "Question_view_count":15702,
        "Owner_creation_date":"2011-03-22 10:28:37.227 UTC",
        "Owner_last_access_date":"2022-09-23 13:51:41.56 UTC",
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-07-02 14:06:18.733 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Can I change the port of my MLflow tracking server?",
        "Question_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-25 05:27:12.427 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"http|port|mlflow",
        "Question_view_count":540,
        "Owner_creation_date":"2019-06-21 08:41:37.313 UTC",
        "Owner_last_access_date":"2022-08-21 05:17:54.15 UTC",
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-25 08:54:16.29 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Experiments disappear when adding --backend-store-uri",
        "Question_body":"<p>I have an EC2 instance running a mlflow server using the following command:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 --default-artifact-root s3:\/\/xxxx\n<\/code><\/pre>\n<p>After running multiple experiments, I was trying to register the best one. However, when trying to register or accessing the tab &quot;Models&quot;, I get the following error:<\/p>\n<blockquote>\n<p>INVALID_PARAMETER_VALUE: Model registry functionality is unavailable; got unsupported URI '.\/mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage<\/a> for how to run an MLflow server against one of the supported backend storage locations.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/63255631\/mlflow-invalid-parameter-value-unsupported-uri-mlruns-for-model-registry-s\">This SO answer<\/a> suggested adding a <code>backend-store-uri<\/code>:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 --default-artifact-root --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>That solved the above issue, however, now all experiments are gone. The Experiments tab is blank. Is there a way to add a <code>backend-store-uri<\/code> after running multiple experiments while keeping all of them?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-07 06:49:14.493 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"mlflow",
        "Question_view_count":208,
        "Owner_creation_date":"2014-03-15 16:29:47.34 UTC",
        "Owner_last_access_date":"2022-09-23 14:38:41.817 UTC",
        "Owner_location":"Curitiba, State of Paran\u00e1, Brazil",
        "Owner_reputation":1490,
        "Owner_up_votes":99,
        "Owner_down_votes":8,
        "Owner_views":346,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow: how to get additional methods from a loaded model?",
        "Question_body":"<p><strong>Use case:<\/strong><\/p>\n<p>A <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.PyFuncModel<\/code><\/a> is defined with some more utilities methods in order to provide a way of parsing its prediction result to different formats.<\/p>\n<p><strong>After the model is loaded from registry, is there a way to access those methods?<\/strong><\/p>\n<p><strong>A contrived example:<\/strong><\/p>\n<p>A <code>mlflow.pyfunc.PyFuncModel<\/code> model defining additional methods:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        prediction = # do some prediction\n        return prediction\n\n    @staticmethod\n    def parse_prediction_to_format_x(prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n\n    def parse_prediction_to_format_y(self, prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n<\/code><\/pre>\n<p>Note: I added one static and one non static, because both use cases are relevant.<\/p>\n<p>Now, some other system goes to MLFlow Registry and loads the model from there:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>        loaded_model = mlflow.pyfunc.load_model(\n            model_uri=saved_model_path.absolute().as_uri()\n        )\n<\/code><\/pre>\n<p>This system, which naturally does not hold the model source code, but the registry path to load it from there, wants to use the additional methods above.\nIt can use predict, since it is part of all pyfunc models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n<\/code><\/pre>\n<p><strong>But how can this system access helper methods in the model class (static or instance methods)?<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n\n# pseudo code:\npredicted_and_formated = loaded_model.parse_prediction_to_format_y(predicted)\n<\/code><\/pre>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 10:04:38.537 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":15,
        "Owner_creation_date":"2020-02-13 21:23:52.807 UTC",
        "Owner_last_access_date":"2022-09-22 23:27:25.757 UTC",
        "Owner_location":null,
        "Owner_reputation":477,
        "Owner_up_votes":59,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Github as artifact repo in mlflow",
        "Question_body":"<p>Can we use github as one of the options in artifact repository in additions to the object storage support.<\/p>\n\n<p>Github seems to be a natural way to capture changes in code between different runs\/experiments, this will also give a way of tying down the revision of code used in a registered model.<\/p>\n\n<p>Model version --> runs --> github version.  Nothing golden than this.<\/p>\n\n<p>Thoughts ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-05 18:55:15.517 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"machine-learning|mlflow",
        "Question_view_count":311,
        "Owner_creation_date":"2016-06-05 04:26:47.473 UTC",
        "Owner_last_access_date":"2022-09-23 19:49:38.557 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Serve online learning models with mlflow",
        "Question_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-26 10:12:57.11 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":360,
        "Owner_creation_date":"2018-01-05 12:55:59.093 UTC",
        "Owner_last_access_date":"2021-09-03 19:38:29.597 UTC",
        "Owner_location":"Fairbanks, AK, United States",
        "Owner_reputation":76,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-04-11 07:04:00.63 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Unable to access to mlflow ui",
        "Question_body":"<p>By following quickstart and tutorial at <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html<\/a>, and <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tutorial.html\" rel=\"noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tutorial.html<\/a>, the execution of train.py works fine. <\/p>\n\n<pre><code>Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\n  RMSE: 0.8222428497595403\n  MAE: 0.6278761410160693\n  R2: 0.12678721972772622\n<\/code><\/pre>\n\n<p>But when launching the ui <code>mlflow ui<\/code>, and accessing to the web page localhost:5000, the browser complains <\/p>\n\n<pre><code>Not Found\n\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n<\/code><\/pre>\n\n<p>What went wrong and how to fix this? <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2018-06-28 09:16:37.51 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"python|python-3.x|mlflow",
        "Question_view_count":7497,
        "Owner_creation_date":"2018-05-07 11:18:44.777 UTC",
        "Owner_last_access_date":"2018-07-06 14:40:51.58 UTC",
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Writing a custom predict method using MLFlow and pyspark",
        "Question_body":"<p>I am having trouble writing a custom predict method using MLFlow and pyspark (2.4.0). What I have so far is a custom transformer that changes the data into the format I need.<\/p>\n<pre><code>class CustomGroupBy(Transformer):\n    def __init__(self):\n        pass\n    def _transform(self, dataset):\n        df = dataset.select(&quot;userid&quot;, explode(split(&quot;widgetid&quot;, ',')).alias(&quot;widgetid&quot;))\n        return(df)\n<\/code><\/pre>\n<p>Then I built a custom estimator to run one of the pyspark machine learning algorithms<\/p>\n<pre><code>class PipelineFPGrowth(Estimator, HasInputCol, DefaultParamsReadable, DefaultParamsWritable): \n    def __init__(self, inputCol=None, minSupport=0.005, minConfidence=0.01):\n        super(PipelineFPGrowth, self).__init__()\n        self.minSupport = minSupport\n        self.minConfidence = minConfidence\n    def setInputCol(self, value):\n        return(self._set(inputCol=value))\n    def _fit(self, dataset):\n        c = self.getInputCol() \n        fpgrowth = FPGrowth(itemsCol=c, minSupport=self.minSupport, minConfidence=self.minConfidence)\n        model = fpgrowth.fit(dataset)\n        return(model)\n<\/code><\/pre>\n<p>This runs in the MLFlow pipeline.<\/p>\n<pre><code>pipeline = Pipeline(stages = [CustomGroupBy,PipelineFPGrowth]).fit(df)\n<\/code><\/pre>\n<p>This all works. If I create a new pyspark dataframe with new data to predict on, I get predictions.<\/p>\n<pre><code>newDF = spark.createDataFrame([(123456,['123ABC', '789JSF'])], [&quot;userid&quot;, &quot;widgetid&quot;])\npipeline.stages[1].transform(newDF).show(3, False)\n\n# How to access frequent itemset.\npipeline.stages[1].freqItemsets.show(3, False)\n<\/code><\/pre>\n<p>Where I run into problems is writing a custom predict. I need to append the frequent itemset that FPGrowth generates to the end of the predictions. I have written the logic for that, but I am having a hard time figuring out how to put it into a custom method. I have tried adding it to my custom estimator but this didn't work. Then I wrote a separate class to take in the returned model and give the extended predictions. This was also unsuccessful.<\/p>\n<p>Eventually I need to log and save the model so I can Dockerize it, which means I will need a custom flavor and to use the pyfunc function. Does anyone have a hint on how to extend the predict method and then log and save the model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-02 09:07:10.127 UTC",
        "Question_favorite_count":0.0,
        "Question_score":3,
        "Question_tags":"python|docker|machine-learning|pyspark|mlflow",
        "Question_view_count":331,
        "Owner_creation_date":"2011-12-06 11:56:12.023 UTC",
        "Owner_last_access_date":"2022-07-04 20:51:05.6 UTC",
        "Owner_location":"Switzerland",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-10-02 09:55:20.82 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does Mflow work only with predictive models?",
        "Question_body":"<p>I am trying to use Mlflow to manage my Bayesian Optimization Model, which has several methods other than the predict (run_optimization() for example). My doubt is that when I log my model to the tracking server the model and retrieve it, it only contains the predict() as it is wrapped as a PyFunctModel; that's a problem because I need the model also to run prescriptions (suggestion of a possible new optimum), does anyone ever tried it? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-14 06:37:15.563 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":40,
        "Owner_creation_date":"2019-04-03 14:27:39.233 UTC",
        "Owner_last_access_date":"2022-06-28 08:32:28.75 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow.pyfunc model can't find artifacts",
        "Question_body":"<p>I'm trying to port over some existing production models from our current system to one that utilizes mlflow.  They're currently run by calling production scripts that, themselves, load up serialized files (including the saved models), run various preprocessing scripts and then predict.  They are structured as follows:<\/p>\n<pre><code>&quot;&quot;&quot; production_script.py &quot;&quot;&quot;\nimport ...\n\nmodel = joblib.load(&quot;.\/model_file.joblib&quot;)\nmodel_meta_data = joblib.load(&quot;.\/model_meta.joblib&quot;)\n\ndef custom_preprocessing(*args):\n   &lt;custom preprocessing stuff here&gt;\n\ndef predict(model, model_meta_data, input_payload):\n   processed_data = custom_preprocessing(input_payload, model_meta_data)\n   prediction = model.predict(processed_data)\n\n   return prediction\n<\/code><\/pre>\n<p>I'm trying to structure the mlflow.pyfunc wrapper such that it imports the predict function from this file, production_script.py, feeds in the payload and returns the response.  Put very simply it would be something like the following:<\/p>\n<pre><code>class Custom_Pyfunc_Wrapper(mlflow.pyfunc.PythonModel):\n   def __init__(self):\n   \n   def load_context(self, context): # one thing I've tried\n      # load in artifacts needed to be called in production_script.py\n      metadata_obj  = context.artifacts[&quot;meta_data_path&quot;]\n      etc...\n\n   def predict(self, context, model_input):\n      from production_script.py import predict\n\n      return predict(model_input)\n<\/code><\/pre>\n<p>When I create the pyfunc model, the files are in the same directory as that script.  I've saved the joblib files as S3 artifacts that I load with load_context().  Alternatively, I've tried including them with the production_script.py file in the code_path as shown below.  Either way, when I try to invoke and endpoint that hosts this pyfunc model, I receive and error that the .joblib files cannot be found.  My question is, how would I give the model access to these files?<\/p>\n<pre><code># trying to load them as artifacts\nartifacts = {&quot;model_path&quot;: &lt;path to model on s3&gt;, meta_data_path: &lt;path to meta file on s3&gt;}\n\n# adding files to code path\nmlflow.pyfunc.log_model(\n        python_model=Custom_Pyfunc_Wrapper(),\n        artifact_path=&quot;model&quot;,\n        # try adding to code path\n        code_path=[&quot;.\/production_script.py&quot;, &quot;.\/model_file.joblib&quot;, &quot;.\/model_meta.joblib&quot;],\n        registered_model_name=&quot;production_model&quot;,\n        artifacts=artifacts, # try loading from artifacts \n        pip_requirements=[&lt;requirments list here&gt;]\n    )\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-24 21:19:59.733 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":95,
        "Owner_creation_date":"2021-12-16 00:24:08.31 UTC",
        "Owner_last_access_date":"2022-09-23 00:00:12.383 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-06-24 21:33:30.527 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow active run does not match environment run id",
        "Question_body":"<p>I am trying to perform an MLFlow run but stuck with the following error after trying a lot of things.<\/p>\n<pre><code>\nrun = mlflow.active_run()\nif run:\n    print(&quot;Active run_id: {}&quot;.format(run.info.run_id))\n    mlflow.end_run()\n\nmlflow.set_experiment('TNF_EXP') \nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000\/&quot;) # Actual Server URI instead of localhost\nexperiment = mlflow.get_experiment_by_name(&quot;TNF_EXP&quot;)\n\nwith mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n...\n...\n\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error -<\/p>\n<pre><code>File &quot;\/...\/ModelTrainer.py&quot;, line 108, in train\n    with mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n  File &quot;\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 207, in start_run\n    &quot;arguments&quot;.format(existing_run_id)\nmlflow.exceptions.MlflowException: Cannot start run with ID e9953eb5918845bb9be1xxxxxx because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments\n2021\/02\/11 09:41:36 ERROR mlflow.cli: === Run (ID 'e9953eb5918845bb9be1xxxxxx') failed ===\n<\/code><\/pre>\n<p>I noticed I had an <code>active run<\/code> earlier so I included the first <code>if block<\/code> to end that run. The code ran successfully and I was able to log the data on MLFlow UI but now when I run it I start getting the same issue. There are no active runs found before starting a new run currently.<\/p>\n<blockquote>\n<p>FYI, I am running the code on Azure server with the respective tracking URI mentioned in the code.<\/p>\n<\/blockquote>\n<p>However the code runs fine if I include an argument <code>--experiment-name=&quot;TNF_EXP&quot;<\/code> in the <code>mlflow run<\/code> command on the CLI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-11 09:55:19.843 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":4094,
        "Owner_creation_date":"2019-06-06 12:06:56.093 UTC",
        "Owner_last_access_date":"2022-09-19 19:12:46.643 UTC",
        "Owner_location":null,
        "Owner_reputation":861,
        "Owner_up_votes":117,
        "Owner_down_votes":6,
        "Owner_views":149,
        "Answer_body":"<p>That is primarily because you have started a run with <code>default experiment name<\/code> and then you are trying to set the <code>experiment_name<\/code> as &quot;TNF_EXP&quot;.<\/p>\n<p>Will suggest you to make use of <code>mlflow.run(..., experiment_name=&quot;TNF_EXP&quot;)<\/code> python method then running it from the <code>CLI<\/code>.<\/p>\n<p>You can find more information <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.run\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-25 07:27:52.013 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-31 01:02:03.883 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pandas|scikit-learn|databricks|xgboost|mlflow",
        "Question_view_count":451,
        "Owner_creation_date":"2019-07-11 07:02:37.217 UTC",
        "Owner_last_access_date":"2022-09-21 21:43:57.89 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":77,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-08 12:56:27.86 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow creates a new experiment run when logging manually along with autolog",
        "Question_body":"<p>I am using MLFlow to log metrics and artefacts in the AzureML workspace. With <code>autolog<\/code>, tensorflow training metrics are available in the experiment run in the AzureML workspace. Along with auto-logging of metrics - I want to log extra metrics and plots in the same experiment run. Doing it with MLFlow - it is creating a new experiment run.<\/p>\n<p><strong>Auto logging:<\/strong><\/p>\n<p><code>mlflow.autolog()<\/code><\/p>\n<p><strong>Manual logging:<\/strong><\/p>\n<p><code>mlflow.log_metric(f&quot;label-A&quot;, random.randint(80, 90))<\/code><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DC8S4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DC8S4.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Expected:<\/strong>\nManually logged metrics are available in the same experiment run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-09 12:19:22.673 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"metrics|mlflow|azure-machine-learning-service",
        "Question_view_count":316,
        "Owner_creation_date":"2015-02-16 15:05:31.103 UTC",
        "Owner_last_access_date":"2022-09-11 17:21:50.487 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":3982,
        "Owner_up_votes":171,
        "Owner_down_votes":17,
        "Owner_views":577,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to copy local MLflow run to remote tracking server?",
        "Question_body":"<p>I am currently tracking my MLflow runs to a local file path URI. I would also like to set up a remote tracking server to share with my collaborators. One thing I would like to avoid is to log everything to the server, as it might soon be flooded with failed runs.<\/p>\n\n<p>Ideally, I'd like to keep my local tracker, and then be able to send only the promising runs to the server.<\/p>\n\n<p>What is the recommended way of copying a run from a local tracker to a remote server?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-23 14:04:36.413 UTC",
        "Question_favorite_count":1.0,
        "Question_score":7,
        "Question_tags":"mlflow",
        "Question_view_count":611,
        "Owner_creation_date":"2017-04-14 20:16:23.87 UTC",
        "Owner_last_access_date":"2022-06-30 12:51:59.29 UTC",
        "Owner_location":null,
        "Owner_reputation":199,
        "Owner_up_votes":132,
        "Owner_down_votes":1,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to create seperate mlflow custom models for training and prediction?",
        "Question_body":"<p>My requirement is to create separate Mlflow custom models for training and prediction.\nI want to create training model and use those training model in prediction model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-29 06:41:58.37 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":45,
        "Owner_creation_date":"2022-04-29 06:10:27.497 UTC",
        "Owner_last_access_date":"2022-09-21 07:40:29.71 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Azure : Ansible role for deploying ML model integrated over databricks",
        "Question_body":"<p>I have developed ML predictive model on historical data in Azure Databricks using python notebook.\nWhich means i have done data extraction, preparation, feature engineering and model training everything done in Databricks using python notebook.\nI have almost completed development part of it, now we want to deploy ML model into production using ansible roles.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-08-10 07:13:27.8 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"deployment|databricks|mlflow|mlmodel",
        "Question_view_count":128,
        "Owner_creation_date":"2020-11-16 06:35:10.493 UTC",
        "Owner_last_access_date":"2021-11-29 05:21:57.803 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-11 06:34:36.903 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to I track loss at epoch using mlflow\/tensorflow?",
        "Question_body":"<p>I want to use mlflow to track the development of a TensorFlow model. How do I log the loss at each epoch? I have written the following code:<\/p>\n<pre><code>mlflow.set_tracking_uri(tracking_uri)\n\nmlflow.set_experiment(&quot;\/deep_learning&quot;)\nwith mlflow.start_run():\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)\n    mlflow.log_param(&quot;learning_rate&quot;, learning_rate)\n    mlflow.log_param(&quot;epochs&quot;, epochs)\n    mlflow.log_param(&quot;Optimizer&quot;, opt)\n    mlflow.log_metric(&quot;train_loss&quot;, train_loss)\n    mlflow.log_metric(&quot;val_loss&quot;, val_loss)\n    mlflow.log_metric(&quot;test_loss&quot;, test_loss)\n    mlflow.log_metric(&quot;test_mse&quot;, test_mse)\n    mlflow.log_artifacts(&quot;.\/model&quot;)\n<\/code><\/pre>\n<p>If I change the train_loss and val_loss to<\/p>\n<pre><code>train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Got invalid value [12.041399002075195] for metric 'train_loss' (timestamp=1649783654667). Please specify value as a valid double (64-bit floating point)\n<\/code><\/pre>\n<p>How to I save the the loss and the val_loss at all epochs, so I can visualise a learning curve within mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-12 17:19:23.393 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|keras|tf.keras|mlflow",
        "Question_view_count":320,
        "Owner_creation_date":"2017-10-10 14:24:52.477 UTC",
        "Owner_last_access_date":"2022-06-13 10:15:22.463 UTC",
        "Owner_location":"Liverpool, UK",
        "Owner_reputation":77,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":"<p>As you can read <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.keras.html#module-mlflow.keras\" rel=\"nofollow noreferrer\">here<\/a>. You can use <code>mlflow.tensorflow.autolog()<\/code> and this, (from doc):<\/p>\n<blockquote>\n<p>Enables (or disables) and configures autologging from Keras to MLflow. Autologging captures the following information:<\/p>\n<blockquote>\n<p>fit() or fit_generator() parameters; optimizer name; learning rate; epsilon\n...<\/p>\n<\/blockquote>\n<\/blockquote>\n<p>For example:<\/p>\n<pre><code># !pip install mlflow\nimport tensorflow as tf\nimport mlflow\nimport numpy as np\n\n\nX_train = np.random.rand(100,100)\ny_train = np.random.randint(0,10,100)\n    \n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(100,))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=.4))\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid'))        \nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              optimizer='Adam', \n              metrics=['accuracy'])\nmodel.summary()\n\n\nmlflow.tensorflow.autolog()\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=50)\n<\/code><\/pre>\n<p>Or as you mention in the comment you can use <code>mlflow.set_tracking_uri()<\/code> like below:<\/p>\n<pre><code>mlflow.set_tracking_uri('http:\/\/127.0.0.1:5000')\ntracking_uri = mlflow.get_tracking_uri()\nwith mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n    batch_size=50\n    history = model.fit(X_train, y_train, epochs=2, batch_size=batch_size)\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)  \n<\/code><\/pre>\n<p>For getting results:<\/p>\n<pre><code>!mlflow ui\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[....] [...] [INFO] Starting gunicorn 20.1.0\n[....] [...] [INFO] Listening at: http:\/\/127.0.0.1:5000 (****)\n[....] [...] [INFO] Using worker: sync\n[....] [...] [INFO] Booting worker with pid: ****\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-04-13 00:26:28.953 UTC",
        "Answer_last_edit_date":"2022-04-14 00:22:12.377 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Packaging MultiLabelBinarizer into scikit-learn Pipeline for inference on new data",
        "Question_body":"<p>I'm building a multilabel classifier to predict labels based on a text field. For example, predicting genres based on movie title. I'd like to use <code>MultiLabelBinarizer()<\/code> to binarize a column containing all applicable genre labels. For example, <code>['action','comedy','drama']<\/code> gets split into three columns with 0\/1 values. <\/p>\n\n<p>The reason I'm using <code>MultiLabelBinarizer()<\/code> is so that I can use the built-in <code>inverse_transform()<\/code> function to turn the output array (e.g. <code>array([0, 0, 1, 0, 1])<\/code> directly into user-friendly text output (<code>['action','drama']<\/code>). <\/p>\n\n<p>The classifier works, but I'm having issues predicting on new data. I can't find a way to integrate the <code>MultiLabelBinarizer()<\/code> into my Pipeline so that it can be saved and re-loaded for inference on new data. One solution is to save it as a pickle object separately and load it back each time, but I'd like to avoid having this dependency in production.<\/p>\n\n<p>I know that this is similar to the tf-idf vector I've built into my Pipeline, but different in the sense that it's applied to the target column (genre labels) instead of my independent variable (the text comment). Here's my code for training the multilabel SVM:<\/p>\n\n<pre><code>def svm_train(df):  \n  mlb = MultiLabelBinarizer()\n  y = mlb.fit_transform(df['Genres'])\n\n  with mlflow.start_run():\n    x_train, x_test, y_train, y_test = train_test_split(df['Movie Title'], y, test_size=0.3)\n\n    # Instantiate TF-IDF Vectorizer and SVM Model\n    tfidf_vect = TfidfVectorizer()\n    mdl = OneVsRestClassifier(LinearSVC(loss='hinge'))\n    svm_pipeline = Pipeline([('tfidf', tfidf_vect), ('clf', mdl)])\n\n    svm_pipeline.fit(x_train, y_train)\n    prediction = svm_pipeline.predict(x_test)\n\n    report = classification_report(y_test, prediction, target_names=mlb.classes_)\n\n    mlflow.sklearn.log_model(svm_pipeline, \"Multilabel Classifier\")\n    mlflow.log_artifact(mlb, \"MLB\")\n\n  return(report)\n\nsvm_train(df)\n<\/code><\/pre>\n\n<p>Inference consists of re-loading the saved model from MLflow (same as loading back in a pickle file) in a separate Databricks notebook and predicting using the Pipeline:<\/p>\n\n<pre><code>def predict_labels(new_data):\n  model_uri = '...MLflow path...'\n  model = mlflow.sklearn.load_model(model_uri)\n  predictions = model.predict(new_data)\n  # If I can't package the MultiLabelBinarizer() into the Pipeline, this \n  # is where I'd have to load the pickle object mlb\n  # so that I can inverse_transform()\n  return mlb.inverse_transform(predictions)\n\nnew_data = ['Some movie title']\npredict_labels(new_data)\n\n['action','comedy']\n<\/code><\/pre>\n\n<p>Here's all of the libraries I'm using:<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport mlflow\nimport mlflow.sklearn\nimport glob, os\nfrom pyspark.sql import DataFrame\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-09-13 13:52:16.8 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|pandas|machine-learning|scikit-learn|mlflow",
        "Question_view_count":1137,
        "Owner_creation_date":"2010-10-07 00:08:50.503 UTC",
        "Owner_last_access_date":"2020-06-08 16:56:03.537 UTC",
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How does MLFlow track the data used in experiments?",
        "Question_body":"<p>I am just starting learning about MLFlow, so apologies if I don't use the correct terminology.<\/p>\n<p>I have done some coding and experiments with MLFlow, in which I named an experiment, and track some metrics, plots and even models.<\/p>\n<p>Later in the MLFlow UI I can see a list of experiments with their tracked elements and artifacts.<\/p>\n<p>My question is how does this work with datasets?<\/p>\n<p>For example if I use a particular data set to train , or to do inference with a model and some metrics are recorded, how can I track that a particular dataset was used to obtain a particular metric?<\/p>\n<p>I am imaging that the <em>entire<\/em> dataset is not stored, is it? Because that would use a lot of disk?<\/p>\n<p>Any pointers about this theme will be greatly appreciated<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-13 00:20:12 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":20,
        "Owner_creation_date":"2015-01-14 01:17:49.333 UTC",
        "Owner_last_access_date":"2022-09-24 09:09:14.427 UTC",
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Error on Spark MLFlow Model Registery using DataBricks after upgrade: cannot load trained XGBoost model",
        "Question_body":"<p>I had some code for training and then using XGBoost models on a Databricks environment. As my runtime version got deprecated, I upgraded it, but I quickly noticed I could not load my trained models anymore. The reason seems to be a change in the naming of functions in Sparkdl:<\/p>\n<pre><code>Error loading metadata: Expected class name sparkdl.xgboost.xgboost_core.XgboostClassifierModel but found class name sparkdl.xgboost.xgboost.XgboostClassifierModel\n<\/code><\/pre>\n<p>Would anyone have advise on how to fix this issue? Maybe modify the metadata?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-28 09:03:38.097 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pyspark|xgboost|azure-databricks|mlflow",
        "Question_view_count":111,
        "Owner_creation_date":"2017-06-24 15:53:38.333 UTC",
        "Owner_last_access_date":"2022-09-08 11:29:05.553 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-31 18:51:28.92 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":904,
        "Owner_creation_date":"2016-06-17 18:38:51.113 UTC",
        "Owner_last_access_date":"2022-09-22 17:49:54.303 UTC",
        "Owner_location":"Michigan",
        "Owner_reputation":414,
        "Owner_up_votes":25,
        "Owner_down_votes":5,
        "Owner_views":39,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-02-21 18:56:57.663 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to log custom Pytorch model with Mlflow?",
        "Question_body":"<p>I have been using Presumm <a href=\"https:\/\/github.com\/nlpyang\/PreSumm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/nlpyang\/PreSumm<\/a> for text summarization.<\/p>\n\n<p>However, in <code>src\/train_abstractive.py<\/code>, the model learner <code>trainer<\/code> is not a <code>torch.nn.Module<\/code>. However, the input <code>AbsSummarizer<\/code> is an extension of the <code>torch.nn.Module<\/code> class.<\/p>\n\n<p>I want to use <code>mlflow.pytorch.log_model<\/code> to save the model as a native pytorch model. But <code>trainer<\/code> is not a <code>torch.nn.Module<\/code>. How do I go about this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-09 15:50:00.773 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pytorch|mlflow",
        "Question_view_count":495,
        "Owner_creation_date":"2017-05-10 23:32:23.587 UTC",
        "Owner_last_access_date":"2022-09-13 21:34:49.32 UTC",
        "Owner_location":null,
        "Owner_reputation":1319,
        "Owner_up_votes":592,
        "Owner_down_votes":5,
        "Owner_views":377,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-18 08:10:47.973 UTC",
        "Question_favorite_count":null,
        "Question_score":6,
        "Question_tags":"mlflow",
        "Question_view_count":3582,
        "Owner_creation_date":"2014-08-18 14:07:01.673 UTC",
        "Owner_last_access_date":"2022-09-15 04:40:38.707 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-09-29 21:39:34.477 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2019-09-18 08:47:48.593 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow Projects can't find conda executable",
        "Question_body":"<p>I am following the tutorial on MLFlow website. I was able to run the train.py and mlflow ui worked fine. Packaging the project tries to use env variable  MLFLOW_CONDA_HOME but can't find conda.\nI have tried setting the variable to the path of anaconda3\/condabin but it doesn't seem to find my executable. This is the error I get:\n ERROR mlflow.cli: === Could not find Conda executable at \/anaconda3\/condabin\\bin\/conda. Ensure Conda is installed as per the inst\nructions at <a href=\"https:\/\/conda.io\/docs\/user-guide\/install\/index.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/install\/index.html<\/a>. You can also configure MLflow to look for a specific Conda executable by setting the MLFLOW_CONDA_HOME environment variable\n to the path of the Conda executable ===<\/p>\n\n<p>Adding \\bin\/conda at the end of my path seems to be the problem, I am not sure why mlflow is doing it. I even tried setting it to my python.exe in my conda env, but no luck. I can't find bin\/conda folder in my Anaconda folder anywhere.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":2,
        "Question_creation_date":"2019-12-14 00:47:58.197 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|anaconda|conda|mlflow",
        "Question_view_count":3001,
        "Owner_creation_date":"2013-03-25 16:26:12.147 UTC",
        "Owner_last_access_date":"2020-08-11 23:36:17.443 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Using MLFlow for commercial use",
        "Question_body":"<p>It seems that from April 2020, we cannot use anaconda for &quot;commercial use&quot; meaning for example (organizations with more than 200 employees for example)<\/p>\n<p>Since MLFlow seems to use yaml files that contain allusions to conda, how is the situation with MLFlow?<\/p>\n<p>Can MLFlow be used for commercial use?<\/p>\n<p>Note: This question <em>is<\/em> about programming since I intend to use MLFlow in our programs and I have to decide if we can or not<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-14 12:16:23.36 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"anaconda|mlflow",
        "Question_view_count":21,
        "Owner_creation_date":"2015-01-14 01:17:49.333 UTC",
        "Owner_last_access_date":"2022-09-24 09:09:14.427 UTC",
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to log a table of metrics into mlflow",
        "Question_body":"<p>I am trying to see if mlflow is the right place to store my metrics in the model tracking.  According to the doc log_metric takes either a key value or a dict of key-values.  I am wondering how to log something like below into mlflow so it can be visualized meaningfully.<\/p>\n<pre><code>          precision    recall  f1-score   support\n\n  class1       0.89      0.98      0.93       174\n  class2       0.96      0.90      0.93        30\n  class3       0.96      0.90      0.93        30\n  class4       1.00      1.00      1.00         7\n  class5       0.93      1.00      0.96        13\n  class6       1.00      0.73      0.85        15\n  class7       0.95      0.97      0.96        39\n  class8       0.80      0.67      0.73         6\n  class9       0.97      0.86      0.91        37\n class10       0.95      0.81      0.88        26\n class11       0.50      1.00      0.67         5\n class12       0.93      0.89      0.91        28\n class13       0.73      0.84      0.78        19\n class14       1.00      1.00      1.00         6\n class15       0.45      0.83      0.59         6\n class16       0.97      0.98      0.97       245\n class17       0.93      0.86      0.89       206\n\naccuracy                           0.92       892\n<\/code><\/pre>\n<p>macro avg       0.88      0.90      0.88       892\nweighted avg       0.93      0.92      0.92       892<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-02-17 00:35:43.563 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":546,
        "Owner_creation_date":"2015-03-18 00:41:20.947 UTC",
        "Owner_last_access_date":"2022-04-21 20:46:05.3 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Nested runs using MLflowClient",
        "Question_body":"<p>In <code>mlflow<\/code>, you can run nested runs using the fluent projects API which are collapsable in the UI. E.g. by using the following code (see <a href=\"https:\/\/databricks.com\/blog\/2018\/11\/21\/mlflow-v0-8-0-features-improved-experiment-ui-and-deployment-tools.html\" rel=\"noreferrer\">this<\/a> for UI support):<\/p>\n\n<pre><code>with mlflow.start_run(nested=True):\n  mlflow.log_param(\"mse\", 0.10)\n  mlflow.log_param(\"lr\", 0.05)\n  mlflow.log_param(\"batch_size\", 512)\n  with mlflow.start_run(nested=True):\n    mlflow.log_param(\"max_runs\", 32)\n    mlflow.log_param(\"epochs\", 20)\n    mlflow.log_metric(\"acc\", 98)\n    mlflow.log_metric(\"rmse\", 98)\n  mlflow.end_run()\n<\/code><\/pre>\n\n<p>Due to database connection issues, I want to use a single mlflow client across my application.<\/p>\n\n<p>How can I stack runs, e.g. for hyperparameter optimization, using created runs via <code>MlflowClient().create_run()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-06-14 12:02:38.62 UTC",
        "Question_favorite_count":null,
        "Question_score":7,
        "Question_tags":"python|mlflow",
        "Question_view_count":4030,
        "Owner_creation_date":"2015-02-02 20:14:19.383 UTC",
        "Owner_last_access_date":"2022-08-10 17:44:00.06 UTC",
        "Owner_location":"Paderborn, Germany",
        "Owner_reputation":391,
        "Owner_up_votes":51,
        "Owner_down_votes":18,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to call a model's artifacts (pickeled vectorizer) when the model is on Production in databricks?",
        "Question_body":"<p>I am using databrick, machine learning view. I have successfully created and saved my model and also logged my pickled vectorizer as artifacts to it. I would like to load it in a different notebook; the model and the artifacts belong to the model which is currently in production.<\/p>\n<pre><code>   import mlflow.pyfunc\n\nmodel_name = &quot;Sentiment&quot;\nstage = 'Production'\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/{model_name}\/{stage}&quot;\n)\n<\/code><\/pre>\n<p>So this code seems to be working but it does not load the artifacts or if it does, i do not know how to display them.<\/p>\n<p>I have found this code but not sure what to do with it to only get the artifacts from the model which is in Production.<\/p>\n<pre><code>  from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\nversion =1\nmodel_uri = MlflowClient.get_model_version_download_uri(name=model_name, version=version)\nModelsArtifactRepository(model_uri).download_artifacts(artifact_path=&quot;&quot;)\n<\/code><\/pre>\n<p>even when i run this i get an error :<\/p>\n<p>TypeError: get_model_version_download_uri() missing 1 required positional argument: 'self'<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-16 12:56:25.133 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"model|databricks|mlflow|artifacts",
        "Question_view_count":10,
        "Owner_creation_date":"2022-06-06 08:24:57.99 UTC",
        "Owner_last_access_date":"2022-09-19 13:50:16.62 UTC",
        "Owner_location":null,
        "Owner_reputation":47,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-16 13:15:02.563 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"can mlflow.spark's saved model loaded as Spark\/Scala Pipeline?",
        "Question_body":"<p>Our algorithm engineer is developing machine learning model using pyspark &amp; mlflow. He's trying to save the model using <code>mlflow.spark<\/code> API &amp; the model format is the native <code>spark MLlib<\/code> format. Could the model be loaded from <code>Spark Scala<\/code> code? It seems that mlflow is quite restricted for cross-language usage.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-19 09:04:39.793 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"scala|pyspark|mlflow",
        "Question_view_count":630,
        "Owner_creation_date":"2019-03-25 09:33:34.607 UTC",
        "Owner_last_access_date":"2021-03-04 14:00:13.497 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Registering models from Databricks to Azure ML and save Azure ML image into provided ACR(Non Default ACR of AML Workspace)",
        "Question_body":"<p>I'm trying to register a data bricks model tp <code>Azure ML<\/code> workspace with <code>mlflow.azure.base_image<\/code> model. But with this method, we can save the <code>Azure ML<\/code> image to default <code>ACR<\/code> connected to the <code>Azure ML<\/code> workspace.<\/p>\n<p>But I want to save the <code>Azure ML<\/code> image to another existing <code>ACR<\/code>. Need help in figuring out the design.<\/p>\n<p>The method I'm using is as follows<\/p>\n<pre><code>    workspace = Workspace.create(name = workspace_name,\n                                 location = workspace_location,\n                                 resource_group = resource_group,\n                                 subscription_id = subscription_id,\n                                 auth=svc_pr,\n                                 exist_ok=True)\n\n    import mlflow.azureml\n\n    model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \n                                                          workspace=workspace,\n                                                          model_name=&quot;winequality&quot;,\n                                                          image_name=&quot;winequality&quot;,\n                                                          description=&quot;Sklearn ElasticNet image for predicting wine quality&quot;,\n                                                          synchronous=True)\n\n    #model_image.wait_for_creation(show_output=True)\n    print(&quot;Access the following URI for build logs: {}&quot;.format(model_image.image_build_log_uri))                                    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-22 11:02:29.39 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|azure-databricks|mlflow|azure-machine-learning-service",
        "Question_view_count":158,
        "Owner_creation_date":"2018-10-30 10:19:53.043 UTC",
        "Owner_last_access_date":"2022-09-22 15:34:05.27 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-10-22 11:10:53.553 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Unable to serve an mlflow model locally",
        "Question_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-28 13:01:27.777 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"deployment|localhost|mlflow",
        "Question_view_count":1277,
        "Owner_creation_date":"2019-11-14 13:58:10.56 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:32.563 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-01-28 13:30:03.1 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"working directory changes to \/tmp\/ when python script runs with mlflow",
        "Question_body":"<p>I have a strange issue with python working directory when running with mlflow run -e build .\nThe script running successfully locally\/using IDE, but when running it with mlflow the problem is that the working directory changes to \/tmp folders instead of the correct working directory where the script resides (I have some path dependencies that certain folders should be present in .\/* so thats why my process fails.<\/p>\n<p>I had a feeling that something with the working directory messed up so I did os.getcwd() prints and saw the issue with temp folders.<\/p>\n<p>I had a similar project that I configured in a similar manner before and didn't have these issues.<\/p>\n<p>any idea what might be the issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-16 19:39:34.247 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|path|mlflow",
        "Question_view_count":286,
        "Owner_creation_date":"2017-11-20 13:50:10.877 UTC",
        "Owner_last_access_date":"2022-02-14 08:54:35.333 UTC",
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Is there a way to manage permissions at an experiment level in MLflow?",
        "Question_body":"<p>Is there a way to manage permissions at an experiment level in MLflow?  We would like to have a shared server but would like to be able to manage permissions at an experiment level - e.g. admin can view all experiments, user_group1 can manage experiment1 - perhaps different groups can see results vs post results.<\/p>\n\n<p>It looks like it is possible in databricks: <a href=\"https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions<\/a>  but I can't find anything in the opensource APIdocs.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-29 15:57:37.393 UTC",
        "Question_favorite_count":2.0,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":1567,
        "Owner_creation_date":"2017-02-17 11:58:49.157 UTC",
        "Owner_last_access_date":"2022-09-21 13:15:40.707 UTC",
        "Owner_location":null,
        "Owner_reputation":289,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFLOW with HDFS artifact store",
        "Question_body":"<p>I need some help to configure setting up hdfs as the artifact store for mlflow. I have mlflow and hdfs all running in separate containers across a docket network. When I try to log the model I get the following error:<\/p>\n\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-35-e54b25688d8e&gt; in &lt;module&gt;\n      1 # log model artifacts\n----&gt; 2 pyfunc.log_model('hdfs:\/\/hdfs:8020\/', python_model=LGBWrapper(), artifacts=artifacts, conda_env=conda_env)\n      3 # pyfunc.save_model('prediction_model8', python_model=LGBWrapper(), artifacts=artifacts, conda_env=conda_env)\n      4 \n      5 # set tag for selecting model\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/__init__.py in log_model(artifact_path, loader_module, data_path, code_path, conda_env, python_model, artifacts, registered_model_name)\n    697                      artifacts=artifacts,\n    698                      conda_env=conda_env,\n--&gt; 699                      registered_model_name=registered_model_name)\n    700 \n    701 \n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/models\/__init__.py in log(cls, artifact_path, flavor, registered_model_name, **kwargs)\n    100             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n    101             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n--&gt; 102             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n    103             try:\n    104                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py in log_artifacts(local_dir, artifact_path)\n    321     \"\"\"\n    322     run_id = _get_or_start_run().info.run_id\n--&gt; 323     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n    324 \n    325 \n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/tracking\/client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n    265         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n    266         \"\"\"\n--&gt; 267         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n    268 \n    269     def _record_logged_model(self, run_id, mlflow_model):\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n    266         run = self.get_run(run_id)\n    267         artifact_repo = get_artifact_repository(run.info.artifact_uri)\n--&gt; 268         artifact_repo.log_artifacts(local_dir, artifact_path)\n    269 \n    270     def list_artifacts(self, run_id, path=None):\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/hdfs_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n     47         hdfs_base_path = _resolve_base_path(self.path, artifact_path)\n     48 \n---&gt; 49         with hdfs_system(host=self.host, port=self.port) as hdfs:\n     50 \n     51             if not hdfs.exists(hdfs_base_path):\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/contextlib.py in __enter__(self)\n     79     def __enter__(self):\n     80         try:\n---&gt; 81             return next(self.gen)\n     82         except StopIteration:\n     83             raise RuntimeError(\"generator didn't yield\") from None\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/hdfs_artifact_repo.py in hdfs_system(host, port)\n    175                                 driver=driver,\n    176                                 kerb_ticket=kerb_ticket,\n--&gt; 177                                 extra_conf=extra_conf)\n    178     yield connected\n    179     connected.close()\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/pyarrow\/hdfs.py in connect(host, port, user, kerb_ticket, driver, extra_conf)\n    213     fs = HadoopFileSystem(host=host, port=port, user=user,\n    214                           kerb_ticket=kerb_ticket, driver=driver,\n--&gt; 215                           extra_conf=extra_conf)\n    216     return fs\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/pyarrow\/hdfs.py in __init__(self, host, port, user, kerb_ticket, driver, extra_conf)\n     36                  driver='libhdfs', extra_conf=None):\n     37         if driver == 'libhdfs':\n---&gt; 38             _maybe_set_hadoop_classpath()\n     39 \n     40         self._connect(host, port, user, kerb_ticket, driver, extra_conf)\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/pyarrow\/hdfs.py in _maybe_set_hadoop_classpath()\n    138             classpath = _hadoop_classpath_glob(hadoop_bin)\n    139     else:\n--&gt; 140         classpath = _hadoop_classpath_glob('hadoop')\n    141 \n    142     os.environ['CLASSPATH'] = classpath.decode('utf-8')\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/site-packages\/pyarrow\/hdfs.py in _hadoop_classpath_glob(hadoop_bin)\n    163 \n    164     hadoop_classpath_args = (hadoop_bin, 'classpath', '--glob')\n--&gt; 165     return subprocess.check_output(hadoop_classpath_args)\n    166 \n    167 \n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/subprocess.py in check_output(timeout, *popenargs, **kwargs)\n    354 \n    355     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n--&gt; 356                **kwargs).stdout\n    357 \n    358 \n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/subprocess.py in run(input, timeout, check, *popenargs, **kwargs)\n    421         kwargs['stdin'] = PIPE\n    422 \n--&gt; 423     with Popen(*popenargs, **kwargs) as process:\n    424         try:\n    425             stdout, stderr = process.communicate(input, timeout=timeout)\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\n    727                                 c2pread, c2pwrite,\n    728                                 errread, errwrite,\n--&gt; 729                                 restore_signals, start_new_session)\n    730         except:\n    731             # Cleanup if the child failed starting.\n\n~\/opt\/anaconda3\/envs\/soptai\/lib\/python3.6\/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\n   1362                         if errno_num == errno.ENOENT:\n   1363                             err_msg += ': ' + repr(err_filename)\n-&gt; 1364                     raise child_exception_type(errno_num, err_msg, err_filename)\n   1365                 raise child_exception_type(err_msg)\n   1366 \n\nFileNotFoundError: [Errno 2] No such file or directory: 'hadoop': 'hadoop'\n<\/code><\/pre>\n\n<p>Access to hdfs is not an issue as they are in the same network and other services running on the same network can access hdfs as well.\nMaybe there are some changes to be made to the core-site.xml or hdfs-site.xml as someone who reported a similar issue suggested (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1466\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1466<\/a>). Unfortunately, I have no idea what those changes need to be. Please assist!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-06 15:27:25.137 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"hdfs|mlflow",
        "Question_view_count":672,
        "Owner_creation_date":"2020-05-06 15:25:39.94 UTC",
        "Owner_last_access_date":"2020-07-29 01:49:03.517 UTC",
        "Owner_location":null,
        "Owner_reputation":32,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"The connection was reset when using Docker even though the port was mapped",
        "Question_body":"<p>I am trying to use mlflow from inside a container.\nMy Dockerfile is<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip install mlflow\n\nRUN pip install sklearn\n<\/code><\/pre>\n<p>I use it with a <code>run.sh<\/code> script<\/p>\n<pre><code>#!\/bin\/bash\ndocker build -t chapter_1_homlflow .\ndocker run -p 8888:8888 -p 5000:5000 -v $(pwd):\/home\/jovyan\/ -it chapter_1_homlflow\n<\/code><\/pre>\n<p>As you can see the ports 8888 and 5000 are being mapped.\nWhen I execute <code>run.sh<\/code> I am immediately inside a jupyter notebook that uses the port 8888 of the local host.<\/p>\n<p>I do my stuff with mlflow and now I want to see its UI<\/p>\n<p>so I enter the container with <code>docker exec -it &lt;container name&gt; bash<\/code> and from inside I do<\/p>\n<pre><code>mlflow ui\n<\/code><\/pre>\n<p>and I get<\/p>\n<pre><code>[2022-09-05 13:15:50 +0000] [4567] [INFO] Starting gunicorn 20.1.0\n[2022-09-05 13:15:50 +0000] [4567] [INFO] Listening at: http:\/\/127.0.0.1:5000 (4567)\n[2022-09-05 13:15:50 +0000] [4567] [INFO] Using worker: sync\n[2022-09-05 13:15:50 +0000] [4568] [INFO] Booting worker with pid: 4568\n<\/code><\/pre>\n<p>So everything seems fine... but ...<\/p>\n<p>when I go to <code>http:\/\/127.0.0.1:5000<\/code> I got<\/p>\n<pre><code>The connection was reset\n\nThe connection to the server was reset while the page was loading.\n<\/code><\/pre>\n<p>I wonder what I might be doing wrong and how to solve it<\/p>\n<p>Btw, when I write the URL I can see clearly there are MLFlow pages there. I just cannot view them<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-05 13:29:14.457 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|networking|mlflow",
        "Question_view_count":14,
        "Owner_creation_date":"2015-01-14 01:17:49.333 UTC",
        "Owner_last_access_date":"2022-09-24 09:09:14.427 UTC",
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Question_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-15 17:51:33.613 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":768,
        "Owner_creation_date":"2018-06-09 20:04:00.85 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:41.153 UTC",
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":10,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Answer_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-19 08:31:17.983 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How does autologging work in MLOps platforms like Comet or MLFlow?",
        "Question_body":"<p>I was wondering how the implementation of logging is done where you just need to create an experiment object from comet_ml and it auto-detects and gives out all the statistics of the trained experiment. Is there some sort of logging system used?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-03 09:14:21.397 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"comet|mlflow|mlops",
        "Question_view_count":55,
        "Owner_creation_date":"2020-08-05 13:21:06.543 UTC",
        "Owner_last_access_date":"2022-03-22 05:41:49.233 UTC",
        "Owner_location":"Nashik, Maharashtra, India",
        "Owner_reputation":18,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow saving weights after each epoch",
        "Question_body":"<p>I have been testing some small examples with MLflow tracking but for my usecase I would like to have the weights saved after each epoch. \nSometimes I kill the runs before they are completely finished (I cannot use earlystopping), but what I experience now is that the weights do not get saved to the tracking ui server.\nIs there a way to do this after each epoch?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-10-24 12:50:01.1 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"keras|mlflow",
        "Question_view_count":1121,
        "Owner_creation_date":"2018-08-15 21:09:08.473 UTC",
        "Owner_last_access_date":"2022-09-23 10:17:25.177 UTC",
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":344,
        "Owner_up_votes":74,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Answer_body":"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints\/weights are saved to disk, you can log them with <code>mlflow_log_artifact()<\/code> or <code>mlflow_log_artifacts()<\/code>.  From the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#logging-functions\" rel=\"nofollow noreferrer\">docs<\/a>,<\/p>\n\n<blockquote>\n  <p><strong>mlflow.log_artifact()<\/strong> logs a local file or directory as an artifact,\n  optionally taking an artifact_path to place it in within the run\u2019s\n  artifact URI. Run artifacts can be organized into directories, so you\n  can place the artifact in a directory this way.<\/p>\n  \n  <p><strong>mlflow.log_artifacts()<\/strong> logs all the files in a given directory as\n  artifacts, again taking an optional artifact_path.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-31 12:38:13.02 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow Registry high availability",
        "Question_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-28 13:18:31.073 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow|mlmodel",
        "Question_view_count":501,
        "Owner_creation_date":"2013-09-07 14:14:09.26 UTC",
        "Owner_last_access_date":"2022-07-27 10:56:50.33 UTC",
        "Owner_location":null,
        "Owner_reputation":470,
        "Owner_up_votes":128,
        "Owner_down_votes":7,
        "Owner_views":45,
        "Answer_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-02 01:27:08.087 UTC",
        "Answer_last_edit_date":"2020-05-02 04:42:51.917 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Saving model artifacts with Kubeflow without Pipeline",
        "Question_body":"<p>I am using mlflow as of now in my jupyterhub environment for model tracking and I feel its easy to keep track of artifacts in mlflow simply by calling the run like:<\/p>\n\n<pre><code>with mlflow.start_run():\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    predicted_qualities = lr.predict(test_x)\n\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    mlflow.log_param(\"alpha\", alpha)\n    mlflow.log_param(\"l1_ratio\", l1_ratio)\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"r2\", r2)\n    mlflow.log_metric(\"mae\", mae)\n\n    mlflow.sklearn.log_model(lr, \"model\")\n<\/code><\/pre>\n\n<p>I am moving to Kubeflow now and not sure if I can do the same thing here without creating a pipleline. What I could find is:<\/p>\n\n<pre><code>client.run_pipleline(exp.id, ....)\n<\/code><\/pre>\n\n<p>Is there any way I can keep track of experiments like mlflow in Kubeflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-26 07:28:20.46 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|kubeflow|mlflow",
        "Question_view_count":472,
        "Owner_creation_date":"2014-05-29 12:37:30.427 UTC",
        "Owner_last_access_date":"2020-10-15 08:35:10.407 UTC",
        "Owner_location":"Kuala Lumpur Federal Territory of Kuala Lumpur Malaysia",
        "Owner_reputation":1990,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":268,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-08-26 11:18:36.08 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Question_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-19 10:31:50.507 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":194,
        "Owner_creation_date":"2020-12-22 10:45:25.527 UTC",
        "Owner_last_access_date":"2022-07-10 06:14:47.843 UTC",
        "Owner_location":"Dubai - United Arab Emirates",
        "Owner_reputation":25,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-19 10:55:13.91 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow and KerasTuner integration",
        "Question_body":"<p>I am trying to integrate together <code>KerasTuner<\/code> and <code>Mlflow<\/code>. I'd like to record the loss at each epoch of each trial of Keras Tuner.<\/p>\n<p>My approach is:<\/p>\n<pre><code>class MlflowCallback(tf.keras.callbacks.Callback):\n    \n    # This function will be called after each epoch.\n    def on_epoch_end(self, epoch, logs=None):\n        if not logs:\n            return\n        # Log the metrics from Keras to MLflow     \n        mlflow.log_metric(&quot;loss&quot;, logs[&quot;loss&quot;], step=epoch)\n    \n\nfrom kerastuner.tuners import RandomSearch\n\nwith mlflow.start_run(run_name=&quot;myrun&quot;, nested=True) as run:\n  \n  tuner = RandomSearch(\n      train_fn,\n      objective='loss',\n      max_trials=25, \n  )\n  tuner.search(train,\n              validation_data=validation, \n              validation_steps=validation_steps,\n              steps_per_epoch=steps_per_epoch, \n              epochs=5, \n              callbacks=[MlflowCallback()]\n  )\n<\/code><\/pre>\n<p>However, the loss values are reported (sequentially) in one single experiment. Is there a way to record them independently?\n<a href=\"https:\/\/i.stack.imgur.com\/awObK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/awObK.png\" alt=\"Loss values\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-22 17:17:09.787 UTC",
        "Question_favorite_count":0.0,
        "Question_score":2,
        "Question_tags":"tensorflow|keras|callback|mlflow|keras-tuner",
        "Question_view_count":390,
        "Owner_creation_date":"2011-10-15 10:06:34.37 UTC",
        "Owner_last_access_date":"2021-07-07 15:02:50.657 UTC",
        "Owner_location":"Rome",
        "Owner_reputation":3651,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":269,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"send post request using curl to mlflow api to multiple records",
        "Question_body":"<p>I have served an mlflow model and am sending POST requests in this format:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; \n--data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, \n&quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, \n&quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, \n6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' \nhttp:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>It is getting scored. However for my particular project, the input to rest api for scoring will always be multiple records in dataframe\/csv format instead of a single record. Can someone point me to how to achieve this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-01 12:17:21.393 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"curl|post|deployment|mlflow|serving",
        "Question_view_count":407,
        "Owner_creation_date":"2019-11-14 13:58:10.56 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:32.563 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow and Hydra causing crash when used together",
        "Question_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-10 05:38:35.617 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"python|docker|exception|mlflow|fb-hydra",
        "Question_view_count":718,
        "Owner_creation_date":"2020-03-01 14:22:35.673 UTC",
        "Owner_last_access_date":"2022-09-23 21:57:09.957 UTC",
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-06-10 07:24:21.797 UTC",
        "Answer_last_edit_date":"2020-06-10 09:41:43.62 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-01 09:15:22.663 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_date":"2014-09-22 04:46:57.027 UTC",
        "Owner_last_access_date":"2022-09-03 08:12:58.187 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-01 10:16:03.553 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Artifact storage and MLFLow on remote server",
        "Question_body":"<p>I am trying to get MLFlow on another machine in a local network to run and I would like to ask for some help because I don't know what to do now.<\/p>\n\n<p>I have a mlflow server running on a <em>server<\/em>. The mlflow server is running under my user on the <em>server<\/em> and has been started like this: <\/p>\n\n<pre><code>mlflow server --host 0.0.0.0 --port 9999 --default-artifact-root sftp:\/\/&lt;MYUSERNAME&gt;@&lt;SERVER&gt;:&lt;PATH\/TO\/DIRECTORY\/WHICH\/EXISTS&gt;\n<\/code><\/pre>\n\n<p>My program which should log all the data to the mlflow server looks like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow import log_metric, log_param, log_artifact, set_tracking_uri\n\nif __name__ == \"__main__\":\n    remote_server_uri = '&lt;SERVER&gt;' # this value has been replaced\n    set_tracking_uri(remote_server_uri)\n    # Log a parameter (key-value pair)\n    log_param(\"param1\", 5)\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(\"foo\", 1)\n    log_metric(\"foo\", 2)\n    log_metric(\"foo\", 3)\n\n    # Log an artifact (output file)\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n    log_artifact(\"output.txt\")\n\n<\/code><\/pre>\n\n<p>The parameters get and metrics get transfered to the server but not the artifacts. Why is that so?<\/p>\n\n<p>Note on the SFTP part:\nI can log in via SFTP and the pysftp package is installed<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-22 13:32:02.373 UTC",
        "Question_favorite_count":null,
        "Question_score":9,
        "Question_tags":"python|mlflow",
        "Question_view_count":3483,
        "Owner_creation_date":"2014-03-21 14:59:04.963 UTC",
        "Owner_last_access_date":"2022-06-30 17:07:48.553 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":422,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":13,
        "Answer_body":"<p>I don't know if I will get an answer to my problem but I did <em>solved<\/em> it this way.<\/p>\n\n<p>On the server I created the directory <code>\/var\/mlruns<\/code>. I pass this directory to mlflow via <code>--backend-store-uri file:\/\/\/var\/mlruns<\/code><\/p>\n\n<p>Then I mount this directory via e.g. <code>sshfs<\/code> on my local machine under the same path.<\/p>\n\n<p>I don't like this solution but it solved the problem good enough for now.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-12-10 08:50:01.237 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2019-11-22 14:43:48.62 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Is there mlflow REST api to hard delete experiments, runs?",
        "Question_body":"<p>Mlfow exp delete api does soft delete and when you create experiment with that name, it gives error RESOURCE_ALREADY_EXISTS.<\/p>\n<p>Is there any way to delete experiment permanently through api?\n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#delete-experiment\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#delete-experiment<\/a><\/p>\n<p>There is similar question here, <a href=\"https:\/\/stackoverflow.com\/questions\/60088889\/how-do-you-permanently-delete-an-experiment-in-mlflow\">How Do You &quot;Permanently&quot; Delete An Experiment In Mlflow?<\/a>\nwhere answers are to run delete sql queries directly by connecting to backend DB which i want to avoid.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-12 09:46:00.843 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"rest|mlflow|mlops",
        "Question_view_count":93,
        "Owner_creation_date":"2017-05-31 04:12:26.49 UTC",
        "Owner_last_access_date":"2022-09-24 08:59:44.06 UTC",
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to add more metrics to a finished MLflow run?",
        "Question_body":"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow<\/code> client and <code>mlflow.get_run(run_id)<\/code> method, but the <code>Run<\/code> object returned by <code>get_run<\/code> seems to be read-only.<\/p>\n<p>Specifically, <code>.log_param<\/code> <code>.log_metric<\/code>, or <code>.log_artifact<\/code> cannot be used on the object returned by <code>get_run<\/code>, raising errors like these:<\/p>\n<pre><code>AttributeError: 'Run' object has no attribute 'log_param'\n<\/code><\/pre>\n<p>If we attempt to run any of the <code>.log_*<\/code> methods on <code>mlflow<\/code>, it would log them into to a new run  with auto-generated run ID in the <code>Default<\/code> experiment.<\/p>\n<p>Example:<\/p>\n<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)\n\nwith mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    \n    \n    # this read operation uses correct run\n    run_id = myrun.info.run_id\n    print(run_id)\n    \n    # this write operation writes to a new run \n    # (with auto-generated random run ID) \n    # in the &quot;Default&quot; experiment (with exp. ID of 0)\n    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)\n   \n<\/code><\/pre>\n<p>Note that the above problem exists regardless of the <code>Run<\/code> status (<code>.info.status<\/code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).<\/p>\n<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()<\/code> context...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-03-29 14:12:06.043 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":1269,
        "Owner_creation_date":"2018-06-19 12:32:08.93 UTC",
        "Owner_last_access_date":"2022-09-24 20:14:44.457 UTC",
        "Owner_location":"EU",
        "Owner_reputation":3260,
        "Owner_up_votes":1100,
        "Owner_down_votes":4,
        "Owner_views":466,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-03-29 17:50:32.537 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow download_artifacts giving Not Found error",
        "Question_body":"<p>I have mlflow and minio running under docker compose. Mlflow successfully logs artifacts to minio and retrieves them. Minio and mlflow have their relevant ports 900 and 5000 exposed by docker.\nIf I run MlflowClient().download_artifacts for predictions from within the docker environment, everything works smoothly.\nIf I run MlflowClient().download_artifacts from outside docker (local machine or remotely), I get the below error. There is no issue in fetching the logged metrics.<\/p>\n<p>botocore.exceptions.ClientError: An error occurred (404) when calling the ListObjectsV2 operation: Not Found<\/p>\n<p>My code:<\/p>\n<pre><code>os.environ['AWS_ACCESS_KEY_ID'] = &quot;x&quot;\nmlflow.set_tracking_uri('http:\/\/10.0.0.1:5000')\nos.environ['AWS_SECRET_ACCESS_KEY'] = &quot;x&quot;\nos.environ['MINIO_ACCESS_KEY_ID'] = &quot;x&quot;\nos.environ['MINIO_SECRET_ACCESS_KEY'] = &quot;x\/me &quot;\n\nfrom mlflow.tracking import MlflowClient\nMlflowClient().download_artifacts(&quot;323e1527d49d4e77bd14c387bbdf6372&quot;, &quot;model&quot;, local_dir)\n<\/code><\/pre>\n<p>Any help would be most appreciated.<\/p>\n<p>Thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-21 09:40:48.143 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"docker|docker-compose|mlflow",
        "Question_view_count":254,
        "Owner_creation_date":"2015-10-28 00:01:57.173 UTC",
        "Owner_last_access_date":"2022-09-24 01:20:28.387 UTC",
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to share models in a multitenant enviroment with Mlflow?",
        "Question_body":"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces<\/a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 16:33:46.997 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|azure-blob-storage|databricks|mlflow",
        "Question_view_count":49,
        "Owner_creation_date":"2021-09-21 04:45:44.943 UTC",
        "Owner_last_access_date":"2022-09-24 22:54:45.03 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Use mlflow to serve a custom python model for scoring",
        "Question_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-25 15:00:24.463 UTC",
        "Question_favorite_count":3.0,
        "Question_score":4,
        "Question_tags":"python|deployment|mlflow|mlops",
        "Question_view_count":3026,
        "Owner_creation_date":"2019-11-14 13:58:10.56 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:32.563 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-01-25 16:41:54.947 UTC",
        "Answer_last_edit_date":"2021-10-14 05:05:40.523 UTC",
        "Answer_score":9.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to get `run_id` when using MLflow Project",
        "Question_body":"<p>When using MLflow Projects (via an <code>MLproject<\/code> file) I get this message at starting time:<\/p>\n<pre><code>INFO mlflow.projects.backend.local: \n=== Running command 'source \/anaconda3\/bin\/..\/etc\/profile.d\/conda.sh &amp;&amp; \nconda activate mlflow-4736797b8261ec1b3ab764c5060cae268b4c8ffa 1&gt;&amp;2 &amp;&amp; \npython3 main.py' in run with ID 'e2f0e8c670114c5887963cd6a1ac30f9' === \n<\/code><\/pre>\n<p>I want to access the <code>run_id<\/code> shown above (<strong>e2f0e8c670114c5887963cd6a1ac30f9<\/strong>) from inside the main script.<\/p>\n<p>I expected a run to be active but:<\/p>\n<pre><code>mlflow.active_run()\n&gt; None\n<\/code><\/pre>\n<p>Initiating a run inside the main script does give me access the correct <code>run_id<\/code>, although any subsequent runs will have a different <code>run_id<\/code>.<\/p>\n<pre><code># first run inside the script - correct run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; e2f0e8c670114c5887963cd6a1ac30f9\n\n# second run inside the script - wrong run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; 417065241f1946b98a4abfdd920239b1\n<\/code><\/pre>\n<p>Seems like a strange behavior, and I was wondering if there's another way to access the <code>run_id<\/code> assigned at the beginning of the <code>MLproject<\/code> run?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-18 18:29:02.007 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":143,
        "Owner_creation_date":"2018-05-30 13:24:03.97 UTC",
        "Owner_last_access_date":"2022-09-16 21:48:03.53 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Differencies between MLflow deployment possibilities",
        "Question_body":"<p>Can someone please explain what are the main use cases when deciding how to serve a model from MLflow:<\/p>\n<ul>\n<li>using command line &quot;mlflow models serve -m ....&quot;<\/li>\n<li>deploying local Docker container with the same model<\/li>\n<li>deploying model online for example on AWS Sagemaker<\/li>\n<\/ul>\n<p>I am mainly interested in differencies between option A and B because as I understand both can be accessed as REST API endpoints. And I assume if network rules are in place then both can be called also externally.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-13 19:00:53.203 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":127,
        "Owner_creation_date":"2021-04-13 19:28:42.277 UTC",
        "Owner_last_access_date":"2022-09-17 14:13:35.527 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to save models in MLFlow with R and get Stages of them in Azure Databricks?",
        "Question_body":"<p>I would like to save a model in MLFlow with Azure Databricks. In Python, I can use the following code to save a model with a name automatically:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.spark.log_model(\n        model,\n        artifact_path = 'model_prueba',\n        registered_model_name = 'model_prueba'\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" alt=\"Registered models window\" \/><\/a><\/p>\n<p>But I am trying to do the same with <strong>R<\/strong> with the following code:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>mlflow_log_model(\n          model,\n          artifact_path = 'model_prueba_R',\n          registered_model_name = 'model_prueba_R'\n    )\n<\/code><\/pre>\n<p>But it does not register any model in the Models section. It only saves the model with the artifact path in the run section.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zfAza.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zfAza.png\" alt=\"Artifact location\" \/><\/a><\/p>\n<p>Anyone could tell me the way to save the model for staging automatically with code in R?<\/p>\n<p>Thank you very much!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-21 11:51:32.693 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|r|model|azure-databricks|mlflow",
        "Question_view_count":201,
        "Owner_creation_date":"2019-09-09 07:55:57.07 UTC",
        "Owner_last_access_date":"2022-01-17 12:39:33.287 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"what are the events (ex MODEL_VERSION_CREATED) associated with ML FLow Databricks CI\/CD",
        "Question_body":"<p>ML Flow has multiple events to subscribe like MODEL_VERSION_CREATED when a model version is created. what are the other events available to subscribe.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-09-28 18:27:45.547 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":26,
        "Owner_creation_date":"2013-06-12 10:44:50.553 UTC",
        "Owner_last_access_date":"2021-11-16 16:58:46.133 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Databricks notebook integrated mlflow artifact location and retention",
        "Question_body":"<ol>\n<li><p>Currently by default in notebook run, it will create an experiment ID, but the Artifact Location would point to something under dbfs:\/databricks\/mlflow\/{experiment id}. If there is a way we may change this in default experiment creation? We like to manage the storage outside databricks.<\/p><\/li>\n<li><p>How long is default TTL for experiment runs and metrics? Is it configurable and how?<\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-01 21:44:58.277 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":587,
        "Owner_creation_date":"2016-03-18 04:43:36.763 UTC",
        "Owner_last_access_date":"2019-12-07 00:33:14.047 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How do you start using MLflow SQL storage instead of the file system storage?",
        "Question_body":"<p>If I were getting started with MLflow, then how would I set up a database store? Is it sufficient to create a new MySQL database or a SQLite database and point MLflow to that?<\/p>\n\n<p>I tried to set the tracking URI, but that didn't create a database if it didn't exist.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-10 21:13:31.39 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":882,
        "Owner_creation_date":"2013-04-02 19:38:08.083 UTC",
        "Owner_last_access_date":"2022-09-23 18:27:42.59 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":31,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does MLflow support distributed XGBoost model?",
        "Question_body":"<p>I  use single-instance xgboost model to train, which works fine with mlflow to log all the model-related parameters by using <code>mlflow.xgboost.autolog()<\/code>, but when i change to distributed xgboost version changing from python package to JVM package by including the xgboost4j.jar and xgboost4j-spark.jar files, and also include mlflow module into it, (<code>mlflow.xgboost.autolog()<\/code>). The mlflow cannot show all the parameters on the page. They are empty.\n<img src=\"https:\/\/i.stack.imgur.com\/dt91a.png\" alt=\"screenshot\" \/><\/p>\n<p>So I looked at the source code in the mlflow.xgboost, in line 271, <code>def autolog(importance_types=[&quot;weight&quot;]):<\/code> says it imports the xgboost package, which i think is the single-instance xgboost model, I wonder if it is support the distributed version? Or is there any other methods to solve the problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-12 06:44:14.15 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"xgboost|mlflow",
        "Question_view_count":133,
        "Owner_creation_date":"2019-08-25 23:46:06.673 UTC",
        "Owner_last_access_date":"2020-11-12 10:03:15.497 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-10-12 08:50:46.443 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Gridsearch on an experiment in Sacred",
        "Question_body":"<p>I'm trying to see some ways to store my ML experiments and I came across some python libraries like Sacred, ModelChimp, MLFlow, ....<\/p>\n\n<p>The one I like the most is Sacred, but I would like to know how to save the <code>GridSearchCV<\/code> sklearn object the way ModelChimp does, for example. Is there any way to include each of the tests that the <code>GridSearchCV<\/code> object does in Sacred like ModelChimp does?<\/p>\n\n<p>Additionally I would like to be able to visualize an interactive map of the folium library (which I would simply export to HTML), but I haven't seen that any of these libraries accept objects to visualize beyond an image.<\/p>\n\n<p>Are Sacred or ModelChimp good options? The little I've seen of MLflow or other libraries hasn't convinced me either but I'm open to suggestions. <a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/bx0apm\/d_how_do_you_manage_your_machine_learning\/\" rel=\"nofollow noreferrer\">Here<\/a> are a few more alternatives. Which one do you use?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-09 14:28:26.603 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|folium|mlflow|python-sacred",
        "Question_view_count":205,
        "Owner_creation_date":"2019-02-15 12:18:22.177 UTC",
        "Owner_last_access_date":"2022-09-22 08:26:53.537 UTC",
        "Owner_location":null,
        "Owner_reputation":621,
        "Owner_up_votes":87,
        "Owner_down_votes":26,
        "Owner_views":103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-10-14 15:55:02.09 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow projects; bash: python: command not found",
        "Question_body":"<p>I'm running MLflow Project for a model using following command from my ubuntu 20.04 terminal<\/p>\n<pre><code>mlflow run . --no-conda -P alpha=0.5\n<\/code><\/pre>\n<p>My system doesn't have conda or python (It does however have python3). So, I added alias for python using terminal<\/p>\n<pre><code>alias python='python3'\n<\/code><\/pre>\n<p>After which I could open python in terminal using <code>python<\/code>. However, I still got the same error<\/p>\n<pre><code>2021\/11\/21 08:07:34 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpp4h595ql for downloading remote URIs passed to arguments of type 'path' ===\n2021\/11\/21 08:07:34 INFO mlflow.projects.backend.local: === Running command 'python tracking.py 0.5 0.1' in run with ID 'e50ca47b3f8848a083906be6220c26fc' === \nbash: python: command not found\n2021\/11\/21 08:07:34 ERROR mlflow.cli: === Run (ID 'e50ca47b3f8848a083906be6220c26fc') failed ===\n<\/code><\/pre>\n<p>How to get rid of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-21 02:44:17.297 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|bash|ubuntu|terminal|mlflow",
        "Question_view_count":281,
        "Owner_creation_date":"2020-09-25 03:31:49.207 UTC",
        "Owner_last_access_date":"2022-09-24 07:17:02.153 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1074,
        "Owner_up_votes":239,
        "Owner_down_votes":21,
        "Owner_views":143,
        "Answer_body":"<p>Change <code>python<\/code> to <code>python3<\/code> in the <code>MLproject<\/code> file to the resolve error.<\/p>\n<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-21 03:02:34.447 UTC",
        "Answer_last_edit_date":"2021-11-21 13:56:33.977 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Adjusting docker environment for each MLflow model",
        "Question_body":"<p>Imagine a situation, where you have two teams working on some models, for example XGBoost. They\ntrain it and then log the model to common MLflow Tracking Server. One of the teams is using older version,\nfor example 1.1 and the other team is using newest 1.6 version.<\/p>\n<p>Is it possible to use both of these models to make predictions inside one container,\nwhich downloades the models from MLflow tracking server? Since these two mentioned models\nuse different versions of XGboost, and the runtime in which they are going to be run has\ncertain version of the pip packages installed, there is going to be compatibility problem -&gt;\nonly one model will be able to run in this enviroment without changes. However, both of these models have\nrequirements.txt file saved as the artifact, and this file specifies the package versions, which the\ngiven model needs. Is it possible to adjust package versions running in the container according to\nthe currently used model using the requirements.txt artifact file? Or is the only\nsolution to create two separate docker images, for each of the models and with the\npackages that the model needs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-04 14:48:23.067 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|pip|mlflow",
        "Question_view_count":95,
        "Owner_creation_date":"2021-11-04 14:41:46.52 UTC",
        "Owner_last_access_date":"2022-09-23 11:46:31.417 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to access Mlflow running on fargate (ECS) with only VPN in\/outbound rules from sagemaker notebook instance?",
        "Question_body":"<p><strong>Context:<\/strong><\/p>\n<p>I have deployed Mlflow on ECS(Fargate) using terraform using this public <a href=\"https:\/\/github.com\/Glovo\/terraform-aws-mlflow\" rel=\"nofollow noreferrer\">git-repo<\/a>. After deploying Mlflow which was publicly accessible using the link, I made some changes in the security group and changed in\/outbound rule to the only company VPN ips, now that link is only accessible under the VPN.<\/p>\n<p><strong>Question:<\/strong><\/p>\n<p>Now I have Sagemake notebook instance and want to access that link inside the notebook and the notebook is running on AWS internet(outside Company-VPN) and I'm not able to access that link. What could be the possible solution?<\/p>\n<p>I don't want to open access of Mlflow-link publicaly to accessible form anywhere on the internet.<\/p>\n<p><strong>Running this code on notebook:<\/strong><\/p>\n<pre><code>!pip install mlflow\nimport mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/mlflow-mlp-xyz-xyz.eu-west-1.elb.amazonaws.com\/&quot;)\nmlflow.get_experiment_by_name('mlpmlflowlogger')\ncurrent_experiment=dict(mlflow.get_experiment_by_name('mlpmlflowlogger'))\nprint(current_experiment)\nexperiment_id=current_experiment['experiment_id']\nprint(experiment_id)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-01 13:31:23.46 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ecs|amazon-vpc|aws-security-group|mlflow",
        "Question_view_count":31,
        "Owner_creation_date":"2017-11-29 13:03:57.98 UTC",
        "Owner_last_access_date":"2022-09-20 13:40:47.487 UTC",
        "Owner_location":"Deggendorf, Germany",
        "Owner_reputation":813,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":115,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Log (and then apply) Spark MLlib model from R to MLflow",
        "Question_body":"<p>I'm using Spark MLlib functions (through the <em>sparklyr<\/em> package) to train a model but now seem unable to save the model in <em>MLflow<\/em> for future use.<\/p>\n<pre><code>iris_tbl &lt;- sparklyr::copy_to(sc, iris, &quot;iris_spark&quot;)\nmdl_mllib &lt;- iris_tbl %&gt;% sparklyr::ml_linear_regression(formula = Sepal_Width ~ Sepal_Length)\nmlflow::mlflow_log_model(mdl_mllib, &quot;artifact_path_where_saved&quot;)\nError in UseMethod(&quot;mlflow_save_model&quot;) : \n  no applicable method for 'mlflow_save_model' applied to an object of class &quot;c('ml_model_linear_regression', 'ml_model_regression', 'ml_model_prediction', 'ml_model')&quot;\n\npackageVersion(&quot;mlflow&quot;)\n[1] \u20181.17.0\u2019\n<\/code><\/pre>\n<p>What is a simple way to save this model in <em>mlflow<\/em> for later use <strong>on a Spark DataFrame<\/strong> such as:<\/p>\n<p><code>mlflow::mlflow_load_model(model_uri = &quot;models:\/mdl_mllib_project01\/Staging&quot;)<\/code><\/p>\n<p>For context, I'm using Azure Databricks as the ecosystem.<\/p>\n<h3>Other places I've looked for answers<\/h3>\n<ul>\n<li>These links don't seem to directly solve my problem (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#id28;\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#id28;<\/a> <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.spark.html#mlflow.spark.log_model\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.spark.html#mlflow.spark.log_model<\/a>), although maybe it's that I need to make a &quot;pipeline&quot; or that this works for Python but not yet for R?<\/li>\n<li>This question seems relevant but offers abstract answers rather than details (<a href=\"https:\/\/stackoverflow.com\/questions\/40533582\/how-to-serve-a-spark-mllib-model\">How to serve a Spark MLlib model?<\/a>).<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-08-02 09:20:06.663 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"apache-spark-mllib|sparklyr|mlflow",
        "Question_view_count":107,
        "Owner_creation_date":"2021-04-21 16:43:43.857 UTC",
        "Owner_last_access_date":"2022-09-20 15:49:55.297 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"FastAPI slow in MLFlow",
        "Question_body":"<p>I have a use case where I want to deploy ML Models with low latency and high throughput.\nIn MLFlow, I couldn't achieve it, so I tried FastAPI and it showed quite good results.<\/p>\n<p>Thus, I tried to replace the Flask engine with FastAPI in MLFlow. But I am getting very low throughput per process on using FastAPI in MLFlow as compared to standalone FastAPI serving for the same model.<\/p>\n<p><a href=\"https:\/\/github.com\/tsenart\/vegeta\" rel=\"nofollow noreferrer\">Vegeta<\/a> benchmarking results:<\/p>\n<p>MLFlow with FastAPI Test1:<\/p>\n<pre><code>(base) [ec2-user@ip-172-31-43-232 testing]$ cat target_all.txt | .\/vegeta attack -duration=1m -rate=100 -max-workers=20 | .\/vegeta report\nRequests [total, rate, throughput] 1174, 100.09, 100.07\nDuration [total, attack, wait] 11.732s, 11.73s, 2.03ms\nLatencies [min, mean, 50, 90, 95, 99, max] 1.71ms, 2.23ms, 1.87ms, 3.394ms, 3.911ms, 7.58ms, 9.813ms\nBytes In [total, mean] 111530, 95.00\nBytes Out [total, mean] 224234, 191.00\nSuccess [ratio] 100.00%\nStatus Codes [code:count] 200:1174\n<\/code><\/pre>\n<p>MLFlow with FastAPI Test2:<\/p>\n<pre><code>(base) [ec2-user@ip-172-31-43-232 testing]$ cat target_all.txt | .\/vegeta attack -duration=1m -rate=200 -max-workers=20 | .\/vegeta report\nRequests [total, rate, throughput] 2159, 158.75, 157.30\nDuration [total, attack, wait] 13.725s, 13.6s, 125.605ms\nLatencies [min, mean, 50, 90, 95, 99, max] 10.256ms, 124.783ms, 125.951ms, 129.294ms, 133.891ms, 148.876ms, 293.218ms\nBytes In [total, mean] 209423, 97.00\nBytes Out [total, mean] 412369, 191.00\nSuccess [ratio] 100.00%\nStatus Codes [code:count] 200:2159\n<\/code><\/pre>\n<p>Standalone FastAPI Test3:<\/p>\n<pre><code>Requests [total, rate, throughput] 36000, 600, 599\nLatencies [min, mean, 50, 90, 95, 99, max] 1.515ms, 2.608ms, 2.734ms, 3.386ms, 3.611ms, 4.204ms, 4.795ms\n<\/code><\/pre>\n<p>On using FastAPI, I am getting throughput to be more than 600 RPS per Gunicorn worker, while on using FastAPI with MLFlow, I am getting 100 RPS at max without affecting the response time of the model.\nI think I am missing something in this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/5599\" rel=\"nofollow noreferrer\">PR<\/a>. Can someone please help?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-05 07:17:27.347 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"fastapi|mlflow|mlops",
        "Question_view_count":296,
        "Owner_creation_date":"2016-09-30 18:28:36.607 UTC",
        "Owner_last_access_date":"2022-05-25 08:19:33.757 UTC",
        "Owner_location":null,
        "Owner_reputation":490,
        "Owner_up_votes":7,
        "Owner_down_votes":11,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Error while loading MLFlow model in python 3.7",
        "Question_body":"<p>I am saving the MLFlow model using databricks. Below are the details:<\/p>\n<pre><code>artifact_path: model\ndatabricks_runtime: 8.4.x-gpu-ml-scala2.12\nflavors:\n  python_function:\n    data: data\n    env: conda.yaml\n    loader_module: mlflow.pytorch\n    pickle_module_name: mlflow.pytorch.pickle_module\n    python_version: 3.8.8\n  pytorch:\n    model_data: data\n    pytorch_version: 1.9.0+cu102\n<\/code><\/pre>\n<p><strong>I am not able to locally load the model using Python 3.7<\/strong>, whereas it works well with Python 3.9.<\/p>\n<p>Any idea what could be the possible resolution to this?<\/p>\n<p>Error Trace:<\/p>\n<pre><code>  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 714, in load_model\n    return _load_model(path=torch_model_artifacts_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 626, in _load_model\n    return torch.load(model_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 607, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 882, in _load\n    result = unpickler.load()\nTypeError: code() takes at most 15 arguments (16 given)\n<\/code><\/pre>\n<p>I have tried specifying the pickle module name explicitly as <code>mlflow.pytorch.load_model(ML_MODEL_PATH,pickle_module=mlflow.pytorch.pickle_module)<\/code><\/p>\n<p>But still getting the same error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-19 06:53:12.843 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|pickle|torch|mlflow",
        "Question_view_count":477,
        "Owner_creation_date":"2021-11-19 06:39:10.9 UTC",
        "Owner_last_access_date":"2022-09-19 10:56:54.76 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow RestException: RESOURCE_ALREADY_EXISTS error when starting runs",
        "Question_body":"<p>Previously was using ML FLow with Databricks on Azure Machine Learning to register and track model Hyperparameter tuning with both SKLearn and Stats model models from start of September with no issues. But since about 23rd October, I started getting these kinds of errors:<\/p>\n<p>RestException: RESOURCE_ALREADY_EXISTS: Failed to create AML experiment for experiment id=863468136127724, name=\/my-experiment3, artifactLocation=dbfs:\/databricks\/mlflow-tracking\/863468136127724. There is an existing AML experiment with id=c74bdea3-e382-4cdf-868a-ee1421de078e and name='\/adb\/5909321886823418\/863468136127724\/my-experiment3' and artifactLocation='' that is not compatible.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/n1qmL.png\" rel=\"nofollow noreferrer\">Even when running a newly created experiment, it will throw this error<\/a><\/p>\n<p>We recently have updated to ml flow v1.21.0  but it doesn't seem to be a bug as there is nothing on the ML Flow github that is similar, just wondering if anyone has encountered anything similar as I run out of ideas of things to look for the issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-08 20:22:30.083 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":618,
        "Owner_creation_date":"2021-11-08 20:13:38.383 UTC",
        "Owner_last_access_date":"2021-12-15 19:26:40.927 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-06 15:40:30.593 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-s3|boto3|mlflow",
        "Question_view_count":237,
        "Owner_creation_date":"2018-10-06 09:06:11.613 UTC",
        "Owner_last_access_date":"2022-09-22 14:26:03.733 UTC",
        "Owner_location":"Berlin",
        "Owner_reputation":647,
        "Owner_up_votes":971,
        "Owner_down_votes":39,
        "Owner_views":61,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-07 09:40:14.37 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-07-06 15:52:51.947 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"EKS Pod S3 Access Denied",
        "Question_body":"<p>I'm trying to create an EKS pod to use as a service for mlflow. The issue that I'm having though is that I'm unable to connect to s3 to store the mlflow run artifacts. I have tried connecting to the pod using <code>kubectl exec -it &lt;pod_name&gt; -- \/bin\/bash<\/code> and setting the aws credentials there as well. When doing this, I'm able to ls the s3 bucket.<\/p>\n\n<p>But when I attempt to save an mlflow artifact to the same s3 location, I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDenied) when calling the AssumeRoleWithWebIdentity operation: Not authorized to perform sts:AssumeRoleWithWebIdentity\n<\/code><\/pre>\n\n<p>What would be the issue causing this? Is there an IAM that needs to be set with the EKS pod or something along those lines?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-23 15:51:47.257 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-iam|amazon-eks|mlflow",
        "Question_view_count":6784,
        "Owner_creation_date":"2014-11-26 14:40:35.813 UTC",
        "Owner_last_access_date":"2021-08-12 15:37:44.573 UTC",
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-06 13:41:23.467 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to solve original size problem of MLFlow Artifacts?",
        "Question_body":"<p>I want to save my feature importance plot into mlflow project. But my figure size is looking like this; x axis names are cropped. How can i fix this problem?<\/p>\n<p>Code;<\/p>\n<pre><code>fig, ax = plt.subplots()\nax.plot(forest_importances[-25:])\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\n    \nmlflow.log_figure(fig, &quot;figure.png&quot;)\n<\/code><\/pre>\n<p>Normal;\n<a href=\"https:\/\/i.stack.imgur.com\/AHpCV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AHpCV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In mlflow ;\n<a href=\"https:\/\/i.stack.imgur.com\/iHY4g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iHY4g.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-17 14:45:10.943 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":13,
        "Owner_creation_date":"2018-07-05 11:57:46.73 UTC",
        "Owner_last_access_date":"2022-09-23 13:39:15.477 UTC",
        "Owner_location":null,
        "Owner_reputation":979,
        "Owner_up_votes":251,
        "Owner_down_votes":110,
        "Owner_views":571,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow: how to read metrics or params from an existing run?",
        "Question_body":"<p>I try to read metrics in this way:<\/p>\n\n<pre><code> data, info = mlflow.get_run(run_id)\n print(data[1].metrics)\n # example of output: {'loss': 0.01}\n<\/code><\/pre>\n\n<p>But it get only last value. It is possible to read manually all steps of a particular metric?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_date":"2020-03-10 11:10:49.443 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|metrics|mlflow",
        "Question_view_count":2995,
        "Owner_creation_date":"2019-01-14 12:03:19.037 UTC",
        "Owner_last_access_date":"2022-09-22 14:18:16.19 UTC",
        "Owner_location":"Busto Arsizio, VA, Italia",
        "Owner_reputation":171,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Answer_body":"<p>I ran into this same problem and was able to do get all of the values for the metric by using using <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\"><code>mlflow.tracking.MlflowClient().get_metric_history<\/code><\/a>. This will return every value you logged using <code>mlflow.log_metric(key, value)<\/code>.<\/p>\n<p>Quick example (untested)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\ntrackingDir = 'file:\/\/\/....'\nregistryDir = 'file:\/\/\/...'\nrunID = 'my run id'\nmetricKey = 'loss'\n\nclient = mlflow.tracking.MlflowClient(\n            tracking_uri=trackingDir,\n            registry_uri=registryDir,\n        )\n\nmetrics = client.get_metric_history(runID, metricKey)\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\">From the docs<\/a><\/p>\n<blockquote>\n<p>get_metric_history(run_id, key)[source] Return a list of metric\nobjects corresponding to all values logged for a given metric.<\/p>\n<p>Parameters run_id \u2013 Unique identifier for run<\/p>\n<p>key \u2013 Metric name within the run<\/p>\n<p>Returns A list of mlflow.entities.Metric entities if logged, else\nempty list<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\ndef print_metric_info(history):\n    for m in history:\n        print(&quot;name: {}&quot;.format(m.key))\n        print(&quot;value: {}&quot;.format(m.value))\n        print(&quot;step: {}&quot;.format(m.step))\n        print(&quot;timestamp: {}&quot;.format(m.timestamp))\n        print(&quot;--&quot;)\n\n# Create a run under the default experiment (whose id is &quot;0&quot;). Since this is low-level\n# CRUD operation, the method will create a run. To end the run, you'll have\n# to explicitly end it. \nclient = MlflowClient() \nexperiment_id = &quot;0&quot; \nrun = client.create_run(experiment_id) \nprint(&quot;run_id:{}&quot;.format(run.info.run_id))\nprint(&quot;--&quot;)\n\n# Log couple of metrics, update their initial value, and fetch each\n# logged metrics' history. \nfor k, v in [(&quot;m1&quot;, 1.5), (&quot;m2&quot;, 2.5)]:\n    client.log_metric(run.info.run_id, k, v, step=0)\n    client.log_metric(run.info.run_id, k, v + 1, step=1)\n    print_metric_info(client.get_metric_history(run.info.run_id, k))\nclient.set_terminated(run.info.run_id) \n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-02 04:51:47.693 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_last_edit_date":"2022-03-14 11:44:28.603 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"geting artifacts from mlflow GridSearch run",
        "Question_body":"<p>I'm running a sklearn pipeline with hyperparameter search (let's say GridSearch). Now, I am logging artifacts such as test results and whole-dataset predictions. I'd like to retrieve these artifacts but the mlflow API is getting in the way...<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.set_tracking_uri(&quot;sqlite:\/\/\/mlruns\/mlruns.db&quot;)\nmlflow.set_registry_uri(&quot;.\/mlruns\/&quot;)\n\nrun_ids = [r.run_id for r in mlflow.list_run_infos(mlflow.get_experiment_by_name(&quot;My Experiment&quot;).experiment_id)]\n<\/code><\/pre>\n<p>With the above code, I can retrieve all runs but I have no way of telling which one is a toplevel run with artifacts logged or a sub-run spawned by the GridSearch procedure.<\/p>\n<p>Is there some way of querying only for <strong>parent<\/strong> runs, so I can retrieve these csv files in order to plot the results? I can of course go to the web api and manually select the run then copy the URI for the file, but I'd like to do it programmatically instead of opening a tab and clicking things.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 09:16:13.517 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":10,
        "Owner_creation_date":"2016-07-07 10:29:22.327 UTC",
        "Owner_last_access_date":"2022-09-22 17:04:45.12 UTC",
        "Owner_location":"Spain",
        "Owner_reputation":624,
        "Owner_up_votes":56,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow: problems with pip installation",
        "Question_body":"<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.\nI installed mlflow with :<\/p>\n\n<pre><code>    pip3 install mlflow\n<\/code><\/pre>\n\n<p>so mlflow is installed in \/usr\/local\/bin\/mlflow<\/p>\n\n<p>Since it is not in \/Users\/xxxx\/opt\/anaconda3\/lib\/python3.7\/site-packages, I get \"ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-16 20:37:11.793 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pip|python-import|python-3.7|importerror|mlflow",
        "Question_view_count":9843,
        "Owner_creation_date":"2018-04-26 22:59:32.553 UTC",
        "Owner_last_access_date":"2021-12-14 18:54:28.437 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":87,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"When to use mlflow.set_tag() vs mlflow.log_params()?",
        "Question_body":"<p>I am confused about the usecase of mlflow.set_tag() vs mlflow.log_params() as both takes key and value pair. Currently, I use mlflow.set_tag() to set tags for data version, code version, etc and mlflow.log_params() to set model training parameters like loss, accuracy, optimizer, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-02 09:23:32.087 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|mlflow",
        "Question_view_count":77,
        "Owner_creation_date":"2016-09-09 08:04:39.707 UTC",
        "Owner_last_access_date":"2022-09-22 15:20:23.897 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":620,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":101,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Is there a way to get log the descriptive stats of a dataset using MLflow?",
        "Question_body":"<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-24 04:52:09.53 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":4592,
        "Owner_creation_date":"2014-09-22 04:46:57.027 UTC",
        "Owner_last_access_date":"2022-09-03 08:12:58.187 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Answer_body":"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"noreferrer\">the docs<\/a>:<\/p>\n<blockquote>\n<p><strong>mlflow.log_artifact(local_path, artifact_path=None)<\/strong>\nLog a local file or directory as an artifact of the currently active run.<\/p>\n<\/blockquote>\n<blockquote>\n<p><strong>Parameters:<\/strong><br \/>\n<em>local_path<\/em> \u2013 Path to the file to write.\n<em>artifact_path<\/em> \u2013 If provided, the directory in artifact_uri to write to.<\/p>\n<\/blockquote>\n<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df<\/code>.<\/p>\n<pre><code>## Write csv from stats dataframe\nstat_df.to_csv('dataset_statistics.csv')\n\n## Log CSV to MLflow\nmlflow.log_artifact('dataset_statistics.csv')\n<\/code><\/pre>\n<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-08 01:32:42.457 UTC",
        "Answer_last_edit_date":"2021-01-26 04:34:50.617 UTC",
        "Answer_score":9.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to set custom path for databricks mlflow artifacts on s3",
        "Question_body":"<p>I've created an empty experiments from databricks experiments console and given the path for my artifacts on s3 i.e. s3:\/\/\/. When i run the scripts, the artifacts are stored at<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/\/&lt;32 char id&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>I want to replace \/\/&lt;32 char id&gt;\/artifacts\/ with \/datetime\/artifacts\/ so something like<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/&lt;datetime&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>Is there any way i could achieve that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Note: experiment_id is from databricks experiment console<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-04 14:12:33.88 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"databricks|mlflow|aws-databricks|mlops",
        "Question_view_count":140,
        "Owner_creation_date":"2018-05-12 15:57:03.12 UTC",
        "Owner_last_access_date":"2022-09-24 08:37:01.693 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":962,
        "Owner_up_votes":106,
        "Owner_down_votes":9,
        "Owner_views":128,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-07 07:21:35.153 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow warning: Experiment ID mismatch for exp Iris. ID recorded as \u20181\u2019 in metadata\u2026",
        "Question_body":"<p>I\u2019m completely new to mlflow.<\/p>\n<p>I\u2019ve started with a couple of the standard tutorials and created examples using the well-known Iris and wine-quality datasets and named the experiments accordingly.<\/p>\n<p>When I run my code I get warnings:<\/p>\n<pre><code>WARNING:root:Experiment ID mismatch for exp iris.  ID recorded as \u20181\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp mlruns. ID recorded as \u20188\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp wine-quality. ID recorded as \u20185\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n<\/code><\/pre>\n<p>However, when I run the following in a jupyter notebook cell:<\/p>\n<pre><code>From mlflow.tracking import MlflowClient\n\nclient=Mlflow.Client()\n\nexperiments=client.list_experiments()\n\nexperiments\n<\/code><\/pre>\n<p>I see the list of experiments, including the 3 above, whose ID matches their names.<\/p>\n<p>When I look in the <code>mlruns\/wine-quality\/1<\/code> folder says, the <code>meta.yaml<\/code> states correctly that <code>Experiment id<\/code> is 5.<\/p>\n<p>Can someone help explain why I am getting these warnings?<\/p>\n<p>(When I run <code>mlflow ui \u2014backend-store-uri file:C:\/Users\/jb8\/mlruns<\/code> I see the experimental runs being logged as I\u2019d hope\u2026)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-10 09:52:00.537 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|warnings|mlflow",
        "Question_view_count":36,
        "Owner_creation_date":"2017-12-22 11:18:19.32 UTC",
        "Owner_last_access_date":"2022-09-23 10:43:46.06 UTC",
        "Owner_location":"Portsmouth, United Kingdom",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-10 15:00:34.743 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"My served model is dependent on my source code",
        "Question_body":"<p>I was serving a model using MLFlow model serve, the model won't serve until I put the source code in the same directory of the MLModel file so that I serve it. I want to serve the MLModel file without the need of the source code. I thought the method load_context() in a wrapper class would help me to avoid this but I couldn't make it didn't have the result I was hoping for either. I implemented it like this:<\/p>\n<pre><code>class Wrapper(PythonModel):\n    def __init__(self):\n        self.logger = get_logger(name='model')\n        self.model = None\n\n    def predict(self, context, model_input):\n        self.logger.warning(model_input)\n        return self.model.predict(model_input.values)\n\n    def load_context(self, context):\n        self.model = mlflow.pyfunc.load_model(path_to_model)\n<\/code><\/pre>\n<p>Any ideas? Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-22 11:14:49.127 UTC",
        "Question_favorite_count":0.0,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":48,
        "Owner_creation_date":"2018-12-29 01:23:01.853 UTC",
        "Owner_last_access_date":"2022-06-27 12:39:04.53 UTC",
        "Owner_location":"Egypt",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-22 11:42:34.06 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Automatic flavor detection with ml_flow load_model?",
        "Question_body":"<p>I am using mlflow and want to handle different flavors (e.g. <code>sklearn<\/code> , <code>tensorflow<\/code> and <code>keras<\/code>) while loading.<\/p>\n<p>Actually I only find the information about the stored <code>flavor<\/code> as string in<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>Run.to_dictionary()['data']['tags']['mlflow.log-model.history']\n<\/code><\/pre>\n<p>output:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>[{&quot;run_id&quot;: &quot;8ea47843f7b446828dbd9cd3a1ed2339&quot;, &quot;artifact_path&quot;: &quot;model&quot;, &quot;utc_time_created&quot;: &quot;2022-04-26 10:39:55.639791&quot;, &quot;flavors&quot;: {&quot;keras&quot;: {&quot;keras_module&quot;: &quot;tensorflow.keras&quot;, &quot;keras_version&quot;: &quot;2.7.0&quot;, &quot;save_format&quot;: &quot;tf&quot;, &quot;data&quot;: &quot;data&quot;, &quot;code&quot;: null}, &quot;python_function&quot;: {&quot;loader_module&quot;: &quot;mlflow.keras&quot;, &quot;python_version&quot;: &quot;3.8.10&quot;, &quot;data&quot;: &quot;data&quot;, &quot;env&quot;: &quot;conda.yaml&quot;}}, &quot;model_uuid&quot;: &quot;3bd37bdb0aa1409aabc65f8314018642&quot;, &quot;mlflow_version&quot;: &quot;1.25.1&quot;}]\n<\/code><\/pre>\n<p>Run is the <code>mlflow.entities.Run<\/code> object.<\/p>\n<p>Using <code>ast.literal_eval<\/code> to transform the string into the dictionary fails.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ast.literal_eval(str(self.run.to_dictionary()['data']['tags']['mlflow.log-model.history'][1:-1]))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-26 11:46:38.64 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|mlflow",
        "Question_view_count":75,
        "Owner_creation_date":"2014-12-23 23:50:02.193 UTC",
        "Owner_last_access_date":"2022-09-21 11:10:16 UTC",
        "Owner_location":null,
        "Owner_reputation":1499,
        "Owner_up_votes":98,
        "Owner_down_votes":14,
        "Owner_views":297,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Why is the MLFlow UI different on installation?",
        "Question_body":"<p>My MLFlow installation results in a significantly different UI experience  that does not neatly stack the Parameters and Metrics columns as in the QuickStart. <\/p>\n\n<p>Here's what my UI looks like after logging some basic information: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas every other example of MLFlow I've come across online looks like this (image taken from MLFlow website quickstart): \n<a href=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The other thing that's missing is the toggle between \"list\" and \"table\" views. Below is what MLFlow documentation says I should see: \n<a href=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas here's what I see in my installation: \n<a href=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My environments are as follows: <\/p>\n\n<ol>\n<li>Ubuntu 16.04, Docker + pip installation of MLFlow<\/li>\n<li>Mac OS, \n\n<ol>\n<li>Conda + pip installation of MLFlow<\/li>\n<li>Brew installation of Python, then pip3 installation of MLFlow<\/li>\n<\/ol><\/li>\n<\/ol>\n\n<p>I've tried tweaking the following: <\/p>\n\n<ol>\n<li>Version of MLFlow from 1.3 to 1.4<\/li>\n<li>Version of Python from 3.7 to 3.8 <\/li>\n<li>Brand new installation vs. existing upgrade <\/li>\n<\/ol>\n\n<p>I'm out of ideas at this point as to why my UI looks so different. It doesn't necessarily affect my usage of MLFlow, but I'm trying to sell it to my colleagues as a good experiment tracking system and I want the UI to be the best possible representation. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-19 18:56:51.567 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":403,
        "Owner_creation_date":"2013-03-29 23:29:22.503 UTC",
        "Owner_last_access_date":"2022-04-08 22:56:08.28 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow: Log steps in evaluation phase using Tensorflow train_and_evaluate",
        "Question_body":"<p>I'm trying to log the steps during the evaluation using Mlflow but have only been able to log the last step. Using\nmlflow.tensorflow.autolog() I am able to log some metrics (like loss) when a checkpoint is saved, every 100 steps that is defined in RunConfig. However I also need to save the accuracy and top3error every 100 steps the model is evaluated. Here is my code:<\/p>\n\n<pre><code>def top3error(features, labels, predictions):\n    return {'top3error': tf.metrics.mean(tf.nn.in_top_k(predictions=predictions['logits'], \n                                                        targets=labels,\n                                                        k=3))}\n# Log metrics\nmlflow.tensorflow.autolog()\n\nwith mlflow.start_run():\n    steps = 1000\n\n    mlflow.log_param(\"Steps\", steps)    \n\n    '''Training &amp; Validation'''\n    train_spec = tf.estimator.TrainSpec(input_fn=generate_input_fn(train), \n                                        max_steps=steps)\n    eval_spec = tf.estimator.EvalSpec(name='validation',\n                                      input_fn=generate_input_fn(test, num_epochs=1))\n\n    tf.logging.info(\"Starting Run...\")\n    results = tf.estimator.train_and_evaluate(m, train_spec, eval_spec)    \n\n    '''Log Run'''\n    mlflow.log_metric(\"accuracy\", results[0]['accuracy'])\n    mlflow.log_metric(\"top3error\", results[0]['top3error'])\n<\/code><\/pre>\n\n<p>Here is the RunConfig used in the model:<\/p>\n\n<pre><code>config=tf.estimator.RunConfig(\n  model_dir=model_dir, \n  save_checkpoints_steps=100,\n)\n<\/code><\/pre>\n\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-08-01 14:57:08.267 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|callback|evaluate|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-04-24 23:13:14.16 UTC",
        "Owner_last_access_date":"2022-09-23 18:12:12.217 UTC",
        "Owner_location":null,
        "Owner_reputation":737,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to connect to MLFlow tracking server that has auth?",
        "Question_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-24 15:30:11.31 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"authorization|tracking|mlflow",
        "Question_view_count":2102,
        "Owner_creation_date":"2021-11-24 15:07:17.853 UTC",
        "Owner_last_access_date":"2022-09-22 09:41:34.783 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-11-24 17:01:13.483 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFLOW on Databricks - Cannot log a Keras model as a mlflow.pyfunc model. Get TypeError: cannot pickle 'weakref' object",
        "Question_body":"<p>Hi all: this is one of my first posts on Stackoverflow - so apologies in advance if i'm not conforming to certain standards!<\/p>\n<p>I'm having trouble saving my Keras model as a <code>mlflow.pyfunc<\/code> model as it's giving me a &quot;cannot pickle a 'weakref' object when I try to log it.<\/p>\n<p><strong>So why am i saving my Keras model as a pyfunc model object in the first place? This is because I want to override the default predict method and output something custom<\/strong>. I also want to do some pre-processing steps on the X_test or new data by encoding it with a tf.keras.StringLookup and then invert it back to get the original categorical variable class. For this reason, I was advised by Databricks that the mlflow.pyfunc flavor is the best way to go for these types of use-cases<\/p>\n<p>The Keras model works just fine and i'm able to log it using <code>mlflow.keras.log_model<\/code>. But it fails when i try to wrap it inside a cutomer &quot;KerasWrapper&quot; class.<\/p>\n<p>Here are some snippets of my code. For the purpose of debugging, the current <code>predict<\/code> method in the custom class is just the default. I simplified it to help debug, but obviously I haven't been able to resolve it.<\/p>\n<p>I would be extremely grateful for any help. Thanks in advance!<\/p>\n<p><strong>ALL CODE ON AZURE DATABRICKS<\/strong><\/p>\n<p><strong>Custom mlflow.pyfunc class<\/strong><\/p>\n<pre><code>class KerasWrapper(mlflow.pyfunc.PythonModel):\n  \n  def __init__(self, keras_model, labelEncoder, labelDecoder, n):         \n    self.keras_model = keras_model\n    self.labelEncoder = labelEncoder\n    self.labelDecoder = labelDecoder\n    self.topn = n\n    \n  def load_context(self, context): \n    self.keras_model = mlflow.keras.load_model(model_uri=context.artifacts[self.keras_model], compile=False)\n  \n  def predict(self, context, input_data):\n    scores = self.keras_model.predict(input_data)\n    return scores\n<\/code><\/pre>\n<p><strong>My Keras Deep Learning Model<\/strong> (this works fine by the way)<\/p>\n<pre><code>def build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):\n  \n  model = None\n\n  i = Input(shape=(None,), dtype=&quot;int64&quot;)\n  \n  #embedding layer\n  e = Embedding(vocab_size, 16)(i)\n  s = SpatialDropout1D(drop_embed)(e)\n\n  x = Conv1D(256, steps, activation='relu')(s)\n  x = GlobalMaxPooling1D()(x)\n  x = Dense(128, activation='relu')(x)\n  x = Dropout(0.2)(x)\n\n  #output layer\n  x = Dense(vocab_size, activation='softmax')(x)\n\n  model = Model(i, x)\n  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  model.summary()\n  \n  return model\n<\/code><\/pre>\n<p><strong>MLFLOW Section<\/strong><\/p>\n<pre><code>with mlflow.start_run(run_name=runName):\n\n  mlflow.tensorflow.autolog()   \n\n  #Build the model, compile and train on the training set\n  #signature: build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):\n  keras_model = build_model((vocab_size + 1), timeSteps, drop_embed, embedding_dimensions, encoder, modelType)      \n\n\n  keras_model.fit(X_train_encoded, y_train_encoded, epochs=epochs, verbose=1, batch_size=32, use_multiprocessing = True, \n                            validation_data=(X_test_encoded, y_test_encoded))\n\n  # Log the model parameters used for this run.  \n  mlflow.log_param(&quot;numofActionsinWorkflow&quot;, numofActionsinWf)\n  mlflow.log_param(&quot;timeSteps&quot;, timeSteps)\n\n  #wrap it up in a pyfunc model\n  wrappedModel = KerasWrapper(keras_model, encoder, decoder, bestActionCount)\n\n  # Create a model signature using the tensor input to store in the MLflow model registry\n  signature = infer_signature(X_test_encoded, wrappedModel.predict(None, X_test_encoded))\n  # Let's check out how it looks\n  print(signature)\n\n  # Create an input example to store in the MLflow model registry\n  input_example = np.expand_dims(X_train[17], axis=0)\n  \n  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n  model_env = mlflow.pyfunc.get_default_conda_env()\n  # Record specific additional dependencies required by the serving model\n  model_env['dependencies'][-1]['pip'] += [\n    f'tensorflow=={tf.__version__}',\n    f'mlflow=={mlflow.__version__}',\n    f'sklearn=={sklearn.__version__}',\n    f'cloudpickle=={cloudpickle.__version__}',\n  ]\n  \n  #log the model to experiment\n  #mlflow.keras.log_model(keras_model, artifact_path = runName, signature=signature, input_example=input_example, conda_env = model_env)\n  \n  wrapped_model_path = runName\n  \n  if (os.path.exists(wrapped_model_path)):\n    shutil.rmtree(wrapped_model_path)\n  \n  #Log model as pyfunc model\n  mlflow.pyfunc.log_model(runName, python_model=wrappedModel, signature=signature, input_example=input_example, conda_env = model_env)\n\n  #return the run ID for model registration\n  run_id = mlflow.active_run().info.run_id\n  \n  mlflow.end_run()\n<\/code><\/pre>\n<p>Here is the error that i receive\n<a href=\"https:\/\/i.stack.imgur.com\/hegaZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hegaZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kZ2oy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kZ2oy.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-12-16 20:22:51.803 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|machine-learning|keras|deep-learning|mlflow",
        "Question_view_count":595,
        "Owner_creation_date":"2015-03-27 21:44:36.943 UTC",
        "Owner_last_access_date":"2022-09-24 05:46:47.85 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFLow artifact logging and retrieve on remote server",
        "Question_body":"<p>I am trying to setup a MLFlow tracking server on a remote machine as a systemd service.\nI have a sftp server running and created a SSH key pair.<\/p>\n<p>Everything seems to work fine except the artifact logging. MLFlow seems to not have permissions to list the artifacts saved in the <code>mlruns<\/code> directory.<\/p>\n<p>I create an experiment and log artifacts in this way:<\/p>\n<pre><code>uri = 'http:\/\/192.XXX:8000' \nmlflow.set_tracking_uri(uri)\n\nmlflow.create_experiment('test', artifact_location='sftp:\/\/192.XXX:_path_to_mlruns_folder_')\n\nexperiment=mlflow.get_experiment_by_name('test')\nwith mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name) as run:\n       mlflow.log_param(_parameter_name_, _parameter_value_)     \n       mlflow.log_artifact(_an_artifact_, _artifact_folder_name_)\n<\/code><\/pre>\n<p>I can see the metrics in the UI and the artifacts in the correct destination folder on the remote machine. However, in the UI I receive this message when trying to see the artifacts:<\/p>\n<blockquote>\n<p>Unable to list artifacts stored\nunder sftp:\/\/192.XXX:<em>path_to_mlruns_folder<\/em>\/<em>run_id<\/em>\/artifacts\nfor the current run. Please contact your tracking server administrator\nto notify them of this error, which can happen when the tracking\nserver lacks permission to list artifacts under the current run's root\nartifact directory.<\/p>\n<\/blockquote>\n<p>I cannot figure out why as the <code>mlruns<\/code> folder has <code>drwxrwxrwx<\/code> permissions and all the subfolders have <code>drwxrwxr-x<\/code>. What am I missing?<\/p>\n<hr \/>\n<p>UPDATE\nLooking at it with fresh eyes, it seems weird that it tries to list files through <code>sftp:\/\/192.XXX:<\/code>, it should just look in the folder <code>_path_to_mlruns_folder_\/_run_id_\/artifacts<\/code>. However, I still do not know how to circumvent that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-10 13:45:56.893 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":2283,
        "Owner_creation_date":"2014-06-18 09:47:32.693 UTC",
        "Owner_last_access_date":"2022-09-25 05:06:29.037 UTC",
        "Owner_location":null,
        "Owner_reputation":2210,
        "Owner_up_votes":1124,
        "Owner_down_votes":117,
        "Owner_views":262,
        "Answer_body":"<p>The problem seems to be that by default the systemd service is run by root.\nSpecifying a user and creating a ssh key pair for that user to access the same remote machine worked.<\/p>\n<pre><code>[Unit]\n\nDescription=MLflow server\n\nAfter=network.target \n\n[Service]\n\nRestart=on-failure\n\nRestartSec=20\n\nUser=_user_\n\nGroup=_group_\n\nExecStart=\/bin\/bash -c 'PATH=_yourpath_\/anaconda3\/envs\/mlflow_server\/bin\/:$PATH exec mlflow server --backend-store-uri postgresql:\/\/mlflow:mlflow@localhost\/mlflow --default-artifact-root sftp:\/\/_user_@192.168.1.245:_yourotherpath_\/MLFLOW_SERVER\/mlruns -h 0.0.0.0 -p 8000' \n\n[Install]\n\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p><code>_user_<\/code> and <code>_group_<\/code> should be the same listed by <code>ls -la<\/code> in the <code>mlruns<\/code> directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-12 10:16:46.663 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":"2021-03-11 08:34:46.85 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to configure `backend-store-uri` with huggingface Trainer",
        "Question_body":"<p>When configuring a Hugging Face TrainingArguments <a href=\"https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html<\/a> you can set the <code>logging_dir<\/code> and <code>output_dir<\/code>.<\/p>\n<p>There is also the <code>mlruns<\/code> directory which according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores<\/a> you can configure using <code>--set-backend-uri<\/code>. Though that is an mlflow doc, not a Hugging Face doc.<\/p>\n<p>What is the best way to specify a different <code>mlruns<\/code> directory programmatically when setting up the Hugging Face Trainer?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-22 16:31:58.393 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow|huggingface",
        "Question_view_count":19,
        "Owner_creation_date":"2017-01-13 11:08:11.263 UTC",
        "Owner_last_access_date":"2022-09-23 11:50:45.823 UTC",
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":161,
        "Owner_up_votes":169,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-06 13:22:27.343 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|mlflow|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":73,
        "Owner_creation_date":"2019-04-05 20:51:24.963 UTC",
        "Owner_last_access_date":"2022-09-24 07:41:48.093 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":438,
        "Owner_up_votes":15,
        "Owner_down_votes":2,
        "Owner_views":120,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 05:29:14.31 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow: active run ID does not match environment run ID",
        "Question_body":"<p>OS: Ubuntu 18<\/p>\n\n<p>Python: Python 3.6<\/p>\n\n<p>MLflow: 1.4<\/p>\n\n<p>I'm trying to get MLflow Projects to run. Here is my project:<\/p>\n\n<ul>\n<li><p>MLflow<\/p>\n\n<ul>\n<li><p>conda.yaml<\/p><\/li>\n<li><p>main.py<\/p><\/li>\n<li><p>prep_data.py<\/p><\/li>\n<li><p>learn.py<\/p><\/li>\n<li><p>List item<\/p><\/li>\n<\/ul><\/li>\n<\/ul>\n\n<p>The project is heavily based up on this repo: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/multistep_workflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/multistep_workflow<\/a>\nI'm trying to run both the prep_data and learn scripts using MLflow Projects and the main.py script as an entry point.\nFor execution I use the following command: <code>mlflow run . -P experiment_name=testproject<\/code><\/p>\n\n<p>But I get the following Error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"prep_data.py\", line 126, in &lt;module&gt;\n    prep_data()\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n   return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"prep_data.py\", line 65, in prep_data\n    with mlflow.start_run() as active_run:\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py\", line 129, in start_run\n    \"arguments\".format(existing_run_id))\nmlflow.exceptions.MlflowException: Cannot start run with ID 405b83bbb61046afa83b8dcd71b4db14 because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments\nTraceback (most recent call last):\n  File \"main.py\", line 75, in &lt;module&gt;\n    workflow()\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"main.py\", line 61, in workflow\n    }, experiment_name)\n  File \"main.py\", line 40, in _get_or_run\n    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/mlflow\/projects\/__init__.py\", line 287, in run\n    _wait_for(submitted_run_obj)\n  File \"\/home\/ubuntu\/venv\/lib\/python3.6\/site-packages\/mlflow\/projects\/__init__.py\", line 304, in _wait_for\n    raise ExecutionException(\"Run (ID '%s') failed\" % run_id)\nmlflow.exceptions.ExecutionException: Run (ID '405b83bbb61046afa83b8dcd71b4db14') failed\n2019\/11\/22 18:51:59 ERROR mlflow.cli: === Run (ID '62c229b2d9194b569a7b2bfc14338800') failed ===\n<\/code><\/pre>\n\n<p>I'm not sure if I understand the error correctly but it seems like it's saying I am using multiple experiments. However I'm fairly certain I am only using 1 (testproject).\nBrowsing SO and Github issues suggested I'd should set the environment variable <code>MLFLOW_TRACKING_URI<\/code> but it wasn't stated on how to set that. Thus I tried two different ways:\n1) exporting it before running the MLflow project: $ export <code>MLFLOW_TRACKING_URI='http:\/\/127.0.0.1:5099'<\/code>\n2) setting it at the beginning of my main.py script using python: <code>os.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/127.0.0.1:5099'<\/code>\nNeither had any effect.\nHere you can see my project:<\/p>\n\n<p>main.py<\/p>\n\n<pre><code>import os\nimport click\nimport mlflow\nfrom mlflow.entities import RunStatus\ndef _already_ran(entry_point, params, experiment_name):\n    # experiment = mlflow.get_experiment_by_name('{}_{}'.format(experiment_name, entry_point))\n    experiment = mlflow.get_experiment_by_name(experiment_name)\n    if experiment == None:\n        return None\n    experiment_id = experiment.experiment_id\n    client = mlflow.tracking.MlflowClient()\n    all_run_infos = reversed(client.list_run_infos(experiment_id))\n    match_failed = False\n    for run_info in all_run_infos\n        full_run = client.get_run(run_info.run_id)\n        for p_key, p_val in params:\n            run_value = full_run.data.params.get(p_key)\n            if run_value != p_val:\n                match_failed = True\n                break\n        if match_failed:\n            continue\n        if run_info.to_proto().status != RunStatus.FINISHED:\n            continue\n        return client.get_run(run_info.run_id)\n    return None\n\n\ndef _get_or_run(entry_point, params, experiment_name, use_cache=True):\n    existing_run = _already_ran(entry_point, params, experiment_name)\n    if use_cache and existing_run:\n        return existing_run\n    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)\n    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)\n\n@click.command()\n@click.option(\"--experiment-name\")\n@click.option('--prep-data-time-avg', default='placeholder')\n@click.option('--prep-data-sensor-id', default='placeholder')\n@click.option('--learn-epochs', default=100, type=int)\n@click.option('--learn-neurons', default=5, type=int)\n@click.option('--learn-layers', default=2, type=int)\ndef workflow(experiment_name, prep_data_time_avg, prep_data_sensor_id, learn_epochs, learn_neurons, learn_layers):\n    # mlflow.set_tracking_uri('http:\/\/127.0.0.1:5099')\n\n    # mlflow.set_experiment(experiment_name)\n    # with mlflow.start_run() as active_run:\n\n    data_run = _get_or_run('prep_data', {\n        'time_avg': prep_data_time_avg,\n        'sensor_id':prep_data_sensor_id,\n        'experiment_name': experiment_name\n    }, experiment_name)\n\n    learn_run = _get_or_run('learn', {\n        'epochs': learn_epochs,\n        'neurons': learn_neurons,\n        'layers': learn_layers,\n        'prep_data_run_id': data_run.run_id,\n        'experiment_name': experiment_name,\n    }, experiment_name)\nif __name__ == '__main__':\n    # os.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/127.0.0.1:5099'\n    workflow()\n\n\n<\/code><\/pre>\n\n<p>prep_data.py<\/p>\n\n<pre><code>@click.command()\n@click.option(\"--experiment-name\")\n@click.option('--time-avg', default='placeholder')\n@click.option('--sensor-id', default='placeholder')\ndef prep_data(experiment_name, time_avg, sensor_id):\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run() as active_run:\n      # logic code of prep_data\n\nif __name__ == '__main__':\n    prep_data()\n\n<\/code><\/pre>\n\n<p>I'm happy about any ideas on how to fix this issue.<\/p>\n\n<p>Thank you very much!<\/p>\n\n<p>Cheers,\nRaphael<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-23 10:00:29.297 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":2256,
        "Owner_creation_date":"2016-09-10 16:12:07.98 UTC",
        "Owner_last_access_date":"2022-06-30 12:40:05.403 UTC",
        "Owner_location":null,
        "Owner_reputation":4045,
        "Owner_up_votes":7,
        "Owner_down_votes":4,
        "Owner_views":73,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-16 01:21:39.507 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"scikit-learn|webserver|mlflow",
        "Question_view_count":787,
        "Owner_creation_date":"2019-03-20 13:27:18.367 UTC",
        "Owner_last_access_date":"2022-09-24 21:19:57.82 UTC",
        "Owner_location":"Tehran, Tehran Province, Iran",
        "Owner_reputation":415,
        "Owner_up_votes":300,
        "Owner_down_votes":2,
        "Owner_views":37,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-16 01:21:39.507 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-07-10 12:27:37.947 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Read csv files in a MLFlow pipeline",
        "Question_body":"<p>I'm following <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/pipelines.html\" rel=\"nofollow noreferrer\">this documentation<\/a> to use ML Flow pipelines, which requires to clone <a href=\"https:\/\/github.com\/mlflow\/mlp-regression-template\" rel=\"nofollow noreferrer\">this repository<\/a>. If I run the complete pipeline as it is It works perfectly:<\/p>\n<pre><code>import os\nfrom mlflow.pipelines import Pipeline\n\nos.chdir(&quot;~\/mlp-regression-template&quot;)\nregression_pipeline = Pipeline(profile=&quot;local&quot;)\n# Display a visual overview of the pipeline graph\nregression_pipeline.inspect()\n# Run the full pipeline\nregression_pipeline.run()\n<\/code><\/pre>\n<p>But when I try to change the first part to read diferent dataset, I get the following error:<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Resolved data file with path '\/tmp\/tmpv201mpms\/precio_leche.csv' does not have the expected format 'parquet'.\n<\/code><\/pre>\n<p>Which is correct, the input file is not in csv format now, I added a new file to the data folder and changed the profile local.yaml file:<\/p>\n<pre><code>INGEST_DATA_LOCATION: &quot;.\/data\/precio_leche.csv&quot; \n<\/code><\/pre>\n<p>What I dont understand is that in the in the pipeline the insgest step, executes the ingest.py code:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fUx48.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fUx48.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Which funtion is to convert the read csv files:<\/p>\n<pre><code>def load_file_as_dataframe(file_path: str, file_format: str) -&gt; DataFrame:\n    &quot;&quot;&quot;\n    Load content from the specified dataset file as a Pandas DataFrame.\n\n    This method is used to load dataset types that are not natively  managed by MLflow Pipelines\n    (datasets that are not in Parquet, Delta Table, or Spark SQL Table format). This method is\n    called once for each file in the dataset, and MLflow Pipelines automatically combines the\n    resulting DataFrames together.\n\n    :param file_path: The path to the dataset file.\n    :param file_format: The file format string, such as &quot;csv&quot;.\n    :return: A Pandas DataFrame representing the content of the specified file.\n    &quot;&quot;&quot;\n\n    if file_format == &quot;csv&quot;:\n        import pandas\n\n        _logger.warning(\n            &quot;Loading dataset CSV using `pandas.read_csv()` with default arguments and assumed index&quot;\n            &quot; column 0 which may not produce the desired schema. If the schema is not correct, you&quot;\n            &quot; can adjust it by modifying the `load_file_as_dataframe()` function in&quot;\n            &quot; `steps\/ingest.py`&quot;\n        )\n        return pandas.read_csv(file_path, index_col=0)\n    else:\n        raise NotImplementedError\n<\/code><\/pre>\n<p>After getting this error I also tried changing the pipeline.yaml file to have .csv as the default format:<\/p>\n<pre><code>  format: {{INGEST_DATA_FORMAT|default('csv')}}\n<\/code><\/pre>\n<p>But it didn't work either, also I notice that when I run the ingest step for the default dataset it return this summary from the pandas profilling library:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/anW8q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/anW8q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I do not see where this is in the code, am I changing the wrong files? or what should I do in order to read csv files in the ingest step? also. I'm looking to read several files, not only one.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-18 00:41:01.67 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|csv|machine-learning|mlflow|mlops",
        "Question_view_count":71,
        "Owner_creation_date":"2015-02-08 23:53:31.84 UTC",
        "Owner_last_access_date":"2022-09-24 00:08:32.523 UTC",
        "Owner_location":null,
        "Owner_reputation":8349,
        "Owner_up_votes":1489,
        "Owner_down_votes":6,
        "Owner_views":949,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does Hugging face defaults allow to log mlflow artifacts and name every run of mlflow log?",
        "Question_body":"<p>I am training a simple binary classification model using Hugging face models using pytorch.<\/p>\n<p>Bert PyTorch HuggingFace.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>import transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom transformers import AutoTokenizer\n\n \nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertConfig\n<\/code><\/pre>\n<pre><code>def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n   \n\n    predictions = np.argmax(logits, axis=-1)\n    \n    acc = np.sum(predictions == labels) \/ predictions.shape[0]\n    return {&quot;accuracy&quot;: acc,\n            'precision': metrics.precision_score(labels, predictions),\n            'recall': metrics.recall_score(labels, predictions),\n            'f1': metrics.f1_score(labels, predictions)}\n\ntraining_args = tr.TrainingArguments(\n    #report_to = 'wandb',\n    output_dir='\/home\/pc\/proj\/Exp2_conv_stampy_data\/results_exp0',          # output directory\n    overwrite_output_dir = True,\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=32,  # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    learning_rate=2e-5,\n    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs_exp0',            # directory for storing logs\n    logging_steps=137,\n    evaluation_strategy=&quot;epoch&quot;\n    ,save_strategy=&quot;epoch&quot;\n    ,load_best_model_at_end=True\n    ,fp16=True\n    ,run_name=&quot;final_model0&quot;\n    \n)\n\n\n# counter = 0\n# results_lst = []\n\nfrom transformers import TrainerCallback\nfrom copy import deepcopy\n\nmodel = tr.XLMRobertaForSequenceClassification.from_pretrained(&quot;\/home\/pc\/multilingual_toxic_xlm_roberta&quot;,problem_type=&quot;single_label_classification&quot;, num_labels=2,ignore_mismatched_sizes=True, id2label={0: 'negative', 1: 'positive'})\n\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\n\n\ntrain_data = SEDataset(train_encodings, train_labels)\nval_data = SEDataset(val_encodings, val_labels)\n\nmodel.to(device)\n\nclass CustomCallback(TrainerCallback):\n    \n    def __init__(self, trainer) -&gt; None:\n        super().__init__()\n        self._trainer = trainer\n    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        if control.should_evaluate:\n            control_copy = deepcopy(control)\n            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=&quot;train&quot;)\n            return control_copy\n\ntrainer = tr.Trainer(\n    model=model,                         # the instantiated Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_data,         # training dataset\n    eval_dataset=val_data,          # evaluation dataset\n    compute_metrics=compute_metrics    # the callback that computes metrics of interest\n)\ntrainer.add_callback(CustomCallback(trainer)) \ntrain = trainer.train()\n\n\n\ntrainer.save_model(&quot;\/home\/pc\/proj\/Exp2_conv_stampy_data\/result_toxic_model_exp0&quot;)\n\n<\/code><\/pre>\n<p>I see by default <code>mlruns<\/code> directory is created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rD25L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rD25L.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>What is <code>0' and what are these 2 folders inside <\/code>0`?<\/strong><\/p>\n<p><strong>How can rename to something useful and understandable.?<\/strong><\/p>\n<p><strong>If I run multiple runs, how can I log every run of model with something like <code>run1<\/code>, <code>run2<\/code> under same experiment?<\/strong><\/p>\n<p><strong>Also I see artifact folder is empty, how to log final model?<\/strong><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-03 07:00:20.4 UTC",
        "Question_favorite_count":2.0,
        "Question_score":0,
        "Question_tags":"pytorch|huggingface-transformers|mlflow",
        "Question_view_count":818,
        "Owner_creation_date":"2018-06-07 08:44:46.053 UTC",
        "Owner_last_access_date":"2022-09-23 09:15:48.837 UTC",
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Is it possible to use authentication and authorization in MLFlow Server?",
        "Question_body":"<p>MLFlow does not have integrated authentication (openID, LDAP, kerberos, AAD...) or authorization (RBAC, ABAC, ACL...)<\/p>\n<p>Is it only possible with a web proxy in MLFlow? p.e: nginx<\/p>\n<p>Does anyone know of an option similar to Apache Sentry or Apache Ranger for MLFlow?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-08 11:01:18.063 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":57,
        "Owner_creation_date":"2016-05-22 18:52:36.183 UTC",
        "Owner_last_access_date":"2022-09-22 08:14:44.89 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Error while running a local mlflow server",
        "Question_body":"<p>I am trying to run an mlflow server locally. <\/p>\n\n<p>To do so, I am using:<\/p>\n\n<pre><code>mlflow server --backend-store-uri=\"sqlite:\/\/\/C:\\\\path\\\\to\\\\project_folder\\\\backend\\\\mlflow_data.db\" \n              --default-artifact-root=\"file:\/\/\/C:\\\\path\\\\to\\\\project_folder\\\\artifact_store\\\\\"\n<\/code><\/pre>\n\n<p>Where <\/p>\n\n<pre><code>- backend-store-uri: URI to which to persist experiment and run data (sqlite database in our case).\n- default-artifact-root: Local or S3 URI to store artifacts, for new experiments (local folder in our case).\n<\/code><\/pre>\n\n<p>I have already install the packages: <\/p>\n\n<pre><code>numpy==1.17.3\npandas==0.25.3\njupyterlab==1.0.10 \nscikit-learn==0.21.3\nmatplotlib==3.1.2\nmlflow==1.4.0\ntorch==1.3.1+cpu \ntorchvision==0.4.2+cpu \nxgboost==0.90\n<\/code><\/pre>\n\n<p>The problem is that I am getting this error: <\/p>\n\n<pre><code>Fatal error in launcher: Unable to create process using \"d:\\bld\\mlflow_1572494804636\\_h_env\\python.exe\" \"C:\\Users\\user\\AppData\\Local\\Continuum\\miniconda3\\envs\\mlflow-tut orial\\Scripts\\mlflow.exe\" server --backend-store-uri=sqlite:\/\/\/C:\\\\projects\\\\user\\\\mlflow_tutorial\\\\backend\\\\mlflow_ui_data.db --default-artifact-root=file:\/\/\/C:\\\\projects \\\\user\\\\mlflow_tutorial\\\\artifact_store\\\\\n<\/code><\/pre>\n\n<p>When I run the <code>mlflow server<\/code> command. Any ideas ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-19 14:59:34.933 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|python-3.x|mlflow",
        "Question_view_count":1113,
        "Owner_creation_date":"2016-02-01 14:54:20.48 UTC",
        "Owner_last_access_date":"2022-09-24 18:36:48.79 UTC",
        "Owner_location":null,
        "Owner_reputation":3527,
        "Owner_up_votes":352,
        "Owner_down_votes":6,
        "Owner_views":440,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Spark 3.2.0 Applying a model",
        "Question_body":"<p>I am using spark 3.2 to load a model to predict probability, and it doesn't seem to be working correctly, cant figure out why.<\/p>\n<pre><code># Load data in spark\npsdf = df.to_pandas_on_spark()\nmodel = mlflow.sklearn.load_model('s3:\/\/bucket\/r1_mlflow\/')\nres = model.predict_proba(psdf)\n\nValueError: Expected 2D array, got 1D array instead: array=['col1' 'col2' 'col3' 'col4' 'col5']\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n<\/code><\/pre>\n<p>Does this have to do with psdf being a <code>pyspark.pandas.frame.DataFrame<\/code> instead of a <code>pandas.core.frame.DataFrame<\/code>, or am I doing something else wrong here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-29 15:58:13.673 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"pandas|apache-spark|pyspark|mlflow",
        "Question_view_count":81,
        "Owner_creation_date":"2012-02-20 15:55:18.11 UTC",
        "Owner_last_access_date":"2022-09-20 02:53:43.513 UTC",
        "Owner_location":null,
        "Owner_reputation":2310,
        "Owner_up_votes":60,
        "Owner_down_votes":5,
        "Owner_views":147,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Question_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-17 20:25:00.84 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"uri|tracking|mlflow",
        "Question_view_count":682,
        "Owner_creation_date":"2020-03-25 10:10:52.3 UTC",
        "Owner_last_access_date":"2022-09-23 08:20:45.11 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-08-18 06:58:43.973 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow \"invocations\" prefix",
        "Question_body":"<p>We are deploying some MLflow models using docker and Kubernetes and\nWe are using Ingress load balancer in K8S is mandatory for security reasons, but right now we need to deploy more than one mlflow image in the same cluster.\nWhen we run the container the model serving start the application using  &quot;\/invocations&quot; path for the POST requests.\nThat means that we cann\u00b4t differentiate the model using the prefix, cause every container is using the same prefix.<\/p>\n<p>My question is,is there any way to change &quot;\/invocations&quot; prefix on model Mlflow images?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-02 20:21:59.17 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":44,
        "Owner_creation_date":"2022-06-10 19:20:31.17 UTC",
        "Owner_last_access_date":"2022-09-23 16:37:58.107 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-03 21:46:53.487 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow can't find .py file",
        "Question_body":"<p>I'm trying to learn to use <code>mlflow<\/code> by creating a very simple project and log it.<\/p>\n\n<p>I've tried following <code>mlflow<\/code>'s example and my code runs properly when running the main.py as a normal bash command.<\/p>\n\n<p>I couldn't make it run using the <code>mlflow<\/code> CLI using project and a simple file.\nI got the following error.<\/p>\n\n<pre><code>(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run src\/main.py \n2019\/05\/11 10:21:41 ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run .\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Created directory \/tmp\/tmpe26oernf for downloading remote URIs passed to arguments of type 'path' ===\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Running command 'source activate mlflow-21497056aed7961402b515847613ed9f950fa9fc &amp;&amp; python src\/main.py 1.0' in run with ID 'ed51446de4c44903ab891d09cfe10e49' === \nbash: activate: No such file or directory\n2019\/05\/11 10:40:25 ERROR mlflow.cli: === Run (ID 'ed51446de4c44903ab891d09cfe10e49') failed ===\n\n<\/code><\/pre>\n\n<p>Needless to say my main has a <code>.py<\/code> suffix.<\/p>\n\n<p>Is there anything wrong that causes this issue?<\/p>\n\n<p>My main.py is:<\/p>\n\n<pre><code>import sys\n\nimport gym\nimport mlflow\n\n\nif __name__ == '__main__':\n    env = gym.make(\"CartPole-v0\")\n    right_percent = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 1.0\n    with mlflow.start_run():\n        obs = env.reset()\n        print(env.action_space)\n        action = 1  # accelerate right\n        print(obs)\n        mlflow.log_param(\"right percent\", right_percent)\n        mlflow.log_metric(\"mean score\", 1)\n        mlflow.log_metric(\"std score\", 0)\n<\/code><\/pre>\n\n<p>conda_env.yaml<\/p>\n\n<pre><code>name: rlearning\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - numpy\n  - pandas\n  - tensorflow-gpu\n  - pip:\n      - mlflow\n      - gym\n<\/code><\/pre>\n\n<p>MLproject<\/p>\n\n<pre><code>name: reinforced learning\n\nconda_env: files\/config\/conda_environment.yaml\n\nentry_points:\n  main:\n    parameters:\n      right_percent: {type: float, default: 1.0}\n    command: \"python src\/main.py {right_percent}\"\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-11 07:32:43.29 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1263,
        "Owner_creation_date":"2015-06-25 08:50:08.62 UTC",
        "Owner_last_access_date":"2022-09-08 07:19:29.283 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":91,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-05-11 07:44:35.393 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"get the run id for an mlflow experiment with the name?",
        "Question_body":"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport mlflow\n\nexperiment_name=&quot;experiment-1&quot;\nmlflow.set_experiment(experiment_name)\n\nno_of_trees=[100,200,300]\ndepths=[2,3,4]\nfor trees in no_of_trees:\n    for depth in depths:\n        with mlflow.start_run() as run:\n            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)\n            model.fit(x_train, y_train)\n            predictions=model.predict(x_cv)\n            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))\n<\/code><\/pre>\n<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-16 02:45:27.487 UTC",
        "Question_favorite_count":null,
        "Question_score":6,
        "Question_tags":"python|mlflow",
        "Question_view_count":6374,
        "Owner_creation_date":"2016-11-15 06:12:07.737 UTC",
        "Owner_last_access_date":"2022-08-15 17:25:10.4 UTC",
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Answer_body":"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.<\/p>\n<pre><code>experiment_name = &quot;experiment-1&quot;\ncurrent_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\nexperiment_id=current_experiment['experiment_id']\n<\/code><\/pre>\n<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)<\/p>\n<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])\nbest_run_id = df.loc[0,'run_id']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-16 02:45:27.487 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":15.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow - How to migrate or copy a run from one experiment to other?",
        "Question_body":"<p>I am trying to move a run in MLflow from one experiment to another. Does anybody know if its possible? If yes, how? (I use Python API)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-17 12:12:19.43 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1074,
        "Owner_creation_date":"2019-01-21 07:32:05.357 UTC",
        "Owner_last_access_date":"2022-09-24 17:03:22.623 UTC",
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":551,
        "Owner_up_votes":9,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow - How to point interface path to show the expected result",
        "Question_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-04 10:15:40.88 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":379,
        "Owner_creation_date":"2015-10-18 08:44:37.637 UTC",
        "Owner_last_access_date":"2021-12-02 14:27:03.627 UTC",
        "Owner_location":null,
        "Owner_reputation":267,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Answer_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-06 08:39:43.72 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Question_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-31 06:46:29.943 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|docker-compose|mlflow",
        "Question_view_count":932,
        "Owner_creation_date":"2011-07-17 08:59:45.21 UTC",
        "Owner_last_access_date":"2022-09-24 02:56:29.02 UTC",
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Answer_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-04 05:46:56.08 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow Tracking On EC2",
        "Question_body":"<p>I'm attempting to follow the instructions given here (<a href=\"https:\/\/medium.com\/@alexanderneshitov\/how-to-run-an-mlflow-tracking-server-on-aws-ec2-d7afd0ac8008\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@alexanderneshitov\/how-to-run-an-mlflow-tracking-server-on-aws-ec2-d7afd0ac8008<\/a>) to test running MLflow tracker on an ec2 instance. I have done the following from the article<\/p>\n\n<ol>\n<li>Install mlflow on ec2<\/li>\n<li>Install and configure NGINX following the steps given<\/li>\n<li>Start mlflow server on ec2 using <code>mlflow server --default-artifact-root s3:\/\/test.bucket.for.mlflow\/ --host 0.0.0.0<\/code><\/li>\n<li>Access server using its public DNS<\/li>\n<\/ol>\n\n<p>According to the article, I should see the mlflow ui when accessing with my ec2 public DNS, but all I see is the following page:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/U5o1C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/U5o1C.png\" alt=\"NGINX page instead of MLflow UI\"><\/a><\/p>\n\n<p>Why would I be seeing this page and not the mlflow page like:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6RiiT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6RiiT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2020-04-10 17:23:33.373 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"nginx|amazon-ec2|dns|mlflow",
        "Question_view_count":1547,
        "Owner_creation_date":"2014-11-26 14:40:35.813 UTC",
        "Owner_last_access_date":"2021-08-12 15:37:44.573 UTC",
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-13 17:01:16.79 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How does one invert an encoded prediction in Keras for model serving?",
        "Question_body":"<p>I have a Keras model in which i have successfully added a <code>StringLookUp<\/code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.<\/p>\n<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.<\/p>\n<p>Here is the implementation of the encoder and decoders:<\/p>\n<pre><code>#generate the encoder and decoders\nencoder = tf.keras.layers.StringLookup(vocabulary=vocab, )\ndecoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)\n<\/code><\/pre>\n<p>Here is the some of the code that makes the inference model<\/p>\n<pre><code># For inference, you can export a model that accepts strings as input\ninputs = Input(shape=(6,), dtype=&quot;string&quot;)\nx = encoder(inputs)\noutputs = keras_model(x)\ninference_model = Model(inputs, outputs)\n\ninference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \ninference_model.summary()\n<\/code><\/pre>\n<p>The <code>encoder<\/code> above is just a function that implements <code>tf.keras.layers.StringLookup<\/code><\/p>\n<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder<\/code> which implements the reverse of <code>StringLookUp<\/code>.<\/p>\n<p><em><strong>Here's my problem<\/strong><\/em>\nWhile this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.<\/p>\n<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks<\/strong><\/em><\/p>\n<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-25 05:35:26.013 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|deep-learning|mlflow",
        "Question_view_count":176,
        "Owner_creation_date":"2015-03-27 21:44:36.943 UTC",
        "Owner_last_access_date":"2022-09-24 05:46:47.85 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-22 17:23:12.93 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|apache-kafka|mlflow|real-time-data",
        "Question_view_count":2716,
        "Owner_creation_date":"2021-02-12 11:44:23.95 UTC",
        "Owner_last_access_date":"2022-09-22 11:40:37.707 UTC",
        "Owner_location":"Turkey",
        "Owner_reputation":44,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-26 14:45:10.747 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-02-22 17:31:38.323 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Log text in mlflow",
        "Question_body":"<p>I know that you can log metrics as your experiment progresses. For example the training loss over epochs for your DL model.<\/p>\n<p>I was wondering if it was possible to do something similar for text. In my particular case I have a text model that generates some example text after each epoch and I wish to see what it's like. For example:<\/p>\n<pre><code>Epoch 1:\ntHi is RubisH\nEpoch 2:\nOk look slight better\nEpoch 3:\nI can speak English better than William Shakespeare\n<\/code><\/pre>\n<p>The workaround I can think of is to log this to a text file and push that as an artifact in mlflow. Was wondering if there was something else more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-25 03:19:19.947 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1295,
        "Owner_creation_date":"2013-06-28 05:53:20.11 UTC",
        "Owner_last_access_date":"2022-09-24 08:20:25.063 UTC",
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":9065,
        "Owner_up_votes":1204,
        "Owner_down_votes":0,
        "Owner_views":952,
        "Answer_body":"<p>You can use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_param\" rel=\"nofollow noreferrer\">log_param\/log_params<\/a> for that. For long texts maybe it's better to use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_text\" rel=\"nofollow noreferrer\">log_text<\/a> instead...<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-03-25 11:24:29.503 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow Rstudio Dockerfile Permission denied",
        "Question_body":"<h1>Goal<\/h1>\n<p>Track R models in the remote MLflow tracking server (running in kubernetes). The model is developed on the local computer from RStudio run in the Docker container.<\/p>\n<h1>Setup<\/h1>\n<p>Based on my research I need to create RStudio image with conda installed. After that I want to run example from MLflow documentation.<\/p>\n<p>Dockerfile<\/p>\n<pre><code>FROM rocker\/rstudio\n\nUSER root\n\nENV PATH=&quot;\/root\/miniconda3\/bin:${PATH}&quot;\nARG PATH=&quot;\/root\/miniconda3\/bin:${PATH}&quot;\nENV MLFLOW_BIN=\/root\/miniconda3\/bin\/mlflow\nENV MLFLOW_PYTHON_BIN=\/root\/miniconda3\/bin\/python\n\nRUN apt-get update\n\nRUN apt-get install -y wget &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget \\\n    https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh \\\n    &amp;&amp; mkdir \/root\/.conda \\\n    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b \\\n    &amp;&amp; rm -f Miniconda3-latest-Linux-x86_64.sh \nRUN conda --version\n\nRUN R -e 'install.packages(&quot;mlflow&quot;)'\nRUN R -e 'install.packages(&quot;glmnet&quot;)'\nRUN R -e 'install.packages(&quot;carrier&quot;)'\n\nRUN pip install -U mlflow==1.19.0\n<\/code><\/pre>\n<p>train.R (adjusted exmple from <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/r_wine\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n<pre><code># The data set used in this example is from http:\/\/archive.ics.uci.edu\/ml\/datasets\/Wine+Quality\n# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n\nlibrary(mlflow)\nlibrary(glmnet)\nlibrary(carrier)\n\nset.seed(40)\n\n# Read the wine-quality csv file\ndata &lt;- read.csv(&quot;wine-quality.csv&quot;)\n\n# Split the data into training and test sets. (0.75, 0.25) split.\nsampled &lt;- sample(1:nrow(data), 0.75 * nrow(data))\ntrain &lt;- data[sampled, ]\ntest &lt;- data[-sampled, ]\n\n# The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\ntrain_x &lt;- as.matrix(train[, !(names(train) == &quot;quality&quot;)])\ntest_x &lt;- as.matrix(test[, !(names(train) == &quot;quality&quot;)])\ntrain_y &lt;- train[, &quot;quality&quot;]\ntest_y &lt;- test[, &quot;quality&quot;]\n\nalpha &lt;- mlflow_param(&quot;alpha&quot;, 0.5, &quot;numeric&quot;)\nlambda &lt;- mlflow_param(&quot;lambda&quot;, 0.5, &quot;numeric&quot;)\n\nSys.setenv(MLFLOW_S3_ENDPOINT_URL=&quot;&lt;EP&gt;&quot;)\nSys.setenv(AWS_ACCESS_KEY_ID=&quot;&lt;some_key&gt;&quot;)\nSys.setenv(AWS_SECRET_ACCESS_KEY=&quot;&lt;some_secret&gt;&quot;)\n\nmlflow_set_experiment(&quot;Wine R experiment&quot;)\nmlflow_set_tracking_uri(&quot;&lt;http...blabla&gt;&quot;)\n\nwith(mlflow_start_run(), {\n    model &lt;- glmnet(train_x, train_y, alpha = alpha, lambda = lambda, family= &quot;gaussian&quot;, standardize = FALSE)\n    predictor &lt;- crate(~ glmnet::predict.glmnet(!!model, as.matrix(.x)), !!model)\n    predicted &lt;- predictor(test_x)\n\n    rmse &lt;- sqrt(mean((predicted - test_y) ^ 2))\n    mae &lt;- mean(abs(predicted - test_y))\n    r2 &lt;- as.numeric(cor(predicted, test_y) ^ 2)\n\n    message(&quot;Elasticnet model (alpha=&quot;, alpha, &quot;, lambda=&quot;, lambda, &quot;):&quot;)\n    message(&quot;  RMSE: &quot;, rmse)\n    message(&quot;  MAE: &quot;, mae)\n    message(&quot;  R2: &quot;, r2)\n\n    mlflow_log_param(&quot;alpha&quot;, alpha)\n    mlflow_log_param(&quot;lambda&quot;, lambda)\n    mlflow_log_metric(&quot;rmse&quot;, rmse)\n    mlflow_log_metric(&quot;r2&quot;, r2)\n    mlflow_log_metric(&quot;mae&quot;, mae)\n\n    mlflow_log_model(predictor, &quot;model&quot;)\n})\n<\/code><\/pre>\n<p>Its POC don't mind the unsafe env variables.<\/p>\n<p>I run the container like this:<\/p>\n<pre><code>docker run --rm -p 8787:8787 -e PASSWORD=password --mount type=bind,source=$(pwd)\/mlflow\/examples\/r_wine,target=\/home\/rstudio rstudio-mlflow\n<\/code><\/pre>\n<h1>Problem<\/h1>\n<p>Everytime I run the file from RStudio I get error on the last line (the <code>mlflow_log_model(predictor, &quot;model&quot;)<\/code>):<\/p>\n<pre><code>cannot start processx process '\/root\/miniconda3\/bin\/mlflow' (system error 13, Permission denied) @unix\/processx.c:608 (processx_exec)\n<\/code><\/pre>\n<p>I am getting permission denied when listing conda bin folder from RStudio terminal. Can you help me how to correctly install conda with RStudio image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-08-05 11:00:11.71 UTC",
        "Question_favorite_count":2.0,
        "Question_score":2,
        "Question_tags":"r|docker|rstudio|conda|mlflow",
        "Question_view_count":351,
        "Owner_creation_date":"2014-12-30 11:34:35.5 UTC",
        "Owner_last_access_date":"2022-09-23 12:37:47.597 UTC",
        "Owner_location":"Bratislava, Slovensko",
        "Owner_reputation":357,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"how to log hydra's multi-run in mlflow",
        "Question_body":"<p>I am trying to manage the results of machine learning with mlflow and hydra.\nSo I tried to run it using the multi-run feature of hydra.\nI used the following code as a test.<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    with mlflow.start_run() :\n        mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>This code will not work.\nI got the following error<\/p>\n<pre><code>Exception: Run with UUID [RUNID] is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>So I modified the code as follows<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    with mlflow.start_run(nested=True) :\n        mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n\n<\/code><\/pre>\n<p>This code works, but the artifact is not saved.\nThe following corrections were made to save the artifacts.<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    \n    mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>As a result, the artifacts are now saved.\nHowever, when I run the following command<\/p>\n<pre><code>python test.py model=A,B hidden=12,212,31 -m\n<\/code><\/pre>\n<p>Only the artifact of the last execution condition was saved.<\/p>\n<p>How can I modify mlflow to manage the parameters of the experiment by taking advantage of the multirun feature of hydra?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-25 20:30:55.417 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|data-science|mlflow|fb-hydra",
        "Question_view_count":1109,
        "Owner_creation_date":"2019-03-21 12:30:34.14 UTC",
        "Owner_last_access_date":"2022-09-23 12:41:46.963 UTC",
        "Owner_location":"Japan",
        "Owner_reputation":581,
        "Owner_up_votes":26,
        "Owner_down_votes":2,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-06-26 07:08:01.413 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Register SageMaker model in MLflow",
        "Question_body":"<p>MLflow can be used to track (hyper)parameters and metrics when training machine learning models. It stores the trained model as an artifact for every experiment. These models then can be directly deployed as <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#deploy-a-python-function-model-on-amazon-sagemaker\" rel=\"nofollow noreferrer\">SageMaker endpoints<\/a>.<\/p>\n<p>Is it possible to do it the other way around, too, i.e. to register models trained in SageMaker into MLflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-23 13:34:36.023 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":133,
        "Owner_creation_date":"2013-10-28 11:07:46.783 UTC",
        "Owner_last_access_date":"2022-09-21 20:35:26.05 UTC",
        "Owner_location":"Vienna, Austria",
        "Owner_reputation":690,
        "Owner_up_votes":707,
        "Owner_down_votes":1,
        "Owner_views":174,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MlflowException: API request (Caused by ResponseError('too many 503 error responses'))",
        "Question_body":"<p>I am using mlflow to register my model. I try to use 'Scenario 4' when artifacts load to S3 bucket from local.<\/p>\n<ol>\n<li><p>Add credentials of S3 bucket to .aws\/credentials<\/p>\n<\/li>\n<li><p>Set endpoint and mlflow URI:<\/p>\n<p>os.environ[&quot;MLFLOW_S3_ENDPOINT_URL&quot;]='https:\/\/storage.yandexcloud.net'\nos.environ[&quot;MLFLOW_TRACKING_URI&quot;]='http:\/\/:8000'<\/p>\n<\/li>\n<li><p>Log model to S3 via mlflow:<\/p>\n<p>import mlflow\nimport mlflow.sklearn\nmlflow.set_experiment(&quot;my&quot;)\n...\nmlflow.sklearn.log_model(model, artifact_path=&quot;models_mlflow&quot;)<\/p>\n<\/li>\n<\/ol>\n<p>But get error:<\/p>\n<pre><code>MlflowException: API request to http:\/\/&lt;IP&gt;:8000\/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl failed with exception HTTPConnectionPool(host='62.84.121.234', port=8000): Max retries exceeded with url: \/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl (Caused by ResponseError('too many 503 error responses'))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-31 22:41:17.113 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|mlflow|mlops|yandexcloud",
        "Question_view_count":38,
        "Owner_creation_date":"2014-04-07 09:58:41.17 UTC",
        "Owner_last_access_date":"2022-09-23 11:17:10.007 UTC",
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":75,
        "Owner_up_votes":105,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"InvalidMountException while loading model from mlflow-registry in databricks",
        "Question_body":"<p>I am trying to load my spark model that I have registered using mlflow in databricks ( referring  this documentation : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-registry-example#load-versions-of-the-registered-model-using-the-api\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-registry-example#load-versions-of-the-registered-model-using-the-api<\/a>)<\/p>\n<p>My Code is :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_uri = &quot;models:\/{model_name}\/{stage_name}&quot;.format(\n   model_name=model_name,stage_name=stage_name)\n\nmodel = mlflow.spark.load_model(model_uri )\n<\/code><\/pre>\n<p>But this gives me following error :<\/p>\n<pre><code>: com.databricks.backend.daemon.data.common.InvalidMountException: \n  Error while using path \/databricks\/mlflow-registry\/run-id\/models\/spark-model\/sparkml \n  for resolving path '\/run-id\/models\/spark-model\/sparkml' \n  within mount at '\/databricks\/mlflow-registry'.\n<\/code><\/pre>\n<p>(Note: <code>artifact_path<\/code> passed by me while registering model was 'spark-model')<\/p>\n<p>How can this error be resolved?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-10-26 17:09:21.65 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"apache-spark|databricks|mlflow",
        "Question_view_count":324,
        "Owner_creation_date":"2020-10-13 08:19:36.863 UTC",
        "Owner_last_access_date":"2022-09-22 10:59:26.177 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-11-01 17:49:03.517 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow: how to return experiment status as failed",
        "Question_body":"<p>I have written out my code and it has the following form <\/p>\n\n<pre><code>def train(run_name, log_basepath, logger, parameters):\n\n    try:\n        metrics = training_functions(parameters) # &lt;---- code that can fail\n\n        # demo for up loading attributes to a given run\n        with mlflow.start_run(run_name=run_name):\n\n            # log parameters\n            mlflow.log_params(parameters)\n\n            # log metrics\n            mlflow.log_metrics(metrics)\n\n            # define experiment tags\n            mlflow.set_tags(tags)\n\n            # upload pertinent files\n            mlflow.log_artifact(artifact_abspath)\n\n            # main model files\n            mlflow.log_artifact(log_basepath)\n\n    except Exception as e:\n        logger.error('\\nFailed model training. Returned following error:\\n {} \\n\\n'.format(e))\n        logger.info( 'Error on line {}'.format(sys.exc_info()[-1].tb_lineno))\n\n        # demo for up loading attributes to a given run\n        with mlflow.start_run(run_name=run_name):\n            # define run tags\n            logger.info('experiment name: {}'.format(experiment_name))\n            logger.info('run_name: {}'.format(run_name))\n            logger.info('run failed')\n\n            mlflow.log_artifact(log_basepath)\n<\/code><\/pre>\n\n<p>my aim is to send the logs to the mlflow server if there is a failure. My questions is, how do I get mlflow to mark this as a failure?  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-09 00:28:12.38 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"machine-learning|mlflow",
        "Question_view_count":522,
        "Owner_creation_date":"2017-03-23 19:39:24.893 UTC",
        "Owner_last_access_date":"2020-04-09 14:57:10.597 UTC",
        "Owner_location":"Nunja",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow S3UploadFailedError: Failed to upload",
        "Question_body":"<p>I've created with docker a MinioS3 artifact storage and a mysql bakend storage using the next docker-compose:<\/p>\n<pre><code>    version: '3.8'\n    services:\n        db:\n           environment:\n              - MYSQL_DATABASE=${MYSQL_DATABASE}\n              - MYSQL_USER=${MYSQL_USER}\n              - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n           expose:\n              - '3306'        \n           volumes:\n              - '(path)\/server_backend:\/var\/lib\/mysql '\n           image: 'mysql'\n           container_name: db\n\n        storage:\n            environment:\n                - MINIO_ACCESS_KEY=${MINIO_USR}\n                - MINIO_SECRET_KEY=${MINIO_PASS}\n            expose:\n                - '9000'\n            ports:\n                - '9000:9000'        \n            depends_on:\n                - db\n            command: server \/data\n            volumes:\n                - '(path)\/server_artifact:\/data'\n            image: minio\/minio:RELEASE.2021-02-14T04-01-33Z\n            container_name: MinIO\n\n        mlflow:\n            build: .\/mlflow\n            environment:\n                - AWS_ACCESS_KEY_ID=${MINIO_USR}\n                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n            expose:\n                - '5000'\n            ports:\n                - '5000:5000'\n            depends_on:\n                - storage                       \n            image: 'mlflow:Dockerfile'\n            container_name: server\n<\/code><\/pre>\n<p>The Mlflow server docker was created using the next Dockerfile:<\/p>\n<pre><code>    FROM python:3.8-slim-buster\n    WORKDIR \/usr\/src\/app\n    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql\n    ENV MLFLOW_S3_ENDPOINT_URL=http:\/\/storage:9000\n    CMD mlflow server \\\n        --backend-store-uri mysql+pymysql:\/\/MLFLOW:temporal@db:3306\/DBMLFLOW \\\n        --default-artifact-root s3:\/\/artifacts \\\n        --host 0.0.0.0\n<\/code><\/pre>\n<p>The credantials are defined in a <code>.env<\/code> file.<\/p>\n<p>The results of the <code>docker-compose<\/code> up command :<\/p>\n<pre><code>\n    [+] Running 21\/22\n     - mlflow Error                                                                                                                              5.6s\n     - storage Pulled                                                                                                                           36.9s\n       - a6b97b4963f5 Pull complete                                                                                                             24.6s\n       - 13948a011eec Pull complete                                                                                                             24.7s\n       - 40cdef9976a6 Pull complete                                                                                                             24.7s\n       - f47162848743 Pull complete                                                                                                             24.8s\n       - 5f2758d8e94c Pull complete                                                                                                             24.9s\n       - c2950439edb8 Pull complete                                                                                                             25.0s\n       - 1b08f8a15998 Pull complete                                                                                                             30.7s\n     - db Pulled                                                                                                                                45.8s\n       - 07aded7c29c6 Already exists                                                                                                             0.0s\n       - f68b8cbd22de Pull complete                                                                                                              0.7s\n       - 30c1754a28c4 Pull complete                                                                                                              2.1s\n       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s\n       - 79a41dc56b9a Pull complete                                                                                                              2.3s\n       - 00a75e3842fb Pull complete                                                                                                              6.7s\n       - b36a6919c217 Pull complete                                                                                                              6.8s\n       - 635b0b84d686 Pull complete                                                                                                              6.8s\n       - 6d24c7242d02 Pull complete                                                                                                             39.4s\n       - 5be6c5edf16f Pull complete                                                                                                             39.5s\n       - cb35eac1242c Pull complete                                                                                                             39.5s\n       - a573d4e1c407 Pull complete                                                                                                             39.6s\n    [+] Building 1.4s (7\/7) FINISHED\n     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s\n     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s\n     =&gt; [internal] load .dockerignore                                                                                                            0.0s\n     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s\n     =&gt; [internal] load metadata for docker.io\/library\/python:3.8-slim-buster                                                                    1.3s\n     =&gt; [1\/3] FROM docker.io\/library\/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s\n     =&gt; CACHED [2\/3] WORKDIR \/usr\/src\/app                                                                                                        0.0s\n     =&gt; CACHED [3\/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s\n     =&gt; exporting to image                                                                                                                       0.0s\n     =&gt; =&gt; exporting layers                                                                                                                      0.0s\n     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s\n     =&gt; =&gt; naming to docker.io\/library\/mlflow:Dockerfile                                                                                         0.0s\n    \n    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n    [+] Running 3\/3\n     - Container db  Created                                                                                                       0.5s\n     - Container MinIO      Created                                                                                                       0.1s\n     - Container server     Created                                                                                                       0.1s\n    Attaching to server, MinIO, db\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files\n    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) initializing of server in progress as process 44\n    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.\n    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend\n    MinIO      | Endpoint: http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Browser Access:\n    MinIO      |    http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Object API (Amazon S3 compatible):\n    MinIO      |    Go:         https:\/\/docs.min.io\/docs\/golang-client-quickstart-guide\n    MinIO      |    Java:       https:\/\/docs.min.io\/docs\/java-client-quickstart-guide\n    MinIO      |    Python:     https:\/\/docs.min.io\/docs\/python-client-quickstart-guide\n    MinIO      |    JavaScript: https:\/\/docs.min.io\/docs\/javascript-client-quickstart-guide\n    MinIO      |    .NET:       https:\/\/docs.min.io\/docs\/dotnet-client-quickstart-guide\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.1 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.3 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.7 seconds\n    server     | 2021\/10\/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 1.5 seconds\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server\n    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 93\n    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 0  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.\n    server     | 2021\/10\/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 3.1 seconds\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/iso3166.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/leap-seconds.list' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone1970.tab' as time zone. Skipping it.\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW\n    db  |\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server\n    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).\n    server     | 2021\/10\/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 6.3 seconds\n    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] \/usr\/sbin\/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped\n    db  |\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.\n    db  |\n    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 1\n    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\n    db  | mbind: Operation not permitted\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\n    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    db  | mbind: Operation not permitted\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http:\/\/0.0.0.0:5000 (17)\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync\n    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19\n    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20\n    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21\n    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22\n\n<\/code><\/pre>\n<p>It makes me suspect because on the second line appears <code>- mlflow Error<\/code> but i think that this is why the other builds haven't finished.<\/p>\n<p>Then I've set my environment variables on the client to create the information flow between my script and the storages:<\/p>\n<pre><code>\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'key'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'\n    \n    remote_server_uri = &quot;http:\/\/localhost:5000\/&quot; # server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n    \n    mlflow.set_experiment(&quot;mnist_mLflow_demo&quot;)\n\n<\/code><\/pre>\n<p>finally i trained a tensorflow network and i didn't have problems storing parameters and metrics but gave me some warnings (refering to next error). But the model haven't been auto log, so i tryed to do it manually:<\/p>\n<pre><code>    with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n    \n        mlflow.keras.log_model(model2, 'model2')\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>It dosen't work and it gives me the next INFO (but essencialy an error):<\/p>\n<pre><code>    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    2021\/10\/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model, flavor: keras)\n    Traceback (most recent call last):\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\environment.py&quot;, line 212, in infer_pip_requirements\n        return _infer_requirements(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 263, in _infer_requirements\n        modules = _capture_imported_modules(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 221, in _capture_imported_modules\n        _run_command(\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 163, in _run_command\n        stderr = stderr.decode(&quot;utf-8&quot;)\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte\n\n<\/code><\/pre>\n<p>And the next error:<\/p>\n<pre><code>\n    ClientError                               Traceback (most recent call last)\n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        278         try:\n    --&gt; 279             future.result()\n        280         # If a client error was raised, add the backwards compatibility layer\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        105             # out of this and propogate the exception.\n    --&gt; 106             return self._coordinator.result()\n        107         except KeyboardInterrupt as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        264         if self._exception:\n    --&gt; 265             raise self._exception\n        266         return self._result\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in __call__(self)\n        125             if not self._transfer_coordinator.done():\n    --&gt; 126                 return self._execute_main(kwargs)\n        127         except Exception as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in _execute_main(self, kwargs)\n        149 \n    --&gt; 150         return_value = self._main(**kwargs)\n        151         # If the task is the final task, then set the TransferFuture's\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\upload.py in _main(self, client, fileobj, bucket, key, extra_args)\n        693         with fileobj as body:\n    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n        695 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\n        385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n    --&gt; 386             return self._make_api_call(operation_name, kwargs)\n        387 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\n        704             error_class = self.exceptions.from_code(error_code)\n    --&gt; 705             raise error_class(parsed_response, operation_name)\n        706         else:\n    \n    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n    \n    During handling of the above exception, another exception occurred:\n    \n    S3UploadFailedError                       Traceback (most recent call last)\n    C:\\Users\\FCAIZA~1\\AppData\\Local\\Temp\/ipykernel_7164\/2476247499.py in &lt;module&gt;\n          1 with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n          2 \n    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')\n          4 \n          5 mlflow.end_run()\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)\n        402             mlflow.keras.log_model(keras_model, &quot;models&quot;)\n        403     &quot;&quot;&quot;\n    --&gt; 404     Model.log(\n        405         artifact_path=artifact_path,\n        406         flavor=mlflow.keras,\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\n        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n        189             try:\n        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py in log_artifacts(local_dir, artifact_path)\n        582     &quot;&quot;&quot;\n        583     run_id = _get_or_start_run().info.run_id\n    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n        585 \n        586 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        975             is_dir: True\n        976         &quot;&quot;&quot;\n    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n        978 \n        979     @contextlib.contextmanager\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        333         &quot;&quot;&quot;\n    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n        335 \n        336     def list_artifacts(self, run_id, path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n        102                 upload_path = posixpath.join(dest_path, rel_path)\n        103             for f in filenames:\n    --&gt; 104                 self._upload_file(\n        105                     s3_client=s3_client,\n        106                     local_file=os.path.join(root, f),\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)\n         78         if environ_extra_args is not None:\n         79             extra_args.update(environ_extra_args)\n    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n         81 \n         82     def log_artifact(self, local_file, artifact_path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\n        128     &quot;&quot;&quot;\n        129     with S3Transfer(self, Config) as transfer:\n    --&gt; 130         return transfer.upload_file(\n        131             filename=Filename, bucket=Bucket, key=Key,\n        132             extra_args=ExtraArgs, callback=Callback)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        283         # client error.\n        284         except ClientError as e:\n    --&gt; 285             raise S3UploadFailedError(\n        286                 &quot;Failed to upload %s to %s: %s&quot; % (\n        287                     filename, '\/'.join([bucket, key]), e))\n    \n    S3UploadFailedError: Failed to upload (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model\\conda.yaml to artifacts\/1\/5ae5fcef2d07432d811c3d7eb534382c\/artifacts\/model2\/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n\n<\/code><\/pre>\n<p>Do you know how to help me with it? I have been looking all this morning but i did not find a solution. Thank you!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-06 13:09:44.987 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mysql|docker|minio|mlflow",
        "Question_view_count":969,
        "Owner_creation_date":"2020-02-04 18:43:25.373 UTC",
        "Owner_last_access_date":"2022-09-21 20:36:53.347 UTC",
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:\/&quot; is &quot;fca\u00f1izares&quot; (Ca\u00f1izares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.<\/p>\n<p>PS: Moral of the issue, get rid of the extrange characters!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-08 08:04:08.313 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Accessing Delta Lake Table in Databricks via Spark in MLflow project",
        "Question_body":"<p>I am currently accessing deltalake table from databricks notebook using spark. However now I need to access delta tables from MLflow project. MLflow spark api only allows logging and loading of SparkML models. Any idea on how can I accomplish this?<\/p>\n<p>Currently I am trying to access spark via this code in MLflow project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nspark = pyspark.sql.SparkSession._instantiatedSession\nif spark is None:\n  # NB: If there is no existing Spark context, create a new local one.\n  # NB: We're disabling caching on the new context since we do not need it and we want to\n  # avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker\n  # (e.g. as part of spark_udf).\n  spark = ( pyspark.sql.SparkSession.builder \\\n   .config(&quot;spark.python.worker.reuse&quot;, True)\n   .config(&quot;spark.databricks.io.cache.enabled&quot;, False)\n   # In Spark 3.1 and above, we need to set this conf explicitly to enable creating\n   # a SparkSession on the workers\n   .config(&quot;spark.executor.allowSparkContext&quot;, &quot;true&quot;)\n   .master(&quot;local[*]&quot;)\n   .appName(&quot;MLflow Project&quot;)\n   .getOrCreate()\n  )\n<\/code><\/pre>\n<p>But I am getting this error:<\/p>\n<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-05 20:21:43 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"apache-spark|pyspark|databricks|delta-lake|mlflow",
        "Question_view_count":282,
        "Owner_creation_date":"2016-10-23 15:09:44.12 UTC",
        "Owner_last_access_date":"2022-09-23 01:20:21.6 UTC",
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-06 17:51:53.797 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Curl returning single output for multiple string inputs",
        "Question_body":"<p>I logged a sentiment-analysis model in <code>Mlflow<\/code> with the custom signature, everything is working fine but as soon as I serve the model and hit it with the curl command, then for my multiple inputs it's returning a single output, please help if someone can point to the issue<\/p>\n<p>Curl command i am using :<\/p>\n<pre><code>curl http:\/\/127.0.0.1:2000\/invocations -H 'Content-Type: application\/json' -d '{&quot;columns&quot;: [&quot;text&quot;],&quot;data&quot;: [[&quot;Its a Bad day&quot;],[&quot;what are you&quot;]]}'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[&quot;negative&quot;]\n<\/code><\/pre>\n<p>Expected Output:<\/p>\n<pre><code>[&quot;negative&quot;,&quot;neutral&quot;]\n<\/code><\/pre>\n<p>Here is the model signature :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have tried two different models, both of them are giving the same issue and if I am trying a model which takes integer values then it's working as expected.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-08-11 15:49:04.257 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|curl|mlflow",
        "Question_view_count":65,
        "Owner_creation_date":"2020-07-08 08:07:31.54 UTC",
        "Owner_last_access_date":"2022-09-23 07:54:56.387 UTC",
        "Owner_location":"India",
        "Owner_reputation":857,
        "Owner_up_votes":83,
        "Owner_down_votes":5,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-11 16:03:25.627 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"ColumnTransformer not fitted after sklearn Pipeline loaded from Mlflow",
        "Question_body":"<p>I am building a machine learning model using sklearn Pipeline which includes a ColumnTransformer as a preprocessor before the actual model. Below is the code how the pipeline is created.<\/p>\n<pre><code>transformers = []\nnum_pipe = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ntransformers.append(('numerical', num_pipe, num_cols))\ncat_pipe = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n])\ntransformers.append(('categorical', cat_pipe, cat_cols))\npreprocessor = ColumnTransformer(transformers, remainder='passthrough')\nmodel = Pipeline([\n  ('prep', preprocessor),\n  ('clf', XGBClassifier())\n])\n<\/code><\/pre>\n<p>I am using <code>Mlflow<\/code> to log the model artifact as sklearn model.<\/p>\n<pre><code>model.fit(X, y)\nmlflow.sklearn.log_model(model, model_uri)\n<\/code><\/pre>\n<p>When I tried to load the model from mlflow for scoring though, I got the error &quot;This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.&quot;<\/p>\n<pre><code>run_model = mlflow.sklearn.load_model(model_uri)\nrun_model.predict(X_pred)\n<\/code><\/pre>\n<p>Only standard sklearn imputers and scalers are being used. Nothing is custom built. Why does it still complain not fitted. Is column transformer not compatible with mlflow?<\/p>\n<p><strong>EDIT<\/strong>\nI ran <code>check_is_fitted<\/code> on the second step of the Pipeline which is the xgboost model itself after loaded from mlflow and it is NOT fitted either.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-02 19:46:45.073 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|mlflow",
        "Question_view_count":139,
        "Owner_creation_date":"2014-02-27 05:08:23.6 UTC",
        "Owner_last_access_date":"2022-09-22 19:28:57.043 UTC",
        "Owner_location":null,
        "Owner_reputation":4303,
        "Owner_up_votes":134,
        "Owner_down_votes":1,
        "Owner_views":536,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-02 20:13:17.5 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Cannot Start mlflow ui on google cloud platform virtual machine instance",
        "Question_body":"<p>after running mlflow ui on command line\nand  clicking <a href=\"http:\/\/127.0.0.1:5000\/\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000\/<\/a>\ni get site cannot be reached\n127.0.0.1 refused to connect.<\/p>\n<p>I have already updated firewall rules on VPC network in GCP and on my local machine and activated the ports<\/p>\n<blockquote>\n<p>This site can\u2019t be reached127.0.0.1 refused to connect.<\/p>\n<p>Try:<\/p>\n<ul>\n<li>Checking the connection<\/li>\n<li>Checking the proxy and the firewall<\/li>\n<\/ul>\n<p>ERR_CONNECTION_REFUSED<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-25 08:35:26.673 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":360,
        "Owner_creation_date":"2018-01-21 16:42:32.317 UTC",
        "Owner_last_access_date":"2021-03-20 09:36:52.253 UTC",
        "Owner_location":"Haryana, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Deploying MLflow Model without Conda environment",
        "Question_body":"<p>Currently working on deploying my MLflow Model in a Docker container. The Docker container is set up with all the necessary dependencies for the model so it seems redundant for MLflow to also then create\/activate a conda environment for the model. Looking at the documentation (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-models-serve\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-models-serve<\/a>) it says you can serve the model with the <code>--no-conda<\/code> flag and that MLflow will assume you are \u201crunning within a Conda environment with the necessary dependencies\u201d. This solution is working for us when we run in any environment with necessary dependencies, not necessarily a Conda environment. Is this correct? Or do we absolutely need to have a Conda environment active when running with the <code>--no-conda<\/code> flag?<\/p>\n\n<p>For example, I can create a virtualenv and, with the virtualenv active, serve the model locally using <code>mlflow models serve -m [model\/path] --no-conda<\/code>. The model then performs properly, but the documentation makes it sound like this shouldn\u2019t work because it explicitly calls for a Conda environment.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-10 21:47:10.877 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"python|docker|virtualenv|conda|mlflow",
        "Question_view_count":4468,
        "Owner_creation_date":"2019-02-28 19:31:25.003 UTC",
        "Owner_last_access_date":"2021-08-13 21:12:25.407 UTC",
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-16 12:59:50.477 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|powerbi|mlflow|azure-machine-learning-service",
        "Question_view_count":405,
        "Owner_creation_date":"2020-09-16 12:42:46.047 UTC",
        "Owner_last_access_date":"2021-03-12 15:06:56.23 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-20 12:28:40.243 UTC",
        "Answer_last_edit_date":"2020-09-23 10:12:37.377 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-09-23 10:11:20.503 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"ValueError: Enum ErrorCode has no value defined for name '403' in mlflow.set_experiment()",
        "Question_body":"<p>I am trying to run some code to train a model, while logging my results to MLflow on Databricks. I keep getting the following error when I try to make a call to <code>mlflow.set_experiment()<\/code>,<\/p>\n<pre><code>    raise ValueError('Enum {} has no value defined for name {!r}'.format(\nValueError: Enum ErrorCode has no value defined for name '403'\n<\/code><\/pre>\n<p>What exactly is going on here?<\/p>\n<p>I am using Databricks Connect to run my code and the section where the error pops up looks like this,<\/p>\n<pre><code>    # set remote tracking server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n\n    # create the MLflow client\n    client = MlflowClient(remote_server_uri)\n\n    # set experiment to log mlflow runs\n    mlflow.set_experiment(experiment_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 04:25:26.99 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mlflow|databricks-connect",
        "Question_view_count":68,
        "Owner_creation_date":"2018-03-24 01:53:05.82 UTC",
        "Owner_last_access_date":"2022-09-24 16:46:35.903 UTC",
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-08 08:21:53.963 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-11 17:17:32.94 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_date":"2018-10-10 22:41:41.843 UTC",
        "Owner_last_access_date":"2022-09-24 01:18:25.137 UTC",
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-18 18:03:05.177 UTC",
        "Answer_last_edit_date":"2021-06-20 15:16:15.903 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2020-03-17 00:07:46.973 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-03 08:15:46.63 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_date":"2016-07-08 02:05:15.393 UTC",
        "Owner_last_access_date":"2022-09-25 05:18:34.54 UTC",
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-06 04:08:45.28 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2020-07-06 04:10:26.393 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"What is a 'XGBoostLabelEncoder' object?",
        "Question_body":"<p>I'm trying to load a model from an mlflow run. When I do that I get an 'XGBoostLabelEncoder' object, an object with no attributes like predict or predict_proba. I don't really know what you can do with it.<\/p>\n<p>I've googled around but can't find any information about what an 'XGBoostLabelEncoder' object is.<\/p>\n<p>Anybody who knows?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-19 07:18:58.73 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|xgboost|mlflow",
        "Question_view_count":25,
        "Owner_creation_date":"2017-08-24 07:25:41.763 UTC",
        "Owner_last_access_date":"2022-09-25 05:29:59.79 UTC",
        "Owner_location":"Malm\u00f6, Sverige",
        "Owner_reputation":796,
        "Owner_up_votes":605,
        "Owner_down_votes":4,
        "Owner_views":136,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-19 07:41:41.817 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Track to database, artifacts to specific destination",
        "Question_body":"<p>I am running <code>mlflow ui<\/code> and PostgreSQL db in docker compose.<\/p>\n<p>Mlflow UI container runs like this: <code>mlflow ui --backend-store-uri &quot;postgresql+psycopg2:\/\/postgres:passw0rd@database:5432\/postgres&quot; --host 0.0.0.0<\/code><\/p>\n<p>Then I run my models locally from jupyter, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>remote_server_uri = &quot;postgresql+psycopg2:\/\/postgres:passw0rd@localhost:5432\/postgres&quot;\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(&quot;exp2&quot;)\n\nX = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\ny = np.array([0, 0, 1, 1, 1, 0])\nlr = LogisticRegression()\nlr.fit(X, y)\nscore = lr.score(X, y)\nprint(&quot;Score: %s&quot; % score)\nwith mlflow.start_run():\n    mlflow.log_metric(&quot;score&quot;, score)\n<\/code><\/pre>\n<p>Everything works fine - experiments get logged into PostgreSQL and mlflow UI can read it from PostgreSQL .<\/p>\n<p>One thing that bothers me is that artifacts are stored locally into .\/mlruns folder. How to change it to save it somewhere else?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-23 09:18:47.63 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|postgresql|mlflow",
        "Question_view_count":361,
        "Owner_creation_date":"2016-04-04 13:56:35.087 UTC",
        "Owner_last_access_date":"2022-09-24 15:49:58.61 UTC",
        "Owner_location":"Czech Republic",
        "Owner_reputation":622,
        "Owner_up_votes":296,
        "Owner_down_votes":11,
        "Owner_views":59,
        "Answer_body":"<p>So apparently <code>--default-artifact-root<\/code> argument has to be used when launching server\/ui. The only downside is that that default artifact root is relative to development environment, so if you are running mlflow server in docker and specify default-artifact-root to e.g. <code>some\/path<\/code> then the artifacts are going to be saved to your <strong>local machine<\/strong> to that path (<strong>not inside docker container<\/strong>). Probably the best solution is to use remote storage such as S3\/Blob.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-01-23 18:03:46.12 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-01-23 09:23:27.49 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-07 10:24:39.323 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|yaml|mlflow",
        "Question_view_count":418,
        "Owner_creation_date":"2020-03-06 10:50:11.22 UTC",
        "Owner_last_access_date":"2022-09-21 15:31:59.043 UTC",
        "Owner_location":"Baku, Azerbaijan",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-11 10:01:04.143 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Connect MLflow server to minio in local",
        "Question_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-09-17 18:17:06.293 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|minio|mlflow",
        "Question_view_count":1136,
        "Owner_creation_date":"2011-07-17 08:59:45.21 UTC",
        "Owner_last_access_date":"2022-09-24 02:56:29.02 UTC",
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Answer_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-20 15:29:11.773 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to provide Dockerfile for mlflow models build-docker",
        "Question_body":"<p>I have created a Model using pyfunc file with mlflow which usage a conda_env to install packages required for model.<\/p>\n<pre><code>pip_env = {\n    'pip': [\n        'pandas==0.24.1',\n        'python-dateutil==2.8.1',\n        'fuzzywuzzy==0.7.0'\n    ]\n}\nconda_env = {\n    'channels': ['defaults'],\n    'dependencies': [\n        'python=3.7.0',\n        'pip=20.2.3',\n        pip_env\n    ]\n}\nmlflow.pyfunc.save_model(path=model_path, python_model=gfeCleanPrediction(), artifacts=artifacts, conda_env=conda_env,code_path=code_path)\n<\/code><\/pre>\n<p>I need to use my own Dockerfile which will build some packages from source and install, Is there a way I can provide it while running below command :<\/p>\n<pre><code>mlflow  models build-docker -m MODEL_FOLDER_V-1-0-1 -n my_model --install-mlflow\n<\/code><\/pre>\n<p>I can see mlflow provide a custom_setup_steps_hook parameter in \/python3.7\/site-packages\/mlflow\/models\/docker_utils.py<\/p>\n<pre><code>def _build_image(image_name, entrypoint, mlflow_home=None, custom_setup_steps_hook=None):\n    &quot;&quot;&quot;\n    :param custom_setup_steps_hook: (Optional) Single-argument function that takes the string path\n           of a dockerfile context directory and returns a string containing Dockerfile commands to\n           run during the image build step.\n    &quot;&quot;&quot;\n    mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None\n    with TempDir() as tmp:\n        cwd = tmp.path()\n        install_mlflow = _get_mlflow_install_step(cwd, mlflow_home)\n        custom_setup_steps = custom_setup_steps_hook(cwd) if custom_setup_steps_hook else &quot;&quot;\n        with open(os.path.join(cwd, &quot;Dockerfile&quot;), &quot;w&quot;) as f:\n            f.write(\n                _DOCKERFILE_TEMPLATE.format(\n                    install_mlflow=install_mlflow,\n                    custom_setup_steps=custom_setup_steps,\n                    entrypoint=entrypoint,\n                )\n            )\n<\/code><\/pre>\n<p>How to use custom_setup_steps_hook OR use my own Dockerfile in <strong>mlflow  models build-docker<\/strong>??<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-21 16:21:45.177 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|dockerfile|mlflow",
        "Question_view_count":190,
        "Owner_creation_date":"2014-11-02 10:27:32.403 UTC",
        "Owner_last_access_date":"2022-07-07 11:05:50.31 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to add coefficients, p-values and relevant variable name in mlflow?",
        "Question_body":"<p>I am running a linear regression model and I would like to add the coefficients and P-values of each variable and the variable name in to the metrics of the mlflow output. I am new to using mlflow and not very familiar in doing this. Below is an example of part of my code<\/p>\n<pre><code>with mlflow.start_run(run_name=p_key + '_' + str(o_key)):\n    \n    lr = LinearRegression(\n      featuresCol = 'features',\n      labelCol = target_var,\n      maxIter = 10,\n      regParam = 0.0,\n      elasticNetParam = 0.0,\n      solver=&quot;normal&quot;\n        )\n    \n    lr_model_item = lr.fit(train_model_data)\n    lr_coefficients_item = lr_model_item.coefficients\n    lr_coefficients_intercept = lr_model_item.intercept\n    \n    lr_predictions_item = lr_model_item.transform(train_model_data)\n    lr_predictions_item_oos = lr_model_item.transform(test_model_data)\n    \n    rsquared = lr_model_item.summary.r2\n    \n    # Log mlflow attributes for mlflow UI\n    mlflow.log_metric(&quot;rsquared&quot;, rsquared)\n    mlflow.log_metric(&quot;intercept&quot;, lr_coefficients_intercept)\n    for i in lr_coefficients_item:\n      mlflow.log_metric('coefficients', lr_coefficients_item[i])\n<\/code><\/pre>\n<p>Would like to know whether this is possible? In the final output I should have the intercept, coefficients, p-values and the relevant variable name.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-21 01:46:29.493 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"databricks|mlflow",
        "Question_view_count":242,
        "Owner_creation_date":"2019-05-08 02:16:04.547 UTC",
        "Owner_last_access_date":"2021-05-19 04:34:38.877 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Can't run mlflow standalone react server",
        "Question_body":"<p>I need to run the frontend UI on a separate server as a stand-alone react app (not with all of the mlflow python server and packages). But when I run the production build, I get the following error:<\/p>\n\n<blockquote>\n  <p>You need to enable JavaScript to run this app.<\/p>\n<\/blockquote>\n\n<p>Reproduce:<\/p>\n\n<pre><code>git clone https:\/\/github.com\/mlflow\/mlflow.git\ncd mlflow\/mlflow\/server\/js\n<\/code><\/pre>\n\n<p>In development mode, \"npm start\" it works just fine:<\/p>\n\n<pre><code>npm install\nnpm start\n<\/code><\/pre>\n\n<p>But when I've run it in production, I am getting the above error.  <\/p>\n\n<pre><code>npm install\nnpm install -g serve\nnpm run build\nserve -s build\n<\/code><\/pre>\n\n<p>I tried many things searching in forums, including this one:\n<a href=\"https:\/\/stackoverflow.com\/questions\/50286927\/i-am-getting-error-in-console-you-need-to-enable-javascript-to-run-this-app-r\">I am getting error in console &quot;You need to enable JavaScript to run this app.&quot; reactjs<\/a>.\nbut I didn't manage to make it work.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-06 13:42:33.067 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"reactjs|mlflow",
        "Question_view_count":286,
        "Owner_creation_date":"2014-11-21 23:30:15.303 UTC",
        "Owner_last_access_date":"2019-07-28 11:08:18.773 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-05-07 17:02:15.987 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow models serve -m mlflow_model",
        "Question_body":"<p>I have a problem with serve MLflow. When I execute this code :<\/p>\n<pre><code>mlflow models serve -m mlflow_model\/\n<\/code><\/pre>\n<p>I have this error:<\/p>\n<pre><code>2022\/07\/24 14:11:15 INFO mlflow.pyfunc.backend: === Running command 'source \/opt\/anaconda3\/bin\/..\/etc\/profile.d\/conda.sh &amp;&amp; conda activate mlflow-a116dac3ec81be8ec538fa60f4402b7d813c2192 1&gt;&amp;2 &amp;&amp; exec gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n[2022-07-24 14:11:16 +0200] [7534] [INFO] Starting gunicorn 20.1.0\n[2022-07-24 14:11:16 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)\n[2022-07-24 14:11:16 +0200] [7534] [ERROR] Retrying in 1 second.\n[2022-07-24 14:11:17 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)\n[2022-07-24 14:11:17 +0200] [7534] [ERROR] Retrying in 1 second.\n[2022-07-24 14:11:18 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)\n[2022-07-24 14:11:18 +0200] [7534] [ERROR] Retrying in 1 second.\n[2022-07-24 14:11:19 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)\n[2022-07-24 14:11:19 +0200] [7534] [ERROR] Retrying in 1 second.\n[2022-07-24 14:11:20 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)\n[2022-07-24 14:11:20 +0200] [7534] [ERROR] Retrying in 1 second.\n[2022-07-24 14:11:21 +0200] [7534] [ERROR] Can't connect to ('127.0.0.1', 5000)\nTraceback (most recent call last):\n File &quot;\/opt\/anaconda3\/bin\/mlflow&quot;, line 8, in &lt;module&gt;\n  sys.exit(cli())\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1128, in __call__\n  return self.main(*args, **kwargs)\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1053, in main\n  rv = self.invoke(ctx)\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1659, in invoke\n  return _process_result(sub_ctx.command.invoke(sub_ctx))\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1659, in invoke\n  return _process_result(sub_ctx.command.invoke(sub_ctx))\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1395, in invoke\n  return ctx.invoke(self.callback, **ctx.params)\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 754, in invoke\n  return __callback(*args, **kwargs)\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/mlflow\/models\/cli.py&quot;, line 68, in serve\n  return _get_flavor_backend(\n File &quot;\/opt\/anaconda3\/lib\/python3.9\/site-packages\/mlflow\/pyfunc\/backend.py&quot;, line 261, in serve\n  raise Exception(\nException: Command 'exec gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n<\/code><\/pre>\n<p>conda.yaml\nchannels:<\/p>\n<ul>\n<li>defaults<\/li>\n<li>anaconda\ndependencies:<\/li>\n<li>python=3.9.12<\/li>\n<li>pip&lt;=22.1.2<\/li>\n<li>pip:\n<ul>\n<li>mlflow<\/li>\n<li>cloudpickle==2.0.0<\/li>\n<li>scikit-learn==1.0.2\nname: mlflow-env<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>I have changed the port to 8502 but in vain. I have this message:<\/p>\n<pre><code>[2022-07-26 02:28:06 +0200] [45925] [INFO] Listening at: http:\/\/127.0.0.1:8502 (45925)\n<\/code><\/pre>\n<p>[2022-07-26 02:28:06 +0200] [45925] [INFO] Using worker: sync\n[2022-07-26 02:28:06 +0200] [45933] [INFO] Booting worker with pid: 45933\n\/opt\/anaconda3\/envs\/mlflow-278b1bb076f93c0a5f2665638173d22fba5482d5\/lib\/python3.9\/site-packages\/lightgbm\/<strong>init<\/strong>.py:40: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\nThis means that in case of installing LightGBM from PyPI via the <code>pip install lightgbm<\/code> command, you don't need to install the gcc compiler anymore.\nInstead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\nYou can install the OpenMP library by the following command: <code>brew install libomp<\/code>.\nwarnings.warn(&quot;Starting from version 2.2.1, the library file in distribution wheels for macOS &quot;<\/p>\n<p>I show you too my ports already used:<\/p>\n<p>COMMAND     PID             USER   FD   TYPE            DEVICE SIZE\/OFF NODE NAME\nloginwind   170 mohamedads    8u  IPv4 0xd7d0f4b08a76d93      0t0  UDP <em>:<\/em>\nControlCe   435 mohamedads   20u  IPv4 0xd7d0f4b08a6a8d3      0t0  UDP <em>:<\/em>\nrapportd    480 mohamedads    3u  IPv4 0xd7d0f4b08a76773      0t0  UDP <em>:<\/em>\nrapportd    480 mohamedads    4u  IPv4 0xd7d0f4b08a76a83      0t0  UDP <em>:<\/em>\nrapportd    480 mohamedads    7u  IPv4 0xd7d0f4b08a78923      0t0  UDP <em>:<\/em>\nidentitys   485 mohamedads   26u  IPv4 0xd7d0f4b08a629d3      0t0  UDP <em>:<\/em>\nidentitys   485 mohamedads   28u  IPv4 0xd7d0f4b08a49c33      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads    4u  IPv4 0xd7d0f4b08a42a83      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads    8u  IPv4 0xd7d0f4b08a42463      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads    9u  IPv4 0xd7d0f4b08a42153      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads   10u  IPv4 0xd7d0f4b08a436c3      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads   14u  IPv4 0xd7d0f4b08a76153      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads   33u  IPv4 0xd7d0f4b08a6ca83      0t0  UDP <em>:<\/em>\nsharingd    493 mohamedads   34u  IPv4 0xd7d0f4b08a6d0a3      0t0  UDP <em>:<\/em>\nWiFiAgent   511 mohamedads    5u  IPv4 0xd7d0f4b08a6f8d3      0t0  UDP <em>:<\/em>\nDropbox     548 mohamedads   71u  IPv4 0xd7d0f4fd5823123      0t0  TCP 192.168.0.14:49185-&gt;162.125.6.20:https (ESTABLISHED)\nDropbox     548 mohamedads  107u  IPv6 0xd7d0f596050d7c3      0t0  TCP *:17500 (LISTEN)\nDropbox     548 mohamedads  108u  IPv4 0xd7d0f4fd4f8d67b      0t0  TCP *:17500 (LISTEN)\nDropbox     548 mohamedads  109u  IPv4 0xd7d0f4b08a75b33      0t0  UDP *:17500\nDropbox     548 mohamedads  147u  IPv4 0xd7d0f4fd4f8abcb      0t0  TCP localhost:17603 (LISTEN)\nDropbox     548 mohamedads  148u  IPv4 0xd7d0f4fd4f8ebcb      0t0  TCP localhost:17600 (LISTEN)\nDropbox     548 mohamedads  155u  IPv4 0xd7d0f4fd5825bcb      0t0  TCP 192.168.0.14:65500-&gt;162.125.19.9:https (ESTABLISHED)\nDropbox     548 mohamedads  167u  IPv4 0xd7d0f4fd582467b      0t0  TCP 192.168.0.14:65483-&gt;162.125.19.131:https (ESTABLISHED)\nassistant   564 mohamedads   45u  IPv4 0xd7d0f4b08a74ef3      0t0  UDP <em>:<\/em>\ncom.apple   793 mohamedads   43u  IPv6 0xd7d0f596050bbc3      0t0  TCP localhost:65493-&gt;localhost:8501 (ESTABLISHED)\nNotes      1226 mohamedads   13u  IPv4 0xd7d0f4fd582267b      0t0  TCP 192.168.0.14:65232-&gt;imap.1and1.fr:imaps (ESTABLISHED)\nNotes      1226 mohamedads   14u  IPv4 0xd7d0f4fd582267b      0t0  TCP 192.168.0.14:65232-&gt;imap.1and1.fr:imaps (ESTABLISHED)\nNotes      1226 mohamedads   37u  IPv6 0xd7d0f59605052c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65084-&gt;wo-in-x6d.1e100.net:imaps (ESTABLISHED)\nNotes      1226 mohamedads   38u  IPv6 0xd7d0f59605052c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65084-&gt;wo-in-x6d.1e100.net:imaps (ESTABLISHED)\npython3.9  1314 mohamedads    5u  IPv4 0xd7d0f4fd5808bcb      0t0  TCP localhost:commplex-main (LISTEN)\nGoogle    14186 mohamedads   20u  IPv4 0xd7d0f4fd5b41bcb      0t0  TCP 192.168.0.14:65486-&gt;192.168.0.17:8009 (ESTABLISHED)\nGoogle    14186 mohamedads   24u  IPv4 0xd7d0f4fd582abcb      0t0  TCP localhost:65495-&gt;localhost:commplex-link (ESTABLISHED)\nGoogle    14186 mohamedads   27u  IPv6 0xd7d0f59605060c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49156-&gt;[2606:4700::6812:1a91]:https (ESTABLISHED)\nGoogle    14186 mohamedads   30u  IPv4 0xd7d0f4fd4f8967b      0t0  TCP 192.168.0.14:49159-&gt;249.195.120.34.bc.googleusercontent.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   31u  IPv6 0xd7d0f4b08a78c33      0t0  UDP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:55745-&gt;par21s22-in-x0a.1e100.net:https\nGoogle    14186 mohamedads   33u  IPv6 0xd7d0f5960507cc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49158-&gt;par10s34-in-x0e.1e100.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   34u  IPv4 0xd7d0f4fd4f8e123      0t0  TCP 192.168.0.14:49155-&gt;249.195.120.34.bc.googleusercontent.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   35u  IPv6 0xd7d0f59605098c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65533-&gt;[2606:4700:4400::ac40:929e]:https (ESTABLISHED)\nGoogle    14186 mohamedads   37u  IPv6 0xd7d0f596050adc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49154-&gt;par21s05-in-x01.1e100.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   38u  IPv4 0xd7d0f4fd581e67b      0t0  TCP 192.168.0.14:49182-&gt;server-52-84-174-19.cdg50.r.cloudfront.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   39u  IPv6 0xd7d0f5960506ec3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65507-&gt;[2606:4700:20::681a:cf5]:https (ESTABLISHED)\nGoogle    14186 mohamedads   40u  IPv4 0xd7d0f4fd583667b      0t0  TCP 192.168.0.14:65506-&gt;ec2-52-202-168-65.compute-1.amazonaws.com:https (CLOSED)\nGoogle    14186 mohamedads   41u  IPv6 0xd7d0f596050b4c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49161-&gt;[2600:9000:218d:a000:b:67f0:7600:93a1]:https (ESTABLISHED)\nGoogle    14186 mohamedads   42u  IPv6 0xd7d0f5960508ac3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49160-&gt;[2a04:4e42:1d::282]:https (ESTABLISHED)\nGoogle    14186 mohamedads   44u  IPv4 0xd7d0f4fd580c123      0t0  TCP 192.168.0.14:65509-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   45u  IPv4 0xd7d0f4fd580b67b      0t0  TCP 192.168.0.14:65508-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   48u  IPv6 0xd7d0f596050e5c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65348-&gt;wo-in-f188.1e100.net:5228 (ESTABLISHED)\nGoogle    14186 mohamedads   51u  IPv4 0xd7d0f4fd581dbcb      0t0  TCP 192.168.0.14:49183-&gt;server-52-84-174-118.cdg50.r.cloudfront.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   53u  IPv4 0xd7d0f4fd5825123      0t0  TCP 192.168.0.14:65510-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   54u  IPv4 0xd7d0f4fd5b3cbcb      0t0  TCP 192.168.0.14:49173-&gt;par21s19-in-f2.1e100.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   55u  IPv4 0xd7d0f4fd5b41123      0t0  TCP 192.168.0.14:65511-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   58u  IPv6 0xd7d0f596050ecc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49163-&gt;[2a04:4e42:1d::720]:https (ESTABLISHED)\nGoogle    14186 mohamedads   59u  IPv6 0xd7d0f59605059c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65527-&gt;[2606:4700:4400::ac40:929e]:https (ESTABLISHED)\nGoogle    14186 mohamedads   62u  IPv6 0xd7d0f5960511dc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49181-&gt;[2001:4860:4802:36::36]:https (ESTABLISHED)\nGoogle    14186 mohamedads   63u  IPv4 0xd7d0f4fd583267b      0t0  TCP 192.168.0.14:49166-&gt;a104-124-109-108.deploy.static.akamaitechnologies.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   65u  IPv6 0xd7d0f59605083c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65522-&gt;[2606:4700::6810:9440]:https (ESTABLISHED)\nGoogle    14186 mohamedads   66u  IPv6 0xd7d0f59605091c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65525-&gt;[2606:4700::6810:9440]:https (ESTABLISHED)\nGoogle    14186 mohamedads   68u  IPv4 0xd7d0f4fd582a123      0t0  TCP 192.168.0.14:65523-&gt;a23-220-25-199.deploy.static.akamaitechnologies.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   71u  IPv4 0xd7d0f4fd5833bcb      0t0  TCP 192.168.0.14:49169-&gt;server-99-86-91-75.cdg50.r.cloudfront.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   72u  IPv6 0xd7d0f59605108c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49172-&gt;[2600:9000:218c:4200:1f:f723:6fc0:93a1]:https (ESTABLISHED)\nGoogle    14186 mohamedads   73u  IPv4 0xd7d0f4fd5808123      0t0  TCP 192.168.0.14:65530-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   74u  IPv4 0xd7d0f4fd580767b      0t0  TCP 192.168.0.14:65531-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)\nGoogle    14186 mohamedads   75u  IPv4 0xd7d0f4fd5837bcb      0t0  TCP 192.168.0.14:49170-&gt;80.142.244.35.bc.googleusercontent.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   76u  IPv6 0xd7d0f59605067c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49167-&gt;[2a04:4e42:54::396]:https (ESTABLISHED)\nGoogle    14186 mohamedads   77u  IPv4 0xd7d0f4fd5833123      0t0  TCP 192.168.0.14:49171-&gt;72.249.186.35.bc.googleusercontent.com:https (ESTABLISHED)\nGoogle    14186 mohamedads   82u  IPv4 0xd7d0f4fd5b3b67b      0t0  TCP 192.168.0.14:49177-&gt;chi.outbrain.com:https (CLOSE_WAIT)\nGoogle    14186 mohamedads   84u  IPv4 0xd7d0f4fd5831bcb      0t0  TCP 192.168.0.14:49178-&gt;chi.outbrain.com:https (CLOSE_WAIT)\nGoogle    14186 mohamedads   85u  IPv4 0xd7d0f4fd5831123      0t0  TCP 192.168.0.14:49179-&gt;151.101.9.140:https (ESTABLISHED)\nGoogle    14186 mohamedads   87u  IPv6 0xd7d0f5960512bc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49174-&gt;[2a04:4e42:1d::720]:https (ESTABLISHED)\nGoogle    14186 mohamedads   93u  IPv4 0xd7d0f4fd581fbcb      0t0  TCP 192.168.0.14:49180-&gt;server-13-32-145-15.cdg50.r.cloudfront.net:https (ESTABLISHED)\npycharm   37549 mohamedads   10u  IPv6 0xd7d0f596050f3c3      0t0  TCP localhost:6942 (LISTEN)\npycharm   37549 mohamedads   46u  IPv6 0xd7d0f5960510fc3      0t0  TCP localhost:63342 (LISTEN)\npython3.9 39875 mohamedads   19u  IPv4 0xd7d0f4fd5823bcb      0t0  TCP *:8501 (LISTEN)\npython3.9 39875 mohamedads   20u  IPv6 0xd7d0f59605116c3      0t0  TCP *:8501 (LISTEN)\npython3.9 39875 mohamedads   29u  IPv6 0xd7d0f596050c2c3      0t0  TCP localhost:8501-&gt;localhost:65493 (ESTABLISHED)\npython3.9 43312 mohamedads    5u  IPv4 0xd7d0f4fd581c67b      0t0  TCP localhost:commplex-link (LISTEN)\npython3.9 44086 mohamedads   10u  IPv4 0xd7d0f4fd581f123      0t0  TCP localhost:ddi-tcp-2 (LISTEN)\npython3.9 44086 mohamedads   11u  IPv6 0xd7d0f59605101c3      0t0  TCP localhost:ddi-tcp-2 (LISTEN)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-24 12:22:45.687 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":85,
        "Owner_creation_date":"2022-07-24 12:15:55.25 UTC",
        "Owner_last_access_date":"2022-07-26 02:17:46.43 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-26 00:36:11.53 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Model is missing inputs ['_automl_sample_weight_ac91']",
        "Question_body":"<p>I am running Azure Databricks AutoML using 12.2 cluster that claims handling the imbalanced dataset. Although, it is not in my input, this cluster adds extra columns as _automl_sample_weight_ac91, _automl_split_col_636d. It drops_automl_split_col_636d column before training, but keeps the _automl_sample_weight_ac91 column. After registering the model, I am trying to predict very separate dataset than the one I used on training...\nApparently, registered automl model adds _automl_sample_weight_ac91 as column, but my very separate test data frame  has not sample weight column, thus gives the following error during prediction:<\/p>\n<p>PythonException: 'mlflow.exceptions. MlflowException: Model is missing inputs ['_automl_sample_weight_ac91'].'. Full traceback below:<\/p>\n<p>How can I solve this error related to _automl_sample_weight_ac91)?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CJm9Q.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-12 16:17:00.61 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow|automl",
        "Question_view_count":14,
        "Owner_creation_date":"2020-10-19 14:59:15.243 UTC",
        "Owner_last_access_date":"2022-09-12 20:18:40.11 UTC",
        "Owner_location":"Charlotte, NC, USA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-12 16:19:30.557 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Run mlflow project on multiple remote servers?",
        "Question_body":"<p>Can <code>MLflow<\/code> be used to dispatch <strong>projects<\/strong> to multiple remote servers?(not aws,azure etc.) from a local tracking server?<br>\nI have the following scenario-<Br>\nMultiple servers, where I would like to dispatch the <code>mlflow<\/code> project to all with different parameters, and let them \"report\" back to the current <strong>tracking server:<\/strong><\/p>\n\n<pre><code>for ip in servers_ips:\n    start_remote_mlflow(entry_point=GITHUBPATH,tracking_server=this_server_ip,hparams)\n<\/code><\/pre>\n\n<p>I see one can dispatch <code>mlflow<\/code> projects to aws or azure by specifying the ip or the remote machine. Can it be done with desktops as well?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-23 16:24:20.187 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|remote-server|mlflow",
        "Question_view_count":221,
        "Owner_creation_date":"2016-10-18 05:35:53.033 UTC",
        "Owner_last_access_date":"2022-09-25 05:02:23.13 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":2057,
        "Owner_up_votes":201,
        "Owner_down_votes":2,
        "Owner_views":269,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow load model fails Python",
        "Question_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 14:21:39.67 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|docker|databricks|mlflow",
        "Question_view_count":109,
        "Owner_creation_date":"2015-04-10 08:31:54.763 UTC",
        "Owner_last_access_date":"2022-09-24 09:37:37.383 UTC",
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Answer_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-14 07:29:05.993 UTC",
        "Answer_last_edit_date":"2022-06-14 10:34:01.06 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-06-13 15:37:50.387 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"ML Model pod keeps restarting in Seldon deployment",
        "Question_body":"<p>I have a Seldon deployment like this:<\/p>\n<pre><code>apiVersion: machinelearning.seldon.io\/v1alpha2\nkind: SeldonDeployment\nmetadata:\n  name: mlflow\nspec:\n  name: wines\n  predictors:\n    - graph:\n        children: []\n        implementation: MLFLOW_SERVER\n        modelUri: gs:\/\/seldon-models\/mlflow\/elasticnet_wine\n        name: classifier\n      name: default\n      replicas: 1     \n<\/code><\/pre>\n<p>Model is downloaded successfully from the server, but, after a while, pods go to state <code>crashloop<\/code> and restart again and again.<\/p>\n<p>When I see the logs, there is no errors since logs have re-started and I can only see how python packages are being downloaded.<\/p>\n<pre><code>PS C:\\Users\\xxx\\mlflow&gt; kubectl logs -p -c wines-classifier model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\n<\/code><\/pre>\n<pre><code>Executing before-run script\n---&gt; Creating environment with Conda...\nINFO:root:Copying contents of \/mnt\/models to local\nINFO:root:Reading MLmodel file\nINFO:root:Creating Conda environment 'mlflow' from conda.yaml\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\nDownloading and Extracting Packages\n_libgcc_mutex-0.1    | 3 KB      | ########## | 100%\nreadline-7.0         | 324 KB    | ########## | 100%\nncurses-6.2          | 817 KB    | ########## | 100%\ntbb4py-2020.0        | 209 KB    | ########## | 100%\nscipy-1.1.0          | 13.2 MB   | ########## | 100%\nzlib-1.2.11          | 103 KB    | ########## | 100%\nxz-5.2.5             | 341 KB    | ########## | 100%\nopenssl-1.1.1g       | 2.5 MB    | ########## | 100%\nmkl_fft-1.0.6        | 135 KB    | ########## | 100%\nblas-1.0             | 6 KB      | ########## | 100%\npip-20.1.1           | 1.8 MB    | ########## | 100%\nwheel-0.34.2         | 51 KB     | ########## | 100%\nlibffi-3.2.1         | 40 KB     | ########## | 100%\nscikit-learn-0.19.1  | 3.9 MB    | ########## | 100%\nlibgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%\nsqlite-3.32.3        | 1.1 MB    | ########## | 100%\nnumpy-1.15.4         | 34 KB     | ########## | 100%\ntk-8.6.10            | 3.0 MB    | ########## | 100%\nlibgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%\nsetuptools-47.3.1    | 514 KB    | ########## | 100%\nmkl_random-1.0.1     | 324 KB    | ########## | 100%\npython-3.6.9         | 30.2 MB   | ########## | 100%\ncertifi-2020.6.20    | 156 KB    | ########## | 100%\nnumpy-base-1.15.4    | 3.4 MB    | ########## | 100%\nintel-openmp-2019.4  | 729 KB    | ########## | 100%\nlibedit-3.1.20191231 | 167 KB    | ########## | 100%\nlibstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%\ntbb-2020.0           | 1.1 MB    | ########## | 100%\nmkl-2018.0.3         | 126.9 MB  | #########  |  91%\n<\/code><\/pre>\n<p>Now, trying with <code>-p<\/code> parameter as proposed by @arghya-sadhu:<\/p>\n<pre><code>PS C:\\Users\\xxx\\mlflow&gt; kubectl logs -p model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp wines-classifier\n<\/code><\/pre>\n<pre><code>---&gt; Creating environment with Conda...\nINFO:root:Copying contents of \/mnt\/models to local\nINFO:root:Reading MLmodel file\nINFO:root:Creating Conda environment 'mlflow' from conda.yaml\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\nDownloading and Extracting Packages\nscikit-learn-0.19.1  | 3.9 MB    | ########## | 100%\nncurses-6.2          | 817 KB    | ########## | 100%\n_libgcc_mutex-0.1    | 3 KB      | ########## | 100%\nzlib-1.2.11          | 103 KB    | ########## | 100%\ntbb4py-2020.0        | 209 KB    | ########## | 100%\nsetuptools-47.3.1    | 514 KB    | ########## | 100%\nlibedit-3.1.20191231 | 167 KB    | ########## | 100%\ntbb-2020.0           | 1.1 MB    | ########## | 100%\nxz-5.2.5             | 341 KB    | ########## | 100%\nmkl_random-1.0.1     | 324 KB    | ########## | 100%\nlibgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%\npython-3.6.9         | 30.2 MB   | ########## | 100%\nlibgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%\nlibffi-3.2.1         | 40 KB     | ########## | 100%\nmkl-2018.0.3         | 126.9 MB  | ########## | 100%\nlibstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%\nreadline-7.0         | 324 KB    | ########## | 100%\nintel-openmp-2019.4  | 729 KB    | ########## | 100%\ntk-8.6.10            | 3.0 MB    | ########## | 100%\npip-20.1.1           | 1.8 MB    | ########## | 100%\nnumpy-base-1.15.4    | 3.4 MB    | ########## | 100%\nwheel-0.34.2         | 51 KB     | ########## | 100%\nscipy-1.1.0          | 13.2 MB   | #########3 |  93%\n<\/code><\/pre>\n<p>And the description of the pod:<\/p>\n<pre><code>PS C:\\Users\\ivarea\\repo\\smartgraph\\mlflow-v2&gt; kubectl describe pod model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\n<\/code><\/pre>\n<pre><code>Name:         model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\nNamespace:    default\nPriority:     0\nNode:         mlops-control-plane\/172.19.0.2\nStart Time:   Thu, 25 Jun 2020 10:08:20 +0200\nLabels:       app=model-a-wines-classifier-0-wines-classifier\n              fluentd=true\n              pod-template-hash=5b8bc7889d\n              seldon-app=model-a-wines-classifier\n              seldon-app-svc=model-a-wines-classifier-wines-classifier\n              seldon-deployment-id=model-a\n              version=wines-classifier\nAnnotations:  prometheus.io\/path: \/prometheus\n              prometheus.io\/scrape: true\nStatus:       Running\nIP:           10.244.0.17\nIPs:\n  IP:           10.244.0.17\nControlled By:  ReplicaSet\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d\nInit Containers:\n  wines-classifier-model-initializer:\n    Container ID:  containerd:\/\/6a3b158cf4218f8c177f6d18eb5d0387946bf9cc36f1173754b68a029483da8b\n    Image:         gcr.io\/kfserving\/storage-initializer:0.2.2\n    Image ID:      gcr.io\/kfserving\/storage-initializer@sha256:7a7d3cf4c5121a3e6bad0acc9e88bbdfa9c7f774d80bd64d8e35a84dcfef8890\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Args:\n      gs:\/\/seldon-models\/mlflow\/model-a\n      \/mnt\/models\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Thu, 25 Jun 2020 10:08:24 +0200\n      Finished:     Thu, 25 Jun 2020 10:08:47 +0200\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     1\n      memory:  1Gi\n    Requests:\n      cpu:        100m\n      memory:     100Mi\n    Environment:  &lt;none&gt;\n    Mounts:\n      \/mnt\/models from wines-classifier-provision-location (rw)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\nContainers:\n  wines-classifier:\n    Container ID:   containerd:\/\/536753d25877994a17d1f1a63bbaf8717dc9180b80f061152688e4c8504c8468\n    Image:          seldonio\/mlflowserver_rest:0.5\n    Image ID:       docker.io\/seldonio\/mlflowserver_rest@sha256:0fd54a0a314fafc82c490c91df0c4776be454702a307b4b76e12ed6958b4ee00\n    Ports:          6000\/TCP, 9000\/TCP\n    Host Ports:     0\/TCP, 0\/TCP\n    State:          Running\n      Started:      Thu, 25 Jun 2020 10:23:28 +0200\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Thu, 25 Jun 2020 10:19:09 +0200\n      Finished:     Thu, 25 Jun 2020 10:20:41 +0200\n    Ready:          False\n    Restart Count:  7\n    Liveness:       tcp-socket :http delay=60s timeout=1s period=5s #success=1 #failure=3\n    Readiness:      tcp-socket :http delay=20s timeout=1s period=5s #success=1 #failure=3\n    Environment:\n      PREDICTIVE_UNIT_SERVICE_PORT:          9000\n      PREDICTIVE_UNIT_ID:                    wines-classifier\n      PREDICTIVE_UNIT_IMAGE:                 seldonio\/mlflowserver_rest:0.5\n      PREDICTOR_ID:                          wines-classifier\n      PREDICTOR_LABELS:                      {&quot;version&quot;:&quot;wines-classifier&quot;}\n      SELDON_DEPLOYMENT_ID:                  model-a\n      PREDICTIVE_UNIT_METRICS_SERVICE_PORT:  6000\n      PREDICTIVE_UNIT_METRICS_ENDPOINT:      \/prometheus\n      PREDICTIVE_UNIT_PARAMETERS:            [{&quot;name&quot;:&quot;model_uri&quot;,&quot;value&quot;:&quot;\/mnt\/models&quot;,&quot;type&quot;:&quot;STRING&quot;}]\n    Mounts:\n      \/etc\/podinfo from podinfo (rw)\n      \/mnt\/models from wines-classifier-provision-location (ro)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\n  seldon-container-engine:\n    Container ID:  containerd:\/\/938e8f7e3ac23355c8a7a475b71ab54b858aff5ca485f26b99feaba09bb60069\n    Image:         docker.io\/seldonio\/seldon-core-executor:1.1.0\n    Image ID:      docker.io\/seldonio\/seldon-core-executor@sha256:661173fcbc6cb4e9b56db353b19e97d04d9c086e9dc445217f84dc1721bdf894\n    Ports:         8000\/TCP, 8000\/TCP, 5001\/TCP\n    Host Ports:    0\/TCP, 0\/TCP, 0\/TCP\n    Args:\n      --sdep\n      model-a\n      --namespace\n      default\n      --predictor\n      wines-classifier\n      --http_port\n      8000\n      --grpc_port\n      5001\n      --transport\n      rest\n      --protocol\n      seldon\n      --prometheus_path\n      \/prometheus\n    State:          Running\n      Started:      Thu, 25 Jun 2020 10:08:51 +0200\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:      100m\n    Liveness:   http-get http:\/\/:8000\/live delay=20s timeout=60s period=5s #success=1 #failure=3\n    Readiness:  http-get http:\/\/:8000\/ready delay=20s timeout=60s period=5s #success=1 #failure=3\n    Environment:\n      ENGINE_PREDICTOR:  &lt;binary ommited&gt;\n      REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX:  http:\/\/default-broker.\n      SELDON_LOG_MESSAGES_EXTERNALLY:          false\n    Mounts:\n      \/etc\/podinfo from podinfo (rw)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             False\n  ContainersReady   False\n  PodScheduled      True\nVolumes:\n  podinfo:\n    Type:  DownwardAPI (a volume populated by information about the pod)\n    Items:\n      metadata.annotations -&gt; annotations\n  wines-classifier-provision-location:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n    SizeLimit:  &lt;unset&gt;\n  default-token-6vqwk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6vqwk\n    Optional:    false\nQoS Class:       Burstable\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age                  From                          Message\n  ----     ------     ----                 ----                          -------\n  Normal   Scheduled  &lt;unknown&gt;            default-scheduler             Successfully assigned default\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp to mlops-control-plane\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;gcr.io\/kfserving\/storage-initializer:0.2.2&quot; already present on machine\n  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier-model-initializer\n  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier-model-initializer\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;seldonio\/mlflowserver_rest:0.5&quot; already present on machine\n  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier\n  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;docker.io\/seldonio\/seldon-core-executor:1.1.0&quot; already present on machine\n  Normal   Created    14m                  kubelet, mlops-control-plane  Created container seldon-container-engine\n  Normal   Started    14m                  kubelet, mlops-control-plane  Started container seldon-container-engine\n  Warning  Unhealthy  14m (x8 over 14m)    kubelet, mlops-control-plane  Readiness probe failed: dial tcp 10.244.0.17:9000: connect: connection refused\n  Warning  Unhealthy  28s (x171 over 14m)  kubelet, mlops-control-plane  Readiness probe failed: HTTP probe failed with statuscode: 503\n<\/code><\/pre>\n<p>How can I disable restarting so I can inspect logs to see the actual error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-25 07:09:40.727 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"kubernetes|mlflow|seldon",
        "Question_view_count":701,
        "Owner_creation_date":"2013-02-28 09:37:55.997 UTC",
        "Owner_last_access_date":"2022-01-03 12:35:46.23 UTC",
        "Owner_location":"Spain",
        "Owner_reputation":2570,
        "Owner_up_votes":183,
        "Owner_down_votes":2,
        "Owner_views":106,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-06-25 08:40:53.297 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"COPY files - next to Dockerfile - don't work and block docker build",
        "Question_body":"<p>I'm trying to building a docker container with mlflow server inside, with poetry toml file for dependency.(the two toml are exactly the same, it was just a way to try  to figure out)<br>\ntree:<\/p>\n\n<p>\u251c\u2500\u2500 docker-entrypoint.sh <br>\n\u251c\u2500\u2500 Dockerfile<br>\n\u251c\u2500\u2500 files<br>\n\u2502   \u2514\u2500\u2500 pyproject.toml<br>\n\u251c\u2500\u2500 git.sh<br>\n\u251c\u2500\u2500 pyproject.toml<br>\n\u2514\u2500\u2500 README.md<br><\/p>\n\n<p>as you can see, my toml file is next to Dockerfile <code>COPY pyproject.toml .\/<\/code> don't work nevertheless<\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>FROM python:3.6.10-alpine3.10 as base\nLABEL maintainer=\"\"\n\nENV PYTHONFAULTHANDLER 1 \nENV    PYTHONHASHSEED random \nENV    PYTHONUNBUFFERED 1\n\nENV MLFLOW_HOME .\/ \nENV SERVER_PORT 5000   \nENV    MLFLOW_VERSION 0.7.0 \nENV    SERVER_HOST 0.0.0.0  \nENV    FILE_STORE ${MLFLOW_HOME}\/fileStore  \nENV    ARTIFACT_STORE ${MLFLOW_HOME}\/artifactStore \nENV PIP_DEFAULT_TIMEOUT 100\nENV    PIP_DISABLE_PIP_VERSION_CHECK on\nENV    PIP_NO_CACHE_DIR  off \nENV    POETRY_VERSION  1.0.0 \n\nWORKDIR ${MLFLOW_HOME}\n\nFROM base as builder\n\nRUN apk update  \\\n    &amp;&amp; apk add --no-cache make gcc musl-dev python3-dev libffi-dev openssl-dev subversion\n#download project file from github  repo \nRUN    svn export https:\/\/github.com\/MChrys\/QuickSign\/trunk\/  \\\n    &amp;&amp; pip install poetry==${POETRY_VERSION} \\\n    &amp;&amp; mkdir -p ${FILE_STORE}  \\\n    &amp;&amp; mkdir -p ${ARTIFACT_STORE}\\\n    &amp;&amp; python -m venv \/venv\n\nCOPY  pyproject.toml .\/\nRUN poetry export -f requirements.txt | \/venv\/bin\/pip install -r  --allow-root-install \/dev\/stdin \n\nCOPY . .\nRUN poetry build &amp;&amp; \/venv\/bin\/pip install dist\/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder \/venv \/venv\nCOPY docker-entrypoint.sh .\/\n\nEXPOSE $SERVER_PORT\n\nVOLUME [\"${FILE_STORE}\", \"${ARTIFACT_STORE}\"]\n\nCMD [\".\/docker-entrypoint.sh\"]\n<\/code><\/pre>\n\n<p>the build command :<\/p>\n\n<pre><code>docker build - &lt; Dockerfile\n<\/code><\/pre>\n\n<p>I get this error  :<\/p>\n\n<pre><code>Step 21\/32 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder335195979\/pyproject.toml: no such file or   directory\n<\/code><\/pre>\n\n<p><strong>pyproject.toml<\/strong><\/p>\n\n<pre><code>requires = [\"poetry&gt;=1.0.0\", \"mlflow&gt;=0.7.0\", \"python&gt;=3.6\"]\nbuild-backend = \"poetry.masonry.api\"\n\n[tool.poetry]\nname = \"Sign\"\ndescription = \"\"\nversion = \"1.0.0\"\nreadme = \"README.md\"\nauthors = [\n  \"\"\n]\n\nlicense = \"MIT\"\n\n\n[tool.poetry.dependencies]\npython = \"3.6\"\nnumpy = \"1.14.3\"\nscipy = \"*\"\npandas = \"0.22.0\"\nscikit-learn = \"0.19.1\"\ncloudpickle = \"*\"\nmlflow =\"0.7.0\"\ntensorflow = \"^2.0.0\"\n\n\n[tool.poetry.dev-dependencies]\n\npylint = \"*\"\ndocker-compose = \"^1.25.0\"\ndocker-image-size-limit = \"^0.2.0\"\ntomlkit = \"^0.5.8\"\n\n<\/code><\/pre>\n\n<p><strong>docker-entrypoint.sh<\/strong><\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\n. \/venv\/bin\/activate\n\nmlflow server \\\n    --file-store $FILE_STORE \\\n    --default-artifact-root $ARTIFACT_STORE \\\n    --host $SERVER_HOST \\\n    --port $SERVER_PORT\n<\/code><\/pre>\n\n<hr>\n\n<hr>\n\n<p>if i add <code>RUN pwd; ls<\/code> just befor the first <code>COPY<\/code> I obtain :<\/p>\n\n<pre><code>Step 20\/31 : RUN pwd; ls\n ---&gt; Running in e8ec36dd6ca8\n\/\nartifactStore\nbin\ndev\netc\nfileStore\nhome\nlib\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\ntrunk\nusr\nvar\nvenv\nRemoving intermediate container e8ec36dd6ca8\n ---&gt; d7bba641bd7c\nStep 21\/31 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder392824737\/pyproject.toml: no such file or directory\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2019-12-24 05:54:19.16 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"docker|dockerfile|mlflow|python-poetry",
        "Question_view_count":940,
        "Owner_creation_date":"2016-04-02 19:35:23.203 UTC",
        "Owner_last_access_date":"2022-03-27 00:55:04.593 UTC",
        "Owner_location":"Grenoble, France",
        "Owner_reputation":56,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":"<p>Try \n<code>docker build -t test .<\/code><\/p>\n\n<p>instead of\n<code>docker build - &lt; Dockerfile<\/code><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-12-24 08:15:50.593 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2019-12-24 07:20:41.963 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow Artifacts Storing But Not Listing In UI",
        "Question_body":"<p>I've run into an issue using MLflow server. When I first ran the command to start an mlflow server on an ec2 instance, everything worked fine. Now, although logs and artifacts are being stored to postgres and s3, the UI is not listing the artifacts. Instead, the artifact section of the UI shows:<\/p>\n\n<pre><code>Loading Artifacts Failed\nUnable to list artifacts stored under &lt;s3-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n\n<p>But when I check in s3, I see the artifact in the s3 location that the error shows. What could possibly have started causing this as it used to work not too long ago and nothing was changed on the ec2 that is hosting mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-18 14:09:54.053 UTC",
        "Question_favorite_count":null,
        "Question_score":9,
        "Question_tags":"amazon-s3|amazon-ec2|artifact|mlflow",
        "Question_view_count":3949,
        "Owner_creation_date":"2014-11-26 14:40:35.813 UTC",
        "Owner_last_access_date":"2021-08-12 15:37:44.573 UTC",
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"logging models in mlflow with a pyspark process in Kerberized HDP 3.1.5",
        "Question_body":"<p>I'm currently testing mlflow to log pyspark models in a HDP3.1.x Cluster KERBERIZED.\nI've configured mlflow to use HDFS (of the same HDP cluster) for model storage.<\/p>\n<p>Whenever I launch a pyspark process to log a model on MLFlow with &quot;spark-submit --deploy-mode=cluster ...&quot;, I've got the exception<\/p>\n<blockquote>\n<p>AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]java.io.IOException: DestHost:destPort\nnamenode01.hdp.site:8020 , LocalHost:localPort\nworker05.hdp.site\/192.168.0.208:0. Failed on local exception:\njava.io.IOException:\n<strong>org.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]<\/strong><\/p>\n<p>(...)<\/p>\n<p>Caused by: java.io.IOException:\norg.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]    at\norg.apache.hadoop.ipc.Client$Connection$1.run(Client.java:758)    at\njava.security.AccessController.doPrivileged(Native Method)    at\njavax.security.auth.Subject.doAs(Subject.java:422)*<\/p>\n<\/blockquote>\n<p>It seems that libhdfs used by mlflow cannot properly authenticate with delegation tokens. Do you know any way to fix or circumvent this problem?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-07 14:09:48.157 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|kerberos|mlflow|hdp",
        "Question_view_count":102,
        "Owner_creation_date":"2021-10-07 13:53:47.87 UTC",
        "Owner_last_access_date":"2022-09-23 09:02:25.037 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-07 15:53:49.737 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Perform GridSearchCV with MLFlow",
        "Question_body":"<p>I just started using MLFlow and I am happy with what it can do. However, I cannot find a way to log different runs in a <code>GridSearchCV<\/code> from scikit learn.<\/p>\n\n<p>For example, I can do this manually<\/p>\n\n<pre><code>params = ['l1', 'l2']\nfor param in params:\n    with mlflow.start_run(experiment_id=1):\n        clf = LogisticRegression(penalty = param).fit(X_train, y_train)\n        y_predictions = clf.predict(X_test)\n\n        precision = precision_score(y_test, y_predictions)\n        recall = recall_score(y_test, y_predictions)\n        f1 = f1_score(y_test, y_predictions)\n\n        mlflow.log_param(\"penalty\", param)\n        mlflow.log_metric(\"Precision\", precision)\n        mlflow.log_metric(\"Recall\", recall)\n        mlflow.log_metric(\"F1\", f1)\n\n        mlflow.sklearn.log_model(clf, \"model\")\n<\/code><\/pre>\n\n<p>But when I want to use the <code>GridSearchCV<\/code> like that <\/p>\n\n<pre><code>pipe = Pipeline([('classifier' , RandomForestClassifier())])\n\nparam_grid = [\n    {'classifier' : [LogisticRegression()],\n     'classifier__penalty' : ['l1', 'l2'],\n    'classifier__C' : np.logspace(-4, 4, 20),\n    'classifier__solver' : ['liblinear']},\n    {'classifier' : [RandomForestClassifier()],\n    'classifier__n_estimators' : list(range(10,101,10)),\n    'classifier__max_features' : list(range(6,32,5))}\n]\n\n\nclf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n\nbest_clf = clf.fit(X_train, y_train)\n<\/code><\/pre>\n\n<p>I cannot think of any way to log all the individual models that the GridSearch tests. Is there any way to do it or I have to keep using the manual process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-02 15:44:47.48 UTC",
        "Question_favorite_count":1.0,
        "Question_score":11,
        "Question_tags":"python|scikit-learn|mlflow",
        "Question_view_count":4620,
        "Owner_creation_date":"2013-04-22 22:07:18.85 UTC",
        "Owner_last_access_date":"2022-09-24 18:16:29.973 UTC",
        "Owner_location":"Greece",
        "Owner_reputation":6815,
        "Owner_up_votes":153,
        "Owner_down_votes":12,
        "Owner_views":875,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-02 15:51:44.26 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How can an mlflow model be scaled to serve more requests?",
        "Question_body":"<p>I would like to have multiple instances of my MLFlow model running in parallel but hidden behind a common the same endpoint\/port so it's not visible to the user. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-17 07:51:03.51 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"multithreading|gunicorn|mlflow",
        "Question_view_count":228,
        "Owner_creation_date":"2020-01-14 16:04:32.663 UTC",
        "Owner_last_access_date":"2021-11-19 17:01:44.77 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Why is MLFLow unable to log metrics, artifacts while using MLFlow Project in Docker environment?",
        "Question_body":"<p>I am trying to store metrics and artifacts on host after running MLProject in a docker environment.I am expecting that when the experiment completes successfully, artifacts, metrics folders in mlruns\/ folder should have values and be shown on mlflow ui but artifacts, metrics folders in mlruns\/ folder are empty. mlflow ui is also not reflecting the new experiment.<\/p>\n<p>\/home\/mlflow_demo\/mlflow-demo.py -<\/p>\n<pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\nfrom random import random\nimport pickle\n\nclient = MlflowClient()\nexperiment_id = client.create_experiment(name='first experiment')\nrun = client.create_run(experiment_id=experiment_id)\nfor i in range(1000):\n client.log_metric(run.info.run_id,&quot;foo&quot;,random(),step=i)\nwith open(&quot;test.txt&quot;,&quot;w&quot;) as f:\n f.write(&quot;This is an artifact file&quot;)\nclient.log_artifact(run.info.run_id,&quot;test.txt&quot;)\nclient.set_terminated(run.info.run_id)\n<\/code><\/pre>\n<p>\/home\/mlflow_demo\/MLProject -<\/p>\n<pre><code>name: test-project\ndocker_env:\n image: kusur\/apex-pytorch-image:latest\nentry_points:\n main:\n  command: &quot;python mlflow-demo.py&quot;\n<\/code><\/pre>\n<p>command (executed in \/home\/mlflow_demo): - <code>mlflow run .<\/code><\/p>\n<p>After running the above code, I get the following log -<\/p>\n<pre><code>2021\/07\/06 12:22:28 INFO mlflow.projects.docker: === Building docker image test-project ===\n2021\/07\/06 12:22:28 INFO mlflow.projects.utils: === Created directory \/home\/mlflow_demo\/mlruns\/tmpwa8ydc5j for downloading remote URIs passed to arguments of type 'path' ===\n2021\/07\/06 12:22:28 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlflow_demo\/mlruns:\/mlflow\/tmp\/mlruns -v \/home\/mlflow_demo\/mlruns\/0\/0978fdd89ba44bf7b49975ab84838e82\/artifacts:\/home\/mlflow_demo\/mlruns\/0\/0978fdd89ba44bf7b49975ab84838e82\/artifacts -e MLFLOW_RUN_ID=0978fdd89ba44bf7b49975ab84838e82 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 test-project:latest python mlflow-demo.py' in run with ID '0978fdd89ba44bf7b49975ab84838e82' ===\n\n...\n\n2021\/07\/06 12:22:33 INFO mlflow.projects: === Run (ID '0978fdd89ba44bf7b49975ab84838e82') succeeded ===\n<\/code><\/pre>\n<p>Still the folders mlruns\/0\/0978fdd89ba44bf7b49975ab84838e82\/artifacts and mlruns\/0\/0978fdd89ba44bf7b49975ab84838e82\/metrics are empty.<\/p>\n<p>Can someone please provide the pointers. Please let me know if the question isn't well framed.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-07 14:38:10.113 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow|mlops",
        "Question_view_count":275,
        "Owner_creation_date":"2012-08-04 05:33:50.707 UTC",
        "Owner_last_access_date":"2022-09-08 13:52:47.757 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":568,
        "Owner_up_votes":97,
        "Owner_down_votes":2,
        "Owner_views":127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-07-16 15:18:22.083 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to display version in Mlflow UI in column",
        "Question_body":"<p>I have developed multiple modeling in mlflow where in I would like to create model versioning so that the version of that model can be track down for the easy identification based on the timestamp.<\/p>\n<p>Kindly provide the documentation specific to model versioning.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-23 05:54:52.247 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":94,
        "Owner_creation_date":"2022-03-23 05:43:50.177 UTC",
        "Owner_last_access_date":"2022-09-19 05:47:27.913 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-23 16:20:54.723 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Train model after load in MLFlow",
        "Question_body":"<p>My goal is to store an empty model into MLFlow Registry and then load it for training.<\/p>\n<p>I have a register_model.py which looks like this:<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n\n    remote_server_uri = &quot;http:\/\/127.0.0.1:5000&quot;\n    mlflow.set_tracking_uri(remote_server_uri)\n\n    # Load and compile Keras model\n    model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n    model.compile(&quot;adam&quot;, &quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])\n\n\n    # Load CIFAR-10 dataset\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n    epochs = 1\n    batch_size = 32\n    mlflow.tensorflow.autolog()\n    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n\n    tf.keras.models.save_model(model, &quot;models\/tensorflow&quot;)\n\n    mlflow.tensorflow.log_model(\n        tf_saved_model_dir='models\/tensorflow',\n        tf_meta_graph_tags=None,\n        tf_signature_def_key='serving_default',\n        artifact_path=&quot;saved\/models\/tensorflow&quot;,\n        registered_model_name=&quot;tensorflow-MobileNetV2-32inputs&quot;\n    )\n<\/code><\/pre>\n<p>Then I'm trying to load it using:<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n\n    remote_server_uri = &quot;http:\/\/127.0.0.1:5000&quot;\n    mlflow.set_tracking_uri(remote_server_uri)\n\n\n    model_name = &quot;tensorflow-MobileNetV2-32inputs&quot;\n    model_version = 1\n\n    model = mlflow.tensorflow.load_model(\n        model_uri=f&quot;models:\/{model_name}\/{model_version}&quot;\n    )\n<\/code><\/pre>\n<p>I would expect my model to be a able to do things like 'model.fit()' and 'model.predict()' but Im always getting:<\/p>\n<pre><code>AttributeError: '_WrapperFunction' object has no attribute 'fit'\n<\/code><\/pre>\n<p>So my question is: it is possible to save a tensorflow\/keras model and to load the architecture to be trained\/retrained and even modified through mlflow? Load the model, add a new layer, store the model either as a new version or as a new model itself, for example.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-24 11:27:48.757 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|mlflow",
        "Question_view_count":116,
        "Owner_creation_date":"2012-05-19 11:47:02.397 UTC",
        "Owner_last_access_date":"2022-09-20 08:15:38.867 UTC",
        "Owner_location":"M\u00e1laga, Spain",
        "Owner_reputation":1176,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to get access of some graphs \/plots in Databricks workflow?",
        "Question_body":"<p>I am running a notebook on another notebook using workflow<\/p>\n<pre><code>dbutils.notebook.run(&quot;python_EDA&quot;, 0, {&quot;param1&quot;: &quot;var1&quot;, &quot;param2&quot;: &quot;var2&quot;})\n<\/code><\/pre>\n<p>In my python_EDA notebook , there are some plots and graphs that are being plotted inside the Notebook job #029309238939093 after running the workflow command .<\/p>\n<p>How to get hold of those since everything is running as a task?\nIs it stored in some location ?<\/p>\n<p>I read its stored as MLflow artifacts, how to access these ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-01 11:45:27.143 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"workflow|databricks|mlflow",
        "Question_view_count":26,
        "Owner_creation_date":"2016-06-14 09:15:17.29 UTC",
        "Owner_last_access_date":"2022-09-23 05:00:12.03 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":559,
        "Owner_up_votes":368,
        "Owner_down_votes":28,
        "Owner_views":171,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does MLflow support darknet framework?",
        "Question_body":"<p>I am learning yolov4 with darknet and using that model for service development.<\/p>\n<p>However, I want to track and manage the performance metric of the model.<\/p>\n<p>So, I've heard of MLflow Tracking and am looking into it.<\/p>\n<p>Does MLflow support darknet?<\/p>\n<p>If so, is there a tracking management tool for using darknet?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 00:57:36.957 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"yolo|mlflow|darknet",
        "Question_view_count":85,
        "Owner_creation_date":"2019-01-02 00:12:28.563 UTC",
        "Owner_last_access_date":"2022-09-02 00:41:48.337 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-19 21:27:38.547 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"scikit-learn|save|xgboost|mlflow|xgbclassifier",
        "Question_view_count":1317,
        "Owner_creation_date":"2020-06-15 23:34:46.427 UTC",
        "Owner_last_access_date":"2022-09-23 19:41:30.79 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-20 13:06:22.583 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Reuse existing conda env for mlflow project?",
        "Question_body":"<p>I have configured a mlflow project file. First hard knock was that the extension is not required. The current problem is that I have exported an existing conda environment using:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>conda env export --name ENVNAME &gt; envname.yml\n<\/code><\/pre>\n<p>substituting the <code>ENVNAME<\/code>. This <strong>envname.yml<\/strong> file has the actual path where the env is located. Next, I have placed the <strong>envname.yml<\/strong> and defined entry points correctly.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: pytorch\nchannels:\n  - defaults\nprefix: \/data\/krishnan\/software\/anaconda3\/envs\/pytorch\n<\/code><\/pre>\n<p>When I run the project using <code>mlflow run .<\/code>, I find that mlflow tries to create yet one more temporary environment based on this Conda file which is Python 2. It ignores that the specified env exists and all packages are correct.<\/p>\n<p>Is there anything incorrect in what I am doing?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2021-12-03 15:45:22.61 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|conda|mlflow",
        "Question_view_count":379,
        "Owner_creation_date":"2016-05-05 05:51:57.847 UTC",
        "Owner_last_access_date":"2022-03-04 13:01:23.96 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-03 19:59:13.4 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Handle different runs concurrently on multiple tracking servers?",
        "Question_body":"<p>Are there any resources or insights on handling multiple tracking servers concurrently? We're trying to deploy some RESTful APIs (with FastAPI) that basically launch, potentially concurrently, multiple runs on different Tracking Servers using the MLflow Python API. We've seen that there's no clear way to explicitly assign the Tracking URI during the <code>mlflow.projects.run<\/code> function and so we're obliged to use <code>set_tracking_uri<\/code> everytime before launching the new run (which I quote &quot;does not affect the currently active run (if one exists), but takes effect for successive runs.&quot;). Problem is that it may happens that multiple runs go in conflict between each other and some random errors like <code>mlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST<\/code> may occur.<\/p>\n<p>Is there a way to handle this use case scenario or MLflow is still too unripe to be handling multiple tracking servers on a single endpoint?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-08 15:15:31.417 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"fastapi|mlflow",
        "Question_view_count":52,
        "Owner_creation_date":"2015-01-11 13:16:26.627 UTC",
        "Owner_last_access_date":"2022-09-24 16:47:22.317 UTC",
        "Owner_location":"Florence, Italy",
        "Owner_reputation":1065,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Question_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-30 14:25:39.347 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|nginx|docker-compose|mlflow",
        "Question_view_count":625,
        "Owner_creation_date":"2020-02-04 18:43:25.373 UTC",
        "Owner_last_access_date":"2022-09-21 20:36:53.347 UTC",
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-11 13:38:19.263 UTC",
        "Answer_last_edit_date":"2022-05-13 13:35:43.407 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Access databricks secrets in pyspark\/python job",
        "Question_body":"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark\/python jobs, especially if they are run using mlflow.<\/p>\n\n<p>I have already tried <a href=\"https:\/\/stackoverflow.com\/questions\/51885332\/how-to-load-databricks-package-dbutils-in-pyspark?rq=1\">How to load databricks package dbutils in pyspark<\/a><\/p>\n\n<p>which does not work for remote jobs or mlflow project runs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-08 21:40:46.997 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":1369,
        "Owner_creation_date":"2017-06-15 11:22:56.727 UTC",
        "Owner_last_access_date":"2021-03-07 17:34:22.217 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":481,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow in container is not mapped to host port",
        "Question_body":"<p>I am learning about MLFlow and Docker Containers.\nI created an ubuntu container and mapped port 5001 of the host to 5000 of the container.<\/p>\n<pre><code>docker run -it -p 5001:5000 -v D:\\Docker\\mlflow:\/home --name mlflow ubuntu:18.04 bash\n<\/code><\/pre>\n<p>Inside the container, I installed the mlflow using pip<\/p>\n<pre><code>pip install mlflow\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I run the mlflow UI it's running but I can't access it from my host PC (localhost:5001) is not working.<\/p>\n<p>Did I do any mistakes anywhere?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-28 04:01:39.483 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"docker|mlflow",
        "Question_view_count":51,
        "Owner_creation_date":"2018-10-17 13:06:23.973 UTC",
        "Owner_last_access_date":"2022-09-23 08:56:16.093 UTC",
        "Owner_location":"Bangkok, Thailand",
        "Owner_reputation":434,
        "Owner_up_votes":141,
        "Owner_down_votes":3,
        "Owner_views":77,
        "Answer_body":"<p>The problem is that you are starting the server on <code>127.0.0.1<\/code> and the port mapping was not pointing to this interface (socket hang up). Starting it on all interfaces <code>0.0.0.0<\/code> works.<\/p>\n<p>You should just run this command in the container.<\/p>\n<pre><code>mlflow ui -h 0.0.0.0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 21:40:54.01 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How do I setup the _SERVER_MODEL_PATH variable?",
        "Question_body":"<p>I'm trying to replicate the quickstart save and serve example.\nI go to the example folder, run the python script and can see the model runs and artifacts when I type <code>mlflow ui<\/code>.\nHowever, when I try the mlflow serve command with different model run Ids and ports I get a 404 in my browser, even though the command seems successful:<\/p>\n<pre><code>mlflow models serve -m runs:\/e1dabe8fc6e84286af5bee28ca89cdde\/model --port 1234\n2022\/07\/11 07:40:01 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2022\/07\/11 07:40:02 INFO mlflow.utils.conda: Conda environment mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 already exists.\n2022\/07\/11 07:40:02 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 &amp; waitress-serve --host=127.0.0.1 --port=1234 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\nINFO:waitress:Serving on http:\/\/127.0.0.1:1234\n<\/code><\/pre>\n<p>I tried running directly from anaconda prompt, and I get the following error:<\/p>\n<pre><code>conda activate mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 &amp; waitress-serve --host=127.0.0.1 --port=1234 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app\n\nTraceback (most recent call last):\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\runpy.py&quot;, line 197, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\runpy.py&quot;, line 87, in run_code\nexec(code, run_globals)\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\Scripts\\waitress-serve.exe_main.py&quot;, line 7, in\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\site-packages\\waitress\\runner.py&quot;, line 283, in run\napp = resolve(module, obj_name)\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\site-packages\\waitress\\runner.py&quot;, line 218, in resolve\nobj = import(module_name, fromlist=segments[:1])\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\site-packages\\mlflow\\pyfunc\\scoring_server\\wsgi.py&quot;, line 6, in\napp = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))\n\nFile &quot;C:\\Users\\sergio ferro.conda\\envs\\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\\lib\\os.py&quot;, line 679, in getitem\nraise KeyError(key) from None\nKeyError: 'pyfunc_model_path'\n<\/code><\/pre>\n<p>I have tried deleting and creating a new anaconda environment, ran from git bash, anaconda prompt, added anaconda3 environment variables. I know it has something to do with the <code>_SERVER_MODEL_PATH<\/code> variable but I wouldn't know how to set it up or which path add to my environment variables so it can read this variable from there.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-11 12:06:46.68 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"anaconda|mlflow",
        "Question_view_count":14,
        "Owner_creation_date":"2017-03-21 14:16:28.373 UTC",
        "Owner_last_access_date":"2022-09-20 12:01:20.917 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-24 11:12:09.07 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"upload a pre-trained model on databricks mlflow experiment",
        "Question_body":"<p>Is it possible to upload  a pre-trained machine learning model (from my local computer, for which I generated a model.pkl) on databricks, and serve it?\nOr is it impossible on Databricks ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-07 14:38:21.163 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"machine-learning|databricks|mlflow|pre-trained-model",
        "Question_view_count":48,
        "Owner_creation_date":"2019-03-03 11:21:20.527 UTC",
        "Owner_last_access_date":"2022-09-23 12:01:19.95 UTC",
        "Owner_location":"France",
        "Owner_reputation":314,
        "Owner_up_votes":18,
        "Owner_down_votes":2,
        "Owner_views":105,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-08 06:55:10.48 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to store my artifcats in the my mlflow tracking server's artifact-root?",
        "Question_body":"<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlflow.db --default-artifact-root $(pwd)\/artifacts --host 0.0.0.0 --port 8000\n<\/code><\/pre>\n<p>I ran this command, so i thought that my default artifact root is tracking server's execution location.<\/p>\n<pre><code>#\/bin\/bash \nexport MLFLOW_TRACKING_URI=&quot;http:\/\/localhost:8000&quot;\nmlflow run mlflow_docker\/ \\\n    --experiment-name AIT.DL.YOLOv5\/yolov5 \\\n    -A gpus=all \\\n    -A volume=&quot;\/home\/chaejin:\/data\/yolov5&quot; \\\n    -A v=&quot;${PWD}:\/usr\/src\/app&quot; \\\n    -A network=host\n<\/code><\/pre>\n<p>And I ran my MLproject in a docker env using shell script..\nThis location was different from tracking server execution location.<\/p>\n<p>As a result, my artifacts was stored in the Shell script location Not my default-artifacts-root.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/80efI.png\" rel=\"nofollow noreferrer\">my MLproject execution location<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bLpjG.png\" rel=\"nofollow noreferrer\">my Tracking server location<\/a><\/p>\n<p>I wanna storing my default artifacts location.. Please help me<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-02 04:33:35.057 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow|mlops",
        "Question_view_count":283,
        "Owner_creation_date":"2022-03-02 04:26:20.14 UTC",
        "Owner_last_access_date":"2022-05-12 10:45:59.14 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow - Serving model by reference to model registry",
        "Question_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-05 11:20:59.307 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1225,
        "Owner_creation_date":"2019-12-05 10:55:57.813 UTC",
        "Owner_last_access_date":"2020-08-06 13:05:49.423 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-12-25 00:04:03.98 UTC",
        "Answer_last_edit_date":"2019-12-25 05:31:38.637 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"\"Tuple index out of range\" error in mlflow pytorch",
        "Question_body":"<p>I am running the first example of <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pytorch.html\" rel=\"nofollow noreferrer\">this<\/a> page(training mnist using mlflow pytorch). Why does it give tuple index out of range?<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/jVaM6.png\" alt=\"enter image description here\" \/><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-06-22 16:48:53.283 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"pytorch|mlflow",
        "Question_view_count":92,
        "Owner_creation_date":"2021-06-22 16:43:08.33 UTC",
        "Owner_last_access_date":"2021-07-30 18:33:05.877 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-06-22 20:13:13.86 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"can I use mlflow python API to register a spark UDF & then use the UDF in Spark scala code?",
        "Question_body":"<p>I'm trying to use mlflow to do the machine learning work. I register the ML model as UDF using the following python code. The question is how can I use the UDF(test_predict) in my scala code? The reason is that our main code is in Scala. The problem is that UDF created below is a temporary UDF and SparkSession scoped. thanks!<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport mlflow\nfrom mlflow import pyfunc\nimport numpy as np\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark import SQLContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\n\nsc=SparkContext()\nspark = SparkSession.builder.appName(\"Python UDF example\").getOrCreate()\npyfunc_udf=mlflow.pyfunc.spark_udf(spark=spark, model_uri=\".\/sk\",result_type=\"float\")\nspark.udf.register(\"test_predict\",pyfunc_udf)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2020-04-25 14:15:05.457 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|apache-spark|pyspark|user-defined-functions|mlflow",
        "Question_view_count":617,
        "Owner_creation_date":"2019-03-25 09:33:34.607 UTC",
        "Owner_last_access_date":"2021-03-04 14:00:13.497 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-28 09:13:51.427 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"No usable temporary directory found with AWS Lambda function",
        "Question_body":"<p>I am trying to download a model with <code>mlflow<\/code> in an <code>aws lambda function<\/code> as described here: <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts<\/a><\/p>\n\n<p>However the following error is thrown:<\/p>\n\n<pre><code>  File \"\/tmp\/mlflow-api-server\/mlflow\/tracking\/artifact_utils.py\", line 66, in _download_artifact_from_uri\n  artifact_path=artifact_path, dst_path=output_path)\n  File \"\/tmp\/mlflow-api-server\/mlflow\/store\/artifact_repo.py\", line 94, in download_artifacts\n  dst_path = tempfile.mkdtemp()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 360, in mkdtemp\n  prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 130, in _sanitize_params\n  dir = gettempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 298, in gettempdir\n  tempdir = _get_default_tempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 233, in _get_default_tempdir\n  dirlist)\nFileNotFoundError: [Errno 2] No usable temporary directory found in ['\/tmp', '\/var\/tmp', '\/usr\/tmp']\n<\/code><\/pre>\n\n<p>The sklearn <code>model.pkl<\/code> file that <code>mlflow<\/code> should download has 627 Byte and the <code>aws lambda<\/code> limit should be 512 MB which should be enough space.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2019-07-03 08:45:32.867 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|aws-lambda|mlflow",
        "Question_view_count":2058,
        "Owner_creation_date":"2017-11-07 14:18:51.503 UTC",
        "Owner_last_access_date":"2022-09-24 17:52:28.29 UTC",
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to log metrics to Azure ML Metrics Tab",
        "Question_body":"<p>I have the following train.py file<\/p>\n<pre><code>import argparse\nimport os\nimport numpy as np\nimport glob\n# import joblib\nimport mlflow\nimport logging\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.dataset import Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\n\n# let user feed in 2 parameters, the dataset to mount or download,\n# and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;\n)\nargs = parser.parse_args()\n\ntablename = args.tablename\n\n\nsubscription_id = ''\nresource_group = 'mlplayground'\nworkspace_name = 'mlplayground'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name=tablename)\ndata = dataset.to_pandas_dataframe()\n\n# use mlflow autologging\nmlflow.autolog()\n\ndata.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)\none_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) \none_hot_city = pd.get_dummies(data.city_name, prefix='city')\n\n#removing categorical features \ndata.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  \n\n#Merging one hot encoded features with our dataset 'data' \ndata=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) \n\ndata['pricepersqm'] = data.price \/ data.house_area\n\nx=data.drop('price',axis=1) \ny=data.price \n\nX_df = DataFrame(x, columns= data.columns)\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)\n\n#Converting the data into proper LGB Dataset Format\nd_train=lgb.Dataset(X_train, label=y_train)\n\n\n#Declaring the parameters\nparams = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 10,\n    'learning_rate': 0.01,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}\n\nprint(&quot;Train a LightGBM Regression model&quot;)\nclf=lgb.train(params,d_train,1000)\n\n#model prediction on X_test\nprint(&quot;Predict the test set&quot;)\ny_pred=clf.predict(X_test)\n\n#using RMSE error metric\nmse =mean_squared_error(y_pred,y_test)\nprint(&quot;RMSE: &quot;, mse**0.5)\nmlflow.log_metric(&quot;RMSE&quot;, mse**0.5)\n<\/code><\/pre>\n<p>And then from a notebook file I use the following:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core import Experiment\n\n# connect to your workspace\nws = Workspace.from_config()\n\nexperiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;\nexp = Experiment(workspace=ws, name=experiment_name)\n\n\n\nfrom azureml.core.environment import Environment\n\n# use a curated environment that has already been built for you\n\nenv = Environment.get(workspace=ws, \n                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, \n                      version=1)\n\nfrom azureml.core import ScriptRunConfig\n\nargs = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;&quot;,\n    script=&quot;train.py&quot;,\n    arguments=args,\n    compute_target=&quot;local&quot;,\n    environment=env,\n)\n\nrun = exp.submit(config=src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.<\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-17 09:44:50.553 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|mlflow",
        "Question_view_count":40,
        "Owner_creation_date":"2011-04-05 19:05:03.093 UTC",
        "Owner_last_access_date":"2022-09-16 12:42:27.473 UTC",
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does MLflow allow to log artifacts from remote locations like S3?",
        "Question_body":"<h2>My setting<\/h2>\n<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>\/opt\/ml\/model<\/code> directory, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">which is reserved by SageMaker to pack models<\/a> as a <code>.tar.gz<\/code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">Scenario 4<\/a>). Note that this is a different S3 bucket than SageMaker's.<\/p>\n<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.<\/p>\n<p>A limitation is that, as I understand it, the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">MLflow API for logging artifacts<\/a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.<\/p>\n<h2>My questions<\/h2>\n<ul>\n<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?<\/li>\n<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-12 10:49:12.913 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow|mlops",
        "Question_view_count":533,
        "Owner_creation_date":"2016-01-08 20:55:48.08 UTC",
        "Owner_last_access_date":"2022-09-23 10:18:38.03 UTC",
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":118,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow: saving signature gives me warning",
        "Question_body":"<p>I am using mlflow with sqlite backend. started the server with:<\/p>\n<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlruns_db\/mlruns.db --default-artifact-root $PWD\/mlruns --host 0.0.0.0 -p 5000\n<\/code><\/pre>\n<p>in the code, I log the model with signature as such<\/p>\n<pre><code>...\nsignature = infer_signature(X, y)\nmlflow.sklearn.log_model(model, model_name, signature=signature)\n...\n<\/code><\/pre>\n<p>then I get warnings<\/p>\n<blockquote>\n<p>2022\/05\/26 19:52:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/d4c8f611d3f24986a32d19c7d8b03f06\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.<\/p>\n<\/blockquote>\n<p>I am using <code>mlflow, version 1.24.0<\/code>, though.<\/p>\n<p>I see that the signature is correctly logged inside <code>MLmodel<\/code> file, but the nice rendering of mlflow ui is lost.<\/p>\n<ol>\n<li><p>with logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/r2FwI.png\" rel=\"nofollow noreferrer\">mlflow ui with logging signature<\/a><\/p>\n<\/li>\n<li><p>without logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/9nQ8w.png\" rel=\"nofollow noreferrer\">mlflow ui without logging signature<\/a><\/p>\n<\/li>\n<\/ol>\n<p>Does this have any consequence later when serving models with signature enforcement?\nAlso, I see many blog examples with postgres instead of sqlite, and sftp\/minio instead of filestore. maybe changing to those setups will solve this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-28 15:14:55.023 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"postgresql|sqlite|metadata|mlflow",
        "Question_view_count":194,
        "Owner_creation_date":"2022-05-28 14:30:27.087 UTC",
        "Owner_last_access_date":"2022-08-24 16:50:33.54 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-04 15:09:56.64 UTC",
        "Question_favorite_count":1.0,
        "Question_score":8,
        "Question_tags":"apache-spark|matplotlib|pyspark|databricks|mlflow",
        "Question_view_count":5219,
        "Owner_creation_date":"2011-09-22 15:25:39.197 UTC",
        "Owner_last_access_date":"2022-09-13 16:17:01.243 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":6711,
        "Owner_up_votes":353,
        "Owner_down_votes":3,
        "Owner_views":819,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-12-04 15:14:14.147 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":7.0,
        "Question_last_edit_date":"2020-12-05 18:10:47.983 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow not configured, set environment variables",
        "Question_body":"<p>Step 1: Installed anaconda ( also installed R and python in it) on AWS EC2 instance with Ubuntu<\/p>\n\n<p>Step 2:  Used \u201cconda install -c conda-forge mlflow\u201d command to install mlflow in conda(which is assumed to be used for both R and python)<\/p>\n\n<p>Step 3: library(mlflow) command to use mlflow in R and import mlflow in python as per the MLFlow documentation but still unable to run the MLFlow as it gives the below error for R &amp; Python<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/holef.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/holef.png\" alt=\"Error Code in Jupyter Notebook for R\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/Wh1xv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wh1xv.png\" alt=\"R Error Code\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/w6t19.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w6t19.png\" alt=\"Python Notebook error code 1\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/IecnB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IecnB.png\" alt=\"Python Notebook error code 2\"><\/a><\/p>\n\n<p>Step 4: Used wine example that\u2019s available in R &amp; Python repository to validate MLFlow logging which is not happening. The MLFlow server is up and running <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2019-10-23 10:54:36.013 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|r|mlflow",
        "Question_view_count":404,
        "Owner_creation_date":"2015-02-16 10:02:28.377 UTC",
        "Owner_last_access_date":"2022-09-18 17:43:16.87 UTC",
        "Owner_location":"India",
        "Owner_reputation":185,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to serve custom MLflow model with Docker?",
        "Question_body":"<p>We have a project following essentially this\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">docker example<\/a> with the only difference that we created a custom model similar to <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">this<\/a> whose code lies in a directory called <code>forecast<\/code>. We succeeded in running the model with <code>mlflow run<\/code>. The problem arises when we try to serve the model. After doing <\/p>\n\n<pre><code>mlflow models build-docker -m \"runs:\/my-run-id\/my-model\" -n \"my-image-name\"\n<\/code><\/pre>\n\n<p>we fail running the container with<\/p>\n\n<pre><code>docker run -p 5001:8080 \"my-image-name\"\n<\/code><\/pre>\n\n<p>with the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'forecast'\n<\/code><\/pre>\n\n<p>It seems that the docker image is not aware of the source code defining our custom model class.\nWith Conda environnement the problem does not arise thanks to the <code>code_path<\/code> argument in <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n\n<p>Our Dockerfile is very basic, with just <code>FROM continuumio\/miniconda3:4.7.12, RUN pip install {model_dependencies}<\/code>.<\/p>\n\n<p>How to let the docker image know about the source code for deserialising the model and run it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-23 14:52:13.58 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"docker|mlflow",
        "Question_view_count":2105,
        "Owner_creation_date":"2015-04-16 17:17:00.943 UTC",
        "Owner_last_access_date":"2022-09-10 21:14:50.857 UTC",
        "Owner_location":"Paris, France",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"fitting and predicting model with mlflow",
        "Question_body":"<p>I'm very new to understanding the use of MLFlow but need assistance, I'm trying to understand on how to try and fit and predict my model once again. I'm able to call my model by:<\/p>\n<pre><code>PLS_model = mlflow.pyfunc.load_model(&quot;runs:\/FFFFF!@#!@#@!#!\/logged_model&quot;, suppress_warnings = True)\n<\/code><\/pre>\n<p>and get:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n  artifact_path: logged_model\n  flavor: mlflow.sklearn\n  run_id: FFFFF!@#!@#@!#!\n<\/code><\/pre>\n<p>But when I try to call any methods as:<\/p>\n<p>1).fit or .predict. I get the following error<\/p>\n<pre><code>AttributeError: 'PyFuncModel' object has no attribute 'fit'\n\nAttributeError: 'PyFuncModel' object has no attribute 'predict'\n<\/code><\/pre>\n<p>Here I encountered on how to actually call these functions but not sure if I'm doing this correctly. In summary, how can I predict, fit to my new data.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-30 14:47:13.613 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1702,
        "Owner_creation_date":"2017-10-24 16:10:51.607 UTC",
        "Owner_last_access_date":"2022-08-31 12:33:53.68 UTC",
        "Owner_location":"Netherlands",
        "Owner_reputation":458,
        "Owner_up_votes":104,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How do I set a different local directory for mlflow?",
        "Question_body":"<p>I want to run the mlflow ui from a different folder. By default it creates a folder called 'mlruns' in  the folder of my user. If create runs in a Jupiter-Notebook using a specific working directory a new Folder 'mlruns' is created in that directory. My goal is to run the mlflow ui acccessing the 'mlruns' Folder in this self specified Directory.\nSo far I have tried:<\/p>\n<p>running mlflow ui in the anaconda prompt PowerShell from the Directory i use<\/p>\n<p>defining the Directory in running the mlflow Server by:<\/p>\n<pre><code>mlflow ui --backend-store-uri file:\/\/\/'directory here'\n<\/code><\/pre>\n<p>I use Windows, Anaconda, Python, Jupiter-Notebook and the latest mlflow Version (1.10)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-21 15:13:41.51 UTC",
        "Question_favorite_count":2.0,
        "Question_score":3,
        "Question_tags":"python|jupyter-notebook|anaconda|mlflow",
        "Question_view_count":3970,
        "Owner_creation_date":"2020-08-11 13:40:43.123 UTC",
        "Owner_last_access_date":"2021-09-03 11:54:12.977 UTC",
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-13 09:08:37.703 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_date":"2017-06-26 09:55:36.987 UTC",
        "Owner_last_access_date":"2021-03-05 03:19:48.693 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-18 17:25:58.667 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to send data to server for Prediction - MLflow",
        "Question_body":"<p>I am able to create ml model server using following command<\/p>\n<pre><code>mlflow models serve -m file:\/\/\/C:\/Users\/SawarkarFamily\/Desktop\/mlflow-master\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/9aeb7ba16d7e4c20870b664e267524ea\/artifacts\/model -p 8000\n2020\/07\/28 17:10:59 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2020\/07\/28 17:11:03 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d &amp; waitress-serve --host=127.0.0.1 --port=8000 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\nc:\\users\\sawarkarfamily\\anaconda3\\envs\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\lib\\site-packages\\waitress\\adjustments.py:441: DeprecationWarning: In future versions of Waitress clear_untrusted_proxy_headers will be set to True by default. You may opt-out by setting this value to False, or opt-in explicitly by setting this to True.\n  warnings.warn(\nServing on http:\/\/DESKTOP-AO59MJC:8000\n<\/code><\/pre>\n<p>In documentation it is given that send that for prediction using curl command as follows:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; --data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, &quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, &quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' http:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>I replaced port number with 8000, but getting error.<\/p>\n<pre><code>curl: (6) Could not resolve host: chlorides,\ncurl: (6) Could not resolve host: citric acid,\ncurl: (6) Could not resolve host: density,\ncurl: (6) Could not resolve host: fixed acidity,\ncurl: (6) Could not resolve host: free sulfur dioxide,\ncurl: (6) Could not resolve host: pH,\ncurl: (6) Could not resolve host: residual sugar,\ncurl: (6) Could not resolve host: sulphates,\ncurl: (6) Could not resolve host: total sulfur dioxide,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 17\ncurl: (6) Could not resolve host: 0.029,\ncurl: (6) Could not resolve host: 0.48,\ncurl: (6) Could not resolve host: 0.98,\ncurl: (6) Could not resolve host: 6.2,\ncurl: (6) Could not resolve host: 29,\ncurl: (6) Could not resolve host: 3.33,\ncurl: (6) Could not resolve host: 1.2,\ncurl: (6) Could not resolve host: 0.39,\ncurl: (6) Could not resolve host: 75,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 5\n{&quot;error_code&quot;: &quot;MALFORMED_REQUEST&quot;, &quot;message&quot;: &quot;Failed to parse input as a Pandas DataFrame. Ensure that the input is a valid JSON-formatted Pandas DataFrame with the `split` orient produced using the `pandas.DataFrame.to_json(..., orient='split')` method.&quot;, &quot;stack_trace&quot;: &quot;Traceback (most recent call last):\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\pyfunc\\\\scoring_server\\\\__init__.py\\&quot;, line 74, in parse_json_input\\n    return _dataframe_from_json(json_input, pandas_orient=orient, schema=schema)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\utils\\\\proto_json_utils.py\\&quot;, line 106, in _dataframe_from_json\\n    return pd.read_json(path_or_str, orient=pandas_orient, dtype=False,\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\util\\\\_decorators.py\\&quot;, line 214, in wrapper\\n    return func(*args, **kwargs)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 608, in read_json\\n    result = json_reader.read()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 731, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 753, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 857, in parse\\n    self._parse_no_numpy()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 1094, in _parse_no_numpy\\n    for k, v in loads(json, precise_float=self.precise_float).items()\\nValueError: Expected object or value\\n&quot;}\n<\/code><\/pre>\n<p>Kindly someone help me with this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-28 12:15:01.813 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|json|curl|mlflow|mlops",
        "Question_view_count":1362,
        "Owner_creation_date":"2020-07-28 12:10:36.26 UTC",
        "Owner_last_access_date":"2021-09-30 13:53:40.19 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-07-28 13:16:24.983 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"deploy model as endpoint in databricks",
        "Question_body":"<p>after creating a simple keras model, I would like to deploy it as an endpoint for real-time inference in azure databricks. I created a simple cluster but unfortunately I ma not able to deploy the model itself. the deployment itself cannot be completed and the status is still yellow (pending)\n<a href=\"https:\/\/i.stack.imgur.com\/RxbN6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RxbN6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>can you please guide me where the error may be? what needs to be checked?  Thank you have a nice day<\/p>\n<p>BR\nTomas<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-11 13:33:12.123 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":48,
        "Owner_creation_date":"2020-08-24 21:06:06.307 UTC",
        "Owner_last_access_date":"2022-07-27 08:33:48.143 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to make predictions using a model that requires an input shape with more than two dimensions using MLflow?",
        "Question_body":"<p>I'm trying to implement a tensorflow (keras) based model into mlflow while learning how it works and if it suite our needs. I'm trying to implement the Fashion MNIST example from tensorflow website <a href=\"https:\/\/www.tensorflow.org\/tutorials\/keras\/classification?hl=it\" rel=\"nofollow noreferrer\">Here the link<\/a><\/p>\n\n<p>I was able to train and to log the model successfully into mlflow using this code:<\/p>\n\n<pre><code>import mlflow\nimport mlflow.tensorflow\nimport mlflow.keras\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images \/ 255.0\n\ntest_images = test_images \/ 255.0\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])\n\nif __name__ == \"__main__\":\n\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n    print('\\nTest accuracy:', test_acc)\n\n    mlflow.log_metric(\"validation accuracy\", float(test_acc))\n    mlflow.log_metric(\"validation loss\", float(test_loss))\n    mlflow.keras.log_model(model, \n                        \"model\", \n                        registered_model_name = \"Fashion MNIST\")\n<\/code><\/pre>\n\n<p>Then I'm now serving it with the models serve subcommand<\/p>\n\n<pre><code>$ mlflow models serve -m [model_path_here] -p 1234\n<\/code><\/pre>\n\n<p>The problem is that I'm not able to make predictions:<\/p>\n\n<pre><code>fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0\nlabels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nurl = \"http:\/\/127.0.0.1:1234\/invocations\"\n\nto_predict = test_images[0]\n\ndata = {\n    \"data\": [to_predict.tolist()]\n}\nheaders = {'Content-type': 'application\/json', 'Accept': 'text\/plain'}\nr = requests.post(url, data=json.dumps(data), headers=headers)\nres = r.json()\n<\/code><\/pre>\n\n<p>I'm getting this error:<\/p>\n\n<pre><code>{'error_code': 'BAD_REQUEST', 'message': 'Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.', 'stack_trace': 'Traceback (most recent call last):\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/keras.py\", line 298, in predict\\n    predicted = pd.DataFrame(self.keras_model.predict(dataframe))\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 909, in predict\\n    use_multiprocessing=use_multiprocessing)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_arrays.py\", line 715, in predict\\n    x, check_steps=True, steps_name=\\'steps\\', steps=steps)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 2472, in _standardize_user_data\\n    exception_prefix=\\'input\\')\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_utils.py\", line 564, in standardize_input_data\\n    \\'with shape \\' + str(data_shape))\\nValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (1, 28)\\n'}\n<\/code><\/pre>\n\n<p>That code above worked fine with a one dimension model<\/p>\n\n<p>The error seems to me related to the fact that a pandas DataFrame is a two dimensional data structure and the model instead requires a three dimensional input.<\/p>\n\n<p>The latest words from the error \"...but got array with shape (1, 28)\". The input shape should be (1, 28, 28) instead<\/p>\n\n<p>There is a way to use this kind of models with mlflow? There is a way to serialize and send numpy arrays directly as input instead of pandas dataframes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-18 15:28:15.563 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|tensorflow|keras|mlflow",
        "Question_view_count":1221,
        "Owner_creation_date":"2010-12-30 14:55:10.407 UTC",
        "Owner_last_access_date":"2022-01-08 07:38:22.447 UTC",
        "Owner_location":null,
        "Owner_reputation":503,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow run: Pass parameters in a file instead of key\/value pairs",
        "Question_body":"<p>Usually when running an MLProject, I would use something similar to:<\/p>\n<pre><code>mlflow run . -P alpha=0.1 -P l1_ratio=0.9\n<\/code><\/pre>\n<p>Is it possible to pass a file containing the key\/value pairs instead ? so something like:<\/p>\n<pre><code>mlflow run . --file .\/parametrs\n<\/code><\/pre>\n<p>where .\/parameters contains the key\/value pairs (like an env file or something)<\/p>\n<p>One way I thought of is to make a seperate bash script that accept the file and extracts the key\/value pairs to be included in the run command, but I wonder if there's a way more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-24 19:18:08.327 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|mlflow|mlops",
        "Question_view_count":222,
        "Owner_creation_date":"2018-10-27 15:39:35.053 UTC",
        "Owner_last_access_date":"2022-09-22 15:17:20.653 UTC",
        "Owner_location":"Tunisia",
        "Owner_reputation":606,
        "Owner_up_votes":42,
        "Owner_down_votes":8,
        "Owner_views":69,
        "Answer_body":"<p>It's not supported functionality according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, and <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/cli.py#L124\" rel=\"nofollow noreferrer\">source code<\/a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-25 08:37:55.96 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-10-23 14:52:36.82 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":1047,
        "Owner_creation_date":"2009-06-14 12:54:00.077 UTC",
        "Owner_last_access_date":"2022-09-23 21:20:51.75 UTC",
        "Owner_location":"New York, NY",
        "Owner_reputation":13408,
        "Owner_up_votes":306,
        "Owner_down_votes":12,
        "Owner_views":687,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-10-23 18:00:53.31 UTC",
        "Answer_last_edit_date":"2022-09-23 18:46:32.437 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-10-23 16:16:05.867 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow model in Heroku",
        "Question_body":"<p>I built an MLflow model and call a prediction on a streamlit dashboard, in it work fine in local.\nIn Heroku, the app which works fine locally failed to send the request online, what am I missing to such deployment?<\/p>\n<p>Below the error code raised.<\/p>\n<p>Procfile :<\/p>\n<pre><code>web: mlflow sagemaker deploy -m mlflow_model\/\nweb: sh setup.sh &amp;&amp; streamlit run app.py\n<\/code><\/pre>\n<p>Heroku logs:<\/p>\n<pre><code>2022-01-07T10:14:15.759252+00:00 app[web.1]: Traceback (most recent call last):\n2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.9\/site-packages\/streamlit\/script_runner.py&quot;, line 354, in _run_script\n2022-01-07T10:14:15.759252+00:00 app[web.1]: exec(code, module.__dict__)\n2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;\/app\/app.py&quot;, line 204, in &lt;module&gt;\n2022-01-07T10:14:15.759252+00:00 app[web.1]: main()\n2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;\/app\/app.py&quot;, line 119, in main\n2022-01-07T10:14:15.759253+00:00 app[web.1]: pred = request_prediction(MLFLOW_URI, ml_data)[0]\n2022-01-07T10:14:15.759253+00:00 app[web.1]: File &quot;\/app\/app.py&quot;, line 63, in request_prediction\n2022-01-07T10:14:15.759253+00:00 app[web.1]: response = requests.request(\n2022-01-07T10:14:15.759253+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.9\/site-packages\/requests\/api.py&quot;, line 61, in request\n2022-01-07T10:14:15.759253+00:00 app[web.1]: return session.request(method=method, url=url, **kwargs)\n2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.9\/site-packages\/requests\/sessions.py&quot;, line 529, in request\n2022-01-07T10:14:15.759254+00:00 app[web.1]: resp = self.send(prep, **send_kwargs)\n2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.9\/site-packages\/requests\/sessions.py&quot;, line 645, in send\n2022-01-07T10:14:15.759254+00:00 app[web.1]: r = adapter.send(request, **kwargs)\n2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.9\/site-packages\/requests\/adapters.py&quot;, line 519, in send\n2022-01-07T10:14:15.759254+00:00 app[web.1]: raise ConnectionError(e, request=request)\n2022-01-07T10:14:15.759259+00:00 app[web.1]: requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: \/invocations (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2452d23790&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-07 08:15:18.273 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"heroku|streamlit|mlflow",
        "Question_view_count":148,
        "Owner_creation_date":"2022-01-03 11:45:30.27 UTC",
        "Owner_last_access_date":"2022-01-31 05:12:53.3 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-07 11:19:14.363 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 08:38:22.897 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-07-28 15:35:17.217 UTC",
        "Owner_last_access_date":"2022-03-28 15:52:24.487 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-14 22:27:30.467 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to handle errors in MLflow when a model has been served using \"mlflow models serve\"?",
        "Question_body":"<p>During training, it is possible to use tags as a way to handle exceptions according to <a href=\"https:\/\/stackoverflow.com\/questions\/59856641\/how-can-i-throw-an-exception-from-within-an-mlflow-project\">this question<\/a>.<\/p>\n<p>If a model has been created using <code>mlflow.pyfunc.PythonModel<\/code>, is it possible to throw exceptions? Is there a way to allow error handling for a model that has been served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-03 13:55:43.31 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|rest|mlflow",
        "Question_view_count":266,
        "Owner_creation_date":"2016-09-03 19:53:45.4 UTC",
        "Owner_last_access_date":"2021-06-15 09:20:57.057 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow - running \"mlflow ui\" throwing file not found error on windows 10",
        "Question_body":"<p>I have done the training with sklearn model along with <code>mlflow<\/code> and it generated the <code>mlruns<\/code> folder.<\/p>\n<p>When I try to run <code>mlflow ui<\/code> it throws the following error.<\/p>\n<p>There is bug in Git <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1670\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1670<\/a> but doesn't have any solution.\nHave anyone faced and have any work around for this ?<\/p>\n<p>Tried re-installation.<\/p>\n<p>Attaching the traces below.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\runpy.py&quot;, line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\Vivek.ananthan\\Anaconda3\\envs\\deepcpu\\Scripts\\mlflow.exe\\__main__.py&quot;, line 7, in &lt;module&gt;\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\click\\core.py&quot;, line 1137, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\click\\core.py&quot;, line 1062, in main\n    rv = self.invoke(ctx)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\click\\core.py&quot;, line 1668, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\click\\core.py&quot;, line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\click\\core.py&quot;, line 763, in invoke\n    return __callback(*args, **kwargs)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\mlflow\\cli.py&quot;, line 280, in ui\n    _run_server(backend_store_uri, default_artifact_root, host, port, None, 1)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\mlflow\\server\\__init__.py&quot;, line 138, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\site-packages\\mlflow\\utils\\process.py&quot;, line 34, in exec_cmd\n    child = subprocess.Popen(\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\subprocess.py&quot;, line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File &quot;c:\\users\\vivek.ananthan\\anaconda3\\envs\\deepcpu\\lib\\subprocess.py&quot;, line 1311, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-06-03 10:57:57.037 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":707,
        "Owner_creation_date":"2012-10-08 01:45:06.233 UTC",
        "Owner_last_access_date":"2022-09-24 18:53:43.03 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":4158,
        "Owner_up_votes":295,
        "Owner_down_votes":2,
        "Owner_views":254,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow run passing Google Application credentials",
        "Question_body":"<p>I want to pass my <code>GOOGLE_APPLICATION_CREDENTIALS<\/code> environmental variable when I run <code>mlflow run<\/code> using a Docker container:<\/p>\n\n<p>This is my current <code>docker run<\/code> when using mlflow run:<\/p>\n\n<pre><code> Running command 'docker run --rm -e MLFLOW_RUN_ID=f18667e37ecb486cac4631cbaf279903 -e MLFLOW_TRACKING_URI=http:\/\/3.1.1.11:5000 -e MLFLOW_EXPERIMENT_ID=0 mlflow_gcp:33156ee python -m trainer.task --job-dir \/tmp\/ \\\n    --num-epochs 10 \\\n    --train-steps 1000 \\\n    --eval-steps 1 \\\n    --train-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv \\\n    --eval-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv \\\n    --batch-size 128\n<\/code><\/pre>\n\n<p>This is how I would normally pass it:<\/p>\n\n<pre><code>docker run \\\n   -p 9090:${PORT} \\\n   -e PORT=${PORT} \\\n   -e GOOGLE_APPLICATION_CREDENTIALS=\/tmp\/keys\/[FILE_NAME].json\n<\/code><\/pre>\n\n<p>What is the best way to option to pass this value to mlflow? I'm writing files in GCS and Docker requires access to GCP.<\/p>\n\n<p>MLproject contents<\/p>\n\n<pre><code>name: mlflow_gcp\ndocker_env:\n  image: mlflow-gcp-example\nentry_points:\n  main:\n    parameters:\n      job_dir:\n        type: string\n        default: '\/tmp\/'\n      num_epochs:\n        type: int\n        default: 10\n      train_steps:\n        type: int\n        default: 1000\n      eval_steps:\n        type: int\n        default: 1\n      batch_size:\n        type: int\n        default: 64\n      train_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv'\n      eval_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv'\n      mlflow_tracking_uri:\n        type: uri\n        default: ''\n\n    command: |\n        python -m trainer.task --job-dir {job_dir} \\\n            --num-epochs {num_epochs} \\\n            --train-steps {train_steps} \\\n            --eval-steps {eval_steps} \\\n            --train-files {train_files} \\\n            --eval-files {eval_files} \\\n            --batch-size {batch_size} \\\n            --mlflow-tracking-uri {mlflow_tracking_uri}\n\n<\/code><\/pre>\n\n<p>I already tried in Python file and fails since Docker has no access to local file system:<\/p>\n\n<pre><code>import os\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/Users\/user\/key.json\"\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-19 00:25:18.127 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"docker|mlflow",
        "Question_view_count":243,
        "Owner_creation_date":"2010-01-28 09:42:15.677 UTC",
        "Owner_last_access_date":"2022-09-25 05:06:35.287 UTC",
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":8619,
        "Owner_up_votes":1916,
        "Owner_down_votes":102,
        "Owner_views":1286,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-12-19 00:30:56.443 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"INTERNAL_SERVER_ERROR in MLFlow UI",
        "Question_body":"<p>I am using MLFlow to connect to MSSQL Server.\nWhen I launch <code>mlflow ui<\/code> using command line, the UI appears with an Internal Error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4W0B5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4W0B5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>On looking into SQL Profiler, I found that one of the queries is not able to execute properly.<\/p>\n<pre><code>[SQL: SELECT DISTINCT runs.run_uuid AS runs_run_uuid, runs.name AS runs_name, runs.source_type AS runs_source_type, runs.source_name AS runs_source_name, runs.entry_point_name AS runs_entry_point_name, runs.user_id AS runs_user_id, runs.status AS runs_status, runs.start_time AS runs_start_time, runs.end_time AS runs_end_time, runs.source_version AS runs_source_version, runs.lifecycle_stage AS runs_lifecycle_stage, runs.artifact_uri AS runs_artifact_uri, runs.experiment_id AS runs_experiment_id, CASE WHEN (runs.start_time IS NULL) THEN ? ELSE ? END AS anon_1\nFROM runs\nWHERE runs.experiment_id IN (?) AND runs.lifecycle_stage IN (?) ORDER BY CASE WHEN (runs.start_time IS NULL) THEN ? ELSE ? END, runs.start_time DESC, runs.run_uuid\n OFFSET ? ROWS\n FETCH FIRST ? ROWS ONLY]\n[parameters: (1, 0, '0', 'active', 1, 0, 0, 100)]\n<\/code><\/pre>\n<p>Any help regarding this is highly appreciated.\nI feel that somehow I am doing something wrong, but I have got same error while setting it up in another machine.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-11 06:48:44.94 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":293,
        "Owner_creation_date":"2013-04-03 18:26:51.123 UTC",
        "Owner_last_access_date":"2022-09-23 05:50:54.99 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":476,
        "Owner_up_votes":41,
        "Owner_down_votes":1,
        "Owner_views":76,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to fix Artifacts not showing in MLflow UI",
        "Question_body":"<p>I'd used MLflow and logged parameters using the function below (from pydataberlin).<\/p>\n<pre><code>def train(alpha=0.5, l1_ratio=0.5):\n    # train a model with given parameters\n    warnings.filterwarnings(&quot;ignore&quot;)\n    np.random.seed(40)\n\n    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n    data_path = &quot;data\/wine-quality.csv&quot;\n    train_x, train_y, test_x, test_y = load_data(data_path)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)    \n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n        print(&quot;  RMSE: %s&quot; % rmse)\n        print(&quot;  MAE: %s&quot; % mae)\n        print(&quot;  R2: %s&quot; % r2)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(key=&quot;alpha&quot;, value=alpha)\n        mlflow.log_param(key=&quot;l1_ratio&quot;, value=l1_ratio)\n        mlflow.log_metric(key=&quot;rmse&quot;, value=rmse)\n        mlflow.log_metrics({&quot;mae&quot;: mae, &quot;r2&quot;: r2})\n        mlflow.log_artifact(data_path)\n        print(&quot;Save to: {}&quot;.format(mlflow.get_artifact_uri()))\n        \n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>Once I run <code>train()<\/code> with its parameters, in UI I cannot see Artifacts, but I can see models and its parameters and Metric.<\/p>\n<p>In artifact tab it's written <code>No Artifacts Recorded Use the log artifact APIs to store file outputs from MLflow runs.<\/code> But in finder in models folders all Artifacts existe with models Pickle.<\/p>\n<p>help<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-24 00:31:11.933 UTC",
        "Question_favorite_count":0.0,
        "Question_score":2,
        "Question_tags":"python|artifacts|mlflow",
        "Question_view_count":8044,
        "Owner_creation_date":"2017-07-19 18:57:23.013 UTC",
        "Owner_last_access_date":"2022-09-15 23:20:31.84 UTC",
        "Owner_location":"France",
        "Owner_reputation":722,
        "Owner_up_votes":703,
        "Owner_down_votes":8,
        "Owner_views":290,
        "Answer_body":"<p>Had a similar issue. In my case, I solved it by running <code>mlflow ui<\/code> inside the <code>mlruns<\/code> directory of your experiment.<\/p>\n<p>See the full discussion on Github <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3030\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>Hope it helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-02 13:47:18.32 UTC",
        "Answer_last_edit_date":"2020-07-05 20:47:50.673 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":"2022-06-27 12:53:59.607 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Question_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-21 02:24:59.153 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|mlflow|scikit-learn-pipeline",
        "Question_view_count":122,
        "Owner_creation_date":"2018-12-10 18:48:11.223 UTC",
        "Owner_last_access_date":"2022-08-01 19:20:26.413 UTC",
        "Owner_location":"Zacatecas, Mexico",
        "Owner_reputation":131,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-22 21:24:47.183 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How can I run Tensorboard with MLFlow's logs?",
        "Question_body":"<p>I use MLFlow with autolog to keep track of my Tensorflow models:<\/p>\n<pre><code>mlflow.tensorflow.autolog(every_n_iter=1)\nwith mlflow.start_run():\n  model = ...\n  model.compile(...)\n  model.fit(...)\n<\/code><\/pre>\n<p>and then I want to use my tensorboard logs located in the artifacts.\nBut when I run:<\/p>\n<pre><code>%tensorboard --logdir=&lt;logs_path&gt;\n<\/code><\/pre>\n<p>I have the error message:\n&quot;No dashboards are active for the current data set.\nProbable causes:<\/p>\n<p>You haven\u2019t written any data to your event files.\nTensorBoard can\u2019t find your event files.&quot;<\/p>\n<p>I work on Databricks, so log_path is something like:<\/p>\n<pre><code>\/dbfs\/databricks\/mlflow-tracking\/..\n<\/code><\/pre>\n<p>Any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-16 18:15:45.067 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|databricks|tensorboard|mlflow",
        "Question_view_count":501,
        "Owner_creation_date":"2021-12-16 18:03:10.923 UTC",
        "Owner_last_access_date":"2022-09-22 13:52:41.243 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow not work after installation (Ubuntu 16, Centos 7)",
        "Question_body":"<p><img src=\"https:\/\/i.stack.imgur.com\/5DC76.png\" alt=\"enter link description here\"><\/p>\n\n<p>I try to install and run the web-based interface mlflow on VM Azure Ubuntu 16 and Centos 7.\nAfter running the command:\nsudo mlflow ui<\/p>\n\n<p>I can not get access url, either through the dns (mydomain.com:5000), or by IP: <a href=\"http:\/\/123.456.789.123:5000\/\" rel=\"nofollow noreferrer\">http:\/\/123.456.789.123:5000\/<\/a><\/p>\n\n<p>Executing on the server:<\/p>\n\n<p>wget <a href=\"http:\/\/localhost:5000\" rel=\"nofollow noreferrer\">http:\/\/localhost:5000<\/a><\/p>\n\n<p>I get the html-page mlflow, ie the server is running, but then why can not I connect to it in a browser? - Error:The connection has timed out<\/p>\n\n<p>p.s. Firewall disabled on this VM.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-02 09:38:26.153 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"ubuntu|centos|gunicorn|mlflow",
        "Question_view_count":218,
        "Owner_creation_date":"2019-04-09 14:32:27.59 UTC",
        "Owner_last_access_date":"2022-05-11 09:54:16.667 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-10-04 07:25:08.487 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Storing multiple metrics in mlflow from a cross validation",
        "Question_body":"<p>I have a logistic regression model for which I'm performing a repeated k-fold cross-validation and I'm wondering on the right way to track the the produced metrics in the mlfflow tracking api.<\/p>\n<pre><code>exp = mlflow.set_experiment(&quot;all_models_repeated_cross_validation_roc_auc&quot;)\nwith mlflow.start_run(experiment_id=exp.experiment_id):\n    rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=random_state)\n    scoring = make_scorer(roc_auc_score, needs_proba=False, multi_class=&quot;ovr&quot;)\n    lr_scores = cross_val_score(lr, X_train, y_train, scoring=scoring, cv=rkf)\n    # log all the 50 metrics in mlfflow tracking api\n<\/code><\/pre>\n<p>What is the proper way to do that with mlflow? Is it storing it as an artifact?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-24 15:14:49.723 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":44,
        "Owner_creation_date":"2013-06-26 10:06:58.2 UTC",
        "Owner_last_access_date":"2022-09-23 17:22:47.64 UTC",
        "Owner_location":"Sofia, Bulgaria",
        "Owner_reputation":547,
        "Owner_up_votes":118,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to delete a run_id from MLflow",
        "Question_body":"<p>I want to permanently delete a <code>run_id<\/code> from an experiment in <code>MLflow<\/code><\/p>\n<p>I am using the following code:<\/p>\n<pre><code>import mlflow\nfrom mlflow.entities import ViewType\n_mlflow_tracking_uri = &quot;the mlflow tracking url&quot;\nexp_id_delete = &quot;2&quot;\nmlflow.set_tracking_uri(_mlflow_tracking_uri)\n           \n\nclient = mlflow.tracking.MlflowClient()\nrun_infos = mlflow.list_run_infos(exp_id_delete, run_view_type=ViewType.DELETED_ONLY)\n               \nfor r in run_infos:\n                    \n   client.delete_run(r.run_id)\n<\/code><\/pre>\n<p>But I get an error <code>INVALID_PARAMETER_VALUE: The run blabla must be in the 'active' state. Current state is deleted. <\/code><\/p>\n<p>Any ideas ?<\/p>\n<p><em>UPDATE<\/em><\/p>\n<p>I tried writing some code with the help of the link that @Alex Ott suggested:<\/p>\n<pre><code>import mlflow\nfrom mlflow.entities import ViewType\nfrom mlflow.store.tracking import DEFAULT_LOCAL_FILE_AND_ARTIFACT_PATH\nfrom mlflow.store.artifact.artifact_repository_registry import get_artifact_repository\nfrom mlflow.tracking import _get_store\nfrom mlflow.entities.lifecycle_stage import LifecycleStage\nfrom mlflow.exceptions import MlflowException\n_mlflow_tracking_uri = &quot;the mlflow tracking url&quot;\nexp_id_delete = &quot;2&quot;\nmlflow.set_tracking_uri(_mlflow_tracking_uri)\n\n\nclient = mlflow.tracking.MlflowClient()\nrun_infos = mlflow.list_run_infos(exp_id_delete, run_view_type=ViewType.DELETED_ONLY)\n\nfor r in run_infos:\n      \n\n print(&quot;- run_id: {}, lifecycle_stage: {}&quot;.format(r.run_id, r.lifecycle_stage))\n\n run = backend_store.get_run(r.run_id)\n if run.info.lifecycle_stage != LifecycleStage.DELETED:\n      raise MlflowException(\n          &quot;Run % is not in `deleted` lifecycle stage. Only runs in &quot;\n          &quot;`deleted` lifecycle stage can be deleted.&quot; % r.run_id\n                    )\n artifact_repo = get_artifact_repository(run.info.artifact_uri)\n artifact_repo.delete_artifacts()\n backend_store._hard_delete_run(r.run_id)\n print(&quot;Run with ID %s has been permanently deleted.&quot; % str(r.run_id))\n<\/code><\/pre>\n<p>But now, even though I get as first message:<\/p>\n<pre><code> Deleted runs: run_id: 24ac591d1af840cfb703131dbe1f92a9, lifecycle_stage: deleted \n<\/code><\/pre>\n<p>then I get the following error<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Run '24ac591d1af840cfb703131dbe1f92a9' not found\n<\/code><\/pre>\n<p>Any ideas ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-29 09:18:27.593 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"python|python-3.x|mlflow",
        "Question_view_count":945,
        "Owner_creation_date":"2016-02-01 14:54:20.48 UTC",
        "Owner_last_access_date":"2022-09-24 18:36:48.79 UTC",
        "Owner_location":null,
        "Owner_reputation":3527,
        "Owner_up_votes":352,
        "Owner_down_votes":6,
        "Owner_views":440,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-04-30 11:44:11.503 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Changing subdirectory of MLflow artifact store",
        "Question_body":"<p>Is there anything in the Python API that lets you alter the artifact subdirectories? For example, I have a .json file stored here:<\/p>\n<p><code>s3:\/\/mlflow\/3\/1353808bf7324824b7343658882b1e45\/artifacts\/feature_importance_split.json<\/code><\/p>\n<p>MlFlow creates a <code>3\/<\/code> key in s3. Is there a way to change to modify this key to something else (a date or the name of the experiment)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-07-13 05:02:26.053 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1493,
        "Owner_creation_date":"2014-03-06 03:54:30.85 UTC",
        "Owner_last_access_date":"2022-09-20 20:56:03.467 UTC",
        "Owner_location":null,
        "Owner_reputation":913,
        "Owner_up_votes":156,
        "Owner_down_votes":5,
        "Owner_views":88,
        "Answer_body":"<p>As I commented above, yes, <code>mlflow.create_experiment()<\/code> does allow you set the artifact location using the <code>artifact_location<\/code> parameter.<\/p>\n<p>However, sort of related, the problem with setting the <code>artifact_location<\/code> using the <code>create_experiment()<\/code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()<\/code> function again.<\/p>\n<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()<\/code> function again. And as of this post, MLfLow does not have <code>check_if_exists<\/code> flag or a <code>create_experiments_if_not_exists()<\/code> function.<\/p>\n<p>To make things more frustrating, you cannot set the <code>artifcact_location<\/code> in the <code>set_experiment()<\/code> function either.<\/p>\n<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.\n:<\/p>\n<pre><code>import os\nfrom random import random, randint\n\nfrom mlflow import mlflow,log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\ntry:\n    experiment = mlflow.get_experiment_by_name('oof')\n    experiment_id = experiment.experiment_id\nexcept AttributeError:\n    experiment_id = mlflow.create_experiment('oof', artifact_location='s3:\/\/mlflow-minio\/sample\/')\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    mlflow.set_tracking_uri('http:\/\/localhost:5000')\n    print(&quot;Running mlflow_tracking.py&quot;)\n\n    log_param(&quot;param1&quot;, randint(0, 100))\n    \n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id<\/code> does not exist and the <code>except<\/code> code block gets executed creating the experiment.<\/p>\n<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try<\/code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-13 22:05:27.413 UTC",
        "Answer_last_edit_date":"2021-07-14 04:26:34.42 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFLOW: Registering a model remotely doesn't work while running locally inside azure VM does",
        "Question_body":"<p>I have been having issues trying to connect to an MLFLOW I created on an azure VM using the following tutorial:\n<a href=\"https:\/\/medium.com\/swlh\/how-to-setup-mlflow-on-azure-5ba67c178e7d\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/how-to-setup-mlflow-on-azure-5ba67c178e7d<\/a>\nWhenever running the following script on the server it works fine, but when running the same script remotely I get an error.\nIs there anyone around here that has experience in deploying mlflow to Azure?<\/p>\n<p>the script (censored IP address intentionally):<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nimport mlflow\nimport mlflow.sklearn\nmlflow.set_tracking_uri(&quot;http:\/\/xx.xxx.xx.xxx:5000\/&quot;)\nmlflow.set_registry_uri(&quot;http:\/\/xx.xxx.xx.xxx:5000\/&quot;)\nmlflow.set_experiment(&quot;test experiment4&quot;)\nwith mlflow.start_run(run_name=&quot;YOUR_RUN_NAME&quot;) as run:\n    sk_learn_rfr = RandomForestRegressor()\n    mlflow.sklearn.log_model(sk_model=sk_learn_rfr,artifact_path=&quot;sklearn-model_local&quot;,registered_model_name=&quot;sk-learn-random-forest-reg-model&quot;)\n<\/code><\/pre>\n<p>error :<\/p>\n<pre><code> File &quot;C:\\Users\\JasperBusschers\\PycharmProjects\\mlflow\\venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py&quot;, line 103, in send\n    self._sender.send(request.http_request, **request.context.options),\n  File &quot;C:\\Users\\JasperBusschers\\PycharmProjects\\mlflow\\venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\base_client.py&quot;, line 333, in send\n    return self._transport.send(request, **kwargs)\n  File &quot;C:\\Users\\JasperBusschers\\PycharmProjects\\mlflow\\venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\base_client.py&quot;, line 333, in send\n    return self._transport.send(request, **kwargs)\n  File &quot;C:\\Users\\JasperBusschers\\PycharmProjects\\mlflow\\venv\\lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py&quot;, line 361, in send\n    raise error\nazure.core.exceptions.ServiceRequestError: &lt;urllib3.connection.HTTPSConnection object at 0x000002B063025AC0&gt;: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n<\/code><\/pre>\n<p>I also tried using socket to try to connect and experienced the same error:<\/p>\n<pre><code>import socket\ns = socket.socket()\ns.connect(('http:\/\/20.XXX.XX.XXX', 5000))\nTraceback (most recent call last):\n  File &quot;&lt;input&gt;&quot;, line 4, in &lt;module&gt;\nsocket.gaierror: [Errno 11001] getaddrinfo failed\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-12 10:34:21.837 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|virtual-machine|mlflow",
        "Question_view_count":163,
        "Owner_creation_date":"2014-01-05 17:37:36.473 UTC",
        "Owner_last_access_date":"2022-09-13 14:14:14.983 UTC",
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-13 10:46:52.673 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Does mlflow support cluster deployment?",
        "Question_body":"<p>We would like to develop an ML platform contains mlflow. We have read the <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">official doc<\/a>, but it only tell us how to deploy on a single node. If this node crashed, the tracking service could be down, so I want to know if there is anyway to deploy a cluster of mlflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2019-04-10 09:08:44.307 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|machine-learning|deployment|mlflow",
        "Question_view_count":102,
        "Owner_creation_date":"2012-10-19 15:35:17.773 UTC",
        "Owner_last_access_date":"2022-09-20 05:16:13.837 UTC",
        "Owner_location":"Dalian, Liaoning, China",
        "Owner_reputation":339,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-02-05 15:49:27.767 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Cannot import data.table := operator into mlflow library , error: Invalid Request. could not find function \":=\"",
        "Question_body":"<p>I am having troubles in bring in the symbol := from data.table into mlflow library.<\/p>\n<p>Basically I am building a docker container to serve machine learning models.<\/p>\n<ul>\n<li>the docker is based off rocker\/r-ver:4.1.3<\/li>\n<li>the ml model is created by R crate function, this model predict function is written in R and uses data.table, including operator :=<\/li>\n<li>the docker is running to serve API request and use ml model to predict a response. during the ml model predict function, i had error:  Invalid Request.  could not find function &quot;:=&quot;<\/li>\n<\/ul>\n<p>I have read the references, tried all of them but none worked.<\/p>\n<p>Basically I am using remotes to install mlflow locally, instead of devtools.<\/p>\n<ul>\n<li>I clone the mlflow repo to local mlflow-debug<\/li>\n<li>add data.table (&gt;= 1.9.6), in the depends section of the DESCRIPTION file<\/li>\n<li>turn on building dependencies by set dependencies and build to TRUE in remotes::install_local, see below dockerfile<\/li>\n<\/ul>\n<pre><code>FROM rocker\/r-ver:4.1.3\nblah blash\n\nRUN pip install mlflow==1.24 protobuf==3.20.0\nRUN Rscript -e 'install.packages(c(&quot;glmnet&quot;, &quot;carrier&quot;, &quot;data.table&quot;, &quot;R.filesets&quot;))'\n\nCOPY mlflow-debug mlflow-debug\nRUN Rscript -e 'install.packages(&quot;remotes&quot;)'\nRUN Rscript -e 'remotes::install_local(\\\n    path=&quot;mlflow-debug\/mlflow\/R\/mlflow&quot;,\\\n    dependencies = TRUE,\\\n    build = TRUE,\\\n    upgrade=&quot;never&quot;\\\n    )'\n\n\n<\/code><\/pre>\n<p>At this point, I suspect that the issue is not bringing in the := symbol into mlflow library.<\/p>\n<p>Any ideas or help will be welcome, thanks a lot!<\/p>\n<p>references:<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/10527072\/using-data-table-package-inside-my-own-package?rq=1\">Using data.table package inside my own package<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/33039194\/making-a-package-in-r-that-depends-on-data-table?rq=1\">Making a package in R that depends on data.table<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/27980835\/r-data-table-works-in-direct-call-but-same-function-in-a-package-fails\">R data.table &#39;:=&#39; works in direct call, but same function in a package fails<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/71620417\/data-table-not-working-in-a-package-function\">data.table := not working in a package function<\/a><\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":10,
        "Question_creation_date":"2022-08-30 15:02:23.987 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"r|docker|data.table|mlflow",
        "Question_view_count":63,
        "Owner_creation_date":"2013-11-04 03:19:11.877 UTC",
        "Owner_last_access_date":"2022-09-23 02:06:10.57 UTC",
        "Owner_location":null,
        "Owner_reputation":502,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Create mlflow experiment: Run with UUID is already active",
        "Question_body":"<p>I'm trying to create a new experiment on mlflow but I have this problem:<\/p>\n<pre><code>Exception: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n<\/code><\/pre>\n<p>This is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport mlflow.sklearn\nimport sys\n\nmlflow.set_experiment(&quot;New experiment 2&quot;)\n\nmlflow.set_tracking_uri('http:\/\/mlflow:5000')\nst= mlflow.start_run(run_name='Test2')\nid = st.info.run_id\nmlflow.log_metric(&quot;score&quot;, score)\nmlflow.sklearn.log_model(model, &quot;wineModel&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-12 07:49:03.553 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":675,
        "Owner_creation_date":"2021-05-11 15:53:08.617 UTC",
        "Owner_last_access_date":"2021-05-13 16:04:47.89 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>You have to run mlflow.end_run() to finish the first experiment. Then you can create another<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-12 07:51:54.963 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Run experiments on Azure ML with Kedro and Mlflow",
        "Question_body":"<p>I'm trying to run the whole Kedro pipeline as an Azure ML experiment. I had two options here. The first one was to use the built-in logging feature of Azure ML and the second one was to use the azumeml-mlflow package that integrates Azure ML with Mlflow.<\/p>\n<p>I only tried the second approach as I did not know how to implement the Run() method of Azure ML inside the Kedro hooks.<\/p>\n<p>So, for the second approach, I presumed everything should be the same as when using Mlflow only. However, I couldn't get it to work even though it worked well outside of the Kedro structure ==&gt; I could launch experiments from other scripts.<\/p>\n<p>What I get with Kedro is that the pipeline runs well but nothing happens on Azure ML.<\/p>\n<p>Here's the code (hooks are inside a ModelTrackingHooks class):<\/p>\n<pre><code>@hook_impl\ndef before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to start an MLflow run\n    with the same run_id as the Kedro pipeline run.\n    &quot;&quot;&quot;\n\n\n    # Get Azure workspace\n    ws = Workspace.get(name=&quot;...&quot;,\n                       subscription_id=&quot;...&quot;,\n                       resource_group=&quot;...&quot;)\n    # Set tracking uri\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n\n    # Create an Azure ML experiment in the workspace\n    experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')\n    mlflow.set_experiment(experiment.name)\n\n    #Start logging\n    mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n    mlflow.log_params(run_params)        \n\n@hook_impl\ndef after_node_run(\n    self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.\n    In this example, we will:\n    * Log the parameters after the data splitting node runs.\n    * Log the model after the model training node runs.\n    * Log the model's metrics after the model evaluating node runs.\n    &quot;&quot;&quot;\n    \n    if node._func_name == &quot;cross_val&quot;:\n        mlflow.log_params(\n            {&quot;best_estimator&quot;: outputs[&quot;best_estimator&quot;],\n             &quot;best_params&quot;: outputs[&quot;best_params&quot;]}\n        )\n        model = outputs[&quot;validated_model&quot;]\n        mlflow.sklearn.log_model(model, &quot;model&quot;)\n\n    elif node._func_name == &quot;fit_and_save_transformer&quot;:\n        transformer = outputs[&quot;custom_transformer&quot;]\n        mlflow.sklearn.log_model(transformer, &quot;customer_transformer&quot;)\n\n    elif node._func_name == &quot;classification_reporting&quot;:\n        mlflow.log_metrics(outputs[&quot;metrics&quot;])\n    \n\n@hook_impl\ndef after_pipeline_run(self) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to end the MLflow run\n    after the Kedro pipeline finishes.\n    &quot;&quot;&quot;\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>Am I doing it the wrong way ?<\/p>\n<p>Do you have any idea or examples on how to use Kedro and Azure ML by leveraging only the built-in capabilities of Azure ML (i.e. without going through Mlflow) ?<\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 19:26:27.91 UTC",
        "Question_favorite_count":3.0,
        "Question_score":0,
        "Question_tags":"python|mlflow|azure-machine-learning-service|kedro|mlops",
        "Question_view_count":266,
        "Owner_creation_date":"2020-04-10 11:23:52.39 UTC",
        "Owner_last_access_date":"2022-08-12 18:19:53.33 UTC",
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-17 21:20:25.437 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How can I use Hyperopt with MLFlow within a pandas_udf?",
        "Question_body":"<p>I'm building multiple Prophet models where each model is passed to a pandas_udf function which trains the model and stores the results with MLflow.<\/p>\n\n<pre><code>@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\ndef forecast(data):\n......\n   with mlflow.start_run() as run: \n......\n<\/code><\/pre>\n\n<p>Then I call this UDF which trains a model for each KPI.<\/p>\n\n<pre><code>df.groupBy('KPI').apply(forecast)\n<\/code><\/pre>\n\n<p>The idea is that, for each KPI a model will be trained with multiple hyperparameters and store the best params for each model in MLflow. I would like to use Hyperopt to make the search more efficient. <\/p>\n\n<p>In this case, where should I place the objective function? Since the data is passed to the UDF for each model I thought of creating an inner function within the UDF that uses the data for each run. Does this make sense?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-18 14:50:29.823 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"apache-spark|mlflow|hyperopt|facebook-prophet",
        "Question_view_count":432,
        "Owner_creation_date":"2012-10-03 14:16:28.797 UTC",
        "Owner_last_access_date":"2022-06-27 13:31:44.52 UTC",
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-09-18 07:30:21.983 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Accessing MLFlow UI with a folder name different than mlruns",
        "Question_body":"<p>I set the <code>tracking_uri<\/code> to a folder name different than <code>mlruns<\/code>. <\/p>\n\n<p>Is there a way I can open the <strong>MLFlow UI<\/strong> pointing to the new folder name for mlruns? <\/p>\n\n<p>I know I can rename the folder back to <code>mlruns<\/code>, which gets me access to all of my metrics and parameters for each experiment, but the artifacts are not accessible, since they were logged to a different folder name than mlruns. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-02-19 19:41:07.453 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":1521,
        "Owner_creation_date":"2019-02-19 19:38:23.667 UTC",
        "Owner_last_access_date":"2019-12-06 03:47:18.253 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-02-19 20:50:20.313 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker",
        "Question_body":"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()<\/code>.<\/p>\n<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))<\/code> to infer input and output signatures.<\/p>\n<p>Model is deployed successfully. When trying to query the model I get <code>ModelError<\/code> (full traceback below).<\/p>\n<p>To query the model, I am using precisely the same <code>test_example<\/code> that I used for <code>infer_signature()<\/code>:<\/p>\n<p><code>test_example = [['This is the subject', 'This is the body']]<\/code><\/p>\n<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()<\/code> as that is not <code>json<\/code>-serializeable.<\/p>\n<p>To query the model I tried two different approaches:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nSAGEMAKER_REGION = 'us-west-2'\nMODEL_NAME = '...'\n\nclient = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)\n\n# Approach 1\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=json.dumps(test_example),\n                ContentType=&quot;application\/json&quot;,\n            )\n\n# Approach 2\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),\n                ContentType=&quot;application\/json; format=pandas-split&quot;,\n            )\n<\/code><\/pre>\n<p>but they result in the same error.<\/p>\n<p>Will be grateful for your suggestions.<\/p>\n<p>Thank you!<\/p>\n<p>Note: I am using Python 3 and all <strong>strings are unicode<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;\n      2                 EndpointName=MODEL_NAME,\n      3                 Body=test_example,\n----&gt; 4                 ContentType=&quot;application\/json; format=pandas-split&quot;,\n      5             )\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    394             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 395             return self._make_api_call(operation_name, kwargs)\n    396 \n    397         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    724             error_class = self.exceptions.from_code(error_code)\n--&gt; 725             raise error_class(parsed_response, operation_name)\n    726         else:\n    727             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\n<\/code><\/pre>\n<p>Environment info:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],\n 'dependencies': ['python=3.6.10',\n  'pip==21.3.1',\n  'pytorch=1.10.2',\n  'cudatoolkit=10.2',\n  {'pip': ['mlflow==1.22.0',\n    'transformers==4.17.0',\n    'datasets==1.18.4',\n    'cloudpickle==1.3.0']}],\n 'name': 'bert_bec_test_env'}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-09 11:56:29.87 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|nlp|amazon-sagemaker|mlflow",
        "Question_view_count":61,
        "Owner_creation_date":"2017-03-23 13:26:01.927 UTC",
        "Owner_last_access_date":"2022-09-22 20:27:37.72 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":83,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-09 14:44:47.963 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"mlflow artifact storage to AWS s3 artifacts",
        "Question_body":"<p>Is there anyway to store the logs stored by mlflow to AWS S3? <\/p>\n\n<pre><code>mlflow server \\\n    --backend-store-uri \/mnt\/persistent-disk \\\n    --default-artifact-root s3:\/\/my-mlflow-bucket\/ \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>Is it possible to only provide the default-artifact-root instead of providing both backend-store-uri and default-artifact-root? <\/p>\n\n<p>Also is there anyway to set default-artifact-root programatically from MlFlowClient or MlFlowContext instead of running mlflow server command line? <\/p>\n\n<p>FYI, I have already defined all AWS_ACCESS_KEY and AWS_SECRET_KEY in my environment variables, and exported ENDPOINTS to S3.<\/p>\n\n<p>Is logArtifacts from ActiveRun class a correct method to set the artifact_uri which points to AWS s3 bucket?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-25 15:57:16.957 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":5057,
        "Owner_creation_date":"2020-01-21 19:46:34.63 UTC",
        "Owner_last_access_date":"2020-03-04 14:53:18.867 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-02-25 18:29:55.24 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Is it possible to load a Scala\/Spark PipelineModel by mlflow?",
        "Question_body":"<p>I try to use <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/spark.html#load_model\" rel=\"nofollow noreferrer\">mlflow<\/a> to load a serialized Scala\/Spark <a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/java\/org\/apache\/spark\/ml\/PipelineModel.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a>.<\/p>\n<p>From <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.spark.html#mlflow.spark.load_model\" rel=\"nofollow noreferrer\">mlflow document<\/a> it seems it only supports <strong>Py<\/strong>Spark. Is this the case?<\/p>\n<p>Is it possible to load a serialized Scala\/Spark PipelineModel in mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-21 21:44:15.09 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|mlflow",
        "Question_view_count":175,
        "Owner_creation_date":"2017-11-13 13:08:21.763 UTC",
        "Owner_last_access_date":"2022-09-23 20:14:30.89 UTC",
        "Owner_location":null,
        "Owner_reputation":1529,
        "Owner_up_votes":92,
        "Owner_down_votes":5,
        "Owner_views":150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-09-24 23:09:15.577 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLFlow model not logging to Azure Blob Storage",
        "Question_body":"<p>I am trying to use MLFlow to log artifacts to Azure Blob Storage. Though the logging to dbfs works fine, when I try to log it to Azure Blob Storage, I only see a folder with the corresponding runid but inside it there are no contents.<\/p>\n\n<p>Here is what I do-<\/p>\n\n<ol>\n<li><p>Create a experiment from Azure Databricks, give it a name and the artifacts location as wasbs:\/\/mlartifacts@myazurestorageaccount.blob.core.windows.net\/ .<\/p><\/li>\n<li><p>In the spark cluster, in the environemtn Variables section pass on the AZURE_STORAGE_ACCESS_KEY=\"ValueoftheKey\" <\/p><\/li>\n<li>In the notebook, use mlflow to log metrics, param and finally the model using a snippet like below<\/li>\n<\/ol>\n\n<pre><code>\nwith mlflow.start_run():\n      lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n      lr.fit(train_x, train_y)\n\n      predicted_qualities = lr.predict(test_x)\n\n      (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n      print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n      print(\"  RMSE: %s\" % rmse)\n      print(\"  MAE: %s\" % mae)\n      print(\"  R2: %s\" % r2)\n\n      mlflow.log_param(\"alpha\", alpha)\n      mlflow.log_param(\"l1_ratio\", l1_ratio)\n      mlflow.log_metric(\"rmse\", rmse)\n      mlflow.log_metric(\"r2\", r2)\n      mlflow.log_metric(\"mae\", mae)\n\n      mlflow.sklearn.log_model(lr, \"model\")\n<\/code><\/pre>\n\n<p>Of course before using it , I set the experiment to the one where I have defined the artifacts store to be azure blob storage<\/p>\n\n<pre><code>experiment_name = \"\/Users\/user@domain.com\/mltestazureblob\"\nmlflow.set_experiment(experiment_name)\n<\/code><\/pre>\n\n<p>The metrices and params I can from the MLFlow  UI within Databricks but as since my artifacts location is Azure Blob Storage , I expect the model, the .pkl and conda.yaml file to be in the container in the Azure Blob Storage but when I go to check it, I only see a folder corresponding to the run id of the experiment but with nothing inside.<\/p>\n\n<p>I do not know what I am missing. In case, someone needs additional details I will be happy to provide.<\/p>\n\n<p>Point to note everything works fine when I use the default location i.e. dbfs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-10-23 09:12:45.343 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":812,
        "Owner_creation_date":"2015-04-10 08:31:54.763 UTC",
        "Owner_last_access_date":"2022-09-24 09:37:37.383 UTC",
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"MLflow unfinished experiment saved as finished",
        "Question_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-04-12 03:26:02.217 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":332,
        "Owner_creation_date":"2020-01-11 13:52:41.197 UTC",
        "Owner_last_access_date":"2022-09-24 12:58:48.047 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-12 10:33:23.527 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to manually or simply clear active run in mlflow",
        "Question_body":"<p>I am working with some NLP stuff. I use hydra and would like to add mlflow tracking for my project. I have added to my code:<\/p>\n<pre><code> set_tracking_uri(&quot;http:\/\/....&quot;)\n set_experiment('bert_adatper')\n with mlflow.start_run() as run:\n        log_param(&quot;a&quot;, 2) # Example of logging\n        mlflow.end_run()\n<\/code><\/pre>\n<p>But script keep telling me that a mlflow run is active and recommends me to use &quot;mlflow.end_run()&quot; which I have tried to use in different locations in code. But i cant get how to do it properly. Do you have any suggestions how to do this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-19 11:54:52.303 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":44,
        "Owner_creation_date":"2021-12-05 17:58:09.347 UTC",
        "Owner_last_access_date":"2022-08-03 10:13:32.077 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"How to log model using mlflow REST api? Does mlflow REST APIs support it?",
        "Question_body":"<p>I'm writing a library using mlflow REST APIs.\nI'm looking for mlflow REST api for logging different mlflow models.<\/p>\n<p>In the doc, <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-model\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-model<\/a> it says the api will be removed in future and doesn't have description about model_json request body.<\/p>\n<p>If I see github, <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/docs\/source\/rest-api.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/docs\/source\/rest-api.rst<\/a> mlflow REST API for Log model is missing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2022-03-30 11:14:03.58 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"machine-learning|mlflow|mlops",
        "Question_view_count":390,
        "Owner_creation_date":"2017-05-31 04:12:26.49 UTC",
        "Owner_last_access_date":"2022-09-24 08:59:44.06 UTC",
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-30 12:06:26.563 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Still on ML-Flow installation in R Studio",
        "Question_body":"<p>Please, after I load the mlflow library in R Studio and I run <code>install_mlflow(python_version = &quot;3.6&quot;)<\/code> on my windows 10 machine, I get the following message printed in my console, which seems to indicate that all requirements are already satisfied:<\/p>\n<pre><code>Requirement already satisfied: mlflow==1.19.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (1.19.0)\nRequirement already satisfied: pandas in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.1.5)\nRequirement already satisfied: pytz in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (2021.1)\nRequirement already satisfied: packaging in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (21.0)\nRequirement already satisfied: requests&gt;=2.17.3 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (2.26.0)\nRequirement already satisfied: alembic&lt;=1.4.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.4.1)\nRequirement already satisfied: entrypoints in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (0.3)\nRequirement already satisfied: Flask in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (2.0.1)\nRequirement already satisfied: docker&gt;=4.0.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (5.0.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (5.4.1)\nRequirement already satisfied: numpy in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.19.5)\nRequirement already satisfied: click&gt;=7.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (8.0.1)\nRequirement already satisfied: querystring-parser in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.2.4)\nRequirement already satisfied: prometheus-flask-exporter in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (0.18.2)\nRequirement already satisfied: protobuf&gt;=3.7.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (3.17.3)\nRequirement already satisfied: sqlparse&gt;=0.3.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (0.4.1)\nRequirement already satisfied: sqlalchemy in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.4.23)\nRequirement already satisfied: gitpython&gt;=2.1.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (3.1.18)\nRequirement already satisfied: waitress in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (2.0.0)\nRequirement already satisfied: databricks-cli&gt;=0.8.7 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (0.15.0)\nRequirement already satisfied: cloudpickle in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from mlflow==1.19.0) (1.6.0)\nRequirement already satisfied: python-editor&gt;=0.3 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.19.0) (1.0.4)\nRequirement already satisfied: Mako in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.19.0) (1.1.5)\nRequirement already satisfied: python-dateutil in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.19.0) (2.8.2)\nRequirement already satisfied: importlib-metadata in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from click&gt;=7.0-&gt;mlflow==1.19.0) (4.8.1)\nRequirement already satisfied: colorama in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from click&gt;=7.0-&gt;mlflow==1.19.0) (0.4.4)\nRequirement already satisfied: tabulate&gt;=0.7.7 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow==1.19.0) (0.8.9)\nRequirement already satisfied: six&gt;=1.10.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow==1.19.0) (1.16.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from docker&gt;=4.0.0-&gt;mlflow==1.19.0) (1.2.1)\nRequirement already satisfied: pywin32==227 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from docker&gt;=4.0.0-&gt;mlflow==1.19.0) (227)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from gitpython&gt;=2.1.0-&gt;mlflow==1.19.0) (4.0.7)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from gitpython&gt;=2.1.0-&gt;mlflow==1.19.0) (3.10.0.1)\nRequirement already satisfied: smmap&lt;5,&gt;=3.0.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=2.1.0-&gt;mlflow==1.19.0) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.19.0) (1.26.6)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.19.0) (3.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.19.0) (2021.5.30)\nRequirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.19.0) (2.0.4)\nRequirement already satisfied: greenlet!=0.4.17 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from sqlalchemy-&gt;mlflow==1.19.0) (1.1.1)\nRequirement already satisfied: Werkzeug&gt;=2.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from Flask-&gt;mlflow==1.19.0) (2.0.1)\nRequirement already satisfied: itsdangerous&gt;=2.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from Flask-&gt;mlflow==1.19.0) (2.0.1)\nRequirement already satisfied: Jinja2&gt;=3.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from Flask-&gt;mlflow==1.19.0) (3.0.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from Jinja2&gt;=3.0-&gt;Flask-&gt;mlflow==1.19.0) (2.0.1)\nRequirement already satisfied: dataclasses in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from Werkzeug&gt;=2.0-&gt;Flask-&gt;mlflow==1.19.0) (0.8)\nRequirement already satisfied: zipp&gt;=0.5 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from importlib-metadata-&gt;click&gt;=7.0-&gt;mlflow==1.19.0) (3.5.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from packaging-&gt;mlflow==1.19.0) (2.4.7)\nRequirement already satisfied: prometheus-client in c:\\users\\ifeanyi\\appdata\\local\\r-mini~1\\envs\\r-mlfl~1.0\\lib\\site-packages (from prometheus-flask-exporter-&gt;mlflow==1.19.0) (0.11.0)\n[1] &quot;mlflow==1.19.0&quot;\n<\/code><\/pre>\n<p>Nevertheless, when I run an API call, such as <code>mlflow_ui()<\/code>, I still get the error message printed in my console:<\/p>\n<pre><code>Error in rethrow_call(c_processx_exec, command, c(command, args), pty,  : \n  Command 'C:\/Users\/IFEANYI\/AppData\/Local\/r-miniconda\/envs\/r-mlflow-1.19.0\/mlflow' not found @win\/processx.c:982 (processx_exec)\n<\/code><\/pre>\n<p>Also, after loading the library and I run <code>install_mlflow()<\/code>, the code runs until it returns the error message in my console:<\/p>\n<pre><code>Error: Error installing package(s): &quot;mlflow==1.20.3&quot;\n<\/code><\/pre>\n<p>I have anaconda installed on my machine, which I believe should take care of conda installation. Please I do not know where to go from here guys.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-20 12:17:50.323 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"r|mlflow",
        "Question_view_count":198,
        "Owner_creation_date":"2016-02-25 21:42:41.25 UTC",
        "Owner_last_access_date":"2022-09-22 21:29:37.373 UTC",
        "Owner_location":null,
        "Owner_reputation":109,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-20 15:46:48.073 UTC",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Custom MLFlow scoring_server for model serving",
        "Question_body":"<p>I would like to know if MLflow currently does support any kind of customization of it's scoring_serving that would allow the ability to register new endpoints to the published Rest API.<\/p>\n<p>By default the scoring server provides \/ping and \/invocations endpoint, but i would like to include more endpoints in addition to those.<\/p>\n<p>I've seen some resources that allow that kind of behaviour using custom WSGI implementations but i would like to know if extension of the provided mlflow scoring_server is possible in any way, so the default supporty provided by mlflow generated docker images and the deployment management is not lost.<\/p>\n<p>I explored existing official and unofficial documentation, and explored existing github issues and the mlflow codebase in it's github repository.<\/p>\n<p>Also i've explored some alternatives such as using custom WSGI server configuration for starting the Rest API.<\/p>\n<p>Any kind of resource\/documentation is greatly appreciated.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-22 08:30:47 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"rest|mlflow|serving|mlops",
        "Question_view_count":96,
        "Owner_creation_date":"2013-05-31 12:29:18.43 UTC",
        "Owner_last_access_date":"2022-06-20 16:38:21.593 UTC",
        "Owner_location":"A Coru\u00f1a",
        "Owner_reputation":303,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"Mlflow - empty artifact folder",
        "Question_body":"<p>All,<\/p>\n<p>I started the <strong>mlflow server<\/strong> as below. I do see the <strong>backend store<\/strong> containing the expected metadata. However, the <strong>artifact folder<\/strong> is <em><strong>empty<\/strong><\/em> despite many runs.<\/p>\n<pre><code>&gt; mlflow server --backend-store-uri mlflow_db --default-artifact-root\n&gt; .\/mlflowruns --host 0.0.0.0 --port 5000\n<\/code><\/pre>\n<p>The mlflow ui has the below message for the artifacts section:<\/p>\n<blockquote>\n<pre><code>              No Artifacts Recorded\n Use the log artifact APIs to store file outputs from MLflow runs.\n<\/code><\/pre>\n<\/blockquote>\n<p>What am I doing wrong?<\/p>\n<p>Thanks,\ngrajee<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-06-27 22:26:29.703 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":599,
        "Owner_creation_date":"2018-02-07 01:32:14.63 UTC",
        "Owner_last_access_date":"2022-06-26 08:34:21.763 UTC",
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":41,
        "Owner_down_votes":1,
        "Owner_views":110,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Question_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 15:41:36.547 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":412,
        "Owner_creation_date":"2014-08-18 14:07:01.673 UTC",
        "Owner_last_access_date":"2022-09-15 04:40:38.707 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Answer_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-22 23:47:09.917 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"MLFlow"
    }
]