[
    {
        "Question_id":62332672,
        "Question_title":"Tracking separate train\/test processes with Trains",
        "Question_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591905931703,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":99,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609771005756,
        "Answer_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1591906575912,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592833417200,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70423979,
        "Question_title":"Export metrics of ClearML to Prometheus and show them in Grafana",
        "Question_body":"<p>Are there any metrics I can get from the API server? or any docker image I can point to the backend and get some metrics?\nMost important is the see how many tasks running in real-time (like we can see on the worker's page) and also check how much time each task is running (also can be found on the worker's page)<\/p>\n<p>If it does not exist, do they have an API for getting all this information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640013390177,
        "Question_score":0,
        "Question_tags":"prometheus|grafana|metrics|clearml",
        "Question_view_count":70,
        "Owner_creation_time":1569092651057,
        "Owner_last_access_time":1662456713617,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70423979",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63586251,
        "Question_title":"Will Trains automagically log Tensorboard HParams?",
        "Question_body":"<p>I know that it's possible to send hyper-params as a dictionary to Trains.<\/p>\n<p>But can it also automagically log hyper-params that are logged using the TF2 HParams module?<\/p>\n<p>Edit: This is done in the <a href=\"https:\/\/www.tensorflow.org\/tensorboard\/hyperparameter_tuning_with_hparams\" rel=\"nofollow noreferrer\">HParams tutorial<\/a> using <code>hp.hparams(hparams)<\/code>.<\/p>\n<p><img src=\"https:\/\/www.tensorflow.org\/tensorboard\/images\/hparams_parallel_coordinates.png?raw=1\" alt=\"Tensorboard HParams\" \/><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598385550273,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":142,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609427497023,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63586251",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65655382,
        "Question_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610294996320,
        "Question_score":2,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1610296597728,
        "Answer_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610303095156,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":67496760,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1620788331577,
        "Question_score":1,
        "Question_tags":"docker|amazon-s3|wsl-2|rclone|clearml",
        "Question_view_count":770,
        "Owner_creation_time":1362580980910,
        "Owner_last_access_time":1663351509150,
        "Owner_location":"Akron, OH, USA",
        "Owner_reputation":4013,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1621022250560,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621009841940,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1621012052112,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62591953,
        "Question_title":"Does clone experiment work on sklearn functions?",
        "Question_body":"<p>I'm trying to run a script and I'm constantly getting this while cloning experiment in allegro.ai\nAttributeError: 'Namespace' object has no attribute 'get'\nCan anybody help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1593164125703,
        "Question_score":0,
        "Question_tags":"trains|clearml",
        "Question_view_count":27,
        "Owner_creation_time":1505152928440,
        "Owner_last_access_time":1593163986227,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1609501001127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62591953",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66320238,
        "Question_title":"ClearML get max value from logged values",
        "Question_body":"<p>I use ClearML to track my tensorboard logs (from PyTorch Lightning) during training.\nAt a point later I start another script which connects to existing task and do some testing.<\/p>\n<p>But unfortenautly I do not have all information in the second script, so I want to query them from the logged values from ClearML server.<\/p>\n<p>How would I do this?<\/p>\n<p>I thought about something like this, but havn't found anything in documentation:<\/p>\n<pre><code>task = Task.init(project_name=&quot;Project&quot;, task_name=&quot;name&quot;, reuse_last_task_id=&quot;Task_id, continue_last_task=True)\nx_value, y_value = task.get_value(key=&quot;val\/acc&quot;, mode=&quot;max&quot;)\nx_value2, y_value2 = task.get_value(key=&quot;epoch&quot;, mode=&quot;x&quot;, x=x_value)\n<\/code><\/pre>\n<ul>\n<li><code>x_value<\/code> would be my epoch or global step<\/li>\n<li><code>y_value<\/code> the maximum value of plot &quot;val\/acc&quot;<\/li>\n<li><code>x_value2<\/code> would be my epoch or global step<\/li>\n<li><code>y_value2<\/code> the value of plot &quot;epoch&quot; at <code>x_value<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614013817123,
        "Question_score":3,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":101,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614165129956,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To get an existing <code>Task<\/code> object for a running (or completed\/failed) experiment, assuming we know Task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(task_id='aabbcc')\n<\/code><\/pre>\n<p>If we only know the Task project\/name<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(project_name='the project', task_name='the name')\n<\/code><\/pre>\n<p>Notice that if you have multiple task under the same name it will return the most updated one.\nOnce we have the <code>Task<\/code> object, we can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>latest_scalar_values_dict = another_task.get_last_scalar_metrics()\n<\/code><\/pre>\n<p>Which would return all the scalars min\/maxm\/last values, for example:<\/p>\n<pre><code>latest_scalar_values_dict = {\n            'title': {\n                'series': {\n                    'last': 0.5,\n                    'min': 0.1,\n                    'max': 0.9\n                    }\n                }\n            }\n<\/code><\/pre>\n<p><a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get%20metrics#clearml.task.Task.get_last_scalar_metrics\" rel=\"nofollow noreferrer\">documentation here<\/a><\/p>\n<p>If you need to get the entire graphs you can use <code>task.get_reported_scalars()<\/code> <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get_reported_scalars#clearml.task.Task.get_reported_scalars\" rel=\"nofollow noreferrer\">see docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1614215061400,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320238",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73456627,
        "Question_title":"ClearML Remote execution does not use requirements.txt",
        "Question_body":"<p>I want to execute a function on a worker with clearml. I use the code below to do this. But worker will always create a new env, although I set it to use my python binary via environmet settings. I also tried to pass a requirements.txt but this will be ignored or misinterpreted.\nThe main issue is that I need opencv_contrib, but clearml always installs opencv afterwards and the contrib package will get overwritten, so the contrib methods are not available and the remote execution fails.<\/p>\n<p>How can I force to use my existing python (conda) environment?\nHow can I force to install the packages I define in my requirements.txt?<\/p>\n<pre><code>    def my_func(path:str):\n        # do calculation\n        return \n    paths = ['\/media\/hdd\/my_folder_1\/']\n    for i, path in enumerate(paths):\n        task.create_function_task(my_func, func_name=my_func-{i}',\n                              task_name=f'my_func - {i}', path=path)\n<\/code><\/pre>\n<p>Starting script for my clearml worker<\/p>\n<pre><code>#!\/bin\/bash\n\nexport CLEARML_HOST_IP=127.0.0.1\nexport CLEARML_AGENT_SKIP_PIP_VENV_INSTALL=\/home\/user\/miniconda3\/envs\/my_env\/bin\/python\nclearml-agent daemon --detached --queue default\n<\/code><\/pre>\n<p>I'm using latest clearml on ubuntu 20.04.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661248448260,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":14,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73456627",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73570817,
        "Question_title":"ClearML - dynamically updating Plotly plots?",
        "Question_body":"<p>I have a question related to ClearML plot logging. We are currently using:<\/p>\n<pre><code>self.task_logger.report_table(&quot;TableSpaceName&quot;, &quot;Some Info&quot;, iteration=0, table_plot=df)\n<\/code><\/pre>\n<p>To report tables. They appear under &quot;PLOTS&quot; section. Similarly, we are reporting plotly graphs:<\/p>\n<pre><code>self.task_logger.report_plotly(\n        title=&quot;PlotTitle&quot;, iteration=0, series='SeriesName', figure=fig\n    )\n<\/code><\/pre>\n<p>Both work fine. The issue is, each new <code>report_plotly<\/code> call, instead of replacing the image in the section, creates a new one, and leaves the previous one present too. This cloggs the PLOTS section (tables and figures). The question is, how does one report a plot, so that it's reported in-place (Such as e.g., scalars, where sample plot gets updated in time)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662041496990,
        "Question_score":0,
        "Question_tags":"python|plot|clearml",
        "Question_view_count":23,
        "Owner_creation_time":1397215330877,
        "Owner_last_access_time":1664044551867,
        "Owner_location":null,
        "Owner_reputation":1968,
        "Owner_up_votes":43,
        "Owner_down_votes":6,
        "Owner_views":190,
        "Question_last_edit_time":1662041933803,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73570817",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62552414,
        "Question_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Question_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592992723417,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":129,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609531417760,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1592997291032,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64636294,
        "Question_title":"Trains: reusing previous task id",
        "Question_body":"<p>I am using <code>reuse_last_task_id=True<\/code> to overwrite an existing task (with same project and task name). But the experiment contains the torch model and therefore does not overwrite the existing task but creates a new one. How can I detach the model from the task?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604260179110,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":133,
        "Owner_creation_time":1410412149420,
        "Owner_last_access_time":1664064601540,
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":1609418272432,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64636294",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73442798,
        "Question_title":"Does ClearML have accounting of information security events",
        "Question_body":"<p>ClearML is one of the most famous MLOps tools existing. It has logging of machine learning processes, however I couldn't find any information regarding its system of accounting of <strong>information security events<\/strong>.<\/p>\n<p>My question is: does ClearML have such system? Does it register\/log events of client-server interaction? If ClearML does, then what format is used?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661159945233,
        "Question_score":1,
        "Question_tags":"logging|clearml",
        "Question_view_count":37,
        "Owner_creation_time":1661159047617,
        "Owner_last_access_time":1663851754370,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73442798",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65509754,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1609343679217,
        "Question_score":5,
        "Question_tags":"trains|clearml",
        "Question_view_count":740,
        "Owner_creation_time":1383083414230,
        "Owner_last_access_time":1663937141383,
        "Owner_location":null,
        "Owner_reputation":2801,
        "Owner_up_votes":371,
        "Owner_down_votes":0,
        "Owner_views":131,
        "Question_last_edit_time":1609427009848,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1609345139523,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661326401412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73716806,
        "Question_title":"How to make ClearML not upload annotations twice when they have the same ID?",
        "Question_body":"<p>The following uploads two annotations, though I expected there to be only one<\/p>\n<pre><code>from typing import List\nfrom allegroai import Dataset, DatasetVersion, SingleFrame, DataView\nfrom allegroai.dataframe.annotation import BoundingBox2D\n\nallegro_frame = SingleFrame(\n    source=&quot;\/irrelevant\/source.png&quot;\n)\nann_id = &quot;the_id&quot;\nlabel = &quot;the_label&quot;\nannotation = BoundingBox2D(id=ann_id)\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\n\nallegro_frames: List[SingleFrame] = [\n    allegro_frame\n]\n\ndataset_name = r&quot;clml_test_dataset&quot;\nversion_name = r&quot;clml_test_version&quot;\ndataset = Dataset.create(dataset_name=dataset_name)\nversion = DatasetVersion.create_version(dataset_name=dataset_name, version_name=version_name)\nversion.add_frames(allegro_frames)\n<\/code><\/pre>\n<p>What's the correct way to make only one annotation be uploaded for the frame?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663158280063,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":5,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73716806",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62654203,
        "Question_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Question_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593508903533,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":228,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609467668432,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1593515579540,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1593557456892,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66279581,
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613745436903,
        "Question_score":1,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":279,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614004159640,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1613773903383,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72593187,
        "Question_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655044800580,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":49,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657799581543,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72670966,
        "Question_title":"clearml-serving-triton | 2022-06-18 16:21:19,790 - clearml.Task - INFO - No repository found, storing script code instead",
        "Question_body":"<p>i found this error when running this command &quot;docker-compose --env-file example.env -f docker-compose-triton.yml up&quot;.\nActually, when i run this command for the first time, it worked. And then when I try to change to my friend's workspace it suddenly give this error when I run the command again.\nI've tried changing example.env file configuration and reinstall the docker, still not worked.\nCan anyone help? or have the solution? Thank You :)\n<a href=\"https:\/\/i.stack.imgur.com\/Fp8XK.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/Fp8XK.png<\/a>\n[Error details on the image]<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655570110730,
        "Question_score":0,
        "Question_tags":"docker|clearml",
        "Question_view_count":43,
        "Owner_creation_time":1655569987980,
        "Owner_last_access_time":1664004364857,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1655570179420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72670966",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":71432331,
        "Question_title":"ALB host based routing without domain name",
        "Question_body":"<p>I'm trying to configure host based routing in AWS ALB for ClearML server using <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/deploying_clearml\/clearml_server_config.html#configuration-procedures\" rel=\"nofollow noreferrer\">this tutorial<\/a>.\nHowever, I don't have a domain name. So can I only use alb's dns for this routing?<\/p>\n<p>For example, I will have the address as app.<em><strong>.ap-north-east-1.elb.amazonaws.com, api.<\/strong><\/em>.ap-north-east-1.elb.amazonaws.com.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646958308150,
        "Question_score":0,
        "Question_tags":"aws-application-load-balancer|clearml",
        "Question_view_count":190,
        "Owner_creation_time":1538398333233,
        "Owner_last_access_time":1660382572563,
        "Owner_location":"\u014csaka-shi, \u5927\u962a\u5e9c \u65e5\u672c",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432331",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":69127220,
        "Question_title":"how to capture logger values using clearml",
        "Question_body":"<p>I am using clearml for testing algorithms and it works well with library <a href=\"https:\/\/github.com\/DLR-RM\/stable-baselines3\/tree\/f8a08690737000e4d23eb643a21d70486ece038b\" rel=\"nofollow noreferrer\">Stable Baselines 3<\/a>, in which clearml automatically captures all the output and plot them in the Scalars tab.<\/p>\n<p>However, when I switched to another library <a href=\"https:\/\/github.com\/pfnet\/pfrl\" rel=\"nofollow noreferrer\">PFRL<\/a> clearml no longer output anything to the Scalar tab. After looking into the code I found PFRL outputs statistics using <code>logger.info<\/code>, which seemed to be the reason of empty Scalars tab (but there was output in the Console tab).\nI am wondering is there any method that I can make clearml automatically collect them into the Scalars tab.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631247565663,
        "Question_score":0,
        "Question_tags":"python|pytorch|clearml",
        "Question_view_count":97,
        "Owner_creation_time":1631247013317,
        "Owner_last_access_time":1658201267683,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69127220",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56762990,
        "Question_title":"How to manually register a sci-kit model with TRAINS python auto-magical experiment manager?",
        "Question_body":"<p>I'm working mostly with scikit-learn, as far as I understand, the TRAINS auto-magic doesn't catch scikit-learn model store\/load automatically.<\/p>\n\n<p>How do I manually register the model after I have 'pickled' it.<\/p>\n\n<p>For Example:<\/p>\n\n<pre><code>import pickle\nwith open(\"model.pkl\", \"wb\") as file:  \n    pickle.dump(my_model, file)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561503148140,
        "Question_score":1,
        "Question_tags":"python|machine-learning|scikit-learn|trains|clearml",
        "Question_view_count":155,
        "Owner_creation_time":1528727701343,
        "Owner_last_access_time":1648026581957,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1609778626447,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56762990",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70592241,
        "Question_title":"How to Deploy a ClearML Agent in a k8s setting?",
        "Question_body":"<p>I want to deploy a ClearML agent in the kubernetes environment while using the ClearML's Free Tier Demo server.\nI was able to deploy the Agent pod in the k8s cluster with the <code>allegroai\/clearml-agent<\/code> docker image. But was not able to link this agent to the ClearML Demo server.\nCan anyone help me with solving this issue of configuring the API access and secret keys for the k8s pod of CLearML Agent.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641382824047,
        "Question_score":0,
        "Question_tags":"kubernetes|clearml",
        "Question_view_count":112,
        "Owner_creation_time":1641382442503,
        "Owner_last_access_time":1647005909747,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70592241",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63606182,
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598478428030,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":107,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609427499190,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598536233436,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72381916,
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653499480970,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":25,
        "Owner_creation_time":1653498830777,
        "Owner_last_access_time":1662113390107,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659992618003,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64305945,
        "Question_title":"pip install trains fails",
        "Question_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1602431071093,
        "Question_score":1,
        "Question_tags":"python|pip|trains|clearml",
        "Question_view_count":1031,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":1609353956110,
        "Answer_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1602767930968,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1604308183096,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65671395,
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610384778623,
        "Question_score":1,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1610390345336,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73108016,
        "Question_title":"Where do augmentations in ClearML run?",
        "Question_body":"<p>In ClearML Dataviews, it is possible to add <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/hyperdatasets\/dataviews\/#data-augmentation\" rel=\"nofollow noreferrer\">augmentations<\/a>.<\/p>\n<p>Where do these augmentations run?<\/p>\n<p>Options<\/p>\n<ol>\n<li>Original data gets downloaded to local, then runs (on which device? How is multiprocessing handled?)<\/li>\n<li>Only augmented data gets downloaded to local cache, augmentations run remotely (who pays for compute? How fast? Should pipelines be changed accordingly?)<\/li>\n<\/ol>\n<p>I couldn't find this in the docs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658746321690,
        "Question_score":0,
        "Question_tags":"deep-learning|data-augmentation|mlops|clearml",
        "Question_view_count":22,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73108016",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":57557070,
        "Question_title":"trains with grid search",
        "Question_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566218806593,
        "Question_score":1,
        "Question_tags":"python|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1416942229380,
        "Owner_last_access_time":1663871932353,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1609778640296,
        "Answer_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1566240704540,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1566242122212,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70397010,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":194,
        "Owner_creation_time":1600124498003,
        "Owner_last_access_time":1661436804020,
        "Owner_location":null,
        "Owner_reputation":46,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640162037740,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":58656998,
        "Question_title":"Parallel Coordinates Plot in TRAINS",
        "Question_body":"<p>Is there a way to create a parallel coordinates plot in TRAINS (<a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">https:\/\/github.com\/allegroai\/trains<\/a>) package to compare several hyper-parameters in respect to a specific metric?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572601152533,
        "Question_score":2,
        "Question_tags":"python|deep-learning|trains|clearml",
        "Question_view_count":57,
        "Owner_creation_time":1374041402270,
        "Owner_last_access_time":1579814770290,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1609781210992,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58656998",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66216294,
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613428648627,
        "Question_score":2,
        "Question_tags":"clearml",
        "Question_view_count":276,
        "Owner_creation_time":1585870038407,
        "Owner_last_access_time":1626728054187,
        "Owner_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Owner_reputation":45,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1613774515323,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56744397,
        "Question_title":"Is there a way to create a graph comparing hyper-parameters vs model accuracy with TRAINS python package?",
        "Question_body":"<p>I would like to run multiple experiments, then report model accuracy per experiment.<\/p>\n\n<p>I'm training a toy MNIST example with pytorch (v1.1.0), but the goal is, once I can compare performance for the toy problem, to have it integrated with the actual code base.<\/p>\n\n<p>As I understand the TRAINS python package, with the \"two lines of code\" all my hyper-parameters are already logged (Command line argparse in my case). <\/p>\n\n<p>What do I need to do in order to report a final scalar and then be able to sort through all the different training experiments (w\/ hyper-parameters) in order to find the best one.<\/p>\n\n<p>What I'd like to get, is a graph\/s where on the X-axis I have hyper-parameter values and on the Y-axis I have the validation accuracy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561413272967,
        "Question_score":1,
        "Question_tags":"python|deep-learning|pytorch|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_time":1342883896543,
        "Owner_last_access_time":1577391054037,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1609861256560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56744397",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":68798737,
        "Question_title":"ClearML How to get configurable hyperparameters?",
        "Question_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1629097990217,
        "Question_score":0,
        "Question_tags":"devops|mlops|clearml|trains",
        "Question_view_count":110,
        "Owner_creation_time":1616008398583,
        "Owner_last_access_time":1652343370277,
        "Owner_location":"Islamabad, Pakistan",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1629122761020,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66640850,
        "Question_title":"How to manage datasets in ClearML Web UI?",
        "Question_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615821875663,
        "Question_score":3,
        "Question_tags":"clearml|trains",
        "Question_view_count":310,
        "Owner_creation_time":1551960780550,
        "Owner_last_access_time":1663614230163,
        "Owner_location":null,
        "Owner_reputation":354,
        "Owner_up_votes":54,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Question_last_edit_time":1615822213440,
        "Answer_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1615831186403,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":71099048,
        "Question_title":"clearml hyperparameter optimization task fails on creating python environment",
        "Question_body":"<p>Trying to use clearml HyperparametersOptimizers. Launching task, but each experiment fails while trying to create python environment. Specificaly the error I get is related to pytorch<\/p>\n<pre><code>clearml_agent: Warning: could not resolve python wheel replacement for torch==1.1.0\nException when trying to resolve python wheel: Could not find pytorch wheel URL for: torch==1.1.0 with cuda 113 support\nclearml_agent: ERROR: Could not install task requirements!\nException when trying to resolve python wheel: Could not find pytorch wheel URL for: torch==1.1.0 with cuda 113 support\n<\/code><\/pre>\n<p>I personally don't need pytorch for the code I am running and I removed it from the environment I am using, but the pytorch channel is present somewhere as it still tries to download it:<\/p>\n<pre><code>Executing Conda: &quot;C:\\Users\\nir.s\\Anaconda3\\Scripts\\conda.exe&quot; install -p &quot;C:\\Users\\nir.s\\.clearml\\venvs-builds\\3.7&quot; -c defaults -c conda-forge -c pytorch &quot;pip&lt;20.2&quot; --quiet --json\n<\/code><\/pre>\n<p>I am not sure if there is some configuration file that I missed or if it's because there are other parts of the repository that use pytorch, either way, as things are now I can't run tasks with clearml-agent since it fails to build an interpreter. Just to clarify, I do have environments working with pytorch for separate projects that use it. So the problem is not with anaconda or pip as it is installed on my computer since I can make it work when running locally.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644740916897,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":95,
        "Owner_creation_time":1629265578777,
        "Owner_last_access_time":1651657361480,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71099048",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73279794,
        "Question_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Question_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659970052813,
        "Question_score":1,
        "Question_tags":"python|catboost|clearml",
        "Question_view_count":54,
        "Owner_creation_time":1659969003173,
        "Owner_last_access_time":1663967691430,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659991938292,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56768436,
        "Question_title":"How to Backup\/Restore TRAINS-server when moving from AMI to local machine",
        "Question_body":"<p>I recently started using TRAINS, with the server in AWS AMI. We are currently using v0.9.0.<\/p>\n\n<p>I would like to move the TRAINS-server to run on our on-premises kubernetes cluster. However, I don't want to lose the data on the current server in AWS (experiments, models, logins, etc...).\nIs there a way to backup the current server and restore it to the local server?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561537653917,
        "Question_score":1,
        "Question_tags":"kubernetes|deep-learning|trains|clearml",
        "Question_view_count":2703,
        "Owner_creation_time":1478632397167,
        "Owner_last_access_time":1561548101617,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1609861259227,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56768436",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":61642549,
        "Question_title":"How to disable crash output in comet ml?",
        "Question_body":"<p>Every time my script is crashing I have such output:<\/p>\n\n<pre><code>COMET INFO:     sys.cpu.percent.43       : (0.0, 0.0)\nCOMET INFO:     sys.cpu.percent.44       : (1.8, 1.8)\nCOMET INFO:     sys.cpu.percent.avg      : (6.579545454545454, 6.579545454545454)\nCOMET INFO:     sys.gpu.0.free_memory    : (34089664512.0, 34089664512.0)\nCOMET INFO:     sys.gpu.0.gpu_utilization: (0.0, 0.0)\nCOMET INFO:     sys.gpu.0.total_memory   : (34089730048.0, 34089730048.0)\nCOMET INFO:     sys.gpu.0.used_memory    : (65536.0, 65536.0)\nCOMET INFO:     sys.load.avg             : (39.42, 39.42)\nCOMET INFO:     sys.ram.total            : (1621711745024.0, 1621711745024.0)\nCOMET INFO:     sys.ram.used             : (78552326144.0, 78552326144.0)\nCOMET INFO:   Other [count]:\nCOMET INFO:     offline_experiment: True\n<\/code><\/pre>\n\n<p>How can I disable it?<\/p>\n\n<p>In comet-ml docs I found but probably it's not what I look for:<\/p>\n\n<blockquote>\n  <p>by setting the environmental variable COMET_DISABLE_AUTO_LOGGING to 1<\/p>\n<\/blockquote>\n\n<pre><code>$ export COMET_DISABLE_AUTO_LOGGING=1                                                                                             \n$ echo $COMET_DISABLE_AUTO_LOGGING \n1\n<\/code><\/pre>\n\n<p>But it didn't help me.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1588789562853,
        "Question_score":1,
        "Question_tags":"python|pytorch|comet|comet-ml",
        "Question_view_count":94,
        "Owner_creation_time":1446931610417,
        "Owner_last_access_time":1662985332847,
        "Owner_location":"Moscow, \u0420\u043e\u0441\u0441\u0438\u044f",
        "Owner_reputation":7659,
        "Owner_up_votes":395,
        "Owner_down_votes":3,
        "Owner_views":776,
        "Question_last_edit_time":1588796193240,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61642549",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":56881430,
        "Question_title":"Is it possible to visualize gradients in comet-ml?",
        "Question_body":"<p>It is straightforward to set up TensorBoard in Keras (it's just a <a href=\"https:\/\/keras.io\/callbacks\/#tensorboard\" rel=\"noreferrer\">callback<\/a>!) and then it is possible to visualize the distribution and magnitude of the weights and gradients. Is it possible to do the same with <a href=\"https:\/\/www.comet.ml\" rel=\"noreferrer\">comet.ml<\/a>? Comet.ml is easy to set up, but visualizes only the loss and accuracy evolution... Is there a way to \"restore\" all the TensorBoard features in comet.ml?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1562219704473,
        "Question_score":6,
        "Question_tags":"python-3.x|tensorflow|keras|tf.keras|comet-ml",
        "Question_view_count":117,
        "Owner_creation_time":1557443756590,
        "Owner_last_access_time":1664031547383,
        "Owner_location":null,
        "Owner_reputation":606,
        "Owner_up_votes":589,
        "Owner_down_votes":211,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881430",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":69642600,
        "Question_title":"Conda UnsatisfiableError of glibc when trying to install Comet ML 3.18 in Dockerfile",
        "Question_body":"<p>I'm trying to install Comet ML version <code>3.18<\/code> in a Dockerfile using Anaconda but the build fails with the following error:<\/p>\n<pre><code>Found conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\nfailed\n\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\n\nOutput in format: Requested package -&gt; Available versionsThe following specifications were found to be incompatible with your system:\n\n  - feature:\/linux-64::__glibc==2.31=0\n  - python=3.8 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']\n\nYour installed version is: 2.31\n<\/code><\/pre>\n<p>Isn't my installed version exactly what is needed according to these lines?! I've googled this issue over and over again but couldn't find a solution that's why I'm now asking it here. Sorry if this is a duplicate but it's driving me nuts! (Had a similar issue trying to install Comet ML on the cluster directly using Anaconda - without Docker - and there the error was with <code>glibc 2.17<\/code>.)<\/p>\n<p>Any help is appreciated!<\/p>\n<hr \/>\n<p>Here's the <strong>full Dockerfile<\/strong>:<\/p>\n<pre><code>FROM continuumio\/anaconda3:latest\n\nRUN conda install -c comet_ml comet_ml=3.18 -y\n\nENTRYPOINT [ &quot;\/bin\/bash&quot; ]\n<\/code><\/pre>\n<p>I also have a version where I install Anaconda manually in Ubuntu 20.04 but it throws the same error and I thought a more concise Dockerfile would be nicer.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1634718135840,
        "Question_score":1,
        "Question_tags":"python|docker|glibc|anaconda3|comet-ml",
        "Question_view_count":499,
        "Owner_creation_time":1389012600070,
        "Owner_last_access_time":1663689374297,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":3054,
        "Owner_up_votes":299,
        "Owner_down_votes":4,
        "Owner_views":487,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69642600",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":47855236,
        "Question_title":"How to configure comet (comet.ml) to create pull requests on GitHub?",
        "Question_body":"<p>Ive followed this this to link my comet.ml project to GitHub - <a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/blob\/master\/github-pullrequest\/README.md\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n\n<p>and had some models already trained (using keras)in my project<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Ive linked my GitHub account and when creating a pull request I get error <\/p>\n\n<p><strong>Cant create pull request<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>please advise<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1513514834923,
        "Question_score":1,
        "Question_tags":"github|keras|comet-ml",
        "Question_view_count":73,
        "Owner_creation_time":1389089640833,
        "Owner_last_access_time":1513514332643,
        "Owner_location":null,
        "Owner_reputation":303,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47855236",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":60839122,
        "Question_title":"ModuleNotFoundError: No module named 'comet_ml'",
        "Question_body":"<p>I am working in a conda environment. If i do<\/p>\n\n<p>(pytorch_env)  sudo pip3 freeze | grep comet<\/p>\n\n<p>I get<\/p>\n\n<p>comet-git-pure==0.19.15\ncomet-ml==3.1.3<\/p>\n\n<p>But on running the python file I get<\/p>\n\n<p>pytorch_env) ubuntu@ Traceback (most recent call last):\n  File \"vgg_11.py\", line 5, in \n    from comet_ml import Experiment \nModuleNotFoundError: No module named 'comet_ml'<\/p>\n\n<p>The code is just this <\/p>\n\n<pre><code>from comet_ml import Experiment\nfrom comet_ml.utils import ConfusionMatrix\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585083094440,
        "Question_score":1,
        "Question_tags":"python-3.x|comet-ml",
        "Question_view_count":507,
        "Owner_creation_time":1458546810273,
        "Owner_last_access_time":1655861032703,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1585083746283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60839122",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46368389,
        "Question_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Question_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506094686057,
        "Question_score":3,
        "Question_tags":"tensorflow|machine-learning|comet-ml",
        "Question_view_count":338,
        "Owner_creation_time":1506066897167,
        "Owner_last_access_time":1506108624433,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1514341154200,
        "Answer_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1506100257932,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1513514205487,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":61239274,
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586987562523,
        "Question_score":0,
        "Question_tags":"crash|google-colaboratory|freeze|comet-ml",
        "Question_view_count":1013,
        "Owner_creation_time":1493314794173,
        "Owner_last_access_time":1658745180780,
        "Owner_location":"Germany",
        "Owner_reputation":844,
        "Owner_up_votes":241,
        "Owner_down_votes":6,
        "Owner_views":170,
        "Question_last_edit_time":1587130797008,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1587034953183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46359436,
        "Question_title":"How to configure comet (comet.ml) to track Keras?",
        "Question_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506066265147,
        "Question_score":3,
        "Question_tags":"python|keras|comet|comet-ml",
        "Question_view_count":1208,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506066568087,
        "Answer_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1506066553020,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1513514919392,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46352435,
        "Question_title":"comet (comet-ml) fails to run with Keras",
        "Question_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506024659377,
        "Question_score":2,
        "Question_tags":"python|tensorflow|keras|comet|comet-ml",
        "Question_view_count":601,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506491876827,
        "Answer_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1506026797448,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1506066589407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":64640949,
        "Question_title":"How to suppress \"Comet.ml Experiment Summary\" console output in the beginning",
        "Question_body":"<p>I'm using <code>comet.ml<\/code> to display learning results of my machine learning models on <code>jupyter<\/code>.<\/p>\n<p>Initializing an experiment as follows<\/p>\n<pre><code>experiment = Experiment(project_name='...', auto_metric_logging=False)\nexperiment.add_tag('...')\n<\/code><\/pre>\n<p>the following information is shown (with light red color background) in the beginning every time:<\/p>\n<pre><code>COMET INFO: ----------------------------\nCOMET INFO: Comet.ml Experiment Summary:\nCOMET INFO:   Data:\nCOMET INFO:     url: https:\/\/www.comet.ml\/...\nCOMET INFO:   Metrics [count] (min, max):\nCOMET INFO:     loss_D_fake [210]            : (1.4689682722091675, 1.999974250793457)\n... 50+ lines ...\nCOMET INFO:     sys.ram.total [8]            : (269603381248.0, 269603381248.0)\nCOMET INFO:     sys.ram.used [8]             : (60270231552.0, 64912928768.0)\nCOMET INFO:   Uploads:\nCOMET INFO:     git-patch: 1\nCOMET INFO: ----------------------------\nCOMET INFO: Experiment is live on comet.ml https:\/\/www.comet.ml\/...\n<\/code><\/pre>\n<p>All I want to see is the last line that shows the URL for this experiment and that seems not to be included in the 'Comet.ml Experiment Summary' block. How can we suppress other information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604299707030,
        "Question_score":0,
        "Question_tags":"python|jupyter|comet-ml",
        "Question_view_count":156,
        "Owner_creation_time":1469364547800,
        "Owner_last_access_time":1663732990757,
        "Owner_location":"Kyoto Prefecture, Japan",
        "Owner_reputation":343,
        "Owner_up_votes":219,
        "Owner_down_votes":15,
        "Owner_views":60,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64640949",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":73435172,
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661090494277,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661173080163,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73602927,
        "Question_title":"Best method to compress large dataset features",
        "Question_body":"<p>10TB featurs which stored on aws s3 bucket.\nin order to decrease costs , we are looking for a loseless compression method for those features (current features are images but it can changed between projects)<\/p>\n<p>I heard about hdf5, but is it the best method nowadays?\nCan the the extraction be paralleled?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1662326892390,
        "Question_score":1,
        "Question_tags":"python|compression|hdf5|dvc",
        "Question_view_count":37,
        "Owner_creation_time":1573845661880,
        "Owner_last_access_time":1664058289083,
        "Owner_location":"Israel",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73602927",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61386810,
        "Question_title":"Data version control (DVC) edit files in place results in cyclic dependency",
        "Question_body":"<p>we have a larger dataset and have several preprocessing scripts.\nThese scripts alter data in place.\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).\nI would assume this is a very common use case.<\/p>\n\n<p>What is the best practice here ?<\/p>\n\n<p>Tried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1587643471830,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":45,
        "Owner_creation_time":1530036936153,
        "Owner_last_access_time":1656059958773,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":121,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61386810",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67505910,
        "Question_title":"Problem running a Docker container in Gitlab CI\/CD",
        "Question_body":"<p>I am trying to build and run my Docker image using Gitlab CI\/CD, but there is one issue I can't fix even though locally everything works well.<\/p>\n<p>Here's my Dockerfile:<\/p>\n<pre><code>FROM &lt;internal_docker_repo_image&gt;\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc\nCOPY requirements.txt \/requirements.txt\n\nRUN pip install --no-cache-dir --user -r \/requirements.txt\n\nCOPY . \/src\nWORKDIR \/src\nENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;dvc&quot;, &quot;repro&quot;]\n<\/code><\/pre>\n<p>This is how I run the container:<\/p>\n<p><code>docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force<\/code><\/p>\n<p>Everything works great when running this locally, but it fails when run on Gitlab CI\/CD.<\/p>\n<pre><code>stages:\n  - build_image\n\nbuild_image:\n  stage: build_image\n  image: &lt;internal_docker_repo_image&gt;\n  script:\n    - echo &quot;Building Docker image...&quot;\n    - mkdir ~\/.docker\n    - cat $GOOGLE_CREDENTIALS &gt; ${CI_PROJECT_DIR}\/key.json\n    - docker build . -t &lt;image_name&gt;\n    - docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force\n  artifacts:\n        paths:\n          - &quot;.\/data\/*csv&quot;\n        expire_in: 1 week\n\n<\/code><\/pre>\n<p>This results in the following error:\n<code>ERROR: you are not inside of a DVC repository (checked up to mount point '\/src')<\/code><\/p>\n<p>Just in case you don't know what DVC is, this is a tool used in machine learning for versioning your models, datasets, metrics, and, in addition, setting up your pipelines, which I use it for in my case.<\/p>\n<p>Essentially, it requires two folders <code>.dvc<\/code> and <code>.git<\/code> in the directory from which <code>dvc repro<\/code> is executed.<\/p>\n<p>In this particular case, I have no idea why it's not able to run this command given that the contents of the folders are exactly the same and both <code>.dvc<\/code> and <code>.git<\/code> exist.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620830695847,
        "Question_score":4,
        "Question_tags":"docker|continuous-integration|gitlab-ci|dvc",
        "Question_view_count":746,
        "Owner_creation_time":1452169170337,
        "Owner_last_access_time":1663996681010,
        "Owner_location":null,
        "Owner_reputation":576,
        "Owner_up_votes":431,
        "Owner_down_votes":4,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67505910",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62529867,
        "Question_title":"Data version control (DVC) commands not working ---> TypeError: public() got an unexpected keyword argument 'SEP'",
        "Question_body":"<p>All of a sudden, dvc has stopped functioning.\nAny command typed fails and throws an exception.\nexample. dvc remote list results in -<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/home\/dev2\/.local\/bin\/dvc&quot;, line 5, in &lt;module&gt;\n    from dvc.main import main\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/main.py&quot;, line 6, in &lt;module&gt;\n    from dvc import analytics\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/analytics.py&quot;, line 16, in &lt;module&gt;\n    from dvc.lock import Lock, LockError\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/lock.py&quot;, line 8, in &lt;module&gt;\n    import flufl.lock\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/__init__.py&quot;, line 3, in &lt;module&gt;\n    from flufl.lock._lockfile import (\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/_lockfile.py&quot;, line 54, in &lt;module&gt;\n    public(SEP=SEP)\nTypeError: public() got an unexpected keyword argument 'SEP'\n \n<\/code><\/pre>\n<p>Any suggestions will be of great help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1592898526370,
        "Question_score":2,
        "Question_tags":"linux|git|dvc",
        "Question_view_count":307,
        "Owner_creation_time":1534165472313,
        "Owner_last_access_time":1663626263697,
        "Owner_location":null,
        "Owner_reputation":449,
        "Owner_up_votes":3,
        "Owner_down_votes":2,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62529867",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70478173,
        "Question_title":"How to track the big data stored in Gdrive through DVC?",
        "Question_body":"<p>I am currently working on the ML project and the data size is around 10 GB. The data I stored in google drive. Its impossible for me to download it on my local machine. So, how to use the DVC (data version control) to track that data? Thank you in advance for your time.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1640410347580,
        "Question_score":2,
        "Question_tags":"machine-learning|version-control|mlops|dvc",
        "Question_view_count":47,
        "Owner_creation_time":1567192416523,
        "Owner_last_access_time":1663279611567,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":76,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1640412287832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70478173",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620170453717,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_time":1379685720450,
        "Owner_last_access_time":1663797728603,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1620171022368,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620171651067,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73084915,
        "Question_title":"How to fix DVC error 'FileNotFoundError: [Errno 2] No such file or directory' in Github actions",
        "Question_body":"<p>Trying to pull a folder with test data into a GitHub actions container, I get<\/p>\n<blockquote>\n<p>FileNotFoundError: [Errno 2] No such file or directory<\/p>\n<\/blockquote>\n<p>I tried running <code>dvc checkout --relink<\/code> locally, but that did not work. I am using Gdrive for the data-repository with a service account. It seems the login works. But strangely the file is not present. Maybe I can push the files again somehow and recreate the data in the repository?<\/p>\n<hr \/>\n<h2><code>dvc doctor<\/code> output:<\/h2>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.7.13 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.14.0),\n    webhdfs (fsspec = 2022.5.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sda1\nRepo: dvc, git\n<\/code><\/pre>\n<p>I was able to clone the GIT repo and pull the data from the DVC data-registry, however, it did not work from GH.<\/p>\n<hr \/>\n<h2>Full traceback<\/h2>\n<pre><code>Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiohttp-retry-2.5.1 aiosignal-1.2.0 appdirs-1.4.4 async-timeout-4.0.2 asynctest-0.13.0 atpublic-2.3 attrs-21.4.0 cached-property-1.5.2 cachetools-4.2.4 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.0.12 colorama-0.4.5 commonmark-0.9.1 configobj-5.0.6 contextvars-2.4 cryptography-37.0.4 dataclasses-0.8 decorator-4.4.2 dictdiffer-0.9.0 diskcache-5.4.0 distro-1.7.0 dpath-2.0.6 dulwich-0.20.45 dvc-2.8.1 flatten-dict-0.4.2 flufl.lock-3.2 frozenlist-1.2.0 fsspec-2022.1.0 ftfy-6.0.3 funcy-1.17 future-0.18.2 gitdb-4.0.9 gitpython-3.1.18 google-api-core-2.8.2 google-api-python-client-2.52.0 google-auth-2.9.1 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.3 grandalf-0.6 httplib2-0.20.4 idna-3.3 idna-ssl-1.1.0 immutables-0.18 importlib-metadata-4.8.3 importlib-resources-5.4.0 mailchecker-4.1.18 multidict-5.2.0 nanotime-0.5.2 networkx-2.5.1 oauth2client-4.1.3 packaging-21.3 pathspec-0.8.1 phonenumbers-8.12.52 ply-3.11 protobuf-3.19.4 psutil-5.9.1 pyOpenSSL-22.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydot-1.4.2 pydrive2-1.10.1 pygit2-1.6.1 pygments-2.12.0 pygtrie-2.5.0 pyparsing-2.4.7 python-benedict-0.25.2 python-dateutil-2.8.2 python-fsutil-0.6.1 python-slugify-6.1.2 requests-2.27.1 rich-12.5.1 rsa-4.9 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 shortuuid-1.0.9 shtab-1.5.5 six-1.16.0 smmap-5.0.0 tabulate-0.8.10 text-unidecode-1.3 toml-0.10.2 tqdm-4.64.0 typing-extensions-4.1.1 uritemplate-4.1.1 urllib3-1.26.10 voluptuous-0.13.1 wcwidth-0.2.5 xmltodict-0.13.0 yarl-1.7.2 zc.lockfile-2.0 zipp-3.6.0\nDVC version: 2.8.1 (pip)\n---------------------------------\nPlatform: Python 3.6.15 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.10.1),\n    webhdfs (fsspec = 2022.1.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sdb1\nRepo: dvc, git\n2022-07-22 19:01:08,361 DEBUG: failed to pull cache for 'tests\/data'\n2022-07-22 19:01:08,364 WARNING: No file hash info found for 'tests\/data'. It won't be created.\n1 file failed\n2022-07-22 19:01:08,365 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/pull.py&quot;, line 44, in pull\n    recursive=recursive,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/checkout.py&quot;, line 110, in checkout\n    raise CheckoutError(stats[&quot;failed&quot;], stats)\ndvc.exceptions.CheckoutError: Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\n2022-07-22 19:01:08,369 DEBUG: Analytics is enabled.\n2022-07-22 19:01:08,412 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\n2022-07-22 19:01:08,413 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\nError: The operation was canceled.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1658515971860,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":209,
        "Owner_creation_time":1444959339707,
        "Owner_last_access_time":1663889479043,
        "Owner_location":null,
        "Owner_reputation":6513,
        "Owner_up_votes":1296,
        "Owner_down_votes":60,
        "Owner_views":1057,
        "Question_last_edit_time":1658518512243,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73084915",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67421254,
        "Question_title":"How to add a file to a dvc-tracked folder without pulling the whole folder's content?",
        "Question_body":"<p>Let's say I am working inside a git\/dvc repo. There is a folder <code>data<\/code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc add data\n<\/code><\/pre>\n<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.<\/p>\n<p>I clone the repo on another workspace, and now I have the <code>data.dvc<\/code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt<\/code> to the <code>data<\/code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data<\/code> locally<\/em> ?<\/p>\n<p>What I have tried for now:<\/p>\n<ol>\n<li><p>Adding the <code>data<\/code> folder again:<\/p>\n<pre><code>mkdir data\nmv path\/to\/newfile.txt data\/newfile.txt\ndvc add data\n<\/code><\/pre>\n<p>The <code>data.dvc<\/code> file is built again from the local state of <code>data<\/code> which only contains <code>newfile.txt<\/code> so this doesn't work.<\/p>\n<\/li>\n<li><p>Adding the file as a single element in <code>data<\/code> folder:<\/p>\n<pre><code> dvc add data\/newfile.txt\n<\/code><\/pre>\n<p>I get :<\/p>\n<pre><code> Cannot add 'data\/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. \n To include 'data\/newfile.txt' in 'data', run 'dvc commit data.dvc'\n<\/code><\/pre>\n<\/li>\n<li><p>Using dvc commit as suggested<\/p>\n<pre><code> mkdir data\n mv path\/to\/newfile.txt data\/newfile.txt\n dvc commit data.dvc\n<\/code><\/pre>\n<p>Similarly as 1., the <code>data.dvc<\/code> is rebuilt again from local state of <code>data<\/code>.<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620314719220,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1781,
        "Owner_creation_time":1620311720937,
        "Owner_last_access_time":1626773929723,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1620314912032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67421254",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_time":1578574709920,
        "Owner_last_access_time":1648636957603,
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1627513406643,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72622280,
        "Question_title":"How does one add individual files with DVC?",
        "Question_body":"<p>Suppose I run the following commands:<\/p>\n<pre><code># set up DVC\n\nmkdir foo\ncd foo &amp;&amp; git init\ndvc init\ngit add * &amp;&amp; git commit -m &quot;dvc init&quot;\n\n\n# make a data file\n\nmkdir -p bar\/biz\ntouch bar\/biz\/boz\n\n\n# add the data file\n\ndvc add bar\/biz\/boz\n<\/code><\/pre>\n<p>And DVC outputs the following:<\/p>\n<pre><code>To track the changes with git, run:\n\n  git add bar\/biz\/.gitignore bar\/biz\/boz.dvc\n<\/code><\/pre>\n<hr \/>\n<p>This last part is what I would like to avoid.  Preferably, DVC would only change the top level <code>.gitignore<\/code> (located at the project root, where <code>git init<\/code> was executed), and will change only DVC files at the top level.<\/p>\n<p><strong>And here's why:<\/strong><\/p>\n<p>I have a rather large dataset developed in an original work more or less ad-hoc. This data is not systematically organized, nor do I want to organize it as-is.<\/p>\n<p>Instead, I want to incrementally add this old, bespoke data to the DVC directory tree.  And each time I add some of the data to the tree, I want to check it in with DVC as I would if I were modifying code or mixing one project's code into another.<\/p>\n<p>However, DVC wants to create a local file and gitignore at every location I add.  This creates a mess and I have no reasonable faith that it will be easy to maintain all of these atomic and distributed datastores.<\/p>\n<hr \/>\n<p><strong>The question:<\/strong><\/p>\n<p>What is the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655234204993,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":112,
        "Owner_creation_time":1405262190020,
        "Owner_last_access_time":1663968225423,
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Assuming bar\/ is the dataset directory you're incrementally adding to, you can instead<\/p>\n<pre><code>dvc add bar\n<\/code><\/pre>\n<p>This creates a bar.dvc file and writes to .gitignore at the top level.<\/p>\n<p>When you update content in bar\/, <code>dvc add<\/code> it again or use <code>dvc commit<\/code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5<\/code> hash that identifies to the latest directory structure.<\/p>\n<p>Some docs:<br \/>\n<a href=\"https:\/\/dvc.org\/doc\/start\/data-management#making-changes\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/start\/data-management#making-changes<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/add<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1655349731160,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655350036863,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72622280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72069555,
        "Question_title":"Second dvc push on AWS Batch using IAM role gets \"Unable to locate credentials\"",
        "Question_body":"<p>I'm running a job on AWS Batch, and this job prepares some data and versions it using <code>dvc<\/code>. Secondly, the job does some transformation generating new data, and it should save this new data using <code>dvc<\/code> again. Also, in this case, i'm setting a instance-profile role to enable the AWS Batch to persist on my S3 bucket.<\/p>\n<p>The first <code>dvc push<\/code> works perfectly. But the second one generates the error <code>Unable to locate credentials<\/code><\/p>\n<p>I have also changed the script to just touch a file, add to dvc and push, and then repeat the process in with other file, and could replicate the problem.<\/p>\n<p>I have already solved, changing the command <code>dvc push<\/code> to <code>dvc push especific-file-to-push<\/code>, but I'm now trying to understand what is the problem with <code>dvc push<\/code> command without the parameter specifying the file.<\/p>\n<p>Does anybody know?<\/p>\n<p>I'm using dvc <code>dvc==2.9.5<\/code> and <code>boto3==1.21.21<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1651330586727,
        "Question_score":2,
        "Question_tags":"amazon-s3|boto3|amazon-iam|aws-batch|dvc",
        "Question_view_count":152,
        "Owner_creation_time":1379631386420,
        "Owner_last_access_time":1663244472720,
        "Owner_location":"Rio de Janeiro, Brazil",
        "Owner_reputation":101,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1651331434543,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72069555",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72333237,
        "Question_title":"Why I got an invalid bucket name error using dvc mlflow on macos",
        "Question_body":"<p>Could anyone tell what's the reason for error:<\/p>\n<p>botocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]<\/em>:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9-]{1,63}$&quot;<\/p>\n<p>I try to use mlflow with docker.\n.env file contains:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/127.0.0.1:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>Also tried to use:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/localhost:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>docker-compose contains:<\/p>\n<pre><code>... \n   mlflow:\n        restart: always\n        image: mlflow_server\n        container_name: mlflow_server\n        ports:\n          - &quot;5000:5000&quot;\n        networks:\n          - postgres\n          - s3\n        environment:\n          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n          - MLFLOW_S3_ENDPOINT_URL=http:\/\/nginx:9000\n        command: mlflow server --backend-store-uri postgresql:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@db\/${POSTGRES_DB} --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/ --host 0.0.0.0\n...\n<\/code><\/pre>\n<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1653167453863,
        "Question_score":0,
        "Question_tags":"docker|mlflow|mlops|dvc",
        "Question_view_count":117,
        "Owner_creation_time":1577631947330,
        "Owner_last_access_time":1663676491067,
        "Owner_location":"Saint Petersburg",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1653372987987,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72333237",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":53213596,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_time":1420765480790,
        "Owner_last_access_time":1663614206113,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549414531500,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56818930,
        "Question_title":"\"dvc push\" after several local commits",
        "Question_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561823734517,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":916,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1561842649416,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561847539150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66505949,
        "Question_title":"problem with dvc import-url from google spreadsheet export",
        "Question_body":"<p>I'm in the process of converting a Makefile-based data workflow to dvc. I have a Google spreadsheet that I'm using in a data workflow to make it easy to update a few things in a makeshift database. Currently this works with something like this:<\/p>\n<pre><code># Makefile\ndata.csv:\n    curl -L https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv &gt; data.csv\n<\/code><\/pre>\n<p>Of course, I can incorporate the same step into my dvc pipeline directly with <code>dvc run<\/code>, but my understanding is that something like <code>dvc import-url<\/code> would be more appropriate but I'm getting an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import-url https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv data.csv\nImporting 'https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv' -&gt; 'data.csv'\nERROR: unexpected error - 'NoneType' object has no attribute 'endswith'\n<\/code><\/pre>\n<p>My guess is that this is because the response data from the Google Spreadsheet export url doesn't have a filename suffix associated with it. Is there a way to work around this problem? Is there a better way to pull data from a google spreadsheet into a dvc workflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1615035920157,
        "Question_score":2,
        "Question_tags":"google-sheets|google-sheets-api|dvc",
        "Question_view_count":59,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66505949",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58163305,
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1569833846627,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569837069043,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61816429,
        "Question_title":"DVC dependencies for derived data without imports",
        "Question_body":"<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.<\/p>\n\n<p><strong>My question:<\/strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import<\/code> to achieve this.<\/p>\n\n<p><strong>Details:<\/strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.<\/p>\n\n<p>Currently my Idea is to <code>dvc get<\/code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.<\/p>\n\n<pre><code>hugeFileRepo\n +metaData.csv\n +dataFolder\n +-- hugeFile_1\n ...\n +-- hugeFile_n\n<\/code><\/pre>\n\n<p>in the derivedData repository I do<\/p>\n\n<pre><code> dvc import hugeFileRepo.git metaData.csv\n dvc run -f derivedData.dvc \\\n    -d metaData.csv \\\n    -d deriveData.py \\\n    -o derivedDataFolder \\\n    python deriveData.py \n<\/code><\/pre>\n\n<p>My deriveData.py does something along the line (pseudocode)<\/p>\n\n<pre><code>metaData = read(metaData.csv)\n\n#Hack because I don't know how to it right:\ngitRevision = getGitRevision(metaData.csv.dvc)          \n...\nfor metaDataForFile, file in metaData:\n   if(iWantFile(metaDataForFile) ):\n      #download specific file\n      !dvc get --rev {gitRevision} -o tempFolder\/{file} hugeFileRepo.git {file}\n\n      #do processing of huge file and store result in derivedDataFolder\n      processAndWrite(tempFolder\/file)\n<\/code><\/pre>\n\n<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589536322927,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":179,
        "Owner_creation_time":1453724533500,
        "Owner_last_access_time":1663059905923,
        "Owner_location":null,
        "Owner_reputation":171,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61816429",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60527213,
        "Question_title":"How does DVC store differences on the directory level into DVC cache?",
        "Question_body":"<p>Can someone explain how DVC stores differences on the directory level into DVC cache. <\/p>\n\n<p>I understand that the DVC-files (.dvc) are metafiles to track data, models and reproduce pipeline stages. However, it is not clear for me how the process of creating branches, commiting them and switching back to a master file is exactly saved in differences. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583328485690,
        "Question_score":4,
        "Question_tags":"version-control|dvc",
        "Question_view_count":623,
        "Owner_creation_time":1583308412843,
        "Owner_last_access_time":1587640257240,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1583332201396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60527213",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643641422880,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_time":1457469301700,
        "Owner_last_access_time":1663223040440,
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1643651143316,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67122683,
        "Question_title":"DVC Files Incomplete",
        "Question_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618565400113,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":548,
        "Owner_creation_time":1348150034833,
        "Owner_last_access_time":1663852884933,
        "Owner_location":"Glasgow, UK",
        "Owner_reputation":2400,
        "Owner_up_votes":66,
        "Owner_down_votes":12,
        "Owner_views":263,
        "Question_last_edit_time":1618826341416,
        "Answer_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1618566517710,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65771384,
        "Question_title":"Data Version Control (dvc) cannot push to remote storage because querying cache",
        "Question_body":"<p>I am setting up a remote storage with dvc using webdavs<\/p>\n<p>I can connect to the remote storage from Finder.<\/p>\n<p>I added the new remote and I see it when I check (dvc remote list)<\/p>\n<p>But when I try to push data, I have the request for password with 0% Querying cache<\/p>\n<p>It stays 0% forever. And when I enter the password, it ends with the following error:<\/p>\n<p>ERROR: unexpected error - No connection with LINK_OF_REMOTE_STORAGE<\/p>\n<p>The only thing I am thinking about is how to check if I can connect to the server from dvc and why querying cache never ends (maybe never starts even)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1610960223987,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":436,
        "Owner_creation_time":1411637192833,
        "Owner_last_access_time":1648687432467,
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1610961648252,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65771384",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67104752,
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_body":"<p>I'm using dvc and connecting to remote S3 for data track and also setting remote dvc cache in same remote S3.\nFollowing is configure file,<\/p>\n<pre><code>[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n[\u2018remote \u201cs3remote\u201d\u2019]\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n[\u2018remote \u201cs3cache\u201d\u2019]\n    url = s3:\/\/dvc-example\/cache\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>I'm able to push and pull from remote repository to local using s3remote.<\/p>\n<p>But when I try to add external data by configuring cache(s3cache), am getting error.<\/p>\n<p>Both s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618474768363,
        "Question_score":4,
        "Question_tags":"git|amazon-web-services|machine-learning|amazon-s3|dvc",
        "Question_view_count":352,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":1618497721960,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67104752",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58511529,
        "Question_title":"Highlight.js not respecting parent regex of a sub mode",
        "Question_body":"<p>I need to write a lexer which highlights my command-line tool commands properly.<\/p>\n\n<pre><code>$ dvc add file.csv\n$ dvc pipeline list\n<\/code><\/pre>\n\n<p>So the command starts with <code>dvc<\/code> and it may have one or two subcommands - <code>add<\/code> or <code>pipeline list<\/code> respectively.<\/p>\n\n<p>Therefore, it should highlight <code>dvc add<\/code> and <code>dvc pipeline list<\/code> in first and second case respectively.<\/p>\n\n<pre><code>contains: [\n          {\n            begin: \/^\\s*\\$\\s(dvc|git) [a-z-]+\/,\n            returnBegin: true,\n            contains: [\n              {\n                begin: \/dvc [a-z-]+ ?\/,\n                lexemes: '[a-z-]+',\n                keywords: {\n                  built_in:\n                    'dvc'\n                },\n                contains: [\n                  {\n                    begin: \/\\w+(?![\\S])\/,\n                    keywords: {\n                      built_in: 'list'\n                    }\n                  }\n                ],\n                className: 'strong'\n              }\n            ]\n          }\n        ]\n<\/code><\/pre>\n\n<p>It matches <code>dvc pipeline list<\/code> even though the parent regex i.e. <code>\/^\\s*\\$\\s(dvc|git) [a-z-]+\/<\/code> should only match till <code>dvc pipeline<\/code>. How is it exactly functioning?<\/p>\n\n<p>How does <code>\/dvc [a-z-]+ ?\/<\/code> override it and continues matching the expression?<\/p>\n\n<p>Please refer to this library docs here: <a href=\"https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html\" rel=\"nofollow noreferrer\">https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1571774550387,
        "Question_score":3,
        "Question_tags":"javascript|syntax-highlighting|highlight|highlight.js|dvc",
        "Question_view_count":177,
        "Owner_creation_time":1562529879143,
        "Owner_last_access_time":1625679183400,
        "Owner_location":null,
        "Owner_reputation":209,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":1572048186932,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58511529",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71378280,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_score":1,
        "Question_tags":"git|dataset|google-colaboratory|dvc",
        "Question_view_count":707,
        "Owner_creation_time":1525227015313,
        "Owner_last_access_time":1663842020310,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1652856778060,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1647022114532,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654718868090,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_time":1562403039337,
        "Owner_last_access_time":1659463717997,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Question_last_edit_time":1655157056467,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1654875420463,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_time":1558529684193,
        "Owner_last_access_time":1642243398997,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1594640021920,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1587029761967,
        "Answer_score":6.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72645665,
        "Question_title":"How to track a folder again when used \"git rm -rf --cached folder_name\" : Error: The following paths are ignored by one of your .gitignore files",
        "Question_body":"<p>I wanted to un-track my git files so I put <code>.dvc<\/code> inside my <code>.gitignore<\/code> file, and run<\/p>\n<pre><code>git rm -rf --cached .dvc\n<\/code><\/pre>\n<p>and then committed.<\/p>\n<p>I realised my mistake soon and then wanted to add the files again . I tried deleting the <code>gitignore<\/code> file, commit, make a new <code>.gitignore<\/code> and then try adding but all is futile. <code>git add .dvc<\/code> does not track my files and using <code>git add .dvc\/*<\/code> gives me error:<\/p>\n<pre><code>The following paths are ignored by one of your .gitignore files:\n.dvc\/cache\n.dvc\/tmp\nUse -f if you really want to add them.\n<\/code><\/pre>\n<p>Running the command <code>git check-ignore -v .dvc\/*<\/code> gives me:<\/p>\n<pre><code>.dvc\/.gitignore:3:\/cache    .dvc\/cache\n.dvc\/.gitignore:2:\/tmp  .dvc\/tmp\n\n<\/code><\/pre>\n<p>What can be done now?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1655381585170,
        "Question_score":1,
        "Question_tags":"git|gitignore|dvc",
        "Question_view_count":41,
        "Owner_creation_time":1561995857583,
        "Owner_last_access_time":1664035105223,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":2716,
        "Owner_up_votes":403,
        "Owner_down_votes":26,
        "Owner_views":668,
        "Question_last_edit_time":1655382233167,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72645665",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70939675,
        "Question_title":"DVC Push KeyError fileSize",
        "Question_body":"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with<\/p>\n<pre><code>ERROR: unexpected error - KeyError('fileSize')\n<\/code><\/pre>\n<p><strong>Edit<\/strong>\nSo searching around it seem that it might help to include the verbose log with regards to the error.<\/p>\n<pre><code>T11:27:08~\/documents\/*****\/data$ dvc push -v\n2022-02-01 11:32:13,186 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/config.local' to gitignore file.\n2022-02-01 11:32:13,199 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp' to gitignore file.\n2022-02-01 11:32:13,200 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to gitignore file.\n2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to '*********'\n2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'\n2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'\n2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files\n2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse\n2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 226, in __getitem__\n    return dict.__getitem__(self, key)\nKeyError: 'fileSize'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 56, in push\n    pushed += self.cloud.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 499, in hashes_exist\n    remote_hashes = set(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 334, in _list_hashes_traverse\n    yield from itertools.chain.from_iterable(in_remote)\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 324, in list_with_update\n    return list(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 107, in find\n    yield from self.fs.find(path)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/fs\/spec.py&quot;, line 323, in find\n    &quot;size&quot;: int(item[&quot;fileSize&quot;]),\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 229, in __getitem__\n    raise KeyError(e)\nKeyError: KeyError('fileSize')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1643714210580,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":54,
        "Owner_creation_time":1333908536530,
        "Owner_last_access_time":1664025870700,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":884,
        "Owner_up_votes":185,
        "Owner_down_votes":3,
        "Owner_views":59,
        "Question_last_edit_time":1643715642688,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70939675",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73700203,
        "Question_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Question_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663059618623,
        "Question_score":1,
        "Question_tags":"git|amazon-s3|caching|pycharm|dvc",
        "Question_view_count":31,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663149911323,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69254525,
        "Question_title":"ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored",
        "Question_body":"<p>I just started with DVC. I have a git repo in which there are heavy models that i want to push to dvc. So I initialized the dvc by<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>and then configured the bucket<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>Now there is <code>\/models<\/code> folders, in which there was <code>.gitkeep<\/code> file and trained models. Following entry was in my <code>.gitignore<\/code><\/p>\n<pre><code>*.tar.gz\n<\/code><\/pre>\n<p>I ran the following command<\/p>\n<pre><code>git rm -r --cached my_server\\models\n<\/code><\/pre>\n<p>and added the following in the <code>.gitignore<\/code><\/p>\n<pre><code>models\n<\/code><\/pre>\n<p>I want to add all the <code>tar.gz<\/code> files to push on dvc<\/p>\n<p>so i tried<\/p>\n<pre><code>dvc add .\/my_server\/models\/*.tar.gz\n<\/code><\/pre>\n<p>but this is showing<\/p>\n<pre><code>ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored.\n<\/code><\/pre>\n<p>If I do\ndvc add .\/my_server\/models\/<\/p>\n<p>then this folder is added and a <code>models.dvc<\/code> file gets created. then git code shows for the changes.<\/p>\n<p>what is the correct way, do i need to mention <code>*.dvc<\/code> to <code>.gitignore<\/code> as well?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632141296913,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":760,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236886743,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69254525",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68284742,
        "Question_title":"DVC experiment is restoring deleted files",
        "Question_body":"<p>I am using DVC to run experiments in my project using<\/p>\n<pre><code>dvc exp run\n<\/code><\/pre>\n<p>Now when i make changes to a file(example train.py) and run &quot;dvc exp run&quot; everything goes well,\nbut my problem is that when making changes by <strong>deleting<\/strong> a file(example train.py or an image in the data folder) as soon as i run the &quot;dvc exp run&quot; the file is restored.\nhow to stop that from happening?<\/p>\n<p>This is my dvc.yaml:<\/p>\n<pre><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n    - train.py\n    metrics:\n    - metrics.txt:\n        cache: false\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1625655452383,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":272,
        "Owner_creation_time":1557314563687,
        "Owner_last_access_time":1645485519443,
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":113,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68284742",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861552,
        "Question_title":"Getting this weird error when trying to run DVC pull",
        "Question_body":"<p>I am new to using DVC and just exploring it. I am trying to pull data from s3 that was pushed by another person on my team. But I am getting this error:<\/p>\n\n<pre><code>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\nname: head_test_file.csv, md5: 45db668193ba44228d61115b1d0304fe\nWARNING: Cache '45db668193ba44228d61115b1d0304fe' not found. File 'head_test_file.csv' won't be created.\nNo changes.\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\nhead_test_file.csv\nDid you forget to fetch?\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585201186813,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":7063,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861552",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56456463,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1559722020690,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":326,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1568948355943,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559758177416,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68517516,
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation:",
        "Question_body":"<p>my ~\/.aws\/credentials looks like<\/p>\n<pre><code>[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<pre><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<p>I have made my .dvc\/config.local to look like<\/p>\n<pre><code>[\u2018remote \u201cmyremote\u201d\u2019]\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test\naccess_key_id = XYZ\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS\nprofile=\u2018testing\u2019\ncredentialpath = \/Users\/nyt21\/.aws\/credentials\n<\/code><\/pre>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n<\/blockquote>\n<p>** Update\nhere is the output of <code>dvc push -v<\/code><\/p>\n<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.\n2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3:\/\/bib-ds-models-testing\/data\/dvc-test'\n2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3:\/\/bib-ds-models-testing\/data\/dvc-test\n2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...\n2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     \n2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes\n2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists\n2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          \n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1057, in _info\n    out = await self._simple_info(path)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 970, in _simple_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Access Denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 50, in do_run\n    return self.run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 51, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 44, in push\n    pushed += self.cloud.push(objs, jobs, remote=remote)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 79, in push\n    return remote_obj.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 57, in wrapper\n    return f(obj, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 494, in push\n    ret = self._process(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 351, in _process\n    dir_status, file_status, dir_contents = self._status(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 195, in _status\n    self.hashes_exist(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 145, in hashes_exist\n    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 438, in hashes_exist\n    remote_hashes = self.list_hashes_exists(hashes, jobs, name)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 389, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 619, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 444, in result\n    return self.__get_result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 389, in __get_result\n    raise self._exception\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 380, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 92, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 87, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 68, in sync\n    raise result[0]\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 24, in _runner\n    result[0] = await coro\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 802, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1061, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1004, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-07-25 22:40:39,712 DEBUG: Version info for developers:\nDVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk3s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk3s1s1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.\n2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n<\/code><\/pre>\n<p>I can upload through python<\/p>\n<pre><code>import boto3\nimport os\nimport pickle\n\nbucket_name = 'bib-ds-models-testing'\nos.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;\nsession = boto3.Session()\ns3_client = boto3.client('s3')\n\ns3_client.upload_file('\/Users\/nyt21\/Devel\/DVC\/test\/data\/iris.csv',\n    'bib-ds-models-testing',\n    'data\/dvc-test\/my_iris.csv')\n<\/code><\/pre>\n<p>I don't use aws CLI but the following also gives an access deny !<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDenied) when calling the ListObjectsV2\noperation: Access Denied<\/p>\n<\/blockquote>\n<p>but it works if I add --profile=testing<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test --profile=testing\n                       \n<\/code><\/pre>\n<blockquote>\n<p>PRE dvc-test\/<\/p>\n<\/blockquote>\n<p>just you know environment variable <code>AWS_PROFILE<\/code> is already set to 'testing'<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>I have tried both <code>AWS_PROFILE='testing'<\/code> and <code>AWS_PROFILE=testing<\/code>, neither of them worked.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_time":1627207477173,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":1526,
        "Owner_creation_time":1297356892887,
        "Owner_last_access_time":1663786584013,
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":4988,
        "Owner_up_votes":350,
        "Owner_down_votes":22,
        "Owner_views":416,
        "Question_last_edit_time":1627383297972,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68517516",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73567700,
        "Question_title":"What DVC does when git merge is executed?",
        "Question_body":"<p>I have two git branches (master and develop). DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662027331777,
        "Question_score":1,
        "Question_tags":"git|dvc",
        "Question_view_count":40,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662097250203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567700",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_time":1331553057367,
        "Owner_last_access_time":1663939436043,
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Question_last_edit_time":1641199464667,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641179335767,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72641284,
        "Question_title":"Undo changes in pandas Dataframe(column drops, row drops, edits performed on a single cell)",
        "Question_body":"<p>I am currently working on developing a 'undo' operation for my interface that deals with changes performed on csv files. I want to provide an option for the user to revert the changes that he had done to the csv file, these changes include edit a cell, deleting column, deleting row, adding row, adding column etc. For this I want to know, does version control works in this scenario? If yes, which data version control should I prefer? If not, please suggest me an another alternative.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1655360134273,
        "Question_score":1,
        "Question_tags":"python|pandas|git|version-control|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1655359768507,
        "Owner_last_access_time":1658381602853,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655716541576,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72641284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":52871630,
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539857086453,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_time":1508231047660,
        "Owner_last_access_time":1663539768267,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Question_last_edit_time":1539881078543,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1540629268432,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861593,
        "Question_title":"How do I specify encryption type when using s3remote for DVC",
        "Question_body":"<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting <\/p>\n\n<p>But when I run the <code>dvc push<\/code> command, I get the generic error saying <\/p>\n\n<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>which I know for a fact that I get that error when I don't specify the encryption.<\/p>\n\n<p>It is similar to running <code>aws s3 cp<\/code> with <code>--sse<\/code> flag or specifying <code>ServerSideEncryption<\/code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585201508167,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":293,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861593",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_time":1587308895270,
        "Owner_last_access_time":1663873987327,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655524559503,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655524277716,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_time":1618255062697,
        "Owner_last_access_time":1645555346683,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1622257491983,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1622257759208,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1622503453296,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":54324146,
        "Question_title":"Azure DataLake with DVC",
        "Question_body":"<p>We are thinking to use DVC for versioning input data for DataScience project.\nmy data resides in Azure DataLake Gen1.<\/p>\n\n<p>how do i configure DVC to push data to Azure DataLake using Service Principal?\ni want DVC to store cache and data into Azure DataLake instead on local disk.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1548236331560,
        "Question_score":5,
        "Question_tags":"azure-data-lake|service-principal|dvc",
        "Question_view_count":473,
        "Owner_creation_time":1257862078920,
        "Owner_last_access_time":1662705544550,
        "Owner_location":"Pune, India",
        "Owner_reputation":6219,
        "Owner_up_votes":227,
        "Owner_down_votes":7,
        "Owner_views":587,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54324146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67215839,
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1619103654467,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1619127242127,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67215839",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67891465,
        "Question_title":"ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored",
        "Question_body":"<p>Getting the error &quot;<em>ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored.<\/em>&quot; while trying to add local files for tracking<\/p>\n<p>Python Version : 3.7<\/p>\n<p>Library used:<\/p>\n<p><code>pip install dvc  pip install dvc[gdrive]   dvc init   <\/code><\/p>\n<p><strong>dvc add -R Training_Batch_Files<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1623171603727,
        "Question_score":3,
        "Question_tags":"python|git|dvc",
        "Question_view_count":1844,
        "Owner_creation_time":1593952964623,
        "Owner_last_access_time":1664007410690,
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67891465",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73565648,
        "Question_title":"DVC shows files not tracked in source control in visual studio code",
        "Question_body":"<p>I'm using DVC extension in VScode inside a python project. The problem is that dvc shows files not tracked by dvc in the source control panel! As in the following picture.\nDVC track only data folder and not the src folder. How can I fix it? Have you also encountered these problems?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662017466370,
        "Question_score":0,
        "Question_tags":"visual-studio-code|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662021092696,
        "Answer_body":"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.\nOnce the files are tracked by one of the tools they should only show up under the appropriate tree.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662022756172,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662022916688,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73565648",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67635688,
        "Question_title":"Installation DVC on MinIO storage",
        "Question_body":"<p>Does anybody install DVC on MinIO storage?<\/p>\n<p>I have read <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">docs<\/a> but not all clear for me.<\/p>\n<p>Which command should I use for setup MinIO storage with this entrance parameters:<\/p>\n<p>storage url: <a href=\"https:\/\/minio.mysite.com\/minio\/bucket-name\/\" rel=\"nofollow noreferrer\">https:\/\/minio.mysite.com\/minio\/bucket-name\/<\/a>\nlogin: my_login\npassword: my_password<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621595380413,
        "Question_score":5,
        "Question_tags":"python|minio|dvc",
        "Question_view_count":1547,
        "Owner_creation_time":1526481416047,
        "Owner_last_access_time":1663922031783,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1621622008696,
        "Answer_body":"<p><strong>Install<\/strong><\/p>\n<p>I usually use it as a Python package, int this case you need to install:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install &quot;dvc[s3]&quot;\n<\/code><\/pre>\n<p><strong>Setup remote<\/strong><\/p>\n<p>By default DVC supports AWS S3 storages and they work fine.<br \/>\nAlso they support &quot;S3-compatible storage&quot;, but setup for this type of remotes is nod described properly. In particular case of MinIO you have <strong>bucket<\/strong> - directory on MinIO server where actual data stores (it is similar to AWS bucket), but DVC uses AWS CLI to authenticate. In case of MinIO you need to pass them explicitly.<\/p>\n<p>Then follow commands to setup your DVC remote:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)\ndvc remote add -d minio s3:\/\/bucket-name -f\n\n# add information about storage url (where &quot;https:\/\/minio.mysite.com&quot; your url)\ndvc remote modify minio endpointurl https:\/\/minio.mysite.com\n\n#  add info about login and password\ndvc remote modify minio access_key_id my_login\ndvc remote modify minio secret_access_key my_password\n<\/code><\/pre>\n<p><strong>If you move from old remote<\/strong>, use follow command to move your data:<\/p>\n<p>Before setup (download all old remote cache to local machine):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>\n<p>After setup (upload all cache to a new remote):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621599285543,
        "Answer_score":6.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67635688",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65847574,
        "Question_title":"Failed to pull existing files from SSH DVC Remote",
        "Question_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1611327752360,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1715,
        "Owner_creation_time":1355002392777,
        "Owner_last_access_time":1663450068853,
        "Owner_location":"Bolzano, Italia",
        "Owner_reputation":530,
        "Owner_up_votes":282,
        "Owner_down_votes":3,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1616082756536,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72651603,
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655411665790,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1655415427550,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655523571800,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655706111940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69725612,
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635260868803,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1635332461183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635332764020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70711015,
        "Question_title":"Is there an alternative to DVC pipelines to create a DAG which is also aware of inputs\/outputs to nodes to cache results?",
        "Question_body":"<p>I recently started to use DVC pipelines to create DAG in my application. I work on Machine Learning projects, and I need to experiment a lot with different nodes of my system. For example:<\/p>\n<p><code>Data preprocessing -&gt; feature extraction -&gt; model training -&gt; model evaluation<\/code><\/p>\n<p>Each node produces an output, and the output of each node is used in another node. What DVC allows me to do is to create a pipeline in which I can specify dependencies between nodes. I also use <code>.yaml<\/code> files to configure parameters of my application, and you can also specify these parameters as dependencies for different nodes. So, whenever a dependency changes between nodes (it can be either configuration parameters or inputs\/outputs specified), DVC is able to detect this, and run the necessary parts of the pipeline. If a dependency hasn't changed for a particular node, DVC can use its cache to skip that step. This is really useful for me, since some nodes take really long time to execute, and they don't always need to be ran (if their dependencies hasn't changed).<\/p>\n<p>I also started to use hydra to manage my config files, and to be honest, DVC doesn't work well with hydra. It expects a static config to specify parameter dependencies, and with hydra it is a bit tricky to do, and complicate things.<\/p>\n<p>My question is: is there any alternative to DVC Pipelines which also goes well with hydra?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1642165507820,
        "Question_score":1,
        "Question_tags":"pipeline|directed-acyclic-graphs|dvc|hydra-core",
        "Question_view_count":181,
        "Owner_creation_time":1548055925357,
        "Owner_last_access_time":1664039002623,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":551,
        "Owner_up_votes":9,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70711015",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64456396,
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603252093370,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_time":1446746840593,
        "Owner_last_access_time":1664083975707,
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Question_last_edit_time":1603305923907,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1603314290768,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603325349590,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56269391,
        "Question_title":"Readding missing files to DVC",
        "Question_body":"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull<\/code> I get the output<\/p>\n\n<pre><code>[##############################] 100% Analysing status.\nWARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data\/2.mp4' won't be created.\nWARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data\/3.mp4' won't be created.\n<\/code><\/pre>\n\n<p>The <code>dvc status<\/code> command gives me:<\/p>\n\n<pre><code>data\/2.mp4.dvc:\n    changed outs:\n        not in cache:       data\/2.mp4\ndata\/3.mp4.dvc:\n    changed outs:\n        not in cache:       data\/3.mp4\n<\/code><\/pre>\n\n<p>It seems that <code>2.mp4<\/code> and <code>3.mp4<\/code> where added under dvc control but <code>dvc push<\/code> command has not been executed.<\/p>\n\n<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:<\/p>\n\n<pre><code>dvc remove data\/2.mp4.dvc\ndvc remove data\/3.mp4.dvc\n\ndvc add data\/2.mp4 \ndvc add data\/3.mp4 \n<\/code><\/pre>\n\n<p>But there is no effect. How can I remove files from under dvc control and add them again?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558593719503,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1065,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558637203487,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56269391",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69265000,
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632209467140,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236873616,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1632210565512,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632211831430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57966851,
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568689927047,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_time":1383611307000,
        "Owner_last_access_time":1664061570950,
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1568693889196,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1568725966083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73126208,
        "Question_title":"Python: Ssl Certificate verify failed",
        "Question_body":"<p>I have installed <code>dvc<\/code> on my <code>ubuntu-18.04-LTS<\/code> system and while trying to download the <code>data<\/code> files from github using dvc, it fails with below error.<\/p>\n<pre><code>$ dvc get https:\/\/github.com\/iterative\/dataset-registry get-started\/data.xml -o data\/data.xml -v\n\n2022-07-22 12:55:22,260 DEBUG: Creating external repo https:\/\/github.com\/iterative\/dataset-registry@None\n2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https:\/\/github.com\/iterative\/dataset-registry' to a temporary dir\n2022-07-22 12:55:23,683 DEBUG: Removing '\/dvc\/dvc_test\/data\/.UEeAzwmJCY3q85YQuCeahx'\n2022-07-22 12:55:23,684 ERROR: failed to get 'get-started\/data.xml' from 'https:\/\/github.com\/iterative\/dataset-registry' - Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;urllib3\/connectionpool.py&quot;, line 703, in urlopen\n  File &quot;urllib3\/connectionpool.py&quot;, line 386, in _make_request\n  File &quot;urllib3\/connectionpool.py&quot;, line 1042, in _validate_conn\n  File &quot;urllib3\/connection.py&quot;, line 414, in connect\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 449, in ssl_wrap_socket\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl\n  File &quot;ssl.py&quot;, line 500, in wrap_socket\n  File &quot;ssl.py&quot;, line 1040, in _create\n  File &quot;ssl.py&quot;, line 1309, in do_handshake\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;dvc\/scm.py&quot;, line 145, in clone\n  File &quot;scmrepo\/git\/__init__.py&quot;, line 143, in clone\n  File &quot;scmrepo\/git\/backend\/dulwich\/__init__.py&quot;, line 199, in clone\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n<\/code><\/pre>\n<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com<\/code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc<\/code>the above errors are occurring, Even the below couldn't solve the issue.<\/p>\n<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;\n\nDefaultVerifyPaths(cafile=None, capath='\/usr\/lib\/ssl\/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='\/usr\/lib\/ssl\/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='\/usr\/lib\/ssl\/certs')\n<\/code><\/pre>\n<pre><code>export SSL_CERT_DIR=\/etc\/ssl\/certs\/\nexport REQUESTS_CA_BUNDLE=\/usr\/local\/lib\/python2.7\/dist-packages\/certifi\/cacert.pem\npip install --upgrade certifi\nexport PYTHONHTTPSVERIFY=0\n\nsudo apt install ca-certificates\nsudo update-ca-certificates --fresh\n<\/code><\/pre>\n<pre><code>$ python --version\nPython 2.7.17\n\n$ dvc doctor\nDVC version: 2.13.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\n<\/code><\/pre>\n<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;<\/code> Similarly do we have option in dvc?<\/p>\n<p>Further what should i update to resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658849780953,
        "Question_score":1,
        "Question_tags":"python|ssl|pip|ssl-certificate|dvc",
        "Question_view_count":157,
        "Owner_creation_time":1432810927473,
        "Owner_last_access_time":1664076190223,
        "Owner_location":null,
        "Owner_reputation":1609,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":447,
        "Question_last_edit_time":1658898077488,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73126208",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620243318287,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_time":1620132740443,
        "Owner_last_access_time":1641927840700,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620292401492,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66409283,
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1614516713937,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":1614699218992,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1614537291720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614698988012,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558708772616,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558674266680,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71663330,
        "Question_title":"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?",
        "Question_body":"<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.<\/p>\n<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.<\/p>\n<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.<\/p>\n<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.<\/p>\n<ol>\n<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?<\/p>\n<\/li>\n<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.<\/p>\n<\/li>\n<\/ol>\n<p>I understand from <a href=\"https:\/\/stackoverflow.com\/a\/35578715\/4533188\">here<\/a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.<\/p>\n<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1648561944137,
        "Question_score":0,
        "Question_tags":"git|git-lfs|dvc|git-annex",
        "Question_view_count":370,
        "Owner_creation_time":1423144219917,
        "Owner_last_access_time":1663937106050,
        "Owner_location":null,
        "Owner_reputation":11374,
        "Owner_up_votes":415,
        "Owner_down_votes":2,
        "Owner_views":845,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71663330",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72451922,
        "Question_title":"How can dvc pipeline recognize when to use encoding pipeline while new data added for the modeling?",
        "Question_body":"<p>I have created separate pipelines for feature encoding and feature scaling in DVC.\nNow, when I will input new data from my flask API, how these DVC pipelines will automatically run and encode and scale data for modelling?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1654017845533,
        "Question_score":0,
        "Question_tags":"machine-learning|pipeline|mlops|dvc",
        "Question_view_count":25,
        "Owner_creation_time":1654016718443,
        "Owner_last_access_time":1664013919797,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72451922",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617399923477,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_time":1407091594730,
        "Owner_last_access_time":1654654035420,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1617643343263,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1617404822540,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1617478114567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58952962,
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574248229420,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1574267475363,
        "Answer_score":13.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1642527991692,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68771979,
        "Question_title":"problems installing a DVC lower version [0.9.4]",
        "Question_body":"<p>I need to install an older version of DVC, namely 0.9.4, in a Python virtual environment.<\/p>\n<p>I used the command:<\/p>\n<pre><code>pip install dvc==0.9.4\n<\/code><\/pre>\n<p>Everything seemed to work fine. However, when I try to run a <code>dvc pull<\/code> command, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\Scripts\\dvc.exe\\__main__.py&quot;, line 4, in &lt;module&gt;\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\main.py&quot;, line 2, in &lt;module&gt;\n    from dvc.cli import parse_args\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cli.py&quot;, line 8, in &lt;module&gt;\n    from dvc.command.init import CmdInit\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\command\\init.py&quot;, line 1, in &lt;module&gt;\n    from dvc.project import Project\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\project.py&quot;, line 15, in &lt;module&gt;\n    from dvc.cloud.data_cloud import DataCloud\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\data_cloud.py&quot;, line 11, in &lt;module&gt;\n    from dvc.cloud.gcp import DataCloudGCP\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\gcp.py&quot;, line 4, in &lt;module&gt;\n    from google.cloud import storage as gc\nModuleNotFoundError: No module named 'google.cloud'\n<\/code><\/pre>\n<p>When I print the dvc version, I see:<\/p>\n<pre><code>0.9.4+6bb66e.mod\n<\/code><\/pre>\n<p>Can anyone please help? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628856076700,
        "Question_score":2,
        "Question_tags":"python|google-cloud-storage|dvc",
        "Question_view_count":234,
        "Owner_creation_time":1569838509623,
        "Owner_last_access_time":1663596099763,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1628882501660,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68771979",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67454531,
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620537862843,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_time":1620537484800,
        "Owner_last_access_time":1642059984007,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1620625906412,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620591578072,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62441146,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1592457436920,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592496929888,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619078487020,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_time":1580932981007,
        "Owner_last_access_time":1663744754923,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619081803750,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619084940030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73651050,
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1662650337132,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662656090387,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60365473,
        "Question_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Question_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582482701247,
        "Question_score":7,
        "Question_tags":"python|sql|r|git|dvc",
        "Question_view_count":689,
        "Owner_creation_time":1504097190907,
        "Owner_last_access_time":1647022258673,
        "Owner_location":null,
        "Owner_reputation":1365,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":193,
        "Question_last_edit_time":1582486128287,
        "Answer_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1582487867856,
        "Answer_score":12.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70752204,
        "Question_title":"How to resolve DVC Pull Error on Pycharm?",
        "Question_body":"<p>When I execute 'DVC Pull', I get the following error<\/p>\n<pre><code>&gt; dvc pull ERROR: unexpected error - invalid syntax (tz.py, line 78)    \n&gt; Traceback (most recent call last):   File\n&gt; &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/main.py&quot;,\n&gt; line 55, in main\n&gt;     ret = cmd.do_run()   File &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/command\/base.py&quot;,\n &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dateutil\/tz.py&quot;,\n&gt; line 78\n&gt;     `self._name`,\n&gt;     ^ SyntaxError: invalid syntax\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/jasma-balasangameshwara\/ml-heroku-fastapi\" rel=\"nofollow noreferrer\">How to resolve it? The link to the Github repository is <\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1642493442793,
        "Question_score":1,
        "Question_tags":"python|dvc",
        "Question_view_count":105,
        "Owner_creation_time":1579349493753,
        "Owner_last_access_time":1652115144133,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70752204",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71155959,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1645092064283,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085253,
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Question_last_edit_time":1645132430276,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1645113091963,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65824766,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_time":1493101921290,
        "Owner_last_access_time":1663968152030,
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1611228029327,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56881619,
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562220779043,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_time":1562220403123,
        "Owner_last_access_time":1563409319090,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881619",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73727709,
        "Question_title":"Can we connect oracle database with DVC ? and if yes then how?",
        "Question_body":"<p>I was trying to connect dvc with oracle database but unable to do it. So, Please can anyone help me with that.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1663229583107,
        "Question_score":1,
        "Question_tags":"oracle|dvc",
        "Question_view_count":20,
        "Owner_creation_time":1663229291407,
        "Owner_last_access_time":1663926949987,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727709",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73276837,
        "Question_title":"Is it possible to use authentication and authorization in MLFlow Server?",
        "Question_body":"<p>MLFlow does not have integrated authentication (openID, LDAP, kerberos, AAD...) or authorization (RBAC, ABAC, ACL...)<\/p>\n<p>Is it only possible with a web proxy in MLFlow? p.e: nginx<\/p>\n<p>Does anyone know of an option similar to Apache Sentry or Apache Ranger for MLFlow?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659956478063,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":57,
        "Owner_creation_time":1463943156183,
        "Owner_last_access_time":1663834484890,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73276837",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67953241,
        "Question_title":"mlflow run git-uri clone to specific directory",
        "Question_body":"<p>I am using mlflow run with a GitHub uri.<\/p>\n<p>When I run using the below command<\/p>\n<pre><code>mlflow run &lt;git-uri&gt;\n<\/code><\/pre>\n<p>The command sets up a conda environment and then <em>clones the Git repo into a <strong>temp<\/strong> directory, But I need it setup in a <strong>specific<\/strong> directory<\/em><\/p>\n<p>I checked the entire document, but I can't find it. Is there no such option to do so in one shot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623534343453,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":239,
        "Owner_creation_time":1575348765723,
        "Owner_last_access_time":1663079223380,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1049,
        "Owner_up_votes":55,
        "Owner_down_votes":68,
        "Owner_views":192,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp<\/code> function (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/1c43176cefb5531fbb243975b9c8c5bfb9775e66\/mlflow\/projects\/utils.py#L140\" rel=\"nofollow noreferrer\">source code<\/a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR<\/code> environment variable as described in <a href=\"https:\/\/docs.python.org\/3\/library\/tempfile.html#tempfile.mkstemp\" rel=\"nofollow noreferrer\">Python docs<\/a> (it lists <code>TMP<\/code> &amp; <code>TEMP<\/code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory\/file names are still will be random.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1623570743820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623614468896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67953241",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70620074,
        "Question_title":"Serving multiple ML models using mlflow in a single VM",
        "Question_body":"<p>I have setup an mlflow service in a VM and I am able to serve the model using mlflow serve command.\nWanted to know if we can host multiple models in a single VM ?<\/p>\n<p>I am using the below command to serve a model using mlflow in a vm.<\/p>\n<p>command:<\/p>\n<pre><code>\/mlflow models serve -m models:\/$Model-Name\/$Version --no-conda -p 443 -h 0.0.0.0\n<\/code><\/pre>\n<p>Above command creates a model serving and runs it on 443 port.\nIs it possible to have an endpoint like below being created with model name in it ?<\/p>\n<p>Current URL:\nhttps:\/\/localhost:443\/invocations<\/p>\n<p>Expected URL:\nhttps:\/\/localhost:443\/model-name\/invocations ?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1641552500160,
        "Question_score":0,
        "Question_tags":"apache-spark|machine-learning|databricks|mlflow",
        "Question_view_count":544,
        "Owner_creation_time":1568969745383,
        "Owner_last_access_time":1663915778593,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1641558820040,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70620074",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69466354,
        "Question_title":"MLflow S3UploadFailedError: Failed to upload",
        "Question_body":"<p>I've created with docker a MinioS3 artifact storage and a mysql bakend storage using the next docker-compose:<\/p>\n<pre><code>    version: '3.8'\n    services:\n        db:\n           environment:\n              - MYSQL_DATABASE=${MYSQL_DATABASE}\n              - MYSQL_USER=${MYSQL_USER}\n              - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n           expose:\n              - '3306'        \n           volumes:\n              - '(path)\/server_backend:\/var\/lib\/mysql '\n           image: 'mysql'\n           container_name: db\n\n        storage:\n            environment:\n                - MINIO_ACCESS_KEY=${MINIO_USR}\n                - MINIO_SECRET_KEY=${MINIO_PASS}\n            expose:\n                - '9000'\n            ports:\n                - '9000:9000'        \n            depends_on:\n                - db\n            command: server \/data\n            volumes:\n                - '(path)\/server_artifact:\/data'\n            image: minio\/minio:RELEASE.2021-02-14T04-01-33Z\n            container_name: MinIO\n\n        mlflow:\n            build: .\/mlflow\n            environment:\n                - AWS_ACCESS_KEY_ID=${MINIO_USR}\n                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n            expose:\n                - '5000'\n            ports:\n                - '5000:5000'\n            depends_on:\n                - storage                       \n            image: 'mlflow:Dockerfile'\n            container_name: server\n<\/code><\/pre>\n<p>The Mlflow server docker was created using the next Dockerfile:<\/p>\n<pre><code>    FROM python:3.8-slim-buster\n    WORKDIR \/usr\/src\/app\n    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql\n    ENV MLFLOW_S3_ENDPOINT_URL=http:\/\/storage:9000\n    CMD mlflow server \\\n        --backend-store-uri mysql+pymysql:\/\/MLFLOW:temporal@db:3306\/DBMLFLOW \\\n        --default-artifact-root s3:\/\/artifacts \\\n        --host 0.0.0.0\n<\/code><\/pre>\n<p>The credantials are defined in a <code>.env<\/code> file.<\/p>\n<p>The results of the <code>docker-compose<\/code> up command :<\/p>\n<pre><code>\n    [+] Running 21\/22\n     - mlflow Error                                                                                                                              5.6s\n     - storage Pulled                                                                                                                           36.9s\n       - a6b97b4963f5 Pull complete                                                                                                             24.6s\n       - 13948a011eec Pull complete                                                                                                             24.7s\n       - 40cdef9976a6 Pull complete                                                                                                             24.7s\n       - f47162848743 Pull complete                                                                                                             24.8s\n       - 5f2758d8e94c Pull complete                                                                                                             24.9s\n       - c2950439edb8 Pull complete                                                                                                             25.0s\n       - 1b08f8a15998 Pull complete                                                                                                             30.7s\n     - db Pulled                                                                                                                                45.8s\n       - 07aded7c29c6 Already exists                                                                                                             0.0s\n       - f68b8cbd22de Pull complete                                                                                                              0.7s\n       - 30c1754a28c4 Pull complete                                                                                                              2.1s\n       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s\n       - 79a41dc56b9a Pull complete                                                                                                              2.3s\n       - 00a75e3842fb Pull complete                                                                                                              6.7s\n       - b36a6919c217 Pull complete                                                                                                              6.8s\n       - 635b0b84d686 Pull complete                                                                                                              6.8s\n       - 6d24c7242d02 Pull complete                                                                                                             39.4s\n       - 5be6c5edf16f Pull complete                                                                                                             39.5s\n       - cb35eac1242c Pull complete                                                                                                             39.5s\n       - a573d4e1c407 Pull complete                                                                                                             39.6s\n    [+] Building 1.4s (7\/7) FINISHED\n     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s\n     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s\n     =&gt; [internal] load .dockerignore                                                                                                            0.0s\n     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s\n     =&gt; [internal] load metadata for docker.io\/library\/python:3.8-slim-buster                                                                    1.3s\n     =&gt; [1\/3] FROM docker.io\/library\/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s\n     =&gt; CACHED [2\/3] WORKDIR \/usr\/src\/app                                                                                                        0.0s\n     =&gt; CACHED [3\/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s\n     =&gt; exporting to image                                                                                                                       0.0s\n     =&gt; =&gt; exporting layers                                                                                                                      0.0s\n     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s\n     =&gt; =&gt; naming to docker.io\/library\/mlflow:Dockerfile                                                                                         0.0s\n    \n    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n    [+] Running 3\/3\n     - Container db  Created                                                                                                       0.5s\n     - Container MinIO      Created                                                                                                       0.1s\n     - Container server     Created                                                                                                       0.1s\n    Attaching to server, MinIO, db\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files\n    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) initializing of server in progress as process 44\n    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.\n    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend\n    MinIO      | Endpoint: http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Browser Access:\n    MinIO      |    http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Object API (Amazon S3 compatible):\n    MinIO      |    Go:         https:\/\/docs.min.io\/docs\/golang-client-quickstart-guide\n    MinIO      |    Java:       https:\/\/docs.min.io\/docs\/java-client-quickstart-guide\n    MinIO      |    Python:     https:\/\/docs.min.io\/docs\/python-client-quickstart-guide\n    MinIO      |    JavaScript: https:\/\/docs.min.io\/docs\/javascript-client-quickstart-guide\n    MinIO      |    .NET:       https:\/\/docs.min.io\/docs\/dotnet-client-quickstart-guide\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.1 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.3 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.7 seconds\n    server     | 2021\/10\/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 1.5 seconds\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server\n    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 93\n    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 0  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.\n    server     | 2021\/10\/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 3.1 seconds\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/iso3166.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/leap-seconds.list' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone1970.tab' as time zone. Skipping it.\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW\n    db  |\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server\n    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).\n    server     | 2021\/10\/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 6.3 seconds\n    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] \/usr\/sbin\/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped\n    db  |\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.\n    db  |\n    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 1\n    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\n    db  | mbind: Operation not permitted\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\n    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    db  | mbind: Operation not permitted\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http:\/\/0.0.0.0:5000 (17)\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync\n    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19\n    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20\n    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21\n    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22\n\n<\/code><\/pre>\n<p>It makes me suspect because on the second line appears <code>- mlflow Error<\/code> but i think that this is why the other builds haven't finished.<\/p>\n<p>Then I've set my environment variables on the client to create the information flow between my script and the storages:<\/p>\n<pre><code>\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'key'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'\n    \n    remote_server_uri = &quot;http:\/\/localhost:5000\/&quot; # server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n    \n    mlflow.set_experiment(&quot;mnist_mLflow_demo&quot;)\n\n<\/code><\/pre>\n<p>finally i trained a tensorflow network and i didn't have problems storing parameters and metrics but gave me some warnings (refering to next error). But the model haven't been auto log, so i tryed to do it manually:<\/p>\n<pre><code>    with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n    \n        mlflow.keras.log_model(model2, 'model2')\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>It dosen't work and it gives me the next INFO (but essencialy an error):<\/p>\n<pre><code>    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    2021\/10\/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model, flavor: keras)\n    Traceback (most recent call last):\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\environment.py&quot;, line 212, in infer_pip_requirements\n        return _infer_requirements(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 263, in _infer_requirements\n        modules = _capture_imported_modules(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 221, in _capture_imported_modules\n        _run_command(\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 163, in _run_command\n        stderr = stderr.decode(&quot;utf-8&quot;)\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte\n\n<\/code><\/pre>\n<p>And the next error:<\/p>\n<pre><code>\n    ClientError                               Traceback (most recent call last)\n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        278         try:\n    --&gt; 279             future.result()\n        280         # If a client error was raised, add the backwards compatibility layer\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        105             # out of this and propogate the exception.\n    --&gt; 106             return self._coordinator.result()\n        107         except KeyboardInterrupt as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        264         if self._exception:\n    --&gt; 265             raise self._exception\n        266         return self._result\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in __call__(self)\n        125             if not self._transfer_coordinator.done():\n    --&gt; 126                 return self._execute_main(kwargs)\n        127         except Exception as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in _execute_main(self, kwargs)\n        149 \n    --&gt; 150         return_value = self._main(**kwargs)\n        151         # If the task is the final task, then set the TransferFuture's\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\upload.py in _main(self, client, fileobj, bucket, key, extra_args)\n        693         with fileobj as body:\n    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n        695 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\n        385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n    --&gt; 386             return self._make_api_call(operation_name, kwargs)\n        387 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\n        704             error_class = self.exceptions.from_code(error_code)\n    --&gt; 705             raise error_class(parsed_response, operation_name)\n        706         else:\n    \n    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n    \n    During handling of the above exception, another exception occurred:\n    \n    S3UploadFailedError                       Traceback (most recent call last)\n    C:\\Users\\FCAIZA~1\\AppData\\Local\\Temp\/ipykernel_7164\/2476247499.py in &lt;module&gt;\n          1 with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n          2 \n    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')\n          4 \n          5 mlflow.end_run()\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)\n        402             mlflow.keras.log_model(keras_model, &quot;models&quot;)\n        403     &quot;&quot;&quot;\n    --&gt; 404     Model.log(\n        405         artifact_path=artifact_path,\n        406         flavor=mlflow.keras,\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\n        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n        189             try:\n        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py in log_artifacts(local_dir, artifact_path)\n        582     &quot;&quot;&quot;\n        583     run_id = _get_or_start_run().info.run_id\n    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n        585 \n        586 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        975             is_dir: True\n        976         &quot;&quot;&quot;\n    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n        978 \n        979     @contextlib.contextmanager\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        333         &quot;&quot;&quot;\n    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n        335 \n        336     def list_artifacts(self, run_id, path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n        102                 upload_path = posixpath.join(dest_path, rel_path)\n        103             for f in filenames:\n    --&gt; 104                 self._upload_file(\n        105                     s3_client=s3_client,\n        106                     local_file=os.path.join(root, f),\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)\n         78         if environ_extra_args is not None:\n         79             extra_args.update(environ_extra_args)\n    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n         81 \n         82     def log_artifact(self, local_file, artifact_path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\n        128     &quot;&quot;&quot;\n        129     with S3Transfer(self, Config) as transfer:\n    --&gt; 130         return transfer.upload_file(\n        131             filename=Filename, bucket=Bucket, key=Key,\n        132             extra_args=ExtraArgs, callback=Callback)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        283         # client error.\n        284         except ClientError as e:\n    --&gt; 285             raise S3UploadFailedError(\n        286                 &quot;Failed to upload %s to %s: %s&quot; % (\n        287                     filename, '\/'.join([bucket, key]), e))\n    \n    S3UploadFailedError: Failed to upload (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model\\conda.yaml to artifacts\/1\/5ae5fcef2d07432d811c3d7eb534382c\/artifacts\/model2\/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n\n<\/code><\/pre>\n<p>Do you know how to help me with it? I have been looking all this morning but i did not find a solution. Thank you!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633525784987,
        "Question_score":1,
        "Question_tags":"python|mysql|docker|minio|mlflow",
        "Question_view_count":969,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:\/&quot; is &quot;fca\u00f1izares&quot; (Ca\u00f1izares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.<\/p>\n<p>PS: Moral of the issue, get rid of the extrange characters!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633680248312,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69466354",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70781419,
        "Question_title":"mlflow static_prefix url in set_tracking_uri is not working",
        "Question_body":"<p>I am starting mlflow with below command<\/p>\n<pre><code>mlflow server --static_prefix=\/myprefix --backend-store-uri postgresql:\/\/psql_user_name:psql_password@localhost\/mlflow_db --default-artifact-root s3:\/\/my-mlflow-bucket\/ --host 0.0.0.0 -p 8000\n<\/code><\/pre>\n<p>everything worked fine and I can see mlflow UI when I open url http:\/\/localhost:8000\/myprefix\nbut when I use mlflow.set_tracking_uri() i have to give url path as &quot;http:\/\/localhost:8000\/&quot;<\/p>\n<p>why cant we use full url , which has static prefix &quot;http:\/\/localhost:8000\/myprefix&quot; ?<\/p>\n<p>if i use full url ,I am getting request to api endpoint fail and api is experiments\/list error 404 !=200\nis there any way to add url with static prefix in set_tracking_uri<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642658164867,
        "Question_score":0,
        "Question_tags":"machine-learning|artificial-intelligence|mlflow",
        "Question_view_count":246,
        "Owner_creation_time":1625731242200,
        "Owner_last_access_time":1659594320980,
        "Owner_location":null,
        "Owner_reputation":145,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70781419",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64209196,
        "Question_title":"How to update a previous run into MLFlow?",
        "Question_body":"<p>I would like to update previous runs done with MLFlow, ie. changing\/updating a parameter value to accommodate a change in the implementation. Typical uses cases:<\/p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.<\/li>\n<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.<\/li>\n<li>Correct a wrong parameter value loggued in the previous runs.<\/li>\n<\/ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.<\/p>\n<p>What is the best way to do this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1601903047533,
        "Question_score":6,
        "Question_tags":"logging|data-science|mlflow",
        "Question_view_count":2834,
        "Owner_creation_time":1347312347147,
        "Owner_last_access_time":1664043273217,
        "Owner_location":null,
        "Owner_reputation":1022,
        "Owner_up_votes":1127,
        "Owner_down_votes":19,
        "Owner_views":66,
        "Question_last_edit_time":1607788863848,
        "Answer_body":"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function<\/p>\n<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:\n    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)\n    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics\n    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file\n<\/code><\/pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs<\/a>.<\/p>\n<p>Source: <a href=\"https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1606920349240,
        "Answer_score":10.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64209196",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71315446,
        "Question_title":"How do I take an already existing MLflow model on my local filesystem and log it to a remote tracking server?",
        "Question_body":"<p>Let's say I already have an existing MLflow model on my local system of the <code>mlflow.pyfunc<\/code> flavor.<\/p>\n<p>The directory looks like this<\/p>\n<pre><code>model\/\n  data\/\n  code\/\n  conda.yml\n  MLmodel\n<\/code><\/pre>\n<p>Where <code>MLmodel<\/code> is something like<\/p>\n<pre><code>flavors:\n  python_function:\n    code: code\n    data: data\n    env: conda.yml\n    loader_module: loader # model\/code\/loader.py has the entrypoint\n<\/code><\/pre>\n<p>I now try and log this model to a remote tracking server using (I'm in the directory above <code>model\/<\/code>, so <code>.\/model\/data<\/code> works, etc)<\/p>\n<pre><code>import mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.pyfunc.log_model(\n  &quot;my-model-artifact&quot;,\n  registered_model_name=&quot;my-model&quot;, # same for all model versions,\n  data_path=&quot;model\/data&quot;,\n  code_path=&quot;model\/code&quot;,\n  loader_module=&quot;model\/code\/loader&quot;\n)\n<\/code><\/pre>\n<p>The tracking server ends up logging a nested MLflow model.. this is inside of the <code>.\/artifacts\/my-model-artifact<\/code> directory on the tracking server<\/p>\n<pre><code>.\/artifacts\/my-model-artifact\n  conda.yaml\n  MLmodel # *not* my MLmodel, one newly generated by MLflow\n  data\/\n  code\/\n<\/code><\/pre>\n<p>Where <code>data<\/code> now points nested to my entire <code>model\/data<\/code> directory and <code>code<\/code> points to a nested <code>model\/code<\/code> directory.<\/p>\n<p>It's like it doesn't understand that I already have this full artifact..<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1646174584637,
        "Question_score":0,
        "Question_tags":"model|mlflow",
        "Question_view_count":336,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71315446",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60088889,
        "Question_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Question_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1580970401043,
        "Question_score":20,
        "Question_tags":"python|mlflow",
        "Question_view_count":13984,
        "Owner_creation_time":1443225809767,
        "Owner_last_access_time":1663898334270,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2332,
        "Owner_up_votes":133,
        "Owner_down_votes":3,
        "Owner_views":560,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1585231513452,
        "Answer_score":22.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70967052,
        "Question_title":"Does Hugging face defaults allow to log mlflow artifacts and name every run of mlflow log?",
        "Question_body":"<p>I am training a simple binary classification model using Hugging face models using pytorch.<\/p>\n<p>Bert PyTorch HuggingFace.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>import transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom transformers import AutoTokenizer\n\n \nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertConfig\n<\/code><\/pre>\n<pre><code>def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n   \n\n    predictions = np.argmax(logits, axis=-1)\n    \n    acc = np.sum(predictions == labels) \/ predictions.shape[0]\n    return {&quot;accuracy&quot;: acc,\n            'precision': metrics.precision_score(labels, predictions),\n            'recall': metrics.recall_score(labels, predictions),\n            'f1': metrics.f1_score(labels, predictions)}\n\ntraining_args = tr.TrainingArguments(\n    #report_to = 'wandb',\n    output_dir='\/home\/pc\/proj\/Exp2_conv_stampy_data\/results_exp0',          # output directory\n    overwrite_output_dir = True,\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=32,  # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    learning_rate=2e-5,\n    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs_exp0',            # directory for storing logs\n    logging_steps=137,\n    evaluation_strategy=&quot;epoch&quot;\n    ,save_strategy=&quot;epoch&quot;\n    ,load_best_model_at_end=True\n    ,fp16=True\n    ,run_name=&quot;final_model0&quot;\n    \n)\n\n\n# counter = 0\n# results_lst = []\n\nfrom transformers import TrainerCallback\nfrom copy import deepcopy\n\nmodel = tr.XLMRobertaForSequenceClassification.from_pretrained(&quot;\/home\/pc\/multilingual_toxic_xlm_roberta&quot;,problem_type=&quot;single_label_classification&quot;, num_labels=2,ignore_mismatched_sizes=True, id2label={0: 'negative', 1: 'positive'})\n\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\n\n\ntrain_data = SEDataset(train_encodings, train_labels)\nval_data = SEDataset(val_encodings, val_labels)\n\nmodel.to(device)\n\nclass CustomCallback(TrainerCallback):\n    \n    def __init__(self, trainer) -&gt; None:\n        super().__init__()\n        self._trainer = trainer\n    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        if control.should_evaluate:\n            control_copy = deepcopy(control)\n            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=&quot;train&quot;)\n            return control_copy\n\ntrainer = tr.Trainer(\n    model=model,                         # the instantiated Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_data,         # training dataset\n    eval_dataset=val_data,          # evaluation dataset\n    compute_metrics=compute_metrics    # the callback that computes metrics of interest\n)\ntrainer.add_callback(CustomCallback(trainer)) \ntrain = trainer.train()\n\n\n\ntrainer.save_model(&quot;\/home\/pc\/proj\/Exp2_conv_stampy_data\/result_toxic_model_exp0&quot;)\n\n<\/code><\/pre>\n<p>I see by default <code>mlruns<\/code> directory is created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rD25L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rD25L.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>What is <code>0' and what are these 2 folders inside <\/code>0`?<\/strong><\/p>\n<p><strong>How can rename to something useful and understandable.?<\/strong><\/p>\n<p><strong>If I run multiple runs, how can I log every run of model with something like <code>run1<\/code>, <code>run2<\/code> under same experiment?<\/strong><\/p>\n<p><strong>Also I see artifact folder is empty, how to log final model?<\/strong><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1643871620400,
        "Question_score":0,
        "Question_tags":"pytorch|huggingface-transformers|mlflow",
        "Question_view_count":818,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70967052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60531166,
        "Question_title":"How to safely shutdown mlflow ui?",
        "Question_body":"<p>After running <code>mlflow ui<\/code> on a remote server, I'm unable to reopen the <code>mlflow ui<\/code> again.<br>\nA workaround is to kill all my processes in the server using <code>pkill -u MyUserName<\/code>.<br>\nOtherwise I get the following error:  <\/p>\n\n<pre><code>[INFO] Starting gunicorn 20.0.4  \n[ERROR] Connection in use: ('127.0.0.1', 5000)\n[ERROR] Retrying in 1 second.  \n...\nRunning the mlflow server failed. Please see ther logs above for details.\n<\/code><\/pre>\n\n<p>I understand the error but I don't understand:<br>\n1. What is the correct way to shutdown <code>mlflow ui<\/code><br>\n2. How can I identify the <code>mlflow ui<\/code> process in order to only kill that process and not use the <code>pkill<\/code>  <\/p>\n\n<p>Currently I close the browser or use ctrl+C <\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1583341247020,
        "Question_score":6,
        "Question_tags":"python|r|machine-learning|mlflow",
        "Question_view_count":9850,
        "Owner_creation_time":1453321149903,
        "Owner_last_access_time":1662925257297,
        "Owner_location":"Israel",
        "Owner_reputation":1153,
        "Owner_up_votes":113,
        "Owner_down_votes":14,
        "Owner_views":168,
        "Question_last_edit_time":1630433880327,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60531166",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66858683,
        "Question_title":"How to integrate mlflow and airflow? Is there any way to connect to mlflow server from airflow",
        "Question_body":"<p>Lets say I have a ML model in mlflow server artifacts. I want to run this model from airflow Dag. Also after running in airflow, metric logs should be visible in mlflow.\nHow can I achieve this?\nThere are connections in airflow, I couldn't find any connection type for mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617038196660,
        "Question_score":2,
        "Question_tags":"airflow|mlflow",
        "Question_view_count":389,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66858683",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60286506,
        "Question_title":"mlflow Exception: Run with UUID is already active",
        "Question_body":"<p>Used mlflow.set_tracking_uri to set up tracking_uri and set_experiment, got an error and check back to run following code again. got an error that \"Exception: Run with UUID  is already active.\"\nTry to use <code>mlflow.end_run<\/code> to end current run, but got RestException: RESOURCE_DOES_NOT_EXIST: Run UUID not found.\nCurrently stuck in this infinite loop. Any suggestion?    <\/p>\n\n<pre><code>    mlflow.set_experiment(\"my_experiment\")\n    mlflow.start_run(run_name='my_project')\n    mlflow.set_tag('input_len',len(input))\n    mlflow.log_param('metrics', r2)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1582047247790,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":2813,
        "Owner_creation_time":1409958744170,
        "Owner_last_access_time":1663993998083,
        "Owner_location":"Los Angeles, CA, USA",
        "Owner_reputation":1997,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":175,
        "Question_last_edit_time":1582049042567,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60286506",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73806968,
        "Question_title":"How to use a @pandas_udf function inside a class with pyspark?",
        "Question_body":"<p>I am trying to use one of the Hugging Face models with ML flow. My input is a pyspark DataFrame.\nThe issue is Mlflow doesn't support directly HuggingFace models, so need to use the flavor pyfunc to save it. So I need create a Python class that inherits from PythonModel and then place everything needed there.\nHow can I use a pandas_udf function inside this PythonModel? It keeps failing because I haven't specified the hint type for all the parameters inside my pandas_udf.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class RobertaClassifier(PythonModel):\n\n    def load_context(self, context: PythonModelContext):\n        import os\n        from transformers.models.auto import AutoConfig,   AutoModelForSequenceClassification\n        from transformers.models.auto.tokenization_auto import AutoTokenizer\n        \n        config_file = os.path.dirname(context.artifacts[&quot;config&quot;])\n        self.config = AutoConfig.from_pretrained(config_file)\n        self.tokenizer = AutoTokenizer.from_pretrained(config_file)\n        self.model = AutoModelForSequenceClassification.from_pretrained(config_file, config=self.config)\n        \n        if torch.cuda.is_available():\n            print('[INFO] Model is being sent to CUDA device as GPU is available')\n            self.model = self.model.cuda()\n        else:\n            print('[INFO] Model will use CPU runtime')\n        \n        _ = self.model.eval()\n    \n    \n    @pandas_udf(&quot;label string, score float&quot;)\n    def predict_batch_udf(self, data: pd.Series) -&gt; pd.Series:\n        import torch\n        import pandas as pd\n        \n        with torch.no_grad():\n            inputs = preprocessing(data['content'])\n            inputs = self.tokenizer(inputs, padding=True, return_tensors='pt', max_length=512, truncation=True)\n        \n            if self.model.device.index != None:\n                torch.cuda.empty_cache()\n                for key in inputs.keys():\n                    inputs[key] = inputs[key].to(self.model.device.index)\n\n            predictions = self.model(**inputs)\n            probs = torch.nn.Softmax(dim=1)(predictions.logits)\n            probs = probs.detach().cpu().numpy()\n\n            labels = probs.argmax(axis=1)\n            scores = probs.max(axis=1)\n\n            return labels, scores\n        \n    def predict(self, context: PythonModelContext, data: pd.Series) -&gt; pd.Series:\n        import math\n        import numpy as np\n        \n        batch_size = 64\n        sample_size = len(data)\n        \n        labels = np.zeros(sample_size)\n        scores = np.zeros(sample_size)\n\n        for batch_idx in range(0, math.ceil(sample_size \/ batch_size)):\n            bfrom = batch_idx * batch_size\n            bto = bfrom + batch_size\n            \n            l, s = self._predict_batch(data.iloc[bfrom:bto])\n            labels[bfrom:bto] = l\n            scores[bfrom:bto] = s\n            \n        return pd.DataFrame({'label': [self.config.id2label[l] for l in labels], \n                             'score': scores })\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663794595057,
        "Question_score":0,
        "Question_tags":"dataframe|class|pyspark|mlflow|pandas-udf",
        "Question_view_count":18,
        "Owner_creation_time":1663793210607,
        "Owner_last_access_time":1663883366877,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663827527230,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73806968",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56866214,
        "Question_title":"No usable temporary directory found with AWS Lambda function",
        "Question_body":"<p>I am trying to download a model with <code>mlflow<\/code> in an <code>aws lambda function<\/code> as described here: <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts<\/a><\/p>\n\n<p>However the following error is thrown:<\/p>\n\n<pre><code>  File \"\/tmp\/mlflow-api-server\/mlflow\/tracking\/artifact_utils.py\", line 66, in _download_artifact_from_uri\n  artifact_path=artifact_path, dst_path=output_path)\n  File \"\/tmp\/mlflow-api-server\/mlflow\/store\/artifact_repo.py\", line 94, in download_artifacts\n  dst_path = tempfile.mkdtemp()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 360, in mkdtemp\n  prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 130, in _sanitize_params\n  dir = gettempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 298, in gettempdir\n  tempdir = _get_default_tempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 233, in _get_default_tempdir\n  dirlist)\nFileNotFoundError: [Errno 2] No usable temporary directory found in ['\/tmp', '\/var\/tmp', '\/usr\/tmp']\n<\/code><\/pre>\n\n<p>The sklearn <code>model.pkl<\/code> file that <code>mlflow<\/code> should download has 627 Byte and the <code>aws lambda<\/code> limit should be 512 MB which should be enough space.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1562143532867,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|aws-lambda|mlflow",
        "Question_view_count":2058,
        "Owner_creation_time":1510064331503,
        "Owner_last_access_time":1664041948290,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56866214",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70010405,
        "Question_title":"Run experiments on Azure ML with Kedro and Mlflow",
        "Question_body":"<p>I'm trying to run the whole Kedro pipeline as an Azure ML experiment. I had two options here. The first one was to use the built-in logging feature of Azure ML and the second one was to use the azumeml-mlflow package that integrates Azure ML with Mlflow.<\/p>\n<p>I only tried the second approach as I did not know how to implement the Run() method of Azure ML inside the Kedro hooks.<\/p>\n<p>So, for the second approach, I presumed everything should be the same as when using Mlflow only. However, I couldn't get it to work even though it worked well outside of the Kedro structure ==&gt; I could launch experiments from other scripts.<\/p>\n<p>What I get with Kedro is that the pipeline runs well but nothing happens on Azure ML.<\/p>\n<p>Here's the code (hooks are inside a ModelTrackingHooks class):<\/p>\n<pre><code>@hook_impl\ndef before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to start an MLflow run\n    with the same run_id as the Kedro pipeline run.\n    &quot;&quot;&quot;\n\n\n    # Get Azure workspace\n    ws = Workspace.get(name=&quot;...&quot;,\n                       subscription_id=&quot;...&quot;,\n                       resource_group=&quot;...&quot;)\n    # Set tracking uri\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n\n    # Create an Azure ML experiment in the workspace\n    experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')\n    mlflow.set_experiment(experiment.name)\n\n    #Start logging\n    mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n    mlflow.log_params(run_params)        \n\n@hook_impl\ndef after_node_run(\n    self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.\n    In this example, we will:\n    * Log the parameters after the data splitting node runs.\n    * Log the model after the model training node runs.\n    * Log the model's metrics after the model evaluating node runs.\n    &quot;&quot;&quot;\n    \n    if node._func_name == &quot;cross_val&quot;:\n        mlflow.log_params(\n            {&quot;best_estimator&quot;: outputs[&quot;best_estimator&quot;],\n             &quot;best_params&quot;: outputs[&quot;best_params&quot;]}\n        )\n        model = outputs[&quot;validated_model&quot;]\n        mlflow.sklearn.log_model(model, &quot;model&quot;)\n\n    elif node._func_name == &quot;fit_and_save_transformer&quot;:\n        transformer = outputs[&quot;custom_transformer&quot;]\n        mlflow.sklearn.log_model(transformer, &quot;customer_transformer&quot;)\n\n    elif node._func_name == &quot;classification_reporting&quot;:\n        mlflow.log_metrics(outputs[&quot;metrics&quot;])\n    \n\n@hook_impl\ndef after_pipeline_run(self) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to end the MLflow run\n    after the Kedro pipeline finishes.\n    &quot;&quot;&quot;\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>Am I doing it the wrong way ?<\/p>\n<p>Do you have any idea or examples on how to use Kedro and Azure ML by leveraging only the built-in capabilities of Azure ML (i.e. without going through Mlflow) ?<\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1637177187910,
        "Question_score":0,
        "Question_tags":"python|mlflow|azure-machine-learning-service|kedro|mlops",
        "Question_view_count":266,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637184025436,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70010405",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73304176,
        "Question_title":"mlflow warning: Experiment ID mismatch for exp Iris. ID recorded as \u20181\u2019 in metadata\u2026",
        "Question_body":"<p>I\u2019m completely new to mlflow.<\/p>\n<p>I\u2019ve started with a couple of the standard tutorials and created examples using the well-known Iris and wine-quality datasets and named the experiments accordingly.<\/p>\n<p>When I run my code I get warnings:<\/p>\n<pre><code>WARNING:root:Experiment ID mismatch for exp iris.  ID recorded as \u20181\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp mlruns. ID recorded as \u20188\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp wine-quality. ID recorded as \u20185\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n<\/code><\/pre>\n<p>However, when I run the following in a jupyter notebook cell:<\/p>\n<pre><code>From mlflow.tracking import MlflowClient\n\nclient=Mlflow.Client()\n\nexperiments=client.list_experiments()\n\nexperiments\n<\/code><\/pre>\n<p>I see the list of experiments, including the 3 above, whose ID matches their names.<\/p>\n<p>When I look in the <code>mlruns\/wine-quality\/1<\/code> folder says, the <code>meta.yaml<\/code> states correctly that <code>Experiment id<\/code> is 5.<\/p>\n<p>Can someone help explain why I am getting these warnings?<\/p>\n<p>(When I run <code>mlflow ui \u2014backend-store-uri file:C:\/Users\/jb8\/mlruns<\/code> I see the experimental runs being logged as I\u2019d hope\u2026)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1660125120537,
        "Question_score":0,
        "Question_tags":"python|warnings|mlflow",
        "Question_view_count":36,
        "Owner_creation_time":1513941499320,
        "Owner_last_access_time":1663929826060,
        "Owner_location":"Portsmouth, United Kingdom",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660143634743,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73304176",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65824711,
        "Question_title":"Mlflow download_artifacts giving Not Found error",
        "Question_body":"<p>I have mlflow and minio running under docker compose. Mlflow successfully logs artifacts to minio and retrieves them. Minio and mlflow have their relevant ports 900 and 5000 exposed by docker.\nIf I run MlflowClient().download_artifacts for predictions from within the docker environment, everything works smoothly.\nIf I run MlflowClient().download_artifacts from outside docker (local machine or remotely), I get the below error. There is no issue in fetching the logged metrics.<\/p>\n<p>botocore.exceptions.ClientError: An error occurred (404) when calling the ListObjectsV2 operation: Not Found<\/p>\n<p>My code:<\/p>\n<pre><code>os.environ['AWS_ACCESS_KEY_ID'] = &quot;x&quot;\nmlflow.set_tracking_uri('http:\/\/10.0.0.1:5000')\nos.environ['AWS_SECRET_ACCESS_KEY'] = &quot;x&quot;\nos.environ['MINIO_ACCESS_KEY_ID'] = &quot;x&quot;\nos.environ['MINIO_SECRET_ACCESS_KEY'] = &quot;x\/me &quot;\n\nfrom mlflow.tracking import MlflowClient\nMlflowClient().download_artifacts(&quot;323e1527d49d4e77bd14c387bbdf6372&quot;, &quot;model&quot;, local_dir)\n<\/code><\/pre>\n<p>Any help would be most appreciated.<\/p>\n<p>Thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1611222048143,
        "Question_score":2,
        "Question_tags":"docker|docker-compose|mlflow",
        "Question_view_count":254,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824711",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_time":1415722650717,
        "Owner_last_access_time":1664051478173,
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Question_last_edit_time":1658083880967,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658304934100,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58956459,
        "Question_title":"How to run authentication on a mlFlow server?",
        "Question_body":"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.<\/p>\n\n<p>I use the following code to run the mlflow server<\/p>\n\n<p><code>mlflow server --host 0.0.0.0 --port 11111<\/code>\nworks perfect,in mybrowser i type <code>myip:11111<\/code> and i see everything (which eventually is the problem)<\/p>\n\n<p>If I understood the documentation and the following <a href=\"https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8\" rel=\"noreferrer\">https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8<\/a> link here correct, I should use nginx to create the authentication.<\/p>\n\n<p>I installed <code>nginx open sourcre<\/code>  and <code>apache2-utils<\/code><\/p>\n\n<p>created <code>sudo htpasswd -c \/etc\/apache2\/.htpasswd user1<\/code> user and passwords.<\/p>\n\n<p>I edited my <code>\/etc\/nginx\/nginx.conf<\/code> to the following:<\/p>\n\n<pre><code>server {\n        listen 80;\n        listen 443 ssl;\n\n        server_name my_ip;\n        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;\n        location \/ {\n            proxy_pass                      my_ip:11111\/;\n            auth_basic                      \"Restricted Content\";\n            auth_basic_user_file \/home\/path to the password file\/.htpasswd;\n        }\n    }\n<\/code><\/pre>\n\n<p><strong>but no authentication appears.<\/strong><\/p>\n\n<p>if I change the conf to listen to  <code>listen 11111<\/code>\nI get an error that the port is already in use ( of course, by the mlflow server....)<\/p>\n\n<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.<\/p>\n\n<p>would be happy to hear any suggestions.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1574259400087,
        "Question_score":11,
        "Question_tags":"nginx|basic-authentication|mlflow",
        "Question_view_count":13870,
        "Owner_creation_time":1554298968017,
        "Owner_last_access_time":1662838530057,
        "Owner_location":"wondeland",
        "Owner_reputation":1540,
        "Owner_up_votes":21,
        "Owner_down_votes":3,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the problem here is that both <code>mlflow<\/code> and <code>nginx<\/code> are trying to run on the <strong>same port<\/strong>... <\/p>\n\n<ol>\n<li><p>first lets deal with nginx:<\/p>\n\n<p>1.1 in \/etc\/nginx\/sites-enable make a new file <code>sudo nano mlflow<\/code> and delete the exist default.<\/p>\n\n<p>1.2 in mlflow file:<\/p><\/li>\n<\/ol>\n\n<pre><code>server {\n    listen YOUR_PORT;\n    server_name YOUR_IP_OR_DOMAIN;\n    auth_basic           \u201cAdministrator\u2019s Area\u201d;\n    auth_basic_user_file \/etc\/apache2\/.htpasswd; #read the link below how to set username and pwd in nginx\n\n    location \/ {\n        proxy_pass http:\/\/localhost:8000;\n        include \/etc\/nginx\/proxy_params;\n        proxy_redirect off;\n    }\n}\n<\/code><\/pre>\n\n<p>1.3.  restart nginx <code>sudo systemctl restart nginx<\/code><\/p>\n\n<ol start=\"2\">\n<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000<\/code><\/li>\n<\/ol>\n\n<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow<\/p>\n\n<ol start=\"3\">\n<li><p>now there are 2 options to tell the mlflow server about it:<\/p>\n\n<p>3.1 set username and pwd as environment variable \n<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd<\/code><\/p>\n\n<p>3.2 edit in your <code>\/venv\/lib\/python3.6\/site-packages\/mlflowpackages\/mlflow\/tracking\/_tracking_service\/utils.py<\/code> the function <\/p><\/li>\n<\/ol>\n\n<pre><code>def _get_rest_store(store_uri, **_):\n    def get_default_host_creds():\n        return rest_utils.MlflowHostCreds(\n            host=store_uri,\n            username=replace with nginx user\n            password=replace with nginx pwd\n            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),\n            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',\n        )\n<\/code><\/pre>\n\n<p>in your .py file where you work with mlflow:<\/p>\n\n<pre><code>import mlflow\nremote_server_uri = \"YOUR_IP_OR_DOMAIN:YOUR_PORT\" # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"\/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n<\/code><\/pre>\n\n<p>A link to nginx authentication doc <a href=\"https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/\" rel=\"noreferrer\">https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1576255052616,
        "Answer_score":8.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58956459",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58519367,
        "Question_title":"MLFlow model not logging to Azure Blob Storage",
        "Question_body":"<p>I am trying to use MLFlow to log artifacts to Azure Blob Storage. Though the logging to dbfs works fine, when I try to log it to Azure Blob Storage, I only see a folder with the corresponding runid but inside it there are no contents.<\/p>\n\n<p>Here is what I do-<\/p>\n\n<ol>\n<li><p>Create a experiment from Azure Databricks, give it a name and the artifacts location as wasbs:\/\/mlartifacts@myazurestorageaccount.blob.core.windows.net\/ .<\/p><\/li>\n<li><p>In the spark cluster, in the environemtn Variables section pass on the AZURE_STORAGE_ACCESS_KEY=\"ValueoftheKey\" <\/p><\/li>\n<li>In the notebook, use mlflow to log metrics, param and finally the model using a snippet like below<\/li>\n<\/ol>\n\n<pre><code>\nwith mlflow.start_run():\n      lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n      lr.fit(train_x, train_y)\n\n      predicted_qualities = lr.predict(test_x)\n\n      (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n      print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n      print(\"  RMSE: %s\" % rmse)\n      print(\"  MAE: %s\" % mae)\n      print(\"  R2: %s\" % r2)\n\n      mlflow.log_param(\"alpha\", alpha)\n      mlflow.log_param(\"l1_ratio\", l1_ratio)\n      mlflow.log_metric(\"rmse\", rmse)\n      mlflow.log_metric(\"r2\", r2)\n      mlflow.log_metric(\"mae\", mae)\n\n      mlflow.sklearn.log_model(lr, \"model\")\n<\/code><\/pre>\n\n<p>Of course before using it , I set the experiment to the one where I have defined the artifacts store to be azure blob storage<\/p>\n\n<pre><code>experiment_name = \"\/Users\/user@domain.com\/mltestazureblob\"\nmlflow.set_experiment(experiment_name)\n<\/code><\/pre>\n\n<p>The metrices and params I can from the MLFlow  UI within Databricks but as since my artifacts location is Azure Blob Storage , I expect the model, the .pkl and conda.yaml file to be in the container in the Azure Blob Storage but when I go to check it, I only see a folder corresponding to the run id of the experiment but with nothing inside.<\/p>\n\n<p>I do not know what I am missing. In case, someone needs additional details I will be happy to provide.<\/p>\n\n<p>Point to note everything works fine when I use the default location i.e. dbfs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1571821965343,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":812,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58519367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67062145,
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":131,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631884865500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70259594,
        "Question_title":"Setting array of tags to MLFlow registered model",
        "Question_body":"<p>I have a model registered in ML Flow and would like associate a list of tags to that model.\nBut when i looked at the reference APIs, it looks like we can add only one tag at a time with a single http request.<\/p>\n<pre><code>https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#set-registered-model-tag\n<\/code><\/pre>\n<p>Is it possible to create an array of tags and associate that with model in a single http call ?\nLike how we do during model creation API ?<\/p>\n<pre><code>https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#create-registeredmodel\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1638877684190,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1568969745383,
        "Owner_last_access_time":1663915778593,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1638878114696,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70259594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71151054,
        "Question_title":"How to log a table of metrics into mlflow",
        "Question_body":"<p>I am trying to see if mlflow is the right place to store my metrics in the model tracking.  According to the doc log_metric takes either a key value or a dict of key-values.  I am wondering how to log something like below into mlflow so it can be visualized meaningfully.<\/p>\n<pre><code>          precision    recall  f1-score   support\n\n  class1       0.89      0.98      0.93       174\n  class2       0.96      0.90      0.93        30\n  class3       0.96      0.90      0.93        30\n  class4       1.00      1.00      1.00         7\n  class5       0.93      1.00      0.96        13\n  class6       1.00      0.73      0.85        15\n  class7       0.95      0.97      0.96        39\n  class8       0.80      0.67      0.73         6\n  class9       0.97      0.86      0.91        37\n class10       0.95      0.81      0.88        26\n class11       0.50      1.00      0.67         5\n class12       0.93      0.89      0.91        28\n class13       0.73      0.84      0.78        19\n class14       1.00      1.00      1.00         6\n class15       0.45      0.83      0.59         6\n class16       0.97      0.98      0.97       245\n class17       0.93      0.86      0.89       206\n\naccuracy                           0.92       892\n<\/code><\/pre>\n<p>macro avg       0.88      0.90      0.88       892\nweighted avg       0.93      0.92      0.92       892<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1645058143563,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":546,
        "Owner_creation_time":1426639280947,
        "Owner_last_access_time":1650573965300,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71151054",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60530176,
        "Question_title":"mlflow How to save a sklearn pipeline with custom transformer?",
        "Question_body":"<p>I am trying to save with mlflow a sklearn machine-learning model, which is a pipeline containing a custom transformer I have defined, and load it in another project.\nMy custom transformer inherits from BaseEstimator and TransformerMixin.<\/p>\n\n<p>Let's say I have 2 projects:<\/p>\n\n<ul>\n<li>train_project: it has the custom transformers in src.ml.transformers.py<\/li>\n<li>use_project: it has other things in src, or has no src catalog at all<\/li>\n<\/ul>\n\n<p>So in my train_project I do :<\/p>\n\n<pre><code>mlflow.sklearn.log_model(preprocess_pipe, 'model\/preprocess_pipe')\n<\/code><\/pre>\n\n<p>and then when I try to load it into use_project :<\/p>\n\n<pre><code>preprocess_pipe = mlflow.sklearn.load_model(f'{ref_model_path}\/preprocess_pipe')\n<\/code><\/pre>\n\n<p>An error occurs :<\/p>\n\n<pre><code>[...]\nFile \"\/home\/quentin\/anaconda3\/envs\/api_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py\", line 210, in _load_model_from_local_file\n    return pickle.load(f)\nModuleNotFoundError: No module named 'train_project'\n<\/code><\/pre>\n\n<p>I tried to use format mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE :<\/p>\n\n<pre><code>mlflow.sklearn.log_model(preprocess_pipe, 'model\/preprocess_pipe', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n<\/code><\/pre>\n\n<p>but I get the same error during load.<\/p>\n\n<p>I saw option <strong>code_path<\/strong> into <strong>mlflow.pyfunc.log_model<\/strong> but its use and purpose is not clear to me. <\/p>\n\n<p>I thought mlflow provide a easy way to save model and serialize them so they can be used anywhere, Is that true only if you have native sklearn models (or keras, ...)?<\/p>\n\n<p>It's seem that this issue is more related to pickle functioning (mlflow use it and pickle needs to have all dependencies installed). <\/p>\n\n<p>The only solution I found so far is to make my transformer a package, import it in both project. Save version of my transformer library with <em>conda_env<\/em> argument of <em>log_model<\/em>, and check if it's same version when I load the model into my use_project.\nBut it's painfull if I have to change my transformer or debug in it...<\/p>\n\n<p>Is anybody have a better solution? \nMore elegent? Maybe there is some mlflow functionality I would have missed?<\/p>\n\n<p>other informations :<br>\nworking on linux (ubuntu)<br>\nmlflow=1.5.0<br>\npython=3.7.3   <\/p>\n\n<p>I saw in test of mlflow.sklearn api that they do a test with custom transformer, but they load it into the same file so it seems not resolve my issue but maybe it can helps other poeple :<\/p>\n\n<p><a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/tests\/sklearn\/test_sklearn_model_export.py\" rel=\"noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/tests\/sklearn\/test_sklearn_model_export.py<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583338083850,
        "Question_score":8,
        "Question_tags":"python|machine-learning|scikit-learn|pickle|mlflow",
        "Question_view_count":3317,
        "Owner_creation_time":1583251456143,
        "Owner_last_access_time":1583925117020,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60530176",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73729205,
        "Question_title":"MLFlow: how to get additional methods from a loaded model?",
        "Question_body":"<p><strong>Use case:<\/strong><\/p>\n<p>A <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.PyFuncModel<\/code><\/a> is defined with some more utilities methods in order to provide a way of parsing its prediction result to different formats.<\/p>\n<p><strong>After the model is loaded from registry, is there a way to access those methods?<\/strong><\/p>\n<p><strong>A contrived example:<\/strong><\/p>\n<p>A <code>mlflow.pyfunc.PyFuncModel<\/code> model defining additional methods:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        prediction = # do some prediction\n        return prediction\n\n    @staticmethod\n    def parse_prediction_to_format_x(prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n\n    def parse_prediction_to_format_y(self, prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n<\/code><\/pre>\n<p>Note: I added one static and one non static, because both use cases are relevant.<\/p>\n<p>Now, some other system goes to MLFlow Registry and loads the model from there:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>        loaded_model = mlflow.pyfunc.load_model(\n            model_uri=saved_model_path.absolute().as_uri()\n        )\n<\/code><\/pre>\n<p>This system, which naturally does not hold the model source code, but the registry path to load it from there, wants to use the additional methods above.\nIt can use predict, since it is part of all pyfunc models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n<\/code><\/pre>\n<p><strong>But how can this system access helper methods in the model class (static or instance methods)?<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n\n# pseudo code:\npredicted_and_formated = loaded_model.parse_prediction_to_format_y(predicted)\n<\/code><\/pre>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663236278537,
        "Question_score":0,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":15,
        "Owner_creation_time":1581629032807,
        "Owner_last_access_time":1663889245757,
        "Owner_location":null,
        "Owner_reputation":477,
        "Owner_up_votes":59,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73729205",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71292619,
        "Question_title":"Error on Spark MLFlow Model Registery using DataBricks after upgrade: cannot load trained XGBoost model",
        "Question_body":"<p>I had some code for training and then using XGBoost models on a Databricks environment. As my runtime version got deprecated, I upgraded it, but I quickly noticed I could not load my trained models anymore. The reason seems to be a change in the naming of functions in Sparkdl:<\/p>\n<pre><code>Error loading metadata: Expected class name sparkdl.xgboost.xgboost_core.XgboostClassifierModel but found class name sparkdl.xgboost.xgboost.XgboostClassifierModel\n<\/code><\/pre>\n<p>Would anyone have advise on how to fix this issue? Maybe modify the metadata?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1646039018097,
        "Question_score":1,
        "Question_tags":"pyspark|xgboost|azure-databricks|mlflow",
        "Question_view_count":111,
        "Owner_creation_time":1498319618333,
        "Owner_last_access_time":1662636545553,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71292619",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72238610,
        "Question_title":"how to save mlflow metrics and paramters to an s3 bucket without a server?",
        "Question_body":"<p>I want to save the parameters and metrics gotten from mlflow into an s3 bucket. Usually I get these from setting the <code>tracking_uri<\/code> in mlflow and that saves it on a server but I can't have a server in this case(was told no) and just want to store my parameters and metrics on the s3 bucket in the same manner as it would using the <code>tracking_uri<\/code>.<\/p>\n<p>I can store the artifacts on the s3 bucket without issue but not the params\/metrics.<\/p>\n<p>Here is some code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def mlflow_testing():\n    \n    tracking_uri =  &quot;s3:\/\/bucket_name\/mlflow\/&quot;,\n    experiment_name = &quot;test&quot;,\n    artifact_uri= &quot;s3:\/\/bucket_name\/mlflow\/&quot;\n    \n    mlflow.set_tracking_uri(tracking_uri)\n    mlflow.create_experiment(experiment_name, artifact_uri)\n    mlflow.set_experiment(experiment_name)\n    \n    with mlflow.start_run() as run:\n        mlflow.log_param(&quot;test1&quot;, 0)\n        mlflow.log_metric(&quot;test2&quot;, 1)\n    \n        with open(&quot;test.txt&quot;, &quot;w&quot;) as f:\n            f.write(&quot;this is an artifact&quot;)\n    \n        mlflow.log_artifact(&quot;test.txt&quot;)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>This is capable of storing the artifact text file on the s3 bucket(so long as I make the uri a local path like <code>local_data\/mlflow<\/code> instead of the s3 bucket).<\/p>\n<p>Setting the s3 bucket for the <code>tracking_uri<\/code> results in this error:<\/p>\n<pre><code>mlflow.tracking.registry.UnsupportedModelRegistryStoreURIException:\nModel registry functionality is unavailable; got unsupported URI\n's3:\/\/bucket_location\/mlflow\/' for model registry data storage.\nSupported URI schemes are: ['', 'file', 'databricks', 'http', 'https',\n'postgresql', 'mysql', 'sqlite', 'mssql']. See\nhttps:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to\nrun an MLflow server against one of the supported backend storage\nlocations.\n<\/code><\/pre>\n<p>Does anyone have advice on getting around this without setting up a server? I just want those metrics and params.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1652517689357,
        "Question_score":2,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow",
        "Question_view_count":818,
        "Owner_creation_time":1615994492347,
        "Owner_last_access_time":1657796021117,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652537683396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72238610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56370096,
        "Question_title":"Deploying ml model using azureml and mlflow",
        "Question_body":"<p>I'm new to databricks and deploying models using mlflow and azureml, I'm trying to deploy my model but haven't found a lot of documentation or examples.<\/p>\n\n<p>I have my model which I save using:<\/p>\n\n<pre><code>mlflow.sklearn.save_model(model, model_path, \n                          conda_env=conda_env_file_name)\n<\/code><\/pre>\n\n<p>I created the workspace and the aci webservice, the next step is to create the image and the webservice:<\/p>\n\n<pre><code># image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                                    runtime = \"python\", \n                                                    conda_file = conda_env_file_name)\n\n# Webservice creation\nmyservice = AciWebservice.deploy_from_model(\n  workspace=ws, \n  name=\"service\",\n  deployment_config = aciconfig,\n  models = [model_path],\n  image_config = myimage_config)\n\nmyservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>However when I try to create the webservice I receive an error and looking at the log:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Could not find an \"MLmodel\" configuration file at \"mode_path\"\n<\/code><\/pre>\n\n<p>My score file init function is like this:<\/p>\n\n<pre><code>def init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('model_path')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>It seems like it cannot find the path to the model. I'm not sure in the moment the image is saved, the model is not saved in it and thus it cannot be found by sklearn.load_model. I'm quite confused cause I've seen that a model can be deployed using mlflow or azureml. I think the problems is that mlflow.save_model does not register the model and then there's no path. Have someone been able to solve this? What is the best way to deploy a model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1559175361693,
        "Question_score":3,
        "Question_tags":"web-services|deployment|mlflow",
        "Question_view_count":933,
        "Owner_creation_time":1461539594160,
        "Owner_last_access_time":1663956732217,
        "Owner_location":null,
        "Owner_reputation":737,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Question_last_edit_time":1559831718176,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56370096",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68219396,
        "Question_title":"MLFlow SparkTrials maxNumConcurrentTasks([]) does not exist",
        "Question_body":"<p>I'm very new at using mlflow and I'm currently having some issues on its SparkTrials. I'm running the following code in my Jupyter notebook using Anaconda:<\/p>\n<pre><code>import mlflow\nfrom hyperopt import hp, fmin, tpe, rand, SparkTrials, STATUS_OK, STATUS_FAIL, space_eval\n\n# replicate input_pd dataframe to workers in Spark cluster\ninputs = sc.broadcast(input_pd)\n\n# configure hyperopt settings to distribute to all executors on workers\nspark_trials = SparkTrials()\n\n# select optimization algorithm\nalgo = tpe.suggest\n\n# perform hyperparameter tuning (logging iterations to mlflow)\nargmin = fmin(\n  fn=evaluate_model,\n  space=search_space,\n  algo=algo,\n  max_evals=100,\n  trials=spark_trials\n  )\n\n# release the broadcast dataset\ninputs.unpersist()\n<\/code><\/pre>\n<p>But, I get the following error:<\/p>\n<pre><code>  Py4JError: An error occurred while calling o233.maxNumConcurrentTasks. Trace:\n    py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n        at py4j.Gateway.invoke(Gateway.java:274)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\n        at java.lang.Thread.run(Unknown Source)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1625197883137,
        "Question_score":2,
        "Question_tags":"python|jupyter|mlflow",
        "Question_view_count":187,
        "Owner_creation_time":1617289926807,
        "Owner_last_access_time":1647594928473,
        "Owner_location":null,
        "Owner_reputation":165,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68219396",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63230793,
        "Question_title":"How to handle errors in MLflow when a model has been served using \"mlflow models serve\"?",
        "Question_body":"<p>During training, it is possible to use tags as a way to handle exceptions according to <a href=\"https:\/\/stackoverflow.com\/questions\/59856641\/how-can-i-throw-an-exception-from-within-an-mlflow-project\">this question<\/a>.<\/p>\n<p>If a model has been created using <code>mlflow.pyfunc.PythonModel<\/code>, is it possible to throw exceptions? Is there a way to allow error handling for a model that has been served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596462943310,
        "Question_score":0,
        "Question_tags":"python|rest|mlflow",
        "Question_view_count":266,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63230793",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60979310,
        "Question_title":"Not able to log model or artifact to Mlflow",
        "Question_body":"<p>I have a MLFlow remote server and I am able to log things from rstudio like:<\/p>\n\n<pre><code>  mlflow_log_param(\"param1\", 5)\n  mlflow_log_param(\"param2\", 5)\n  mlflow_log_metric(\"foo\", 1)\n  mlflow_log_metric(\"foo\", 2)\n  mlflow_log_metric(\"foo\", 3)\n<\/code><\/pre>\n\n<p>But when I try to log things like:<\/p>\n\n<pre><code>  writeLines(\"Hello world!\", \"output.txt\") \n  mlflow_log_artifact(\"output.txt\")\n<\/code><\/pre>\n\n<p>I have this error:<\/p>\n\n<pre><code>2020\/04\/01 21:45:27 INFO mlflow.store.artifact.cli: Logged artifact from local file output.txt to artifact_path=None\nRoot URI: .\/mlruns\/12\/3256cfd3cd1b44b99334040bd5c7c9ee\/artifacts\n<\/code><\/pre>\n\n<p>And when I try to log a model:<\/p>\n\n<pre><code> mlflow_log_model(predictor, \"model1\")\n<\/code><\/pre>\n\n<p>I have next error:<\/p>\n\n<pre><code> 2020\/04\/01 21:56:44 INFO mlflow.store.artifact.cli: Logged artifact from local dir D:\/user\/AppData\/Local\/Temp\/RtmpcBwDOP\/model1 to artifact_path=model1\n    Root URI: .\/mlruns\/12\/07d84e0f252a4248bb2473229297d318\/artifacts\n    # A tibble: 0 x 0\n    Warning message:\n    In value[[3L]](cond) :\n      Logging model metadata to the tracking server has failed, possibly due to older server version. The model artifacts have been logged successfully. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n<\/code><\/pre>\n\n<p>The server is updated and also the client.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1585771185263,
        "Question_score":2,
        "Question_tags":"r|mlflow",
        "Question_view_count":605,
        "Owner_creation_time":1521885997267,
        "Owner_last_access_time":1663857830210,
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1627866203927,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60979310",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57878280,
        "Question_title":"How do you start using MLflow SQL storage instead of the file system storage?",
        "Question_body":"<p>If I were getting started with MLflow, then how would I set up a database store? Is it sufficient to create a new MySQL database or a SQLite database and point MLflow to that?<\/p>\n\n<p>I tried to set the tracking URI, but that didn't create a database if it didn't exist.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568150011390,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":882,
        "Owner_creation_time":1364931488083,
        "Owner_last_access_time":1663957662590,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":31,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57878280",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73746436,
        "Question_title":"Mlflow-got error no host supplied,provided uri tracking,help me to resolve it",
        "Question_body":"<p>In below image can see i mention tracking uri and trying to load model but facing error in host supplied. <a href=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663338869023,
        "Question_score":0,
        "Question_tags":"mlflow|mlops",
        "Question_view_count":12,
        "Owner_creation_time":1576127245140,
        "Owner_last_access_time":1664053852373,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73746436",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65939058,
        "Question_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Question_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611843895217,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1611841996690,
        "Owner_last_access_time":1637883583270,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611845294603,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59051212,
        "Question_title":"MLFlow deployment example",
        "Question_body":"<p>I have  models  create  I want learn to deploy   ML Flow  model on production. can I get setp by sep  tutorial  whee  I can deply model.on my PC [assuming it to production env]<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1574772113833,
        "Question_score":0,
        "Question_tags":"deep-learning|artificial-intelligence|mlflow",
        "Question_view_count":70,
        "Owner_creation_time":1555011534987,
        "Owner_last_access_time":1647953103740,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59051212",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71687131,
        "Question_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Question_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648703157780,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":394,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":1648705670280,
        "Answer_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648708838088,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68356746,
        "Question_title":"Changing subdirectory of MLflow artifact store",
        "Question_body":"<p>Is there anything in the Python API that lets you alter the artifact subdirectories? For example, I have a .json file stored here:<\/p>\n<p><code>s3:\/\/mlflow\/3\/1353808bf7324824b7343658882b1e45\/artifacts\/feature_importance_split.json<\/code><\/p>\n<p>MlFlow creates a <code>3\/<\/code> key in s3. Is there a way to change to modify this key to something else (a date or the name of the experiment)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626152546053,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1493,
        "Owner_creation_time":1394078070850,
        "Owner_last_access_time":1663707363467,
        "Owner_location":null,
        "Owner_reputation":913,
        "Owner_up_votes":156,
        "Owner_down_votes":5,
        "Owner_views":88,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As I commented above, yes, <code>mlflow.create_experiment()<\/code> does allow you set the artifact location using the <code>artifact_location<\/code> parameter.<\/p>\n<p>However, sort of related, the problem with setting the <code>artifact_location<\/code> using the <code>create_experiment()<\/code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()<\/code> function again.<\/p>\n<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()<\/code> function again. And as of this post, MLfLow does not have <code>check_if_exists<\/code> flag or a <code>create_experiments_if_not_exists()<\/code> function.<\/p>\n<p>To make things more frustrating, you cannot set the <code>artifcact_location<\/code> in the <code>set_experiment()<\/code> function either.<\/p>\n<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.\n:<\/p>\n<pre><code>import os\nfrom random import random, randint\n\nfrom mlflow import mlflow,log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\ntry:\n    experiment = mlflow.get_experiment_by_name('oof')\n    experiment_id = experiment.experiment_id\nexcept AttributeError:\n    experiment_id = mlflow.create_experiment('oof', artifact_location='s3:\/\/mlflow-minio\/sample\/')\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    mlflow.set_tracking_uri('http:\/\/localhost:5000')\n    print(&quot;Running mlflow_tracking.py&quot;)\n\n    log_param(&quot;param1&quot;, randint(0, 100))\n    \n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id<\/code> does not exist and the <code>except<\/code> code block gets executed creating the experiment.<\/p>\n<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try<\/code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626213927412,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626236794420,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68356746",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62579563,
        "Question_title":"NameError: name 'dbutils' is not defined",
        "Question_body":"<p>I've .py file with following code line and it lives in git.<\/p>\n<pre><code>dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234') \n<\/code><\/pre>\n<p>I am using mlflow to run it in remote databricks job cluster. I've conda.yml and MLProject file to pick it up from git and run it in databricks job cluster but I am getting following error.<\/p>\n<pre><code>  File &quot;tea\/src\/cltv_xgb_tea.py&quot;, line 40, in &lt;module&gt;\n    dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234')\nNameError: name 'dbutils' is not defined\n<\/code><\/pre>\n<p>Any help\/solution is much appreciated.<\/p>\n<hr \/>\n<p>My current files in git<\/p>\n<p>Conda.yml has<\/p>\n<pre><code>name: cicd-environment\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - pip=19.0.3\n  - pip:\n    - mlflow==1.7.2\n    - DBUtils==1.3\n    - ipython==7.14.0\n    - databricks-connect==6.5.1\n    - invoke==1.4.1\n    - awscli==1.18.87\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1593101026770,
        "Question_score":0,
        "Question_tags":"pyspark|conda|databricks|mlflow",
        "Question_view_count":810,
        "Owner_creation_time":1555347036127,
        "Owner_last_access_time":1663694487237,
        "Owner_location":"Minnesota, USA",
        "Owner_reputation":352,
        "Owner_up_votes":27,
        "Owner_down_votes":1,
        "Owner_views":88,
        "Question_last_edit_time":1593403896032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62579563",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63133902,
        "Question_title":"How to send data to server for Prediction - MLflow",
        "Question_body":"<p>I am able to create ml model server using following command<\/p>\n<pre><code>mlflow models serve -m file:\/\/\/C:\/Users\/SawarkarFamily\/Desktop\/mlflow-master\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/9aeb7ba16d7e4c20870b664e267524ea\/artifacts\/model -p 8000\n2020\/07\/28 17:10:59 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2020\/07\/28 17:11:03 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d &amp; waitress-serve --host=127.0.0.1 --port=8000 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\nc:\\users\\sawarkarfamily\\anaconda3\\envs\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\lib\\site-packages\\waitress\\adjustments.py:441: DeprecationWarning: In future versions of Waitress clear_untrusted_proxy_headers will be set to True by default. You may opt-out by setting this value to False, or opt-in explicitly by setting this to True.\n  warnings.warn(\nServing on http:\/\/DESKTOP-AO59MJC:8000\n<\/code><\/pre>\n<p>In documentation it is given that send that for prediction using curl command as follows:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; --data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, &quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, &quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' http:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>I replaced port number with 8000, but getting error.<\/p>\n<pre><code>curl: (6) Could not resolve host: chlorides,\ncurl: (6) Could not resolve host: citric acid,\ncurl: (6) Could not resolve host: density,\ncurl: (6) Could not resolve host: fixed acidity,\ncurl: (6) Could not resolve host: free sulfur dioxide,\ncurl: (6) Could not resolve host: pH,\ncurl: (6) Could not resolve host: residual sugar,\ncurl: (6) Could not resolve host: sulphates,\ncurl: (6) Could not resolve host: total sulfur dioxide,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 17\ncurl: (6) Could not resolve host: 0.029,\ncurl: (6) Could not resolve host: 0.48,\ncurl: (6) Could not resolve host: 0.98,\ncurl: (6) Could not resolve host: 6.2,\ncurl: (6) Could not resolve host: 29,\ncurl: (6) Could not resolve host: 3.33,\ncurl: (6) Could not resolve host: 1.2,\ncurl: (6) Could not resolve host: 0.39,\ncurl: (6) Could not resolve host: 75,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 5\n{&quot;error_code&quot;: &quot;MALFORMED_REQUEST&quot;, &quot;message&quot;: &quot;Failed to parse input as a Pandas DataFrame. Ensure that the input is a valid JSON-formatted Pandas DataFrame with the `split` orient produced using the `pandas.DataFrame.to_json(..., orient='split')` method.&quot;, &quot;stack_trace&quot;: &quot;Traceback (most recent call last):\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\pyfunc\\\\scoring_server\\\\__init__.py\\&quot;, line 74, in parse_json_input\\n    return _dataframe_from_json(json_input, pandas_orient=orient, schema=schema)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\utils\\\\proto_json_utils.py\\&quot;, line 106, in _dataframe_from_json\\n    return pd.read_json(path_or_str, orient=pandas_orient, dtype=False,\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\util\\\\_decorators.py\\&quot;, line 214, in wrapper\\n    return func(*args, **kwargs)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 608, in read_json\\n    result = json_reader.read()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 731, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 753, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 857, in parse\\n    self._parse_no_numpy()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 1094, in _parse_no_numpy\\n    for k, v in loads(json, precise_float=self.precise_float).items()\\nValueError: Expected object or value\\n&quot;}\n<\/code><\/pre>\n<p>Kindly someone help me with this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1595938501813,
        "Question_score":3,
        "Question_tags":"python|json|curl|mlflow|mlops",
        "Question_view_count":1362,
        "Owner_creation_time":1595938236260,
        "Owner_last_access_time":1633010020190,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1595942184983,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63133902",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69841126,
        "Question_title":"Adjusting docker environment for each MLflow model",
        "Question_body":"<p>Imagine a situation, where you have two teams working on some models, for example XGBoost. They\ntrain it and then log the model to common MLflow Tracking Server. One of the teams is using older version,\nfor example 1.1 and the other team is using newest 1.6 version.<\/p>\n<p>Is it possible to use both of these models to make predictions inside one container,\nwhich downloades the models from MLflow tracking server? Since these two mentioned models\nuse different versions of XGboost, and the runtime in which they are going to be run has\ncertain version of the pip packages installed, there is going to be compatibility problem -&gt;\nonly one model will be able to run in this enviroment without changes. However, both of these models have\nrequirements.txt file saved as the artifact, and this file specifies the package versions, which the\ngiven model needs. Is it possible to adjust package versions running in the container according to\nthe currently used model using the requirements.txt artifact file? Or is the only\nsolution to create two separate docker images, for each of the models and with the\npackages that the model needs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636037303067,
        "Question_score":0,
        "Question_tags":"docker|pip|mlflow",
        "Question_view_count":95,
        "Owner_creation_time":1636036906520,
        "Owner_last_access_time":1663933591417,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69841126",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71203995,
        "Question_title":"RuntimeError: Java gateway process exited before sending its port number when Deploying Pyspark model to Azure Container Instance",
        "Question_body":"<p>I am trying to deploy a PySpark model trained in Azure Databricks with MLflow to an ACI in Azure Machine Learning.<\/p>\n<p>I am following the steps in this link:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks<\/a><\/p>\n<p>but I get this error:<\/p>\n<pre><code>SPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2022-02-21 09:29:30,269 | root | INFO | Starting up app insights client\nlogging socket was found. logging is available.\nlogging socket was found. logging is available.\n2022-02-21 09:29:30,270 | root | INFO | Starting up request id generator\n2022-02-21 09:29:30,270 | root | INFO | Starting up app insight hooks\n2022-02-21 09:29:30,270 | root | INFO | Invoking user's init function\nJAVA_HOME is not set\n2022-02-21 09:29:31,267 | root | ERROR | User's init function failed\n2022-02-21 09:29:31,268 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 191, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/execution_script.py&quot;, line 15, in init\n    model = load_model(model_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/spark.py&quot;, line 703, in _load_pyfunc\n    pyspark.sql.SparkSession.builder.config(&quot;spark.python.worker.reuse&quot;, True)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py&quot;, line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 144, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 339, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py&quot;, line 108, in launch_gateway\n    raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>from mlflow.deployments import get_deploy_client\n\n# set the tracking uri as the deployment client\nclient = get_deploy_client(mlflow.get_tracking_uri())\n\n# set the model path \nmodel_path = &quot;k_means_model&quot;\n\n    # define the model path and the name is the service name\n    # the model gets registered automatically and a name is autogenerated using the &quot;name&quot; parameter below \n    client.create_deployment(model_uri='runs:\/{}\/{}'.format(run_id, model_path), name = 'k-means-model-ml-flow')\n<\/code><\/pre>\n<p>While my model settings are:<\/p>\n<pre><code>artifact_path: k_means_model\ndatabricks_runtime: 10.3.x-cpu-ml-scala2.12\nflavors:\n  python_function:\n    data: sparkml\n    env: conda.yaml\n    loader_module: mlflow.spark\n    python_version: 3.8.10\n  spark:\n    model_data: sparkml\n    pyspark_version: 3.2.1\nmodel_uuid: 76ba9dfb01e1428ab8145a161ec3cf32\nrun_id: c0090fa9-b382-45b8-be08-d05e16f3cd62\nutc_time_created: '2022-02-21 08:47:34.967167'\n<\/code><\/pre>\n<p>Can someone help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1645436044400,
        "Question_score":1,
        "Question_tags":"pyspark|azure-databricks|azure-machine-learning-service|mlflow",
        "Question_view_count":289,
        "Owner_creation_time":1635865186583,
        "Owner_last_access_time":1657599189513,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1645439952347,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71203995",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62160734,
        "Question_title":"Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks",
        "Question_body":"<p>I'm using Azure Databricks + Hyperopt + MLflow for some hyperparameter tuning on a small dataset.  Seem like the job is running, and I get output in MLflow, but the job ends with the following error message:<\/p>\n\n<pre><code>Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks\n<\/code><\/pre>\n\n<p>Here is my code code with some information redacted:<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n\n# spark session initialization\nspark = (SparkSession.builder.getOrCreate())\nsc = spark.sparkContext\n\n# Data Processing\nimport pandas as pd\nimport numpy as np\n# Hyperparameter Tuning\nfrom hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, SparkTrials, STATUS_OK\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n# Modeling\nfrom sklearn.ensemble import RandomForestClassifier\n# cleaning\nimport gc\n# tracking\nimport mlflow\n# track runtime\nfrom datetime import date, datetime\n\nmlflow.set_experiment('\/user\/myname\/myexp')\n# notebook settings \\ variable settings\nn_splits = #\nn_repeats = #\nmax_evals = #\n\ndfL = pd.read_csv(\"\/my\/data\/loc\/mydata.csv\")\n\nx_train = dfL[['f1','f2','f3']]\ny_train = dfL['target']\n\ndef define_model(params):\n    model = RandomForestClassifier(n_estimators=int(params['n_estimators']),\n                                   criterion=params['criterion'], \n                                   max_depth=int(params['max_depth']), \n                                   min_samples_split=params['min_samples_split'], \n                                   min_samples_leaf=params['min_samples_leaf'], \n                                   min_weight_fraction_leaf=params['min_weight_fraction_leaf'], \n                                   max_features=params['max_features'], \n                                   max_leaf_nodes=None, \n                                   min_impurity_decrease=params['min_impurity_decrease'], \n                                   min_impurity_split=None, \n                                   bootstrap=params['bootstrap'], \n                                   oob_score=False, \n                                   n_jobs=-1, \n                                   random_state=int(params['random_state']), \n                                   verbose=0, \n                                   warm_start=False, \n                                   class_weight={0:params['class_0_weight'], 1:params['class_1_weight']})\n        return model\n\n\nspace = {'n_estimators': hp.quniform('n_estimators', #, #, #),\n         'criterion': hp.choice('#', ['#','#']),\n         'max_depth': hp.quniform('max_depth', #, #, #),\n         'min_samples_split': hp.quniform('min_samples_split', #, #, #),\n         'min_samples_leaf': hp.quniform('min_samples_leaf', #, #, #),\n         'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', #, #, #),\n         'max_features': hp.quniform('max_features', #, #, #),\n         'min_impurity_decrease': hp.quniform('min_impurity_decrease', #, #, #),\n         'bootstrap': hp.choice('bootstrap', [#,#]),\n         'random_state': hp.quniform('random_state', #, #, #),\n         'class_0_weight': hp.choice('class_0_weight', [#,#,#]),\n         'class_1_weight': hp.choice('class_1_weight', [#,#,#])}\n\n# define hyperopt objective\ndef objective(params, n_splits=n_splits, n_repeats=n_repeats):\n\n    # define model\n    model = define_model(params)\n    # get cv splits\n    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1331)\n    # define and run sklearn cv scorer\n    scores = cross_val_score(model, x_train, y_train, cv=kfold, scoring='roc_auc')\n    score = scores.mean()\n\n    return {'loss': score*(-1), 'status': STATUS_OK}\n\nspark_trials = SparkTrials(parallelism=36, spark_session=spark)\nwith mlflow.start_run():\n  best = fmin(objective, space, algo=tpe.suggest, trials=spark_trials, max_evals=max_evals)\n<\/code><\/pre>\n\n<p>and then at the end I get..<\/p>\n\n<pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200\/200 [1:35:28&lt;00:00, 100.49s\/trial, best loss: -0.9584565527065526]\n\nHyperopt failed to execute mlflow.end_run() with tracking URI: databricks\n\nException: 'MLFLOW_RUN_ID'\n\nTotal Trials: 200: 200 succeeded, 0 failed, 0 cancelled.\n<\/code><\/pre>\n\n<p>My Azure Databricks cluster is..<\/p>\n\n<pre><code>6.6 ML (includes Apache Spark 2.4.5, Scala 2.11)\nStandard_DS3_v2\nmin 9 max 18 nodes\n<\/code><\/pre>\n\n<p>Am I doing something wrong or is this a bug?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1591129195507,
        "Question_score":2,
        "Question_tags":"pyspark|databricks|azure-databricks|mlflow|hyperopt",
        "Question_view_count":604,
        "Owner_creation_time":1528603052363,
        "Owner_last_access_time":1663971435213,
        "Owner_location":null,
        "Owner_reputation":247,
        "Owner_up_votes":637,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62160734",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73448308,
        "Question_title":"How to configure `backend-store-uri` with huggingface Trainer",
        "Question_body":"<p>When configuring a Hugging Face TrainingArguments <a href=\"https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html<\/a> you can set the <code>logging_dir<\/code> and <code>output_dir<\/code>.<\/p>\n<p>There is also the <code>mlruns<\/code> directory which according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores<\/a> you can configure using <code>--set-backend-uri<\/code>. Though that is an mlflow doc, not a Hugging Face doc.<\/p>\n<p>What is the best way to specify a different <code>mlruns<\/code> directory programmatically when setting up the Hugging Face Trainer?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661185918393,
        "Question_score":0,
        "Question_tags":"mlflow|huggingface",
        "Question_view_count":19,
        "Owner_creation_time":1484305691263,
        "Owner_last_access_time":1663933845823,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":161,
        "Owner_up_votes":169,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73448308",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68668190,
        "Question_title":"Shape error while converting Image to JSON file",
        "Question_body":"<p>I am trying to convert image to JSON file and POST it with REST API by using MLFLow. Below you can see my code. I got an error like &quot;cannot reshape array of size 535500 into shape (1,4096)&quot;. Can you please help me. Thank you in advance.<\/p>\n<pre><code>import json\nimport cv2\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\n\n\ndata = np.asarray(Image.open('Dataset\/test2\/dog_PNG50348.png').convert('LA'))\ndata = data.reshape((1, 64*64))\ncolumns = [f&quot;col_{c}&quot; for c in range(0, data[0].shape[0])]\ndct = {&quot;columns&quot;: columns, &quot;data&quot;: [data[0].tolist()]}\nprint(json.dumps(dct, indent=2) + &quot;\\n&quot;)\n\n#print(data)\nheaders = {'Content-Type': 'application\/json'}\nrequest_uri = 'http:\/\/127.0.0.1:5000\/invocations'\n\nif __name__ == '__main__':\n    try:\n        response = requests.post(request_uri, data=json.dumps(dct,indent=2)+&quot;\\n&quot;, headers=headers)\n        print(response.content)\n        print('done!!!')\n    except Exception as ex:\n        raise (ex)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1628172503093,
        "Question_score":0,
        "Question_tags":"python|json|numpy|rest|mlflow",
        "Question_view_count":70,
        "Owner_creation_time":1460657116247,
        "Owner_last_access_time":1652910305520,
        "Owner_location":null,
        "Owner_reputation":98,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68668190",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73789674,
        "Question_title":"How can I know how many iterations are left when tuning accross multiple hyperparameters in SparkML?",
        "Question_body":"<p>I'm running a crossvalidation accross a grid of multiple hyperparameters with XgBoost model using Pyspark in Databricks and I would like to know the progress of this operation...So far it has been running for almost 24 hours and I have no idea if it's halfway done or only 10% of the way. I have a 128k combinations of hyperparameters of 5 folds each so a total of 640k runs...<\/p>\n<p>I've tried clicking on MLflow logged run but it's an empty page with an UNFINISHED status. Is there any way to know the progress ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663690227917,
        "Question_score":0,
        "Question_tags":"databricks|cross-validation|apache-spark-mllib|hyperparameters|mlflow",
        "Question_view_count":13,
        "Owner_creation_time":1558118783690,
        "Owner_last_access_time":1663939366183,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663694262408,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73789674",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57693162,
        "Question_title":"Specify database backend store creation in specific schema",
        "Question_body":"<p>When creating an mlflow tracking server and specifying that a SQL Server database is to be used as a backend store, mlflow creates a bunch of table within the dbo schema. Does anyone know if it is possible to specify a different schema in which to create these tables?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1566997596983,
        "Question_score":1,
        "Question_tags":"python|sqlalchemy|mlflow",
        "Question_view_count":823,
        "Owner_creation_time":1566996760530,
        "Owner_last_access_time":1630269065473,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57693162",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72368541,
        "Question_title":"Does mlflow support spacy model serving\/ life-cycle management",
        "Question_body":"<p>How much does MLflow support for spaCy model lifecycle management?<\/p>\n<p>SpaCy model building <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/spacy\" rel=\"nofollow noreferrer\">example<\/a> is given here.<\/p>\n<p>But model serving is failing and showing below error:\n<a href=\"https:\/\/i.stack.imgur.com\/04InD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/04InD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653420858453,
        "Question_score":0,
        "Question_tags":"spacy|cicd|mlflow",
        "Question_view_count":75,
        "Owner_creation_time":1495572503257,
        "Owner_last_access_time":1664063965120,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1653464335820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72368541",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63175684,
        "Question_title":"fitting and predicting model with mlflow",
        "Question_body":"<p>I'm very new to understanding the use of MLFlow but need assistance, I'm trying to understand on how to try and fit and predict my model once again. I'm able to call my model by:<\/p>\n<pre><code>PLS_model = mlflow.pyfunc.load_model(&quot;runs:\/FFFFF!@#!@#@!#!\/logged_model&quot;, suppress_warnings = True)\n<\/code><\/pre>\n<p>and get:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n  artifact_path: logged_model\n  flavor: mlflow.sklearn\n  run_id: FFFFF!@#!@#@!#!\n<\/code><\/pre>\n<p>But when I try to call any methods as:<\/p>\n<p>1).fit or .predict. I get the following error<\/p>\n<pre><code>AttributeError: 'PyFuncModel' object has no attribute 'fit'\n\nAttributeError: 'PyFuncModel' object has no attribute 'predict'\n<\/code><\/pre>\n<p>Here I encountered on how to actually call these functions but not sure if I'm doing this correctly. In summary, how can I predict, fit to my new data.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596120433613,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1702,
        "Owner_creation_time":1508861451607,
        "Owner_last_access_time":1661949233680,
        "Owner_location":"Netherlands",
        "Owner_reputation":458,
        "Owner_up_votes":104,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63175684",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68059536,
        "Question_title":"Could not connect to MLFlow model hosted on Docker",
        "Question_body":"<p>I hosted a model inside a docker container.\nOn running the DockerFile, It runs the following command:<\/p>\n<p><code>mlflow models serve -m model --port 8080 --no-conda<\/code><\/p>\n<p>It serves the model succesfully , And I can now make calls to it.\nBut, I keep getting Max retries exceeded with url<\/p>\n<p>When I host the same model without using Docker(And follow the same steps), it works perfectly.<\/p>\n<p>I use the following command to run the docker container\n<code>docker run -it --rm --network host imagename:random<\/code><\/p>\n<p>I have also tried mapping port 8080, But still not able to get a response.<\/p>\n<p>Not able to understand what the possible issues could be.<\/p>\n<p>Dockerfile for reference<\/p>\n<pre><code>  \nFROM ubuntu:20.04\nENV DEBIAN_FRONTEND=noninteractive\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential software-properties-common\\\n    libboost-dev libboost-serialization-dev libssl-dev \\\n    cmake vim\\\n    wget \\\n    make libbz2-dev libexpat1-dev swig python-dev\nRUN add-apt-repository -y ppa:ubuntugis\/ppa &amp;&amp; apt-get -q update\nRUN apt-get -y install gdal-bin libgdal-dev\nRUN apt-get update\n\nRUN apt install -y python3-pip\nRUN pip3 install --upgrade pip\nRUN pip install mlflow\nRUN pip install pandas\n\nRUN mkdir -p \/tmp\nCOPY .\/main.py \/tmp\/\nCOPY .\/run.sh \/tmp\/\n\nENV LC_ALL=C.UTF-8\nENV LANG=C.UTF-8\nRUN chmod +x run.sh\nCMD .\/run.sh\n<\/code><\/pre>\n<p>Where, run.sh is<\/p>\n<pre><code>python3 main.py\nmlflow models serve -m \/tmp\/mlflow_model --port 8080 --no-conda\n<\/code><\/pre>\n<p>When I run the commands of run.sh file outside of docker container, It is able to serve the model correctly,And I get the correct response.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624218724787,
        "Question_score":1,
        "Question_tags":"docker|python-requests|port|mlflow",
        "Question_view_count":195,
        "Owner_creation_time":1539037621213,
        "Owner_last_access_time":1658940050953,
        "Owner_location":"New Delhi, Delhi, India",
        "Owner_reputation":75,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68059536",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69905990,
        "Question_title":"Registering model without weights with MLFLow",
        "Question_body":"<p>I would like to be able to register untrained models with MLFLow to use as prototypes for instantiating models for training. I need this because we need to train thousands of models of the same type. Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636497891150,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":46,
        "Owner_creation_time":1636497545873,
        "Owner_last_access_time":1640893909690,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69905990",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61266578,
        "Question_title":"How can an mlflow model be scaled to serve more requests?",
        "Question_body":"<p>I would like to have multiple instances of my MLFlow model running in parallel but hidden behind a common the same endpoint\/port so it's not visible to the user. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587109863510,
        "Question_score":0,
        "Question_tags":"multithreading|gunicorn|mlflow",
        "Question_view_count":228,
        "Owner_creation_time":1579017872663,
        "Owner_last_access_time":1637341304770,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61266578",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73499320,
        "Question_title":"Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00",
        "Question_body":"<p>I'm using Databricks Connect version 9.1.16 to connect to a databricks external cluster with spark version 3.1 and download a Pyspark ML model that's been trained and saved using mlflow.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(&quot;databricks&quot;)\nmodel_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n<\/code><\/pre>\n<p>I get the following output and error:<\/p>\n<pre><code>2022\/08\/26 11:54:18 INFO mlflow.spark: 'models:\/model_name\/model_version' resolved as 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model'\n2022\/08\/26 11:54:25 INFO mlflow.spark: URI 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' does not point to the current DFS.\n2022\/08\/26 11:54:25 INFO mlflow.spark: File 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' not found on DFS. Will attempt to upload the file.\n2022\/08\/26 11:55:06 INFO mlflow.spark: Copied SparkML model to \/tmp\/mlflow\/model_id\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\nc:\\Users\\carlafernandez\\Documents\\my_notebook.ipynb Celda 5 in &lt;cell line: 2&gt;()\n      1 mlflow.set_tracking_uri(&quot;databricks&quot;)\n----&gt; 2 model_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:711, in load_model(model_uri, dfs_tmpdir)\n    708 local_model_path = _download_artifact_from_uri(model_uri)\n    709 _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n--&gt; 711 return _load_model(model_uri=model_uri, dfs_tmpdir_base=dfs_tmpdir)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:660, in _load_model(model_uri, dfs_tmpdir_base)\n    658     return _load_model_databricks(model_uri, dfs_tmpdir)\n    659 model_uri = _HadoopFileSystem.maybe_copy_from_uri(model_uri, dfs_tmpdir)\n--&gt; 660 return PipelineModel.load(model_uri)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:463, in MLReadable.load(cls, path)\n    460 @classmethod\n    461 def load(cls, path):\n    462     &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;\n--&gt; 463     return cls.read().load(path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\pipeline.py:258, in PipelineModelReader.load(self, path)\n    256 metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n    257 if 'language' not in metadata['paramMap'] or metadata['paramMap']['language'] != 'Python':\n--&gt; 258     return JavaMLReader(self.cls).load(path)\n    259 else:\n    260     uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:413, in JavaMLReader.load(self, path)\n    411 if not isinstance(path, str):\n    412     raise TypeError(&quot;path should be a string, got type %s&quot; % type(path))\n--&gt; 413 java_obj = self._jread.load(path)\n    414 if not hasattr(self._clazz, &quot;_from_java&quot;):\n    415     raise NotImplementedError(&quot;This Java ML type cannot be loaded into Python currently: %r&quot;\n    416                               % self._clazz)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\java_gateway.py:1304, in JavaMember.__call__(self, *args)\n   1298 command = proto.CALL_COMMAND_NAME +\\\n   1299     self.command_header +\\\n   1300     args_command +\\\n   1301     proto.END_COMMAND_PART\n   1303 answer = self.gateway_client.send_command(command)\n-&gt; 1304 return_value = get_return_value(\n   1305     answer, self.gateway_client, self.target_id, self.name)\n   1307 for temp_arg in temp_args:\n   1308     temp_arg._detach()\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\sql\\utils.py:117, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    115 def deco(*a, **kw):\n    116     try:\n--&gt; 117         return f(*a, **kw)\n    118     except py4j.protocol.Py4JJavaError as e:\n    119         converted = convert_exception(e.java_exception)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         &quot;An error occurred while calling {0}{1}{2}.\\n&quot;.\n    328         format(target_id, &quot;.&quot;, name), value)\n    329 else:\n    330     raise Py4JError(\n    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;.\n    332         format(target_id, &quot;.&quot;, name, value))\n\nPy4JJavaError: An error occurred while calling o645.load.\n: java.io.StreamCorruptedException: invalid type code: 00\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n    at sun.reflect.GeneratedMethodAccessor311.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6631)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6616)\n    at com.databricks.service.SparkServiceRPCHandler.execute0(SparkServiceRPCHandler.scala:728)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC0$1(SparkServiceRPCHandler.scala:477)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:372)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:323)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:309)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC$1(SparkServiceRPCHandler.scala:359)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:336)\n    at com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:167)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n    at org.eclipse.jetty.server.Server.handle(Server.java:516)\n    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>\n<p>So it seems like it's able to find a copy the model, but then somehow it cannot read it. It's worth noting that the same <strong>works in a databricks notebook<\/strong>, the problem only occurs using databricks connect.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661508308487,
        "Question_score":0,
        "Question_tags":"pyspark|databricks|mlflow|databricks-connect",
        "Question_view_count":23,
        "Owner_creation_time":1506240907953,
        "Owner_last_access_time":1663933103760,
        "Owner_location":"Madrid, Espa\u00f1a",
        "Owner_reputation":56,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73499320",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71679081,
        "Question_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Question_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648650339347,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|nginx|docker-compose|mlflow",
        "Question_view_count":625,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652276299263,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1652448943407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56328367,
        "Question_title":"Linear regression model with integration of MLFlow",
        "Question_body":"<p>Does anybody has a link to sample Linear Regression code integrated with MLFlow and explaining all three concepts of MLFlow i.e. Tracking, Project and Model? <\/p>\n\n<p>I'm particularly looking for a demo link to the same.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1558968873863,
        "Question_score":1,
        "Question_tags":"python|machine-learning|linear-regression|azure-databricks|mlflow",
        "Question_view_count":204,
        "Owner_creation_time":1453907226643,
        "Owner_last_access_time":1604947189130,
        "Owner_location":null,
        "Owner_reputation":329,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56328367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":54233746,
        "Question_title":"Filter mlflow runs by commit ID",
        "Question_body":"<p>When using the UI of MlFlow, is it possible to filter\/search the runs using the (git) commit ID? I manage to search by parameters but it doesn't seem like there's a way to filter by the commit ID.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/npkFO.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/npkFO.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1547720560897,
        "Question_score":4,
        "Question_tags":"machine-learning|version-control|mlflow",
        "Question_view_count":982,
        "Owner_creation_time":1300789717227,
        "Owner_last_access_time":1663941101560,
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Question_last_edit_time":1547796230323,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54233746",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60633328,
        "Question_title":"Mlflow it is possible to log confusion matrix every step?",
        "Question_body":"<p>It is possible to log with mlflow the confusion matrix every step like a simple metrics?\nIf it is possible it have a visualization like this?\n<a href=\"https:\/\/i.stack.imgur.com\/bYbgo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bYbgo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1583920669630,
        "Question_score":0,
        "Question_tags":"data-visualization|confusion-matrix|mlflow",
        "Question_view_count":2334,
        "Owner_creation_time":1547467399037,
        "Owner_last_access_time":1663856296190,
        "Owner_location":"Busto Arsizio, VA, Italia",
        "Owner_reputation":171,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1584009263430,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60633328",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69482686,
        "Question_title":"logging models in mlflow with a pyspark process in Kerberized HDP 3.1.5",
        "Question_body":"<p>I'm currently testing mlflow to log pyspark models in a HDP3.1.x Cluster KERBERIZED.\nI've configured mlflow to use HDFS (of the same HDP cluster) for model storage.<\/p>\n<p>Whenever I launch a pyspark process to log a model on MLFlow with &quot;spark-submit --deploy-mode=cluster ...&quot;, I've got the exception<\/p>\n<blockquote>\n<p>AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]java.io.IOException: DestHost:destPort\nnamenode01.hdp.site:8020 , LocalHost:localPort\nworker05.hdp.site\/192.168.0.208:0. Failed on local exception:\njava.io.IOException:\n<strong>org.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]<\/strong><\/p>\n<p>(...)<\/p>\n<p>Caused by: java.io.IOException:\norg.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]    at\norg.apache.hadoop.ipc.Client$Connection$1.run(Client.java:758)    at\njava.security.AccessController.doPrivileged(Native Method)    at\njavax.security.auth.Subject.doAs(Subject.java:422)*<\/p>\n<\/blockquote>\n<p>It seems that libhdfs used by mlflow cannot properly authenticate with delegation tokens. Do you know any way to fix or circumvent this problem?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1633615788157,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|kerberos|mlflow|hdp",
        "Question_view_count":102,
        "Owner_creation_time":1633614827870,
        "Owner_last_access_time":1663923745037,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1633622029736,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69482686",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70680222,
        "Question_title":"Does MLflow allow to log artifacts from remote locations like S3?",
        "Question_body":"<h2>My setting<\/h2>\n<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>\/opt\/ml\/model<\/code> directory, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">which is reserved by SageMaker to pack models<\/a> as a <code>.tar.gz<\/code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">Scenario 4<\/a>). Note that this is a different S3 bucket than SageMaker's.<\/p>\n<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.<\/p>\n<p>A limitation is that, as I understand it, the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">MLflow API for logging artifacts<\/a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.<\/p>\n<h2>My questions<\/h2>\n<ul>\n<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?<\/li>\n<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641984552913,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow|mlops",
        "Question_view_count":533,
        "Owner_creation_time":1452286548080,
        "Owner_last_access_time":1663928318030,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":118,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70680222",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59363969,
        "Question_title":"mlflow: problems with pip installation",
        "Question_body":"<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.\nI installed mlflow with :<\/p>\n\n<pre><code>    pip3 install mlflow\n<\/code><\/pre>\n\n<p>so mlflow is installed in \/usr\/local\/bin\/mlflow<\/p>\n\n<p>Since it is not in \/Users\/xxxx\/opt\/anaconda3\/lib\/python3.7\/site-packages, I get \"ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1576528631793,
        "Question_score":1,
        "Question_tags":"pip|python-import|python-3.7|importerror|mlflow",
        "Question_view_count":9843,
        "Owner_creation_time":1524783572553,
        "Owner_last_access_time":1639508068437,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":87,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59363969",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72604450,
        "Question_title":"MLflow load model fails Python",
        "Question_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655130099670,
        "Question_score":2,
        "Question_tags":"python|docker|databricks|mlflow",
        "Question_view_count":109,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1655134670387,
        "Answer_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655191745992,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655202841060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67456172,
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1151,
        "Owner_creation_time":1316620102630,
        "Owner_last_access_time":1643692547643,
        "Owner_location":null,
        "Owner_reputation":1308,
        "Owner_up_votes":177,
        "Owner_down_votes":1,
        "Owner_views":151,
        "Question_last_edit_time":1620554070856,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1620627546968,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67835498,
        "Question_title":"MLflow - How to point interface path to show the expected result",
        "Question_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622801740880,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":379,
        "Owner_creation_time":1445157877637,
        "Owner_last_access_time":1638455223627,
        "Owner_location":null,
        "Owner_reputation":267,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622968783720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67835498",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59887618,
        "Question_title":"Is it possible to use MLFlow and H2o.ai sparkling water in a Scala based project?",
        "Question_body":"<p>I'm solving a Scala data science problem in Intellij using maven. I noticed that MLFlow spark (<a href=\"https:\/\/mvnrepository.com\/artifact\/org.mlflow\/mlflow-spark\/1.5.0\" rel=\"nofollow noreferrer\">https:\/\/mvnrepository.com\/artifact\/org.mlflow\/mlflow-spark\/1.5.0<\/a>) is dependent on scala 2.12 while h2o.ai sparkling water is dependent on scala 2.11 (<a href=\"https:\/\/mvnrepository.com\/artifact\/ai.h2o\/sparkling-water-core\" rel=\"nofollow noreferrer\">https:\/\/mvnrepository.com\/artifact\/ai.h2o\/sparkling-water-core<\/a>). Is there any way to use both of these together using Scala? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1579816453420,
        "Question_score":0,
        "Question_tags":"scala|apache-spark|h2o|sparkling-water|mlflow",
        "Question_view_count":217,
        "Owner_creation_time":1579815947657,
        "Owner_last_access_time":1661917140247,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59887618",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68402406,
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626398499507,
        "Question_score":1,
        "Question_tags":"scikit-learn|webserver|mlflow",
        "Question_view_count":787,
        "Owner_creation_time":1553088438367,
        "Owner_last_access_time":1664054397820,
        "Owner_location":"Tehran, Tehran Province, Iran",
        "Owner_reputation":415,
        "Owner_up_votes":300,
        "Owner_down_votes":2,
        "Owner_views":37,
        "Question_last_edit_time":1657456057947,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626398499507,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73620580,
        "Question_title":"Hi everyone, I'm trying to authentication and authorize MLFlow my infra is in AWS and my mlflow will run in eks ? can we do it with mlflow plugin",
        "Question_body":"<p>Do we have any way to do this?<\/p>\n<ol>\n<li>I tried to authenticate using AWS ALB that did not work for me.<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1662460519567,
        "Question_score":0,
        "Question_tags":"kubernetes|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1562213601020,
        "Owner_last_access_time":1663912595940,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73620580",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67968777,
        "Question_title":"How to get run id from run name in MLflow",
        "Question_body":"<p>To download artifacts from a run, you need run id. I get the run id from the UI as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FJtAe.png\" rel=\"nofollow noreferrer\">Run id from the UI<\/a><\/p>\n<p>But when I set the run name parameter, run id is not visible in the UI. How to find the run Id of a particular run in MLflow ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1623667148760,
        "Question_score":2,
        "Question_tags":"mlflow",
        "Question_view_count":1492,
        "Owner_creation_time":1585919790787,
        "Owner_last_access_time":1641475609990,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67968777",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64645798,
        "Question_title":"How to log a sklearn pipeline with a Keras step using mlflow.pyfunc.log_model()? TypeError: can't pickle _thread.RLock objects",
        "Question_body":"<p>I would like to log into MlFlow a <code>sklearn<\/code> pipeline with a Keras step.<\/p>\n<p>The pipeline has 2 steps: a <code>sklearn<\/code> StandardScale and a Keras TensorFlow model.<\/p>\n<p>I am using mlflow.pyfunc.log_model() as possible solution, but I am having this error:<\/p>\n<pre><code>TypeError: can't pickle _thread.RLock objects\n---&gt;   mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)\n<\/code><\/pre>\n<p>Here is my code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport keras\nfrom keras import layers, Input\nfrom keras.wrappers.scikit_learn import KerasRegressor\nimport mlflow.pyfunc\nfrom sklearn.pipeline import Pipeline\nfrom mlflow.models.signature import infer_signature\n\n#toy dataframe\ndf1 = pd.DataFrame([[1,2,3,4,5,6], [10,20,30,40,50,60],[100,200,300,400,500,600]] )\n\n#create train test datasets\nX_train, X_test = train_test_split(df1, random_state=42, shuffle=True)\n\n#scale X_train\nscaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_train_s = pd.DataFrame(X_train_s)\n\n#wrap the keras model to use it inside of sklearn pipeline\ndef create_model(optimizer='adam', loss='mean_squared_error', s = X_train.shape[1]):\n  input_layer = keras.Input(shape=(s,))\n  # &quot;encoded&quot; is the encoded representation of the input\n  encoded = layers.Dense(25, activation='relu')(input_layer)\n  encoded = layers.Dense(2, activation='relu')(encoded)\n\n  # &quot;decoded&quot; is the lossy reconstruction of the input\n  decoded = layers.Dense(2, activation='relu')(encoded)\n  decoded = layers.Dense(25, activation='relu')(encoded)\n  decoded = layers.Dense(s, activation='linear')(decoded)\n  \n  model = keras.Model(input_layer, decoded)\n  model.compile(optimizer, loss)\n  return model\n\n# wrap the model\nmodel = KerasRegressor(build_fn=create_model, verbose=1)\n\n# create the pipeline\npipe = Pipeline(steps=[\n    ('scale', StandardScaler()),\n    ('model',model)\n])\n\n#function to wrap the pipeline to be logged by mlflow\nclass SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n    \n  def predict(self, context, model_input):\n    return self.model.predict(model_input)[:,1]\n  \n  \nmlflow.end_run()\nwith mlflow.start_run(run_name='test1'):\n\n  #train the pipeline\n  pipe.fit(X_train, X_train_s, model__epochs=2)\n  \n  #wrap the model for mlflow log\n  wrappedModel = SklearnModelWrapper(pipe)\n\n  # Log the model with a signature that defines the schema of the model's inputs and outputs. \n  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n  mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)\n  \n<\/code><\/pre>\n<p>From what I googled, it seems like this type of error is related to concurrency of threads. It could be then related to the TensorFlow, since it distributes the code during the model training phase.<\/p>\n<p>However, the offending code line is after the training phase. If I remove this line, the rest of the code works, which makes me think that it happens after the concurrency phase of the model training. I have no idea why I am getting this error in this context.\nI am a beginner? Can someone please help me?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1604320799543,
        "Question_score":4,
        "Question_tags":"python|keras|scikit-learn|mlflow",
        "Question_view_count":787,
        "Owner_creation_time":1339148818537,
        "Owner_last_access_time":1616602620223,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1604334305112,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64645798",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61615818,
        "Question_title":"Setting-up MLflow on Google Colab",
        "Question_body":"<p>I frequently use Google Colab to train TF\/PyTorch models as Colab provides me with GPU\/TPU runtime. Besides, I like working with MLflow to store and compare trained models, tracking progress, sharing, etc.  What are the available solutions to use MLflow with Google Colab?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1588689839033,
        "Question_score":7,
        "Question_tags":"google-colaboratory|mlflow|mlops",
        "Question_view_count":6053,
        "Owner_creation_time":1552661046210,
        "Owner_last_access_time":1663946284313,
        "Owner_location":"Paris, France\/ Ternopil, Ukraine",
        "Owner_reputation":1131,
        "Owner_up_votes":55,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":1621937850167,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61615818",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63973530,
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600550858547,
        "Question_score":3,
        "Question_tags":"scikit-learn|save|xgboost|mlflow|xgbclassifier",
        "Question_view_count":1317,
        "Owner_creation_time":1592264086427,
        "Owner_last_access_time":1663962090790,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600607182583,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63973530",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65316586,
        "Question_title":"get the run id for an mlflow experiment with the name?",
        "Question_body":"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport mlflow\n\nexperiment_name=&quot;experiment-1&quot;\nmlflow.set_experiment(experiment_name)\n\nno_of_trees=[100,200,300]\ndepths=[2,3,4]\nfor trees in no_of_trees:\n    for depth in depths:\n        with mlflow.start_run() as run:\n            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)\n            model.fit(x_train, y_train)\n            predictions=model.predict(x_cv)\n            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))\n<\/code><\/pre>\n<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608086727487,
        "Question_score":6,
        "Question_tags":"python|mlflow",
        "Question_view_count":6374,
        "Owner_creation_time":1479190327737,
        "Owner_last_access_time":1660584310400,
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.<\/p>\n<pre><code>experiment_name = &quot;experiment-1&quot;\ncurrent_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\nexperiment_id=current_experiment['experiment_id']\n<\/code><\/pre>\n<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)<\/p>\n<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])\nbest_run_id = df.loc[0,'run_id']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1608086727487,
        "Answer_score":15.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65316586",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60695933,
        "Question_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Question_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584294693613,
        "Question_score":4,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":768,
        "Owner_creation_time":1528574640850,
        "Owner_last_access_time":1663976741153,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":10,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611045077983,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":55943088,
        "Question_title":"Databricks notebook integrated mlflow artifact location and retention",
        "Question_body":"<ol>\n<li><p>Currently by default in notebook run, it will create an experiment ID, but the Artifact Location would point to something under dbfs:\/databricks\/mlflow\/{experiment id}. If there is a way we may change this in default experiment creation? We like to manage the storage outside databricks.<\/p><\/li>\n<li><p>How long is default TTL for experiment runs and metrics? Is it configurable and how?<\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1556747098277,
        "Question_score":1,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":587,
        "Owner_creation_time":1458276216763,
        "Owner_last_access_time":1575678794047,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55943088",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72855204,
        "Question_title":"MLflow is taking longer than expected time to finish logging metrics and parameters",
        "Question_body":"<p>I'm running a code where I have to perform multiple iterations for a set of products to select the best performing model. While running multiple iterations for a single product, I need to log details of every single run using mlflow(using mlflow with pandas-udf). While logging for individual iterations are taking around 2 seconds but the parent run under which I'm tracking every iteration details is taking 1.5 hours to finish. Here is the code -<\/p>\n<pre><code>@F.pandas_udf( model_results_schema, F.PandasUDFType.GROUPED_MAP )\ndef get_gam_pe_results( model_input ):\n    ...\n    ...\n    for j, gam_terms in enumerate(term_list[-1]):\n        results_iteration_output_1, results_iteration_output, results_iteration_all = run_gam_model(gam_terms)\n        \n        results_iteration_version = results_iteration_version.append(results_iteration_output)\n        unique_id = uuid.uuid1()\n        metric_list = [&quot;AIC&quot;, &quot;AICc&quot;, &quot;GCV&quot;, &quot;adjusted_R2&quot;, &quot;deviance&quot;, &quot;edof&quot;, &quot;elasticity_in_k&quot;, &quot;loglikelihood&quot;,\n                      &quot;scale&quot;]\n        param_list = [&quot;features&quot;]\n        start_time = str(datetime.now())\n        with mlflow.start_run(run_id=parent_run_id, experiment_id=experiment_id):\n            with mlflow.start_run(run_name=str(model_input['prod_id'].iloc[1]) + &quot;-&quot; + unique_id.hex,\n                                  experiment_id=experiment_id, nested=True):\n                for item in results_iteration_output.columns.values.tolist():\n                        if item in metric_list:\n                            mlflow.log_metric(item, results_iteration_output[item].iloc[0])\n                        if item in param_list:\n                            mlflow.log_param(item, results_iteration_output[item].iloc[0])\n                            \n                end_time = str(datetime.now())\n                mlflow.log_param(&quot;start_time&quot;, start_time)\n                mlflow.log_param(&quot;end_time&quot;, end_time)\n<\/code><\/pre>\n<p>Outside pandas-udf -<\/p>\n<pre><code>current_time = str(datetime.today().replace(microsecond=0))\nrun_id = None\nwith mlflow.start_run(run_name=&quot;MLflow_pandas_udf_testing-&quot;+current_time, experiment_id=experiment_id) as run:\n    run_id = run.info.run_uuid\n    gam_model_output = (Product_data\n                        .withColumn(&quot;run_id&quot;, F.lit(run_id))\n                        .groupby(['prod_id'])\n                        .apply(get_gam_pe_results)\n                       )\n<\/code><\/pre>\n<p>Note - Running this entire code in Databricks(cluster has 8 cores and 28gb ram).<\/p>\n<p>Any idea why this parent run is taking so long to finish while it's only 2 seconds to finish each iterations?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jzqkx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jzqkx.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656930055540,
        "Question_score":0,
        "Question_tags":"python-3.x|machine-learning|azure-databricks|mlflow|pandas-udf",
        "Question_view_count":76,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1663949207680,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72855204",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73320708,
        "Question_title":"Set run description programmatically in mlflow",
        "Question_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660221143727,
        "Question_score":1,
        "Question_tags":"python|artificial-intelligence|mlflow",
        "Question_view_count":89,
        "Owner_creation_time":1527525798183,
        "Owner_last_access_time":1664059587093,
        "Owner_location":"Sarajevo, Bosnia and Herzegovina",
        "Owner_reputation":736,
        "Owner_up_votes":829,
        "Owner_down_votes":8,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1660231264752,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660307815687,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64771247,
        "Question_title":"Logging a PySpark dataframe into a MLFlow Artifact",
        "Question_body":"<p>I am currently writing an MLFlow artifact to the dbfs but I am using pandas using the code below...<\/p>\n<pre><code>temp = tempfile.NamedTemporaryFile(prefix=&quot;*****&quot;, suffix=&quot;.csv&quot;)\ntemp_name = temp.name\ntry:\n  df.to_csv(temp_name, index=False)\n  mlflow.log_artifact(temp_name, &quot;******&quot;)\nfinally:\n  temp.close() # Delete the temp file\n<\/code><\/pre>\n<p>How would I write this if 'df' was a spark dataframe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1605020000127,
        "Question_score":1,
        "Question_tags":"python|pyspark|mlflow",
        "Question_view_count":810,
        "Owner_creation_time":1593519939237,
        "Owner_last_access_time":1660311748437,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64771247",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68233056,
        "Question_title":"Logging SKLearn Models in the same folder while running multiple models in Pandas UDF",
        "Question_body":"<p>I am trying to run multiple XGBoost models and save the resulting models in the form of experiments. However, when I call the UDF function on my pyspark dataframe the models are being saved in a multiple folders.<\/p>\n<p>It appears that they are being randomly split in smaller batches and stored. Is there a way to ensure that all models are saved in the same run\/ folder such that I can easily load them back later.<\/p>\n<pre><code>def classification_xgb(df):\n  #modeling code\n  mlflow.sklearn.log_model(xgb, model_name)\n\n\ndat_m.groupBy(&quot;Product&quot;).applyInPandas(classification_xgb, schema).show(10000,False)\n<\/code><\/pre>\n<p>I have over 100 products for which I need to create models and save in the same run instance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1625283818387,
        "Question_score":1,
        "Question_tags":"pandas|pyspark|azure-databricks|sklearn-pandas|mlflow",
        "Question_view_count":79,
        "Owner_creation_time":1625282679850,
        "Owner_last_access_time":1641308459047,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1625285948292,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68233056",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58554979,
        "Question_title":"Cannot Start mlflow ui on google cloud platform virtual machine instance",
        "Question_body":"<p>after running mlflow ui on command line\nand  clicking <a href=\"http:\/\/127.0.0.1:5000\/\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000\/<\/a>\ni get site cannot be reached\n127.0.0.1 refused to connect.<\/p>\n<p>I have already updated firewall rules on VPC network in GCP and on my local machine and activated the ports<\/p>\n<blockquote>\n<p>This site can\u2019t be reached127.0.0.1 refused to connect.<\/p>\n<p>Try:<\/p>\n<ul>\n<li>Checking the connection<\/li>\n<li>Checking the proxy and the firewall<\/li>\n<\/ul>\n<p>ERR_CONNECTION_REFUSED<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571992526673,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":360,
        "Owner_creation_time":1516552952317,
        "Owner_last_access_time":1616233012253,
        "Owner_location":"Haryana, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1592644375060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58554979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66855807,
        "Question_title":"How to add more metrics to a finished MLflow run?",
        "Question_body":"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow<\/code> client and <code>mlflow.get_run(run_id)<\/code> method, but the <code>Run<\/code> object returned by <code>get_run<\/code> seems to be read-only.<\/p>\n<p>Specifically, <code>.log_param<\/code> <code>.log_metric<\/code>, or <code>.log_artifact<\/code> cannot be used on the object returned by <code>get_run<\/code>, raising errors like these:<\/p>\n<pre><code>AttributeError: 'Run' object has no attribute 'log_param'\n<\/code><\/pre>\n<p>If we attempt to run any of the <code>.log_*<\/code> methods on <code>mlflow<\/code>, it would log them into to a new run  with auto-generated run ID in the <code>Default<\/code> experiment.<\/p>\n<p>Example:<\/p>\n<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)\n\nwith mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    \n    \n    # this read operation uses correct run\n    run_id = myrun.info.run_id\n    print(run_id)\n    \n    # this write operation writes to a new run \n    # (with auto-generated random run ID) \n    # in the &quot;Default&quot; experiment (with exp. ID of 0)\n    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)\n   \n<\/code><\/pre>\n<p>Note that the above problem exists regardless of the <code>Run<\/code> status (<code>.info.status<\/code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).<\/p>\n<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()<\/code> context...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617027126043,
        "Question_score":3,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":1269,
        "Owner_creation_time":1529411528930,
        "Owner_last_access_time":1664050484457,
        "Owner_location":"EU",
        "Owner_reputation":3260,
        "Owner_up_votes":1100,
        "Owner_down_votes":4,
        "Owner_views":466,
        "Question_last_edit_time":1617040232536,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66855807",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69796285,
        "Question_title":"Artifacts generations for pmml models in mlflow",
        "Question_body":"<p>I generated a sample pmml file which contains lightgbm model, when i am trying to log it and generate artifacts in mlflow i am getting error: 'Model' object has no attribute 'save_model'<\/p>\n<p>The code i used for logging:<\/p>\n<pre><code>import mlflow\n\nfrom pypmml import Model\n\nlr = Model.fromFile('iris.pmml') \n\nmlflow.lightgbm.log_model(lr,&quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1635766055407,
        "Question_score":0,
        "Question_tags":"mlflow|pmml",
        "Question_view_count":95,
        "Owner_creation_time":1635765403513,
        "Owner_last_access_time":1642406857823,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1635843903448,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69796285",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71121324,
        "Question_title":"Mlflow registed model version increment in python",
        "Question_body":"<p>I want to keep versions of the model in mlflow but not as version[1,2,3,...]\ninstead, i want to increment model's versions like 1.1 1.2 and when I feel that there is some major change I want increment to 2.0<\/p>\n<p>please let me know how this can be done.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644901430677,
        "Question_score":0,
        "Question_tags":"python|model|version-control|mlflow",
        "Question_view_count":24,
        "Owner_creation_time":1516006161920,
        "Owner_last_access_time":1645106959933,
        "Owner_location":"Arunachal Pradesh, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71121324",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62711259,
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593764146630,
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1594008626392,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594008525280,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51064366,
        "Question_title":"Can't run MLflow web-based user interface",
        "Question_body":"<p>I've installed <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">MLflow<\/a> on Ubuntu Server 18.04 LTS, in a virtual environment (Python 3), using its <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">Quickstart documentation<\/a>:<\/p>\n\n<pre><code>$ python3 -m venv mlflow\n$ source \/home\/emre\/mlflow\/bin\/activate\n$ pip install mlflow\n<\/code><\/pre>\n\n<p>that gave the following output during install:<\/p>\n\n<pre><code>Collecting mlflow\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e8\/b3\/cf358e182be34a62fcd6843e5df793f278bd9d24f78f565509cb927c6a22\/mlflow-0.1.0.tar.gz (4.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 323kB\/s\nCollecting Flask (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/e7\/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b\/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 9.4MB\/s\nCollecting awscli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/32\/d6d254f6ccc2ed21f02d81f38709ff06feca9cbdb2e68ea90635fa483a73\/awscli-1.15.46-py2.py3-none-any.whl (1.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 1.0MB\/s\nCollecting boto3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/e0\/a98898b94d8093bbd8fd4576fb2e89620adac1e24a2bfc28d11c4ce29a5b\/boto3-1.7.46-py2.py3-none-any.whl (128kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.8MB\/s\nCollecting click&gt;=6.7 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/34\/c1\/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77\/click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.3MB\/s\nCollecting databricks-cli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/58\/78\/4bda6f29a091ab7b0ad29efdba2491e5d0b56bd09d608857e6f0b799be48\/databricks-cli-0.7.2.tar.gz\nCollecting gitpython (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ac\/c9\/96d7c86c623cb065976e58c0f4898170507724d6b4be872891d763d686f4\/GitPython-2.1.10-py2.py3-none-any.whl (449kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.9MB\/s\nCollecting numpy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/68\/1e\/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2\/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2MB 110kB\/s\nCollecting pandas (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/eb\/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f\/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.8MB 116kB\/s\nCollecting protobuf (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/fc\/f0\/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2\/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.1MB 177kB\/s\nCollecting pygal (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/5f\/b7\/201c9254ac0d2b8ffa3bb2d528d23a4130876d9ba90bc28e99633f323f17\/pygal-2.4.0-py2.py3-none-any.whl (127kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 9.7MB\/s\nCollecting python-dateutil (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/cf\/f5\/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825\/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 6.0MB\/s\nCollecting pyyaml (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/10\/7d\/6efe0bd69580fecd40adf47ebaf8d807238308ccb851f0549881fa7605aa\/PyYAML-4.1.tar.gz (153kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 7.8MB\/s\nCollecting querystring_parser (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/64\/3086a9a991ff3aca7b769f5b0b51ff8445a06337ae2c58f215bcee48f527\/querystring_parser-1.2.3.tar.gz\nCollecting requests&gt;=2.17.3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/65\/47\/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda\/requests-2.19.1-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 8.2MB\/s\nCollecting scikit-learn (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/3d\/2d\/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9\/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.4MB 108kB\/s\nCollecting scipy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a8\/0b\/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730\/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.2MB 42kB\/s\nCollecting six&gt;=1.10.0 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/4b\/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a\/six-1.11.0-py2.py3-none-any.whl\nCollecting uuid (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ce\/63\/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d\/uuid-1.30.tar.gz\nCollecting zipstream (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/1a\/a4\/58f0709cef999db1539960aa2ae77100dc800ebb8abb7afc97a1398dfb2f\/zipstream-1.1.4.tar.gz\nCollecting itsdangerous&gt;=0.24 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/b4\/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4\/itsdangerous-0.24.tar.gz (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.4MB\/s\nCollecting Werkzeug&gt;=0.14 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/20\/c4\/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243\/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 4.0MB\/s\nCollecting Jinja2&gt;=2.10 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/ff\/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731\/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.2MB\/s\nCollecting rsa&lt;=3.5.0,&gt;=3.1.2 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e1\/ae\/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e\/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.5MB\/s\nCollecting botocore==1.10.46 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b4\/04\/ddaad5574f70a539d106e8d53b4685e3de4387de7a16884a95459f8c7691\/botocore-1.10.46-py2.py3-none-any.whl (4.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 314kB\/s\nCollecting s3transfer&lt;0.2.0,&gt;=0.1.12 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/d7\/14\/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d\/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.6MB\/s\nCollecting colorama&lt;=0.3.9,&gt;=0.2.5 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/db\/c8\/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf\/colorama-0.3.9-py2.py3-none-any.whl\nCollecting docutils&gt;=0.10 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/36\/fa\/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d\/docutils-0.14-py3-none-any.whl (543kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 2.5MB\/s\nCollecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b7\/31\/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365\/jmespath-0.9.3-py2.py3-none-any.whl\nCollecting configparser&gt;=0.3.5 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/69\/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c\/configparser-3.5.0.tar.gz\nCollecting tabulate&gt;=0.7.7 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/12\/c2\/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc\/tabulate-0.8.2.tar.gz (45kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 7.9MB\/s\nCollecting gitdb2&gt;=2.0.0 (from gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e0\/95\/c772c13b7c5740ec1a0924250e6defbf5dfdaee76a50d1c47f9c51f1cabb\/gitdb2-2.0.3-py2.py3-none-any.whl (63kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 11.2MB\/s\nCollecting pytz&gt;=2011k (from pandas-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/83\/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2\/pytz-2018.4-py2.py3-none-any.whl (510kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 421kB\/s\nRequirement already satisfied: setuptools in .\/mlflow\/lib\/python3.6\/site-packages (from protobuf-&gt;mlflow)\nCollecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bc\/a9\/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8\/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.7MB\/s\nCollecting idna&lt;2.8,&gt;=2.5 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4b\/2a\/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165\/idna-2.7-py2.py3-none-any.whl (58kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.3MB\/s\nCollecting urllib3&lt;1.24,&gt;=1.21.1 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bd\/c9\/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb\/urllib3-1.23-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.3MB\/s\nCollecting certifi&gt;=2017.4.17 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/e6\/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5\/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 8.0MB\/s\nCollecting MarkupSafe&gt;=0.23 (from Jinja2&gt;=2.10-&gt;Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4d\/de\/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b\/MarkupSafe-1.0.tar.gz\nCollecting pyasn1&gt;=0.1.3 (from rsa&lt;=3.5.0,&gt;=3.1.2-&gt;awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a0\/70\/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9\/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 10.9MB\/s\nCollecting smmap2&gt;=2.0.0 (from gitdb2&gt;=2.0.0-&gt;gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e3\/59\/4e22f692e65f5f9271252a8e63f04ce4ad561d4e06192478ee48dfac9611\/smmap2-2.0.3-py2.py3-none-any.whl\nBuilding wheels for collected packages: mlflow, databricks-cli, pyyaml, querystring-parser, uuid, zipstream, itsdangerous, configparser, tabulate, MarkupSafe\n  Running setup.py bdist_wheel for mlflow ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/mlflow\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp10fdrz2ypip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for mlflow\n  Running setup.py clean for mlflow\n  Running setup.py bdist_wheel for databricks-cli ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/databricks-cli\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpy_2acqi3pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for databricks-cli\n  Running setup.py clean for databricks-cli\n  Running setup.py bdist_wheel for pyyaml ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/pyyaml\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp4bs2fwrtpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for pyyaml\n  Running setup.py clean for pyyaml\n  Running setup.py bdist_wheel for querystring-parser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/querystring-parser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp_cnm9w_tpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for querystring-parser\n  Running setup.py clean for querystring-parser\n  Running setup.py bdist_wheel for uuid ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/uuid\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpenr2igaxpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for uuid\n  Running setup.py clean for uuid\n  Running setup.py bdist_wheel for zipstream ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/zipstream\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpnzsjh5e2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for zipstream\n  Running setup.py clean for zipstream\n  Running setup.py bdist_wheel for itsdangerous ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/itsdangerous\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp7imi3zv2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for itsdangerous\n  Running setup.py clean for itsdangerous\n  Running setup.py bdist_wheel for configparser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/configparser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpyk9qtmi1pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for configparser\n  Running setup.py clean for configparser\n  Running setup.py bdist_wheel for tabulate ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/tabulate\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpjim2qr00pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for tabulate\n  Running setup.py clean for tabulate\n  Running setup.py bdist_wheel for MarkupSafe ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/MarkupSafe\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpsdpdd8ulpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for MarkupSafe\n  Running setup.py clean for MarkupSafe\nFailed to build mlflow databricks-cli pyyaml querystring-parser uuid zipstream itsdangerous configparser tabulate MarkupSafe\nInstalling collected packages: click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, pyasn1, rsa, jmespath, six, python-dateutil, docutils, botocore, s3transfer, colorama, pyyaml, awscli, boto3, configparser, chardet, idna, urllib3, certifi, requests, tabulate, databricks-cli, smmap2, gitdb2, gitpython, numpy, pytz, pandas, protobuf, pygal, querystring-parser, scikit-learn, scipy, uuid, zipstream, mlflow\n  Running setup.py install for itsdangerous ... done\n  Running setup.py install for MarkupSafe ... done\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for configparser ... done\n  Running setup.py install for tabulate ... done\n  Running setup.py install for databricks-cli ... done\n  Running setup.py install for querystring-parser ... done\n  Running setup.py install for uuid ... done\n  Running setup.py install for zipstream ... done\n  Running setup.py install for mlflow ... done\nSuccessfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 awscli-1.15.46 boto3-1.7.46 botocore-1.10.46 certifi-2018.4.16 chardet-3.0.4 click-6.7 colorama-0.3.9 configparser-3.5.0 databricks-cli-0.7.2 docutils-0.14 gitdb2-2.0.3 gitpython-2.1.10 idna-2.7 itsdangerous-0.24 jmespath-0.9.3 mlflow-0.1.0 numpy-1.14.5 pandas-0.23.1 protobuf-3.6.0 pyasn1-0.4.3 pygal-2.4.0 python-dateutil-2.7.3 pytz-2018.4 pyyaml-4.1 querystring-parser-1.2.3 requests-2.19.1 rsa-3.4.2 s3transfer-0.1.13 scikit-learn-0.19.1 scipy-1.1.0 six-1.11.0 smmap2-2.0.3 tabulate-0.8.2 urllib3-1.23 uuid-1.30 zipstream-1.1.4\n<\/code><\/pre>\n\n<p>After that I checked the following didn't give any errors:<\/p>\n\n<pre><code>import os\nfrom mlflow import log_metric, log_param, log_artifact\n<\/code><\/pre>\n\n<p>But when I try to run the web-based user interface, I get the following errors:<\/p>\n\n<pre><code>$ mlflow ui\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 574, in _build_master\n    ws.require(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 892, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/bin\/mlflow\", line 6, in &lt;module&gt;\n    from pkg_resources import load_entry_point\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3088, in &lt;module&gt;\n    @_call_aside\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3072, in _call_aside\n    f(*args, **kwargs)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3101, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 576, in _build_master\n    return cls._build_from_requirements(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 589, in _build_from_requirements\n    dists = ws.resolve(reqs, Environment())\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n<\/code><\/pre>\n\n<p>Any ideas how I can fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1530106954863,
        "Question_score":0,
        "Question_tags":"python|python-3.x|pip|mlflow",
        "Question_view_count":791,
        "Owner_creation_time":1261400320737,
        "Owner_last_access_time":1663928633650,
        "Owner_location":"Antwerp, Belgium",
        "Owner_reputation":7876,
        "Owner_up_votes":4051,
        "Owner_down_votes":47,
        "Owner_views":924,
        "Question_last_edit_time":1530112131487,
        "Answer_body":"<p>Apparently I had to install the <code>wheel<\/code> module inside my virtual environment. I deleted the virtual environment, re-created it, and then installed the <code>wheel<\/code> module:<\/p>\n\n<pre><code>pip install wheel\n<\/code><\/pre>\n\n<p>after that <code>pip install mlflow<\/code>, as well as <code>mlflow ui<\/code> worked successfully.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530109158067,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51064366",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65145994,
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607094596640,
        "Question_score":8,
        "Question_tags":"apache-spark|matplotlib|pyspark|databricks|mlflow",
        "Question_view_count":5219,
        "Owner_creation_time":1316705139197,
        "Owner_last_access_time":1663085821243,
        "Owner_location":"Boston, MA",
        "Owner_reputation":6711,
        "Owner_up_votes":353,
        "Owner_down_votes":3,
        "Owner_views":819,
        "Question_last_edit_time":1607191847983,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1607094854147,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63515370,
        "Question_title":"How to add coefficients, p-values and relevant variable name in mlflow?",
        "Question_body":"<p>I am running a linear regression model and I would like to add the coefficients and P-values of each variable and the variable name in to the metrics of the mlflow output. I am new to using mlflow and not very familiar in doing this. Below is an example of part of my code<\/p>\n<pre><code>with mlflow.start_run(run_name=p_key + '_' + str(o_key)):\n    \n    lr = LinearRegression(\n      featuresCol = 'features',\n      labelCol = target_var,\n      maxIter = 10,\n      regParam = 0.0,\n      elasticNetParam = 0.0,\n      solver=&quot;normal&quot;\n        )\n    \n    lr_model_item = lr.fit(train_model_data)\n    lr_coefficients_item = lr_model_item.coefficients\n    lr_coefficients_intercept = lr_model_item.intercept\n    \n    lr_predictions_item = lr_model_item.transform(train_model_data)\n    lr_predictions_item_oos = lr_model_item.transform(test_model_data)\n    \n    rsquared = lr_model_item.summary.r2\n    \n    # Log mlflow attributes for mlflow UI\n    mlflow.log_metric(&quot;rsquared&quot;, rsquared)\n    mlflow.log_metric(&quot;intercept&quot;, lr_coefficients_intercept)\n    for i in lr_coefficients_item:\n      mlflow.log_metric('coefficients', lr_coefficients_item[i])\n<\/code><\/pre>\n<p>Would like to know whether this is possible? In the final output I should have the intercept, coefficients, p-values and the relevant variable name.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597974389493,
        "Question_score":1,
        "Question_tags":"databricks|mlflow",
        "Question_view_count":242,
        "Owner_creation_time":1557281764547,
        "Owner_last_access_time":1621398878877,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63515370",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73808627,
        "Question_title":"Can't get model inference using mlflow.pytorch.log_model, but could get it with mlflow.pyfunc.log_model",
        "Question_body":"<p>I've used <code>mlflow.pyfunc.log_model<\/code> and I was able to get model inference with this, but not with<code>mlflow.pytorch.log_model<\/code>. The error was Verify that the serialized input Dataframe is compatible with the model for inference.<\/p>\n<pre><code>    data = torch.randn(10, 3, 224, 224)  # shape: [bs, channel, size, size]\n    model_input = {\n                &quot;inputs&quot;: { \n                    &quot;x&quot;: data.tolist() }\n                }\n    request = json.dumps(model_input)\n    headers = {&quot;content-type&quot;: &quot;application\/json&quot;}\n    response = requests.post(URL, data=request, headers=headers) # to mlflow\n    response = response.json() \n    print(response)\n<\/code><\/pre>\n<p>The very same input to the model, but I could get inference on one but not the other? Am I missing something here? I would like to use <code>mlflow.pytorch.log_model<\/code> so I don't have to do a model wrapper for generalisation with <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n<p>Can anyone help me with this please.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663812765467,
        "Question_score":0,
        "Question_tags":"rest|deployment|pytorch|mlflow|serving",
        "Question_view_count":14,
        "Owner_creation_time":1487831504143,
        "Owner_last_access_time":1664005735500,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1663812910112,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73808627",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68722437,
        "Question_title":"Azure : Ansible role for deploying ML model integrated over databricks",
        "Question_body":"<p>I have developed ML predictive model on historical data in Azure Databricks using python notebook.\nWhich means i have done data extraction, preparation, feature engineering and model training everything done in Databricks using python notebook.\nI have almost completed development part of it, now we want to deploy ML model into production using ansible roles.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1628579607800,
        "Question_score":1,
        "Question_tags":"deployment|databricks|mlflow|mlmodel",
        "Question_view_count":128,
        "Owner_creation_time":1605508510493,
        "Owner_last_access_time":1638163317803,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1628663676903,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68722437",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68887243,
        "Question_title":"Autolog metrics mlflow",
        "Question_body":"<p>I have a question about autologging. I use it and I want to record another metrics.\nCan I change the recorded metrics for autologging?<\/p>\n<p>I found a class in the documentation <code>_AutologgingMetricsManager<\/code> but I don't know how can I use it.<\/p>\n<p>Thanks,\nIrina<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1629692108827,
        "Question_score":1,
        "Question_tags":"metrics|mlflow",
        "Question_view_count":54,
        "Owner_creation_time":1629691503070,
        "Owner_last_access_time":1657106037790,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1629701490176,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68887243",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72660569,
        "Question_title":"mlflow UI server doesn't start",
        "Question_body":"<p>When I run mlflow UI inside my repository and environment (anaconda), I receive the following error<\/p>\n<pre><code>Cannot open C:\\Users\\XXX\\Anaconda3\\envs\\haea\\Scripts\\waitress-serve-script.py\nRunning the mlflow server failed. Please see the logs above for details\n<\/code><\/pre>\n<p>When I checked the anaconda environment folder, I don't see a waitress-serve-script there, that might be the reason, but I can't find other online resources for the issue. Any recommendations would help.<\/p>\n<p><em>What I have tried so far<\/em><\/p>\n<ol>\n<li>Reinstalling mlflow<\/li>\n<li>Creating a new environment<\/li>\n<li>Manually install waitress (pip install waitress)<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655474363667,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":158,
        "Owner_creation_time":1466863372793,
        "Owner_last_access_time":1658241993080,
        "Owner_location":"Michigan",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72660569",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71612603,
        "Question_title":"How does one invert an encoded prediction in Keras for model serving?",
        "Question_body":"<p>I have a Keras model in which i have successfully added a <code>StringLookUp<\/code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.<\/p>\n<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.<\/p>\n<p>Here is the implementation of the encoder and decoders:<\/p>\n<pre><code>#generate the encoder and decoders\nencoder = tf.keras.layers.StringLookup(vocabulary=vocab, )\ndecoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)\n<\/code><\/pre>\n<p>Here is the some of the code that makes the inference model<\/p>\n<pre><code># For inference, you can export a model that accepts strings as input\ninputs = Input(shape=(6,), dtype=&quot;string&quot;)\nx = encoder(inputs)\noutputs = keras_model(x)\ninference_model = Model(inputs, outputs)\n\ninference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \ninference_model.summary()\n<\/code><\/pre>\n<p>The <code>encoder<\/code> above is just a function that implements <code>tf.keras.layers.StringLookup<\/code><\/p>\n<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder<\/code> which implements the reverse of <code>StringLookUp<\/code>.<\/p>\n<p><em><strong>Here's my problem<\/strong><\/em>\nWhile this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.<\/p>\n<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks<\/strong><\/em><\/p>\n<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648186526013,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|deep-learning|mlflow",
        "Question_view_count":176,
        "Owner_creation_time":1427492676943,
        "Owner_last_access_time":1663998407850,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71612603",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68319208,
        "Question_title":"Unable to import mlflow, getting ModuleNotFoundError: No module named 'mlflow'",
        "Question_body":"<p>Unable to import <code>mlflow<\/code> in a .py script.<\/p>\n<pre><code>ModuleNotFoundError: No module named 'mlflow'\n<\/code><\/pre>\n<p>The script runs in a <code>python:3.7-stretch Docker<\/code> container<\/p>\n<p>Use <code>requirements.txt<\/code> to pip install packages.<\/p>\n<pre><code>(...)\nsqlalchemy==1.4.1\npsycopg2==2.8.6\nmlflow==1.18.0\n<\/code><\/pre>\n<pre><code>RUN pip3 install --default-timeout=5000 --use-deprecated=legacy-resolver -r \/root\/requirements.txt\n<\/code><\/pre>\n<p>Can see that it is installed.<\/p>\n<pre><code>root@abc:~# pip uninstall mlflow\nFound existing installation: mlflow 1.18.0\nUninstalling mlflow-1.18.0:\n  Would remove:\n    \/usr\/local\/bin\/mlflow\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow-1.18.0.dist-info\/*\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow\/*\nProceed (y\/n)? n\n<\/code><\/pre>\n<p>Can do an import from python shell.<\/p>\n<pre><code>root@abc:~# python\nPython 3.7.10 (default, Feb 16 2021, 19:46:13)\n[GCC 6.3.0 20170516] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; import mlflow\n&gt;&gt;&gt;\n<\/code><\/pre>\n<p>But no joy when running from .py script.<\/p>\n<p>Other packages installed from <code>requirements.txt<\/code> can be imported.<\/p>\n<p>Any ideas what is wrong ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1625844282463,
        "Question_score":0,
        "Question_tags":"python-3.x|docker|machine-learning|python-module|mlflow",
        "Question_view_count":814,
        "Owner_creation_time":1461829551257,
        "Owner_last_access_time":1663945644783,
        "Owner_location":null,
        "Owner_reputation":375,
        "Owner_up_votes":241,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1650287875787,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68319208",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63232368,
        "Question_title":"Storing mlflow artifacts to s3, while having SQL databse as backend",
        "Question_body":"<p>When using a SQL database as backend for <code>mlflow<\/code> are the artifacts stored in the same database or in default <code>.\/mlruns<\/code> directory?<\/p>\n<p>Is it possible to store them in different location as in AWS S3?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1596468560843,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":610,
        "Owner_creation_time":1596468107063,
        "Owner_last_access_time":1624957000150,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1596534279300,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63232368",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56851463,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562066976100,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":353,
        "Owner_creation_time":1384343462317,
        "Owner_last_access_time":1663916912330,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":478,
        "Owner_up_votes":65,
        "Owner_down_votes":4,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562240543612,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59401800,
        "Question_title":"MLFlow run passing Google Application credentials",
        "Question_body":"<p>I want to pass my <code>GOOGLE_APPLICATION_CREDENTIALS<\/code> environmental variable when I run <code>mlflow run<\/code> using a Docker container:<\/p>\n\n<p>This is my current <code>docker run<\/code> when using mlflow run:<\/p>\n\n<pre><code> Running command 'docker run --rm -e MLFLOW_RUN_ID=f18667e37ecb486cac4631cbaf279903 -e MLFLOW_TRACKING_URI=http:\/\/3.1.1.11:5000 -e MLFLOW_EXPERIMENT_ID=0 mlflow_gcp:33156ee python -m trainer.task --job-dir \/tmp\/ \\\n    --num-epochs 10 \\\n    --train-steps 1000 \\\n    --eval-steps 1 \\\n    --train-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv \\\n    --eval-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv \\\n    --batch-size 128\n<\/code><\/pre>\n\n<p>This is how I would normally pass it:<\/p>\n\n<pre><code>docker run \\\n   -p 9090:${PORT} \\\n   -e PORT=${PORT} \\\n   -e GOOGLE_APPLICATION_CREDENTIALS=\/tmp\/keys\/[FILE_NAME].json\n<\/code><\/pre>\n\n<p>What is the best way to option to pass this value to mlflow? I'm writing files in GCS and Docker requires access to GCP.<\/p>\n\n<p>MLproject contents<\/p>\n\n<pre><code>name: mlflow_gcp\ndocker_env:\n  image: mlflow-gcp-example\nentry_points:\n  main:\n    parameters:\n      job_dir:\n        type: string\n        default: '\/tmp\/'\n      num_epochs:\n        type: int\n        default: 10\n      train_steps:\n        type: int\n        default: 1000\n      eval_steps:\n        type: int\n        default: 1\n      batch_size:\n        type: int\n        default: 64\n      train_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv'\n      eval_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv'\n      mlflow_tracking_uri:\n        type: uri\n        default: ''\n\n    command: |\n        python -m trainer.task --job-dir {job_dir} \\\n            --num-epochs {num_epochs} \\\n            --train-steps {train_steps} \\\n            --eval-steps {eval_steps} \\\n            --train-files {train_files} \\\n            --eval-files {eval_files} \\\n            --batch-size {batch_size} \\\n            --mlflow-tracking-uri {mlflow_tracking_uri}\n\n<\/code><\/pre>\n\n<p>I already tried in Python file and fails since Docker has no access to local file system:<\/p>\n\n<pre><code>import os\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/Users\/user\/key.json\"\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1576715118127,
        "Question_score":2,
        "Question_tags":"docker|mlflow",
        "Question_view_count":243,
        "Owner_creation_time":1264671735677,
        "Owner_last_access_time":1664082395287,
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":8619,
        "Owner_up_votes":1916,
        "Owner_down_votes":102,
        "Owner_views":1286,
        "Question_last_edit_time":1576715456443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59401800",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58306468,
        "Question_title":"Gridsearch on an experiment in Sacred",
        "Question_body":"<p>I'm trying to see some ways to store my ML experiments and I came across some python libraries like Sacred, ModelChimp, MLFlow, ....<\/p>\n\n<p>The one I like the most is Sacred, but I would like to know how to save the <code>GridSearchCV<\/code> sklearn object the way ModelChimp does, for example. Is there any way to include each of the tests that the <code>GridSearchCV<\/code> object does in Sacred like ModelChimp does?<\/p>\n\n<p>Additionally I would like to be able to visualize an interactive map of the folium library (which I would simply export to HTML), but I haven't seen that any of these libraries accept objects to visualize beyond an image.<\/p>\n\n<p>Are Sacred or ModelChimp good options? The little I've seen of MLflow or other libraries hasn't convinced me either but I'm open to suggestions. <a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/bx0apm\/d_how_do_you_manage_your_machine_learning\/\" rel=\"nofollow noreferrer\">Here<\/a> are a few more alternatives. Which one do you use?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1570631306603,
        "Question_score":1,
        "Question_tags":"python|machine-learning|folium|mlflow|python-sacred",
        "Question_view_count":205,
        "Owner_creation_time":1550233102177,
        "Owner_last_access_time":1663835213537,
        "Owner_location":null,
        "Owner_reputation":621,
        "Owner_up_votes":87,
        "Owner_down_votes":26,
        "Owner_views":103,
        "Question_last_edit_time":1571068502088,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58306468",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73769728,
        "Question_title":"What is a 'XGBoostLabelEncoder' object?",
        "Question_body":"<p>I'm trying to load a model from an mlflow run. When I do that I get an 'XGBoostLabelEncoder' object, an object with no attributes like predict or predict_proba. I don't really know what you can do with it.<\/p>\n<p>I've googled around but can't find any information about what an 'XGBoostLabelEncoder' object is.<\/p>\n<p>Anybody who knows?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663571938730,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|xgboost|mlflow",
        "Question_view_count":25,
        "Owner_creation_time":1503559541763,
        "Owner_last_access_time":1664083799790,
        "Owner_location":"Malm\u00f6, Sverige",
        "Owner_reputation":796,
        "Owner_up_votes":605,
        "Owner_down_votes":4,
        "Owner_views":136,
        "Question_last_edit_time":1663573301816,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73769728",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68622242,
        "Question_title":"Copying Experiments from a MLFlow server to another MLFlow server",
        "Question_body":"<p>I have a user in a Linux machine and I run a mlflow server from this user. Artifacts are stored in local mlruns folder. Lets call this user as user A. Then I run another mlflow server from another Linux user and call this user as user B. I wanted to move older experiments that resides in mlruns directory of user A to mlflow that run in user B. I simply moved mlruns directory of user A to the home directory of user B and run mlflow from there again. When I accessed to mlflow UI by browser I saw that artifact location is configured correctly to mlruns folder of user B, but I couldn't see the experiments that moved from user A's mlruns directory. How can I see them in the UI too?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1627910044223,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":303,
        "Owner_creation_time":1533205766067,
        "Owner_last_access_time":1663159397120,
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Owner_reputation":275,
        "Owner_up_votes":167,
        "Owner_down_votes":2,
        "Owner_views":85,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68622242",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58666136,
        "Question_title":"How can I set run_name in mlflow command line?",
        "Question_body":"<p>MLFlow version: 1.4.0\nPython version: 3.7.4<\/p>\n\n<p>I'm running the UI as <code>mlflow server...<\/code> with all the required command line options. <\/p>\n\n<p>I am logging to MLFlow as an MLFlow project, with the appropriate <code>MLproject.yaml<\/code> file. The project is being run on a Docker container, so the CMD looks like this: <\/p>\n\n<p><code>mlflow run . -P document_ids=${D2V_DOC_IDS} -P corpus_path=...  --no-conda --experiment-name=${EXPERIMENT_NAME}<\/code><\/p>\n\n<p>Running the experiment like this results in a blank run_name. I know there's a run_id but I'd also like to see the run_name and set it in my code -- either in the command line, or in my code as <code>mlflow.log....<\/code>.  <\/p>\n\n<p>I've looked at <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation\">Is it possible to set\/change mlflow run name after run initial creation?<\/a> but I want to programmatically set the run name instead of changing it manually on the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572643824847,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":2230,
        "Owner_creation_time":1364599762503,
        "Owner_last_access_time":1649458568280,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>One of the parameters to <code>mlflow.start_run()<\/code> is <code>run_name<\/code>.  This would give you programmatic access to set the run name with each iteration. See the docs <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Here's an example:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\n## Define the name of our run\nname = \"this run is gonna be bananas\" + datetime.now()\n\n## Start a new mlflow run and set the run name\nwith mlflow.start_run(run_name = name):\n\n    ## ...train model, log metrics\/params\/model...\n\n    ## End the run\n    mlflow.end_run()\n<\/code><\/pre>\n\n<p>If you want to include set the name as part of an MLflow Project, you'll have to specify it as a parameter in the entry points to the project.  This is located in in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/projects.html#mlproject-file\" rel=\"nofollow noreferrer\">MLproject file<\/a>.  Then you can pass those values into the <code>mlflow.start_run()<\/code> function from the command line.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1573412741883,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58666136",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58198968,
        "Question_title":"mlflow not work after installation (Ubuntu 16, Centos 7)",
        "Question_body":"<p><img src=\"https:\/\/i.stack.imgur.com\/5DC76.png\" alt=\"enter link description here\"><\/p>\n\n<p>I try to install and run the web-based interface mlflow on VM Azure Ubuntu 16 and Centos 7.\nAfter running the command:\nsudo mlflow ui<\/p>\n\n<p>I can not get access url, either through the dns (mydomain.com:5000), or by IP: <a href=\"http:\/\/123.456.789.123:5000\/\" rel=\"nofollow noreferrer\">http:\/\/123.456.789.123:5000\/<\/a><\/p>\n\n<p>Executing on the server:<\/p>\n\n<p>wget <a href=\"http:\/\/localhost:5000\" rel=\"nofollow noreferrer\">http:\/\/localhost:5000<\/a><\/p>\n\n<p>I get the html-page mlflow, ie the server is running, but then why can not I connect to it in a browser? - Error:The connection has timed out<\/p>\n\n<p>p.s. Firewall disabled on this VM.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570009106153,
        "Question_score":0,
        "Question_tags":"ubuntu|centos|gunicorn|mlflow",
        "Question_view_count":218,
        "Owner_creation_time":1554820347590,
        "Owner_last_access_time":1652262856667,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1570173908487,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58198968",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73716706,
        "Question_title":"Using MLFlow for commercial use",
        "Question_body":"<p>It seems that from April 2020, we cannot use anaconda for &quot;commercial use&quot; meaning for example (organizations with more than 200 employees for example)<\/p>\n<p>Since MLFlow seems to use yaml files that contain allusions to conda, how is the situation with MLFlow?<\/p>\n<p>Can MLFlow be used for commercial use?<\/p>\n<p>Note: This question <em>is<\/em> about programming since I intend to use MLFlow in our programs and I have to decide if we can or not<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663157783360,
        "Question_score":0,
        "Question_tags":"anaconda|mlflow",
        "Question_view_count":21,
        "Owner_creation_time":1421198269333,
        "Owner_last_access_time":1664010554427,
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73716706",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59415473,
        "Question_title":"Why is the MLFlow UI different on installation?",
        "Question_body":"<p>My MLFlow installation results in a significantly different UI experience  that does not neatly stack the Parameters and Metrics columns as in the QuickStart. <\/p>\n\n<p>Here's what my UI looks like after logging some basic information: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas every other example of MLFlow I've come across online looks like this (image taken from MLFlow website quickstart): \n<a href=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The other thing that's missing is the toggle between \"list\" and \"table\" views. Below is what MLFlow documentation says I should see: \n<a href=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas here's what I see in my installation: \n<a href=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My environments are as follows: <\/p>\n\n<ol>\n<li>Ubuntu 16.04, Docker + pip installation of MLFlow<\/li>\n<li>Mac OS, \n\n<ol>\n<li>Conda + pip installation of MLFlow<\/li>\n<li>Brew installation of Python, then pip3 installation of MLFlow<\/li>\n<\/ol><\/li>\n<\/ol>\n\n<p>I've tried tweaking the following: <\/p>\n\n<ol>\n<li>Version of MLFlow from 1.3 to 1.4<\/li>\n<li>Version of Python from 3.7 to 3.8 <\/li>\n<li>Brand new installation vs. existing upgrade <\/li>\n<\/ol>\n\n<p>I'm out of ideas at this point as to why my UI looks so different. It doesn't necessarily affect my usage of MLFlow, but I'm trying to sell it to my colleagues as a good experiment tracking system and I want the UI to be the best possible representation. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1576781811567,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":403,
        "Owner_creation_time":1364599762503,
        "Owner_last_access_time":1649458568280,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59415473",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60398594,
        "Question_title":"mlflow artifact storage to AWS s3 artifacts",
        "Question_body":"<p>Is there anyway to store the logs stored by mlflow to AWS S3? <\/p>\n\n<pre><code>mlflow server \\\n    --backend-store-uri \/mnt\/persistent-disk \\\n    --default-artifact-root s3:\/\/my-mlflow-bucket\/ \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>Is it possible to only provide the default-artifact-root instead of providing both backend-store-uri and default-artifact-root? <\/p>\n\n<p>Also is there anyway to set default-artifact-root programatically from MlFlowClient or MlFlowContext instead of running mlflow server command line? <\/p>\n\n<p>FYI, I have already defined all AWS_ACCESS_KEY and AWS_SECRET_KEY in my environment variables, and exported ENDPOINTS to S3.<\/p>\n\n<p>Is logArtifacts from ActiveRun class a correct method to set the artifact_uri which points to AWS s3 bucket?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582646236957,
        "Question_score":1,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":5057,
        "Owner_creation_time":1579635994630,
        "Owner_last_access_time":1583333598867,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1582655395240,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60398594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68745216,
        "Question_title":"Curl returning single output for multiple string inputs",
        "Question_body":"<p>I logged a sentiment-analysis model in <code>Mlflow<\/code> with the custom signature, everything is working fine but as soon as I serve the model and hit it with the curl command, then for my multiple inputs it's returning a single output, please help if someone can point to the issue<\/p>\n<p>Curl command i am using :<\/p>\n<pre><code>curl http:\/\/127.0.0.1:2000\/invocations -H 'Content-Type: application\/json' -d '{&quot;columns&quot;: [&quot;text&quot;],&quot;data&quot;: [[&quot;Its a Bad day&quot;],[&quot;what are you&quot;]]}'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[&quot;negative&quot;]\n<\/code><\/pre>\n<p>Expected Output:<\/p>\n<pre><code>[&quot;negative&quot;,&quot;neutral&quot;]\n<\/code><\/pre>\n<p>Here is the model signature :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have tried two different models, both of them are giving the same issue and if I am trying a model which takes integer values then it's working as expected.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1628696944257,
        "Question_score":0,
        "Question_tags":"python|curl|mlflow",
        "Question_view_count":65,
        "Owner_creation_time":1594195651540,
        "Owner_last_access_time":1663919696387,
        "Owner_location":"India",
        "Owner_reputation":857,
        "Owner_up_votes":83,
        "Owner_down_votes":5,
        "Owner_views":35,
        "Question_last_edit_time":1628697805627,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68745216",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71795643,
        "Question_title":"Error when loading ML model from the remote MLflow instance",
        "Question_body":"<p>I tried to load a model from the remote MLflow instance, using <code>load_model<\/code> function:<\/p>\n<pre><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;http:\/\/remote_IP_address:5000\/runs:\/&lt;run_id&gt;\/model&quot;)\n<\/code><\/pre>\n<p>I found the run_id by using the REST API:<\/p>\n<pre><code>import requests\n\nrequests.get(&quot;http:\/\/remote_IP_address:5000\/api\/2.0\/preview\/mlflow\/runs\/search&quot;,params={&quot;experiment_ids&quot;:[0,1]})\n<\/code><\/pre>\n<p>But I am receiving an error:<\/p>\n<pre><code>ValueError: not enough values to unpack (expected 2, got 1)\n<\/code><\/pre>\n<p>I suppose the error is in the URI that I am using. Can you tell me the correct way to access the remote Mlflow instance and load the model?<\/p>\n<p>p.s.\nI also tried:<\/p>\n<pre><code>mlflow.pyfunc.load_model(&quot;http:\/\/remote_Ip_address:5000\/models:\/&lt;model_name&gt;\/production&quot;)\n<\/code><\/pre>\n<p>but I received the same error.<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649414682147,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|mlflow",
        "Question_view_count":286,
        "Owner_creation_time":1528365488027,
        "Owner_last_access_time":1663858441063,
        "Owner_location":null,
        "Owner_reputation":499,
        "Owner_up_votes":167,
        "Owner_down_votes":1,
        "Owner_views":59,
        "Question_last_edit_time":1649415164790,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71795643",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72723433,
        "Question_title":"Does MLflow support darknet framework?",
        "Question_body":"<p>I am learning yolov4 with darknet and using that model for service development.<\/p>\n<p>However, I want to track and manage the performance metric of the model.<\/p>\n<p>So, I've heard of MLflow Tracking and am looking into it.<\/p>\n<p>Does MLflow support darknet?<\/p>\n<p>If so, is there a tracking management tool for using darknet?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655945856957,
        "Question_score":0,
        "Question_tags":"yolo|mlflow|darknet",
        "Question_view_count":85,
        "Owner_creation_time":1546387948563,
        "Owner_last_access_time":1662079308337,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72723433",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72280328,
        "Question_title":"MLflow artifacts on S3 but not in UI",
        "Question_body":"<p>I'm running mlflow on my local machine and logging everything through a remote tracking server with my artifacts going to an S3 bucket.  I've confirmed that they are present in S3 after a run but when I look at the UI the artifacts section is completely blank.  There's no error, just empty space.  <a href=\"https:\/\/i.stack.imgur.com\/ZeHJ8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZeHJ8.png\" alt=\"enter image description here\" \/><\/a>\nAny idea why this is?  I've included a picture from the UI.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1652819462347,
        "Question_score":1,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":502,
        "Owner_creation_time":1639614248310,
        "Owner_last_access_time":1663891212383,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1659109833416,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72280328",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59856641,
        "Question_title":"How can I throw an exception from within an MLflow project?",
        "Question_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579686060327,
        "Question_score":0,
        "Question_tags":"python|exception|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579719419647,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579728648287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69554275,
        "Question_title":"MLFLow: Install github-package dependency via pip and 1 building-job",
        "Question_body":"<p>I want to use MLFlow and I have to specify a Github python package as a pip dependency in the yaml-file.\nThe problem is, that I need to force pip to only use 1 job to build it (otherwise it would run out of memory).\nHow can I do this?<\/p>\n<p>I tried already: mlflow run hello_ml -n 1\nBut n is no option. Nether j (job).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634122665053,
        "Question_score":0,
        "Question_tags":"python|pip|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1582009662117,
        "Owner_last_access_time":1652282252807,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69554275",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70169519,
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638276045390,
        "Question_score":3,
        "Question_tags":"python|scikit-learn|mlflow|mlops",
        "Question_view_count":323,
        "Owner_creation_time":1550233102177,
        "Owner_last_access_time":1663835213537,
        "Owner_location":null,
        "Owner_reputation":621,
        "Owner_up_votes":87,
        "Owner_down_votes":26,
        "Owner_views":103,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638361888372,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56967364,
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_score":0,
        "Question_tags":"apache-spark|parameters|hadoop-yarn|spark-submit|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1413431014113,
        "Owner_last_access_time":1566529894493,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562766864887,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72416831,
        "Question_title":"mlflow: saving signature gives me warning",
        "Question_body":"<p>I am using mlflow with sqlite backend. started the server with:<\/p>\n<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlruns_db\/mlruns.db --default-artifact-root $PWD\/mlruns --host 0.0.0.0 -p 5000\n<\/code><\/pre>\n<p>in the code, I log the model with signature as such<\/p>\n<pre><code>...\nsignature = infer_signature(X, y)\nmlflow.sklearn.log_model(model, model_name, signature=signature)\n...\n<\/code><\/pre>\n<p>then I get warnings<\/p>\n<blockquote>\n<p>2022\/05\/26 19:52:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/d4c8f611d3f24986a32d19c7d8b03f06\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.<\/p>\n<\/blockquote>\n<p>I am using <code>mlflow, version 1.24.0<\/code>, though.<\/p>\n<p>I see that the signature is correctly logged inside <code>MLmodel<\/code> file, but the nice rendering of mlflow ui is lost.<\/p>\n<ol>\n<li><p>with logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/r2FwI.png\" rel=\"nofollow noreferrer\">mlflow ui with logging signature<\/a><\/p>\n<\/li>\n<li><p>without logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/9nQ8w.png\" rel=\"nofollow noreferrer\">mlflow ui without logging signature<\/a><\/p>\n<\/li>\n<\/ol>\n<p>Does this have any consequence later when serving models with signature enforcement?\nAlso, I see many blog examples with postgres instead of sqlite, and sftp\/minio instead of filestore. maybe changing to those setups will solve this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653750895023,
        "Question_score":1,
        "Question_tags":"postgresql|sqlite|metadata|mlflow",
        "Question_view_count":194,
        "Owner_creation_time":1653748227087,
        "Owner_last_access_time":1661359833540,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72416831",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65330061,
        "Question_title":"working directory changes to \/tmp\/ when python script runs with mlflow",
        "Question_body":"<p>I have a strange issue with python working directory when running with mlflow run -e build .\nThe script running successfully locally\/using IDE, but when running it with mlflow the problem is that the working directory changes to \/tmp folders instead of the correct working directory where the script resides (I have some path dependencies that certain folders should be present in .\/* so thats why my process fails.<\/p>\n<p>I had a feeling that something with the working directory messed up so I did os.getcwd() prints and saw the issue with temp folders.<\/p>\n<p>I had a similar project that I configured in a similar manner before and didn't have these issues.<\/p>\n<p>any idea what might be the issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608147574247,
        "Question_score":0,
        "Question_tags":"python|path|mlflow",
        "Question_view_count":286,
        "Owner_creation_time":1511185810877,
        "Owner_last_access_time":1644828875333,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65330061",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71610688,
        "Question_title":"Is it possible to change the legend of the plot chart in mlflow metrics?",
        "Question_body":"<p>Thanks for the development of mlflow. I love it very much.<\/p>\n<p>I want to compare several runs with different hyper parameters, but I found that it is very difficult to differenciate these runs from the legend (some random numbers as the run ID) as shown in the screenshot.<\/p>\n<p>I hope the legend could be set to the hyper parameters in which these runs have different values. For instance, the legend could be set to different <code>patch size<\/code>, or different <code>learning rate<\/code>, etc.<\/p>\n<p>So is it possible for the current mlflow? If not, do you have the plan to develop this feature?<\/p>\n<p>This question is similar with this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/5523\" rel=\"nofollow noreferrer\">issue<\/a>. But the issue proposed to use the customed <code>name<\/code> as the legend, while I think it is better to set it as the different hyperparemeters. Or it is best to let users to choose how to set the legend.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Wagl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Wagl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648165795930,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":68,
        "Owner_creation_time":1516197597947,
        "Owner_last_access_time":1663970048630,
        "Owner_location":"Leiden, \u8377\u5170",
        "Owner_reputation":908,
        "Owner_up_votes":568,
        "Owner_down_votes":12,
        "Owner_views":151,
        "Question_last_edit_time":1648167115347,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71610688",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73015639,
        "Question_title":"Issues with deploying spark and mlflow to sagemaker",
        "Question_body":"<p>My goal is to deploy a spark\/mlflow to sagemaker with the following command:<\/p>\n<pre><code>    mlflow sagemaker deploy .. \n<\/code><\/pre>\n<p>I've successfully pushed a image to EC2 with<\/p>\n<pre><code>mlflow sagemaker build-and-push-container\n<\/code><\/pre>\n<p>I encounter errors when attempting to run mlflow sagemaker deploy:<\/p>\n<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/127.0.0.1:8000\/ping&quot;, host: &quot;model.aws.local:8080&quot;\njava.io.IOException: Failed to connect to model.aws.local\/172.17.0.2:34473\n<\/code><\/pre>\n<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \nspark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          \n<\/code><\/pre>\n<p>However this outputted the following error:<\/p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org\/apache\/commons\/configuration\/Configuration\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)\n    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)\n    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)\n    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)\n    at scala.Option.getOrElse(Option.scala:189)\n    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)\n    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)\n    at scala.Option.map(Option.scala:230)\n    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)\n    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n    ... 20 more\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nInput In [7], in &lt;cell line: 3&gt;()\n      1 # Create Spark Session\n      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \n----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py:228, in SparkSession.Builder.getOrCreate(self)\n    226         sparkConf.set(key, value)\n    227     # This SparkContext may be an existing one.\n--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)\n    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230 # by all sessions.\n    231 session = SparkSession(sc)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:384, in SparkContext.getOrCreate(cls, conf)\n    382 with SparkContext._lock:\n    383     if SparkContext._active_spark_context is None:\n--&gt; 384         SparkContext(conf=conf or SparkConf())\n    385     return SparkContext._active_spark_context\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    140     raise ValueError(\n    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;\n    142         &quot; is not allowed as it is a security risk.&quot;)\n--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145 try:\n    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n    147                   conf, jsc, profiler_cls)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    329 with SparkContext._lock:\n    330     if not SparkContext._gateway:\n--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)\n    332         SparkContext._jvm = SparkContext._gateway.jvm\n    334     if instance:\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)\n    105     time.sleep(0.1)\n    107 if not os.path.isfile(conn_info_file):\n--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    110 with open(conn_info_file, &quot;rb&quot;) as info:\n    111     gateway_port = read_int(info)\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658095201223,
        "Question_score":0,
        "Question_tags":"python|apache-spark|deployment|amazon-sagemaker|mlflow",
        "Question_view_count":59,
        "Owner_creation_time":1642068779037,
        "Owner_last_access_time":1663949787333,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73015639",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73153142,
        "Question_title":"Mlflow UI can't show artifacts",
        "Question_body":"<p>I have mlflow running on an azure VM and connected to Azure Blob as the artifact storage.<\/p>\n<p>After uploading artifacts to the storage from the Client.<\/p>\n<p>I tried the MLflow UI and successfully was able to show the uploaded file.<\/p>\n<p>The problem happens when I try to run MLFLOW with Docker, I get the error:\n<strong>Unable to list artifacts stored under <code>{artifactUri}<\/code> for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory<\/strong><\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM python:3.7-slim-buster\n# Install python packages\nRUN pip install mlflow pymysql\n\nRUN pip install azure-storage-blob\n\nENV AZURE_STORAGE_ACCESS_KEY=&quot;#########&quot;\nENV AZURE_STORAGE_CONNECTION_STRING=&quot;#######&quot;\n<\/code><\/pre>\n<p>docker-compose.yml<\/p>\n<pre><code>web:\n        restart: always\n        build: .\/mlflow_server\n        image: mlflow_server\n        container_name: mlflow_server\n        expose:\n            - &quot;5000&quot;\n        networks:\n            - frontend\n            - backend\n        environment:\n            - AZURE_STORAGE_ACCESS_KEY=&quot;#####&quot;\n            - AZURE_STORAGE_CONNECTION_STRING=&quot;#####&quot;\n        command: mlflow server --backend-store-uri mysql+pymysql:\/\/mlflow_user:123456@db:3306\/mlflow --default-artifact-root wasbs:\/\/etc..\n<\/code><\/pre>\n<p>I tried multiple solutions:<\/p>\n<ol>\n<li>Making sure that boto3 is installed (Didn't do anything)<\/li>\n<li>Adding Environment Variables in the Dockerfile so the command runs after they're set<\/li>\n<li>I double checked the url of the storage blob<\/li>\n<\/ol>\n<p>And MLFLOW doesn't show any logs it just kills the process and restarts again.<\/p>\n<p>Anyone got any idea what might be the solution or how can i access the logs<\/p>\n<p>here're the docker logs of the container:<\/p>\n<pre><code>[2022-07-28 12:23:33 +0000] [10] [INFO] Starting gunicorn 20.1.0\n[2022-07-28 12:23:33 +0000] [10] [INFO] Listening at: http:\/\/0.0.0.0:5000 (10)\n[2022-07-28 12:23:33 +0000] [10] [INFO] Using worker: sync\n[2022-07-28 12:23:33 +0000] [13] [INFO] Booting worker with pid: 13\n[2022-07-28 12:23:33 +0000] [14] [INFO] Booting worker with pid: 14\n[2022-07-28 12:23:33 +0000] [15] [INFO] Booting worker with pid: 15\n[2022-07-28 12:23:33 +0000] [16] [INFO] Booting worker with pid: 16\n[2022-07-28 12:24:24 +0000] [10] [CRITICAL] WORKER TIMEOUT (pid:14)\n[2022-07-28 12:24:24 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2022-07-28 12:24:24 +0000] [21] [INFO] Booting worker with pid: 21\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659012211500,
        "Question_score":0,
        "Question_tags":"docker|docker-compose|azure-blob-storage|mlflow",
        "Question_view_count":97,
        "Owner_creation_time":1659010615837,
        "Owner_last_access_time":1663625482013,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73153142",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71950167,
        "Question_title":"What is the use of git commits in mlflow?",
        "Question_body":"<p>Why mlflow tracks git commits, we already have run_id for tracking experiment. Can we use those commits to go back to previous commit like we do in git.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650524426890,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":161,
        "Owner_creation_time":1608194065657,
        "Owner_last_access_time":1662033766403,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71950167",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56984854,
        "Question_title":"fcntl error with \u201cmlflow ui\u201d on windows - mlflow 1.0",
        "Question_body":"<p>I am getting the following error message when trying mlflow examples and running 'mlflow ui'.<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'fcntl' Running the mlflow server\n  failed. Please see the logs above for details<\/p>\n<\/blockquote>\n\n<p>Is anyone aware of a solution to this issue?<\/p>\n\n<p>I have tried the solutions suggested at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/1080\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/1080<\/a><\/p>\n\n<p>without success. Replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for with the following:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\thesis_mlflow\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\cli.py\", line 198, in ui\n    _run_server(backend_store_uri, default_artifact_root, \"127.0.0.1\", port, None, 1)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 90, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n    stdin=subprocess.PIPE, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 729, in __init__\n    restore_signals, start_new_session)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562833873410,
        "Question_score":1,
        "Question_tags":"python|windows|mlflow",
        "Question_view_count":725,
        "Owner_creation_time":1529408888483,
        "Owner_last_access_time":1649930832187,
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1562909693510,
        "Answer_body":"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562919127687,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56984854",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61975133,
        "Question_title":"Run mlflow project on multiple remote servers?",
        "Question_body":"<p>Can <code>MLflow<\/code> be used to dispatch <strong>projects<\/strong> to multiple remote servers?(not aws,azure etc.) from a local tracking server?<br>\nI have the following scenario-<Br>\nMultiple servers, where I would like to dispatch the <code>mlflow<\/code> project to all with different parameters, and let them \"report\" back to the current <strong>tracking server:<\/strong><\/p>\n\n<pre><code>for ip in servers_ips:\n    start_remote_mlflow(entry_point=GITHUBPATH,tracking_server=this_server_ip,hparams)\n<\/code><\/pre>\n\n<p>I see one can dispatch <code>mlflow<\/code> projects to aws or azure by specifying the ip or the remote machine. Can it be done with desktops as well?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590251060187,
        "Question_score":2,
        "Question_tags":"python|remote-server|mlflow",
        "Question_view_count":221,
        "Owner_creation_time":1476768953033,
        "Owner_last_access_time":1664082143130,
        "Owner_location":"Israel",
        "Owner_reputation":2057,
        "Owner_up_votes":201,
        "Owner_down_votes":2,
        "Owner_views":269,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61975133",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70099987,
        "Question_title":"Setting Mlflow region in Python script",
        "Question_body":"<p>Hi all I deployed my minio on my localhost using Kuberenetes @ url: mlflow-minio.local<\/p>\n<p>I also deployed my mlflow server <a href=\"http:\/\/mlflow-server.local\" rel=\"nofollow noreferrer\">http:\/\/mlflow-server.local<\/a> and set the parameters as below in my python script<\/p>\n<pre><code>def savemlflow(lm, output, test_size, random_state, coeff_df):\n    mlflow.set_tracking_uri('http:\/\/mlflow-server.local')\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/mlflow-minio.local\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio'\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(lm, &quot;model&quot;)\n        mlflow.log_metric(&quot;MAE&quot;, metrics.mean_absolute_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;]))\n        mlflow.log_metric(&quot;MSE&quot;, metrics.mean_squared_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;]))\n        mlflow.log_metric(&quot;RMSE&quot;, np.sqrt(metrics.mean_squared_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;])))\n<\/code><\/pre>\n<p>However, I keep getting a <code>boto3.exceptions.S3UploadFailedError: Failed to upload \/var\/folders\/xw\/6jppt2490z30qk9pz1fxm2c80000gp\/T\/tmpjen_w4gh\/model\/requirements.txt to mlflow\/0\/adf90c9806e64f64aaf14c4513a00dbf\/artifacts\/model\/requirements.txt: An error occurred (InvalidRegion) when calling the PutObject operation: Region does not match.<\/code><\/p>\n<p>Im guessing I need to set the region somehow for my minio in my python. But how do i do know what is the current region so I can be able to set it?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1637772817887,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":62,
        "Owner_creation_time":1381186868793,
        "Owner_last_access_time":1664071153520,
        "Owner_location":null,
        "Owner_reputation":1069,
        "Owner_up_votes":43,
        "Owner_down_votes":0,
        "Owner_views":178,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70099987",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62841162,
        "Question_title":"How can I password protect my mlflow portal",
        "Question_body":"<p>I have installed my mlflow on centos7 and hosting it at a port 5000.\nI followed this article for reference: <a href=\"https:\/\/medium.com\/analytics-vidhya\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">Install MLFlow with postgres<\/a><\/p>\n<p>I am looking to secure my mlflow UI with username and password. Any authentication method should be fine, however, Single Sign On is preferred.<\/p>\n<p>I looked at this article: <a href=\"https:\/\/karimlahrichi.com\/2020\/03\/13\/add-authentication-to-mlflow\/\" rel=\"nofollow noreferrer\">Add Authentication to MLFlow<\/a> It allows me to secure all the traffic going from port 80. After successful authentication I will be redirected to port 5000 where my MLFlow application is running. However, if I directly go to host:5000 my mlflow doesn't ask me for any authentication.\nPlease help me understand how I can enable mandatory authentication before you can reach the mlflow dashboard.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594409600090,
        "Question_score":1,
        "Question_tags":"python|authentication|nginx|centos7|mlflow",
        "Question_view_count":1682,
        "Owner_creation_time":1490818955347,
        "Owner_last_access_time":1619802281030,
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62841162",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70968664,
        "Question_title":"How to run data bricck notebook with mlflow in azure data factory pipeline?",
        "Question_body":"<p>My colleagues and I are facing an issue when trying to run my databricks notebook in Azure Data Factory and the error is coming from MLFlow.<\/p>\n<p>The command that is failing is the following:<\/p>\n<pre><code># Take the parent notebook path to use as path for the experiment\ncontext = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\nnb_base_path = context['extraContext']['notebook_path'][:-len(&quot;00_training_and_validation&quot;)]\n\nexperiment_path = nb_base_path + 'trainings'\nmlflow.set_experiment(experiment_path)\nexperiment = mlflow.get_experiment_by_name(experiment_path)\nexperiment_id = experiment.experiment_id\n\nrun = mlflow.start_run(experiment_id=experiment_id, run_name=f&quot;run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;)\n<\/code><\/pre>\n<p>And the error that is throwing is:<\/p>\n<p>An exception was thrown from a UDF: 'mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: No experiment ID was specified. An experiment ID must be specified in Databricks Jobs and when logging to the MLflow server from outside the Databricks workspace. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;\/path\/to\/experiment\/in\/workspace&quot;) at the start of your program.', from , line 32.<\/p>\n<p>The pipeline just runs the notebook from ADF, it does not have any other step and the cluster we are using is type 7.3 ML.<\/p>\n<p>Could you please help us?<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643880182260,
        "Question_score":0,
        "Question_tags":"azure-data-factory|databricks|mlflow",
        "Question_view_count":392,
        "Owner_creation_time":1586330708360,
        "Owner_last_access_time":1643881412287,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968664",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69175969,
        "Question_title":"Data bricks:- Cannot display the predicted output by using ml flow registered model",
        "Question_body":"<p>I have created a model using diabetes dataset for prediction. I have trained, evaluated, logged and registered it as a new model in ML flow. Now I am trying to load the registered model and trying to predict on new data. All though I was able to predict the results. I am not able to display it. When I try to display using command <code>.show()<\/code> or <code>display()<\/code> it is throwing an error. What is the cause of the error? and How do I display the results?<\/p>\n<p>Note: I have programmed using pure pyspark and all the ML flow operation was done on Data bricks<\/p>\n<p>Code:-<\/p>\n<pre><code>model_details = mlflow.tracking.MlflowClient().get_latest_versions('model1',stages=['staging'])[0]\nmodel = mlflow.pyfunc.spark_udf(spark,model_details.source)\ninput_df = sdf.drop('progression')\ncolumns = list(map(lambda c: f&quot;{c}&quot;, input_df.columns))\ndf = input_df.withColumn(&quot;progression&quot;, model(*columns))\ndf.show(truncate=False)\n<\/code><\/pre>\n<p>Error :-<\/p>\n<pre><code>PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:\nPythonException                           Traceback (most recent call last)\n&lt;command-1343735193245452&gt; in &lt;module&gt;\n     34 df = input_df.withColumn(&quot;progression&quot;, model(*columns))\n     35 \n---&gt; 36 df.show(truncate=False)\n\n\/databricks\/spark\/python\/pyspark\/sql\/dataframe.py in show(self, n, truncate, vertical)\n    441             print(self._jdf.showString(n, 20, vertical))\n    442         else:\n--&gt; 443             print(self._jdf.showString(n, int(truncate), vertical))\n    444 \n    445     def __repr__(self):\n\n\/databricks\/spark\/python\/lib\/py4j-0.10.9-src.zip\/py4j\/java_gateway.py in __call__(self, *args)\n   1303         answer = self.gateway_client.send_command(command)\n   1304         return_value = get_return_value(\n-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)\n   1306 \n   1307         for temp_arg in temp_args:\n\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in deco(*a, **kw)\n    131                 # Hide where the exception came from that shows a non-Pythonic\n    132                 # JVM exception message.\n--&gt; 133                 raise_from(converted)\n    134             else:\n    135                 raise\n\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in raise_from(e)\n\nPythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:\nTraceback (most recent call last):\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 654, in main\n    process()\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 281, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 97, in dump_stream\n    for batch in iterator:\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 271, in init_stream_yield_batches\n    for series in iterator:\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 467, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 467, in &lt;genexpr&gt;\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 111, in &lt;lambda&gt;\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File &quot;\/databricks\/spark\/python\/pyspark\/util.py&quot;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 827, in predict\n    model = SparkModelCache.get_or_load(archive_path)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/spark_model_cache.py&quot;, line 64, in get_or_load\n    SparkModelCache._models[archive_path] = load_pyfunc(temp_dir)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/utils\/annotations.py&quot;, line 43, in deprecated_func\n    return func(*args, **kwargs)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 693, in load_pyfunc\n    return load_model(model_uri, suppress_warnings)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/spark.py&quot;, line 707, in _load_pyfunc\n    .master(&quot;local[1]&quot;)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/session.py&quot;, line 189, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 134, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 333, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/java_gateway.py&quot;, line 105, in launch_gateway\n    raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1631614691113,
        "Question_score":1,
        "Question_tags":"pyspark|apache-spark-sql|user-defined-functions|databricks|mlflow",
        "Question_view_count":168,
        "Owner_creation_time":1628599797537,
        "Owner_last_access_time":1637600430000,
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69175969",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70539698,
        "Question_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Question_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640933189943,
        "Question_score":1,
        "Question_tags":"amazon-s3|docker-compose|mlflow",
        "Question_view_count":932,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641275216080,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70539698",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73040570,
        "Question_title":"How to share models in a multitenant enviroment with Mlflow?",
        "Question_body":"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces<\/a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658248426997,
        "Question_score":0,
        "Question_tags":"azure|azure-blob-storage|databricks|mlflow",
        "Question_view_count":49,
        "Owner_creation_time":1632199544943,
        "Owner_last_access_time":1664060085030,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73040570",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66566031,
        "Question_title":"MLFLow artifact logging and retrieve on remote server",
        "Question_body":"<p>I am trying to setup a MLFlow tracking server on a remote machine as a systemd service.\nI have a sftp server running and created a SSH key pair.<\/p>\n<p>Everything seems to work fine except the artifact logging. MLFlow seems to not have permissions to list the artifacts saved in the <code>mlruns<\/code> directory.<\/p>\n<p>I create an experiment and log artifacts in this way:<\/p>\n<pre><code>uri = 'http:\/\/192.XXX:8000' \nmlflow.set_tracking_uri(uri)\n\nmlflow.create_experiment('test', artifact_location='sftp:\/\/192.XXX:_path_to_mlruns_folder_')\n\nexperiment=mlflow.get_experiment_by_name('test')\nwith mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name) as run:\n       mlflow.log_param(_parameter_name_, _parameter_value_)     \n       mlflow.log_artifact(_an_artifact_, _artifact_folder_name_)\n<\/code><\/pre>\n<p>I can see the metrics in the UI and the artifacts in the correct destination folder on the remote machine. However, in the UI I receive this message when trying to see the artifacts:<\/p>\n<blockquote>\n<p>Unable to list artifacts stored\nunder sftp:\/\/192.XXX:<em>path_to_mlruns_folder<\/em>\/<em>run_id<\/em>\/artifacts\nfor the current run. Please contact your tracking server administrator\nto notify them of this error, which can happen when the tracking\nserver lacks permission to list artifacts under the current run's root\nartifact directory.<\/p>\n<\/blockquote>\n<p>I cannot figure out why as the <code>mlruns<\/code> folder has <code>drwxrwxrwx<\/code> permissions and all the subfolders have <code>drwxrwxr-x<\/code>. What am I missing?<\/p>\n<hr \/>\n<p>UPDATE\nLooking at it with fresh eyes, it seems weird that it tries to list files through <code>sftp:\/\/192.XXX:<\/code>, it should just look in the folder <code>_path_to_mlruns_folder_\/_run_id_\/artifacts<\/code>. However, I still do not know how to circumvent that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615383956893,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":2283,
        "Owner_creation_time":1403084852693,
        "Owner_last_access_time":1664082389037,
        "Owner_location":null,
        "Owner_reputation":2210,
        "Owner_up_votes":1124,
        "Owner_down_votes":117,
        "Owner_views":262,
        "Question_last_edit_time":1615451686848,
        "Answer_body":"<p>The problem seems to be that by default the systemd service is run by root.\nSpecifying a user and creating a ssh key pair for that user to access the same remote machine worked.<\/p>\n<pre><code>[Unit]\n\nDescription=MLflow server\n\nAfter=network.target \n\n[Service]\n\nRestart=on-failure\n\nRestartSec=20\n\nUser=_user_\n\nGroup=_group_\n\nExecStart=\/bin\/bash -c 'PATH=_yourpath_\/anaconda3\/envs\/mlflow_server\/bin\/:$PATH exec mlflow server --backend-store-uri postgresql:\/\/mlflow:mlflow@localhost\/mlflow --default-artifact-root sftp:\/\/_user_@192.168.1.245:_yourotherpath_\/MLFLOW_SERVER\/mlruns -h 0.0.0.0 -p 8000' \n\n[Install]\n\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p><code>_user_<\/code> and <code>_group_<\/code> should be the same listed by <code>ls -la<\/code> in the <code>mlruns<\/code> directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1615544206663,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66566031",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68918223,
        "Question_title":"Databricks: Migrate a registered model from one workspace to another?",
        "Question_body":"<p>We have multiple Databricks Workspaces on Azure. On one of them we trained multiple models and registered them in the MLflow registry. Our goal is to move those model from one databricks workspace to another and so far, i could not find a straight forwared way to do this except running the training script again on the new databricks workspace.<\/p>\n<p>Downloading the model an registering them in the new workspace didn't work so far. Should I create a &quot;dummy&quot; training script, that just loads the model, does nothing with it and then logs it away in the new workspace?<\/p>\n<p>Seems to me like databricks never anticipated, that someone might want to migrate ML models?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629874787863,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|migration|databricks|mlflow",
        "Question_view_count":588,
        "Owner_creation_time":1485121974840,
        "Owner_last_access_time":1659449289970,
        "Owner_location":null,
        "Owner_reputation":188,
        "Owner_up_votes":0,
        "Owner_down_votes":1,
        "Owner_views":22,
        "Question_last_edit_time":1629877329852,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68918223",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56701139,
        "Question_title":"Model-logging for \"hybrid models\" (e.g. SKlearn Pipeline including KerasWrapper) possible?",
        "Question_body":"<p>I have wrapped my keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. I want to serialize this model and capture its dependencies via MLflow.<\/p>\n\n<p>I have tried <code>mlflow.keras.save_model()<\/code>, which seems not appropriate. (it's not a \"pure\" keras model and as no <code>save()<\/code> attribute)<\/p>\n\n<p>I also tried <code>mlflow.sklearn.save_model()<\/code> and <code>mlflow.pyfunc.save_model()<\/code>, which both lead my to the same error: <\/p>\n\n<p><code>NotImplementedError: numpy() is only available when eager execution is enabled.<\/code><\/p>\n\n<p>(This error seems to stem from a clash between python and tensorflow, maybe?)<\/p>\n\n<p>I wonder, should it already\/ generally be possible to serialize these kind of \"hybrid\" models with mlflow?<\/p>\n\n<h3>Please finde a minimal example below<\/h3>\n\n<pre><code># In[1]:\n\n\nfrom mlflow.sklearn import save_model\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n\n# ### Save Keras Model\n\n# In[2]:\n\n\niris_data = load_iris() \n\nx = iris_data.data\ny_ = iris_data.target.reshape(-1, 1)\n\n# One Hot encode the class labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y_)\n\n# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\nmodel.add(Dense(10, activation='relu', name='fc2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=2, batch_size=5, epochs=20)\n\n\n# In[3]:\n\n\nimport mlflow.keras\n\nmlflow.keras.save_model(model, \"modelstorage\/model40\")\n\n\n# ### Save Minimal SKlearn-Pipeline (with Keras)\n\n# In[4]:\n\n\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# In[5]:\n\n\ndef define_model():\n    \"\"\"\n    Create fully connected network with given parameters.\n    \"\"\"\n    keras_model = Sequential()\n\n    keras_model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n    keras_model.add(Dense(10, activation='relu', name='fc2'))\n    keras_model.add(Dense(3, activation='softmax', name='output'))\n\n    optimizer = Adam(lr=0.001)\n    keras_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# In[6]:\n\n\n# target_encoder = TargetEncoder() \nscaler = StandardScaler()\nkeras_model = KerasClassifier(define_model, batch_size=5, epochs=20)\n\n\n# In[7]:\n\n\npipeline = Pipeline([\n#     ('encoding', target_encoder),\n    ('scaling', scaler),\n    ('modeling', keras_model)\n])\n\n\n# In[8]:\n\n\npipeline.fit(train_x, train_y)\n\n\n# In[9]:\n\n\nmlflow.keras.save_model(pipeline, \"modelstorage\/model42\")   #not working\n\n\n# In[10]:\n\n\nimport mlflow.sklearn\n\nmlflow.sklearn.save_model(pipeline, \"modelstorage\/model43\")\n\nOutput from modelstorage\/model43\/conda.yaml:\n\n======================\nchannels:\n- defaults\ndependencies:\n- python=3.6.7\n- scikit-learn=0.21.2\n- pip:\n  - mlflow\n  - cloudpickle==1.2.1\nname: mlflow-env\n======================\n\nDoesn't seem to capture Tensorflow.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561111222150,
        "Question_score":2,
        "Question_tags":"python|keras|scikit-learn|mlflow",
        "Question_view_count":1470,
        "Owner_creation_time":1336936830510,
        "Owner_last_access_time":1663079651823,
        "Owner_location":null,
        "Owner_reputation":948,
        "Owner_up_votes":592,
        "Owner_down_votes":1,
        "Owner_views":132,
        "Question_last_edit_time":1561122889067,
        "Answer_body":"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:<\/p>\n\n<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()\n  conda_env[\"dependencies\"] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[\"dependencies\"]\n  mlflow.sklearn.log_model(pipeline, \"modelstorage\/model43\", conda_env = conda_env)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1572628704763,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56701139",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73562615,
        "Question_title":"MlflowException: API request (Caused by ResponseError('too many 503 error responses'))",
        "Question_body":"<p>I am using mlflow to register my model. I try to use 'Scenario 4' when artifacts load to S3 bucket from local.<\/p>\n<ol>\n<li><p>Add credentials of S3 bucket to .aws\/credentials<\/p>\n<\/li>\n<li><p>Set endpoint and mlflow URI:<\/p>\n<p>os.environ[&quot;MLFLOW_S3_ENDPOINT_URL&quot;]='https:\/\/storage.yandexcloud.net'\nos.environ[&quot;MLFLOW_TRACKING_URI&quot;]='http:\/\/:8000'<\/p>\n<\/li>\n<li><p>Log model to S3 via mlflow:<\/p>\n<p>import mlflow\nimport mlflow.sklearn\nmlflow.set_experiment(&quot;my&quot;)\n...\nmlflow.sklearn.log_model(model, artifact_path=&quot;models_mlflow&quot;)<\/p>\n<\/li>\n<\/ol>\n<p>But get error:<\/p>\n<pre><code>MlflowException: API request to http:\/\/&lt;IP&gt;:8000\/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl failed with exception HTTPConnectionPool(host='62.84.121.234', port=8000): Max retries exceeded with url: \/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl (Caused by ResponseError('too many 503 error responses'))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661985677113,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|mlflow|mlops|yandexcloud",
        "Question_view_count":38,
        "Owner_creation_time":1396864721170,
        "Owner_last_access_time":1663931830007,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":75,
        "Owner_up_votes":105,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73562615",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58611088,
        "Question_title":"Is there a way to manage permissions at an experiment level in MLflow?",
        "Question_body":"<p>Is there a way to manage permissions at an experiment level in MLflow?  We would like to have a shared server but would like to be able to manage permissions at an experiment level - e.g. admin can view all experiments, user_group1 can manage experiment1 - perhaps different groups can see results vs post results.<\/p>\n\n<p>It looks like it is possible in databricks: <a href=\"https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions<\/a>  but I can't find anything in the opensource APIdocs.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572364657393,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":1567,
        "Owner_creation_time":1487332729157,
        "Owner_last_access_time":1663766140707,
        "Owner_location":null,
        "Owner_reputation":289,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58611088",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68034523,
        "Question_title":"How to download artifacts from mlflow in python",
        "Question_body":"<p>I am creating an mlflow experiment which logs a logistic regression model together with a metric and an artifact.<\/p>\n<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\n\nwith mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n\n        logreg = LogisticRegression()\n        logreg.fit(x_train, y_train)\n        print('training over', flush=True)\n        y_pred = logreg.predict(x_test)\n        mlflow.sklearn.log_model(logreg, &quot;model&quot;)\n   \n        mlflow.log_metric(&quot;f1&quot;, precision_recall_fscore_support(y_test, y_pred, average='weighted')[2])\n        mlflow.log_artifact(x_train.to_csv('train.csv')\n<\/code><\/pre>\n<p>for some data (<code>x_train, y_train, x_test, y_test<\/code>)<\/p>\n<p>Is there any way to access the artifacts for that specific experiment_id for this run_name and read the <code>train.csv<\/code> and also read the <code>model<\/code> ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624016436840,
        "Question_score":3,
        "Question_tags":"python|python-3.x|mlflow",
        "Question_view_count":5182,
        "Owner_creation_time":1454338460480,
        "Owner_last_access_time":1664044608790,
        "Owner_location":null,
        "Owner_reputation":3527,
        "Owner_up_votes":352,
        "Owner_down_votes":6,
        "Owner_views":440,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is a <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts\" rel=\"noreferrer\">download_artifacts function<\/a> that allows you to get access to the logged artifact:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>local_path = client.download_artifacts(run_id, &quot;train.csv&quot;, local_dir)\n<\/code><\/pre>\n<p>The model artifact could either downloaded using the same function (there should be the object called <code>model\/model.pkl<\/code> (for scikit-learn, or something else), or you can load model by run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>loaded_model = mlflow.pyfunc.load_model(f&quot;runs:\/{run_id}\/model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1624022186892,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68034523",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72341647,
        "Question_title":"MLFlow -> ModuleNotFoundError: No module named 'sqlalchemy.future'",
        "Question_body":"<p>It seems to use MLFlow Model Registry locally, one option is to build my own backend database with SQLite.<\/p>\n<p>I've found a site, which advised to run:<\/p>\n<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlflow.db --default-artifact-root .\/artifacts --host 0.0.0.0 --port 5000\n<\/code><\/pre>\n<p>When running the command above, I get the following error message:<\/p>\n<pre><code>2022\/05\/22 23:08:58 ERROR mlflow.cli: Error initializing backend store\n2022\/05\/22 23:08:58 ERROR mlflow.cli: No module named 'sqlalchemy.future'\nTraceback (most recent call last):\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/cli.py&quot;, line 426, in server\n    initialize_backend_stores(backend_store_uri, default_artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 259, in initialize_backend_stores\n    _get_tracking_store(backend_store_uri, default_artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 244, in _get_tracking_store\n    _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py&quot;, line 39, in get_store\n    return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py&quot;, line 49, in _get_store_with_resolved_uri\n    return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 110, in _get_sqlalchemy_store\n    from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py&quot;, line 11, in &lt;module&gt;\n    from sqlalchemy.future import select\nModuleNotFoundError: No module named 'sqlalchemy.future'\n<\/code><\/pre>\n<p>This seems odd, because if I run <code>pip freeze<\/code>, the sqlalchemy shows up, or if I do <code>from sqlalchemy.future import select<\/code> in a notebook, I get no error.<\/p>\n<p>I think this may related to using a virtual environment. The current one I'm using is in <code>\/home\/username\/folder\/mlflow\/.mlflow<\/code> but mlflow seems to be looking elsewhere for the file...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653257749153,
        "Question_score":1,
        "Question_tags":"python|sqlalchemy|mlflow",
        "Question_view_count":274,
        "Owner_creation_time":1396288958297,
        "Owner_last_access_time":1664053410610,
        "Owner_location":null,
        "Owner_reputation":714,
        "Owner_up_votes":128,
        "Owner_down_votes":6,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72341647",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71738738,
        "Question_title":"How to set custom path for databricks mlflow artifacts on s3",
        "Question_body":"<p>I've created an empty experiments from databricks experiments console and given the path for my artifacts on s3 i.e. s3:\/\/\/. When i run the scripts, the artifacts are stored at<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/\/&lt;32 char id&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>I want to replace \/\/&lt;32 char id&gt;\/artifacts\/ with \/datetime\/artifacts\/ so something like<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/&lt;datetime&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>Is there any way i could achieve that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Note: experiment_id is from databricks experiment console<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1649081553880,
        "Question_score":2,
        "Question_tags":"databricks|mlflow|aws-databricks|mlops",
        "Question_view_count":140,
        "Owner_creation_time":1526140623120,
        "Owner_last_access_time":1664008621693,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":962,
        "Owner_up_votes":106,
        "Owner_down_votes":9,
        "Owner_views":128,
        "Question_last_edit_time":1649316095152,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738738",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67786052,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622538922663,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622542563552,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72250896,
        "Question_title":"PowerShell Get request with body",
        "Question_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1652637573657,
        "Question_score":1,
        "Question_tags":"powershell|rest|python-requests|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1652640935860,
        "Answer_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1652649592320,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73501103,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661517215980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1661625379892,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51335594,
        "Question_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Question_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1531546446273,
        "Question_score":1,
        "Question_tags":"python|windows|fcntl|mlflow",
        "Question_view_count":4688,
        "Owner_creation_time":1308552848513,
        "Owner_last_access_time":1664030809627,
        "Owner_location":null,
        "Owner_reputation":1177,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1531837117032,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1531837039907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70098779,
        "Question_title":"How to connect to MLFlow tracking server that has auth?",
        "Question_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637767811310,
        "Question_score":1,
        "Question_tags":"authorization|tracking|mlflow",
        "Question_view_count":2102,
        "Owner_creation_time":1637766437853,
        "Owner_last_access_time":1663839694783,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1637773273483,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70098779",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73241326,
        "Question_title":"Can't see artifact ui in mlflow dashboard",
        "Question_body":"<p>mlflow server <br \/>\n--host 0.0.0.0 <br \/>\n--port 5000 <br \/>\n--backend-store-uri sqlite:\/\/\/\/tmp\/test.db <br \/>\n--artifacts-destination s3:\/\/mlflow <br \/>\n--serve-artifacts<\/p>\n<p>Using minio as S3\nAnd env. Variable as secret key &amp; access key<\/p>\n<p>#mlflow #artifactui #proxyartifact<a href=\"https:\/\/i.stack.imgur.com\/Q3h0B.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659641244373,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":65,
        "Owner_creation_time":1542045989230,
        "Owner_last_access_time":1663926597850,
        "Owner_location":"Gandhinagar, Gujarat, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660023754676,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73241326",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70377991,
        "Question_title":"Setting tracking and artifact locations",
        "Question_body":"<p>I read the documentation for setting the tracking and artifact location. For tracking URI, the options are:<\/p>\n<ol>\n<li>set the <code>MLFLOW_TRACKING_URI<\/code> in bash or the environment I use<\/li>\n<li>set the location with <code>mlflow.set_tracking_uri<\/code> inside the python code<\/li>\n<li>Start a server and then set the above parameters to reflect the server information.<\/li>\n<\/ol>\n<p>How can I set the tracking uri in MLproject? I want to use minimal external code in my project. One way I think of is to use the environment section of the MLproject file environment, like <code>[[&quot;NEW_ENV_VAR&quot;, &quot;new_var_value&quot;]<\/code> Is this correct? Or is there any other way to do it? I could find no example for this except under docker section.<\/p>\n<p>Secondly, same for the artifact registry. Can this be set somewhere in the MLproject file?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1639653097383,
        "Question_score":0,
        "Question_tags":"tracking|artifact|mlflow",
        "Question_view_count":146,
        "Owner_creation_time":1462427517847,
        "Owner_last_access_time":1646398883960,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1639662138947,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70377991",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61258979,
        "Question_title":"EKS Docker Image Pull CrashLoopBackOff",
        "Question_body":"<p>I'm trying to deploy a Docker image from ECR to my EKS. When attempting to deploy my docker image to a pod, I get the following events from a CrashLoopBackOff:<\/p>\n\n<pre><code>Events:\n  Type     Reason                  Age               From                                   Message\n  ----     ------                  ----              ----                                   -------\n  Normal   Scheduled               62s               default-scheduler                      Successfully assigned default\/mlflow-tracking-server to &lt;EC2 IP&gt;.internal\n  Normal   SuccessfulAttachVolume  60s               attachdetach-controller                AttachVolume.Attach succeeded for volume \"&lt;PVC&gt;\"\n  Normal   Pulling                 56s               kubelet, &lt;IP&gt;.ec2.internal             Pulling image \"&lt;ECR Image UI&gt;\"\n  Normal   Pulled                  56s               kubelet, &lt;IP&gt;.ec2.internal             Successfully pulled image \"&lt;ECR Image UI&gt;\"\n  Normal   Created                 7s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Created container mlflow-tracking-server\n  Normal   Pulled                  7s (x3 over 54s)  kubelet, &lt;IP&gt;.ec2.internal             Container image \"&lt;ECR Image UI&gt;\" already present on machine\n  Normal   Started                 6s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Started container mlflow-tracking-server\n  Warning  BackOff                 4s (x5 over 52s)  kubelet, &lt;IP&gt;.ec2.internal             Back-off restarting failed container\n<\/code><\/pre>\n\n<p>I don't understand why it keeps looping like this and failing. Would anyone know why this is happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1587067011163,
        "Question_score":1,
        "Question_tags":"docker|amazon-eks|mlflow",
        "Question_view_count":604,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>CrashLoopBackError<\/code> can be related to these possible reasons:<\/p>\n\n<ul>\n<li><p>the application inside your pod is not starting due to an error;<\/p><\/li>\n<li><p>the image your pod is based on is not present in the registry, or the\nnode where your pod has been scheduled cannot pull from the registry;<\/p><\/li>\n<li><p>some parameters of the pod has not been configured correctly.<\/p><\/li>\n<\/ul>\n\n<p>In your case it seems an application error, inside the container.\nTry to view the logs with:<\/p>\n\n<pre><code>kubectl logs &lt;your_pod&gt; -n &lt;namespace&gt;\n<\/code><\/pre>\n\n<p>For more info on how to troubleshoot this kind of error refer to:<\/p>\n\n<p><a href=\"https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html\" rel=\"nofollow noreferrer\">https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588674359532,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61258979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57987999,
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568794247973,
        "Question_score":6,
        "Question_tags":"mlflow",
        "Question_view_count":3582,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":1568796468592,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1569793174476,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73619538,
        "Question_title":"geting artifacts from mlflow GridSearch run",
        "Question_body":"<p>I'm running a sklearn pipeline with hyperparameter search (let's say GridSearch). Now, I am logging artifacts such as test results and whole-dataset predictions. I'd like to retrieve these artifacts but the mlflow API is getting in the way...<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.set_tracking_uri(&quot;sqlite:\/\/\/mlruns\/mlruns.db&quot;)\nmlflow.set_registry_uri(&quot;.\/mlruns\/&quot;)\n\nrun_ids = [r.run_id for r in mlflow.list_run_infos(mlflow.get_experiment_by_name(&quot;My Experiment&quot;).experiment_id)]\n<\/code><\/pre>\n<p>With the above code, I can retrieve all runs but I have no way of telling which one is a toplevel run with artifacts logged or a sub-run spawned by the GridSearch procedure.<\/p>\n<p>Is there some way of querying only for <strong>parent<\/strong> runs, so I can retrieve these csv files in order to plot the results? I can of course go to the web api and manually select the run then copy the URI for the file, but I'd like to do it programmatically instead of opening a tab and clicking things.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662455773517,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":10,
        "Owner_creation_time":1467887362327,
        "Owner_last_access_time":1663866285120,
        "Owner_location":"Spain",
        "Owner_reputation":624,
        "Owner_up_votes":56,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73619538",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69484727,
        "Question_title":"Pyspark: How to save and apply IndexToString to convert labels back to original values in a new predicted dataset",
        "Question_body":"<p>I am using pyspark.ml.RandomForestClassifier and one of the steps here involves <strong>StringIndexer<\/strong> on the training data target variable to convert it into labels.<\/p>\n<pre><code>indexer = StringIndexer(inputCol = target_variable_name, outputCol = 'label').fit(df)\ndf = indexer.transform(df)\n<\/code><\/pre>\n<p>After fitting the final model I am saving it using mlflow.spark.log_model(). So, when applying the model on a new dataset in future, I just load the model again and apply to the new data:<\/p>\n<pre><code>model = mlflow.sklearn.load_model(&quot;models:\/RandomForest_model\/None&quot;)\npredictions = rfModel.transform(new_data)\n<\/code><\/pre>\n<p>In the new_data the prediction will come as <strong>labels<\/strong> and not in original value. So, if I have to get the original values I have to use <strong>IndexToString<\/strong><\/p>\n<pre><code>labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;,labels=indexer.labels)\npredictions = labelConverter.transform(predictions)\n<\/code><\/pre>\n<p>So, the question is, my model doesn't save the <strong>indexer.labels<\/strong> as only the model gets saved. How do, I save and use the indexer.labels from my training dataset on any new dataset. Can this be saved and retrived in mlflow ?<\/p>\n<p>Apologies, if Iam sounding na\u00efve here . But, getting back the original values in the new dataset is really getting me confused.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1633624197133,
        "Question_score":1,
        "Question_tags":"pyspark|databricks|random-forest|apache-spark-mllib|mlflow",
        "Question_view_count":115,
        "Owner_creation_time":1501160366927,
        "Owner_last_access_time":1663743894843,
        "Owner_location":null,
        "Owner_reputation":459,
        "Owner_up_votes":100,
        "Owner_down_votes":0,
        "Owner_views":61,
        "Question_last_edit_time":1633676810887,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69484727",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61879913,
        "Question_title":"ModuleNotFoundError: No module named 'pyspark.dbutils' while running multiple.py file\/notebook on job clusters in databricks",
        "Question_body":"<p>I am working in TravisCI, MlFlow and Databricks environment where .tavis.yml sits at git master branch and detects any change in <code>.py<\/code> file and whenever it gets updated, It will run mlflow command to run .py file in databricks environment. \nmy MLProject file looks as following:<\/p>\n\n<pre><code>name: mercury_cltv_lib\nconda_env: conda-env.yml\n\n\nentry_points:    \n  main:\n    command: \"python3 run-multiple-notebooks.py\"\n<\/code><\/pre>\n\n<p>Workflow is as following:\nTravisCI detects change in master branch-->triggers build which will run MLFlow command and it'll spin up a job cluster in databricks to run .py file from repo.<\/p>\n\n<p>It worked fine with one .py file but when I tried to run multiple notebook using dbutils, it is throwing <\/p>\n\n<pre><code>  File \"run-multiple-notebooks.py\", line 3, in &lt;module&gt;\n    from pyspark.dbutils import DBUtils\nModuleNotFoundError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>Please find below the relevant code section from run-multiple-notebooks.py<\/p>\n\n<pre><code>  def get_spark_session():\n    from pyspark.sql import SparkSession\n    return SparkSession.builder.getOrCreate()\n\n  def get_dbutils(self, spark = None):\n    try:\n        if spark == None:\n            spark = spark\n\n        from pyspark.dbutils import DBUtils #error line\n        dbutils = DBUtils(spark) #error line\n    except ImportError:\n        import IPython\n        dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n    return dbutils\n\n  def submitNotebook(notebook):\n    print(\"Running notebook %s\" % notebook.path)\n    spark = get_spark_session()\n    dbutils = get_dbutils(spark)\n<\/code><\/pre>\n\n<p>I tried all the options and tried <\/p>\n\n<pre><code>https:\/\/stackoverflow.com\/questions\/61546680\/modulenotfounderror-no-module-named-pyspark-dbutils\n<\/code><\/pre>\n\n<p>as well. It is not working :(<\/p>\n\n<p>Can someone please suggest if there is fix for the above-mentioned error while running .py in job cluster. My code works fine inside databricks local notebook but running from outside using TravisCI and MLFlow isn't working which is must requirement for pipeline automation.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1589840102237,
        "Question_score":2,
        "Question_tags":"pyspark|travis-ci|databricks|mlflow|dbutils",
        "Question_view_count":401,
        "Owner_creation_time":1555347036127,
        "Owner_last_access_time":1663694487237,
        "Owner_location":"Minnesota, USA",
        "Owner_reputation":352,
        "Owner_up_votes":27,
        "Owner_down_votes":1,
        "Owner_views":88,
        "Question_last_edit_time":1589982476476,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61879913",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68547133,
        "Question_title":"Using pytorch_lightning.loggers.MLFlowLogger with azure machine learning studio raises exception mlflow.exceptions.RestException: BAD_REQUEST",
        "Question_body":"<p>I'm trying to locally train pytorch_lightning model and log metrics using  pytorch_lightning.loggers.MLFlowLogger.<\/p>\n<p>It was working fine until last weekend. Now training crashes with error:<\/p>\n<pre><code>mlflow.exceptions.RestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Metric once published using sync API should always use sync API to publish following metrics', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '', 'request': ''}, 'Environment': 'northeurope', 'Location': 'northeurope', 'Time': '2021-07-27T14:06:23.7035319+00:00', 'ComponentName': 'run-history', 'error_code': 'BAD_REQUEST'}\n<\/code><\/pre>\n<p>How to fix this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627397531183,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|mlflow|pytorch-lightning",
        "Question_view_count":176,
        "Owner_creation_time":1625582525933,
        "Owner_last_access_time":1656680057217,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68547133",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73321771,
        "Question_title":"Mlflow authorization with spnego",
        "Question_body":"<p>I saw this topic about Kerberos authntication - <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2678\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/2678<\/a> . It was in 2020 . Our team trying to do authentication with kerberos by spnego. We did spnego on nginx server and it is fine - and get code 200 when we do curl to mlflow http uri . BUT we can't do it with mlflow environment variable .<\/p>\n<p>The question is - Does mlflow has some feature to make authentication with spnego or not? Or it has just these environment variables for authentication and such methods :<\/p>\n<ul>\n<li>MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables .<\/li>\n<li>MLFLOW_TRACKING_TOKEN - token to use with HTTP Bearer authentication. Basic authentication takes precedence if set.<\/li>\n<li>MLFLOW_TRACKING_INSECURE_TLS - If set to the literal true, MLflow does not verify the TLS connection, meaning it does not validate certificates or hostnames for https:\/\/ tracking URIs. This flag is not recommended for production environments. If this is set to true then MLFLOW_TRACKING_SERVER_CERT_PATH must not be set.<\/li>\n<li>MLFLOW_TRACKING_SERVER_CERT_PATH - Path to a CA bundle to use. Sets the verify param of the requests.request function (see <a href=\"https:\/\/requests.readthedocs.io\/en\/master\/api\/\" rel=\"nofollow noreferrer\">https:\/\/requests.readthedocs.io\/en\/master\/api\/<\/a>). When you use a self-signed server certificate you can use this to verify it on client side. If this is set MLFLOW_TRACKING_INSECURE_TLS must not be set (false).<\/li>\n<li>MLFLOW_TRACKING_CLIENT_CERT_PATH - Path to ssl client cert file (.pem). Sets the cert param of the requests.request function (see <a href=\"https:\/\/requests.readthedocs.io\/en\/master\/api\/\" rel=\"nofollow noreferrer\">https:\/\/requests.readthedocs.io\/en\/master\/api\/<\/a>). This can be used to use a (self-signed) client certificate.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660225498580,
        "Question_score":1,
        "Question_tags":"authentication|kerberos|mlflow|spnego",
        "Question_view_count":25,
        "Owner_creation_time":1536337420893,
        "Owner_last_access_time":1662282588817,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73321771",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65627039,
        "Question_title":"MLflow stores tags but does not return them",
        "Question_body":"<p>I am running the below code to store tags and then to retrieve them. As you can see below, Mlflow is storing one set of tags and returning another.<\/p>\n<pre><code>import mlflow\nwith mlflow.start_run() as active_run:\n    tw = { &quot;run_id&quot;: 1}\n    mlflow.set_tags(tw)            \n    print(&quot;Tags are &quot;, active_run.data.tags)\n    print(type(active_run.data.tags))\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>Tags are  {'mlflow.source.name': '\/media\/Space\/AI\/anaconda4\/lib\/python3.7\/site-packages\/ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'adeel'}\n<\/code><\/pre>\n<p>Looking at the stored tags through mlflow ui, I can see that the tag &quot;run_id&quot; set by the code is actually stored in the run. However, only the header information of the run seems to be getting returned by active_run.data.tags.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610100575263,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1611139529420,
        "Answer_body":"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)<\/code> to achieve this.<\/p>\n<pre><code>import mlflow\n\n\nwith mlflow.start_run() as active_run:\n  tags = { &quot;my_tag&quot;: 1}\n  mlflow.set_tags(tags)            \n  # Keep track of the run ID of the active run\n  run_id = active_run.info.run_id\n\nrun = mlflow.get_run(run_id)\nprint(&quot;The tags are &quot;, run.data.tags)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610128097448,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65627039",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69088149,
        "Question_title":"MlFlow: Can't find runs using api",
        "Question_body":"<p>I try get list of runs, but get empty list.<\/p>\n<p>There are my runs:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/v8uF2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v8uF2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But if I try get it using api:\nI expect(<a href=\"https:\/\/mlflow.org\/docs\/latest\/rest-api.html#get-experiment\" rel=\"nofollow noreferrer\">by API<\/a>) that I also watch &quot;runs&quot;, but watch &quot;experiment&quot; only<\/p>\n<pre><code>http:\/\/localhost:5000\/api\/2.0\/mlflow\/experiments\/get?experiment_id=0\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/78a3Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/78a3Y.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I read in doc, that &quot;This field is deprecated. Please use the \u201cSearch Runs\u201d API to fetch runs within an experiment.&quot;, Ok, I try \u201cSearch Runs\u201d<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EExIn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EExIn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Nothing again.<\/p>\n<p>But I try get run by id(from ui):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4eHB7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4eHB7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I need get list of run ids by experiment id. How can I do it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1631017253323,
        "Question_score":0,
        "Question_tags":"python|api|mlflow",
        "Question_view_count":306,
        "Owner_creation_time":1525702055273,
        "Owner_last_access_time":1664047417137,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":155,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69088149",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60667610,
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584090517703,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_time":1498470936987,
        "Owner_last_access_time":1614914388693,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1584552358667,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63806335,
        "Question_title":"How to migrate MlFlow experiments from one Databricks workspace to another with registered models?",
        "Question_body":"<p>so unfortunatly we have to redeploy our Databricks Workspace in which we use the MlFlow functonality with the Experiments and the registering of Models.<\/p>\n<p>However if you export the user folder where the eyperiment is saved with a DBC and import it into the new workspace, the Experiments are not migrated and are just missing.<\/p>\n<p>So the easiest solution did not work. The next thing I tried was to create a new experiment in the new workspace. Copy all the experiment data from the dbfs of the old workspace (with dbfs cp -r dbfs:\/databricks\/mlflow source, and then the same again to upload it to the new workspace) to the new one. And then just reference the location of the data to the experiment like in the following picture:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/emgGs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emgGs.png\" alt=\"Create Experiment with existing path\" \/><\/a><\/p>\n<p>This is also not working, no run is visible, although the path is already existing.<\/p>\n<p>The next idea was that the registred models are the most important one so at least those should be there and accessible. For that I used the documentation here: <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/model-registry.html<\/a>.<\/p>\n<p>With the following code you get a list of the registred models on the old workspace with the reference on the run_id and location.<\/p>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nfor rm in client.list_registered_models():\n    pprint(dict(rm), indent=4)\n<\/code><\/pre>\n<p>And with this code you can add models to a model registry with a reference to the location of the artifact data (on the new workspace):<\/p>\n<pre><code># first the general model must be defined\nclient.create_registered_model(name='MyModel')\n\n# and then the run of the model you want to registre will be added to the model as version one\nclient.create_model_version( name='MyModel', run_id='9fde022012046af935fe52435840cf1', source='dbfs:\/databricks\/mlflow\/experiment_id\/run_id\/artifacts\/model')\n<\/code><\/pre>\n<p>But that did also not worked out. if you go into the Model Registry you get a message like this: <a href=\"https:\/\/i.stack.imgur.com\/Ham4y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ham4y.png\" alt=\"error message of the registred model\" \/><\/a>.<\/p>\n<p>And I really checked, at the given path (the source) there the data is really uploaded and also a model is existing.<\/p>\n<p>Do you have any new ideas to migrate those models in Databricks?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1599634821840,
        "Question_score":4,
        "Question_tags":"migration|databricks|azure-databricks|mlflow",
        "Question_view_count":1704,
        "Owner_creation_time":1437927419340,
        "Owner_last_access_time":1655189509997,
        "Owner_location":null,
        "Owner_reputation":416,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63806335",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73469166,
        "Question_title":"Can we print the configurations on which the MLflow server has started?",
        "Question_body":"<p>I am using the following command to start the MLflow server:<\/p>\n<pre><code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow@localhost\/mlflow  --artifacts-destination &lt;S3 bucket location&gt; --serve-artifacts  -h 0.0.0.0 -p 8000\n<\/code><\/pre>\n<p>Before production deployment, we have a requirement that we need to print or fetch the under what configurations the server is running. For example, the above command uses localhost postgres connection and S3 bucket.<\/p>\n<p>Is there a way to achieve this?<\/p>\n<p>Also, how do I set the server's environment as &quot;production&quot;? So finally I should see a log like this:<\/p>\n<pre><code>[LOG] Started MLflow server:\nEnv: production\npostgres: localhost:5432\nS3: &lt;S3 bucket path&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1661326653987,
        "Question_score":0,
        "Question_tags":"machine-learning|mlflow|model-management",
        "Question_view_count":17,
        "Owner_creation_time":1484504952247,
        "Owner_last_access_time":1664042089473,
        "Owner_location":"Mumbai, India",
        "Owner_reputation":4433,
        "Owner_up_votes":121,
        "Owner_down_votes":31,
        "Owner_views":885,
        "Question_last_edit_time":1661327087603,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73469166",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72443352,
        "Question_title":"How to save summary of pyGAM using MLflow?",
        "Question_body":"<p>Getting None in return while trying to save output of pyGAM summary function in a variable or file to log it using MLflow .<\/p>\n<pre><code>gam = LinearGAM(s(0, n_splines=20) + s(1) + s(2)+ s(3)+s(4)+s(5)+s(6)+s(7)+s(8)+s(9)+s(10)+s(11)+s(12)+s(13)+s(14)+s(15)+s(16)+s(17)).fit(X_GAM, Y_GAM)\ngam.summary()\noutput = gam.summary()\ntype(output)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ub0Gs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ub0Gs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>NoneType<\/p>\n<p>Is there any efficient way to store pyGAM summary output using MLflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653979981097,
        "Question_score":0,
        "Question_tags":"python-3.x|machine-learning|mlflow|pygam",
        "Question_view_count":28,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1653981428128,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72443352",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72473826,
        "Question_title":"When to use mlflow.set_tag() vs mlflow.log_params()?",
        "Question_body":"<p>I am confused about the usecase of mlflow.set_tag() vs mlflow.log_params() as both takes key and value pair. Currently, I use mlflow.set_tag() to set tags for data version, code version, etc and mlflow.log_params() to set model training parameters like loss, accuracy, optimizer, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1654161812087,
        "Question_score":1,
        "Question_tags":"machine-learning|mlflow",
        "Question_view_count":77,
        "Owner_creation_time":1473408279707,
        "Owner_last_access_time":1663860023897,
        "Owner_location":"Germany",
        "Owner_reputation":620,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72473826",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69573565,
        "Question_title":"How to configure artifact store of mlflow tracking service to connect to minio S3 using minio STS generated acces_key, secret_key and session_token",
        "Question_body":"<ul>\n<li><p>Minio is configured with LDAP and am generating credentials of user\nwith AssumeRoleWithLDAPIdentity using STS API (<a href=\"https:\/\/docs.min.io\/minio\/baremetal\/security\/ad-ldap-external-identity-management\/AssumeRoleWithLDAPIdentity.html#assumerolewithldapidentity\" rel=\"nofollow noreferrer\">reference<\/a>)<\/p>\n<\/li>\n<li><p>From above values, I'm setting the variables AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_SESSION_TOKEN (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#amazon-s3-and-s3-compatible-storage\" rel=\"nofollow noreferrer\">reference<\/a>)<\/p>\n<\/li>\n<\/ul>\n<p>I'm getting error when am trying to push model to mlflow to store in minio artifact<\/p>\n<pre><code>S3UploadFailedError: Failed to upload \/tmp\/tmph68xubhm\/model\/MLmodel to mlflow\/1\/xyz\/artifacts\/model\/MLmodel: An error occurred (InvalidTokenId) when calling the PutObject operation: The security token included in the request is invalid\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634225910190,
        "Question_score":2,
        "Question_tags":"amazon-s3|minio|mlflow|mlops",
        "Question_view_count":255,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69573565",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72239105,
        "Question_title":"MLFlow Webhook calling Azure DevOps pipeline - retrieve body",
        "Question_body":"<p>I am using the MLFlow Webhooks , mentioned <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-registry-webhooks\" rel=\"nofollow noreferrer\">here<\/a>. I am using that to queue an Azure Devops Pipeline.<\/p>\n<p>However, I can't seem to to find a way to retrieve the payload variables inside my pipeline.<\/p>\n<p>E.g. during transition of models, according to the document, such a payload is passed<\/p>\n<pre><code>POST\n\/your\/endpoint\/for\/event\/model-versions\/stage-transition\n--data {\n  &quot;event&quot;: &quot;MODEL_VERSION_TRANSITIONED_STAGE&quot;,\n  &quot;webhook_id&quot;: &quot;c5596721253c4b429368cf6f4341b88a&quot;,\n  &quot;event_timestamp&quot;: 1589859029343,\n  &quot;model_name&quot;: &quot;Airline_Delay_SparkML&quot;,\n  &quot;version&quot;: &quot;8&quot;,\n  &quot;to_stage&quot;: &quot;Production&quot;,\n  &quot;from_stage&quot;: &quot;None&quot;,\n  &quot;text&quot;: &quot;Registered model 'someModel' version 8 transitioned from None to Production.&quot;\n}\n<\/code><\/pre>\n<p>My webhook is created like this:<\/p>\n<pre><code>mlflow_webhook_triggerDevOps={\n  &quot;events&quot;: [&quot;TRANSITION_REQUEST_CREATED&quot;, &quot;REGISTERED_MODEL_CREATED&quot;],\n  &quot;description&quot;: &quot;Integration with Azure DevOps&quot;,\n  &quot;status&quot;: &quot;ACTIVE&quot;,\n  &quot;http_url_spec&quot;: {\n                    &quot;url&quot;: &quot;https:\/\/dev.azure.com\/orgname\/ProjectName\/_apis\/build\/builds?definitionId=742&amp;api-version=6.0&quot;,\n                    &quot;authorization&quot;: &quot;Basic &quot; + base64_message\n                    }\n }\n\nmlflow_createwebhook=requests.post('https:\/\/databricksurl\/api\/2.0\/mlflow\/registry-webhooks\/create', headers=header, proxies=proxies, json=mlflow_webhook_body)\n<\/code><\/pre>\n<p>How do I then retrieve the payload variable e.g. model_name, inside my pipeline definition in Azure Devops?.<\/p>\n<p>I looked at <a href=\"https:\/\/stackoverflow.com\/questions\/50838651\/vsts-use-api-to-set-build-parameters-at-queue-time\">this post<\/a>, but I can't seem to see any payload information (like mentioned above) under the Network-payload tab (or I am not using properly).<\/p>\n<p>Right now, I can trigger the pipeline, but can't seem to find a way to retrieve the payload.<\/p>\n<p>Is it possible? Am I missing something?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1652522121283,
        "Question_score":1,
        "Question_tags":"azure-devops|databricks|webhooks|azure-databricks|mlflow",
        "Question_view_count":125,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72239105",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67472983,
        "Question_title":"Can MLFlow log new metrics in a terminated run?",
        "Question_body":"<p>I would like to use MLFlow (with Python) to log time series with time interval equal to 1 day.\nMy idea would be to create a new run with a certain ID and to use function <code>log_metric<\/code> every day (say, with a cron job) with a new value. Once my run is terminated, can I &quot;reopen&quot; it and log a new metric ?\nWhat I have in mind is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Day 1\nimport mlflow\n\ntracking_uri = &quot;my_uri&quot;\nmlflow.set_tracking_uri(tracking_uri)\nxp_id = 0\nmlflow.start_run(run_name=&quot;test&quot;, experiment_id=xp_id)\nmlflow.log_metric(&quot;test_metric&quot;, 1)\nmlflow.end_run()\n<\/code><\/pre>\n<p>And the following days:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\ndef log_daily_metric(daily_value_metric):\n  tracking_uri = &quot;my_uri&quot;\n  mlflow.set_tracking_uri(tracking_uri)\n  xp_id = 0\n  mlflow.restart_run(run_name=&quot;test&quot;, experiment_id=xp_id)  # \/!\\ function mlflow.restart does not exist\n  mlflow.log_metric(&quot;test_metric&quot;, daily_value_metric)\n  mlflow.end_run()\n<\/code><\/pre>\n<p>so that run <code>&quot;test&quot;<\/code> would have new metrics logged every day.<\/p>\n<p>Any idea to achieve this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1620659322073,
        "Question_score":2,
        "Question_tags":"python-3.x|time-series|mlflow",
        "Question_view_count":274,
        "Owner_creation_time":1539181664153,
        "Owner_last_access_time":1663922812787,
        "Owner_location":"Nice, France",
        "Owner_reputation":1247,
        "Owner_up_votes":690,
        "Owner_down_votes":9,
        "Owner_views":191,
        "Question_last_edit_time":1620662360136,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67472983",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70962095,
        "Question_title":"what is the difference between Duration, TT (Training Time), RunTime on ML Performance Report of mlflow",
        "Question_body":"<p>I compared the performance of machine learning algorithms by applying pycaret and k-fold on a data and reported it on mlflow. There are three time columns in the report, these are duration, TT(training time) and runtime. When I look at these times, they are all different from each other. I know the training time, but what are the other times?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1643831868733,
        "Question_score":0,
        "Question_tags":"machine-learning|classification|mlflow|pycaret",
        "Question_view_count":54,
        "Owner_creation_time":1642027182437,
        "Owner_last_access_time":1648553902727,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1643835272060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70962095",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70792868,
        "Question_title":"Serving MLFlow artifacts through `--serve-artifacts` without passing credentials",
        "Question_body":"<p>A new version of MLFlow (1.23) provided a <code>--serve-artifacts<\/code> option (via <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/5045\" rel=\"nofollow noreferrer\">this<\/a> pull request) along with some example code. This <em>should<\/em> allow me to simplify the rollout of a server for data scientists by only needing to give them one URL for the tracking server, rather than a URI for the tracking server, URI for the artifacts server, and a username\/password for the artifacts server. At least, that's how I understand it.<\/p>\n<p>A complication that I have is that I need to use <code>podman<\/code> instead of <code>docker<\/code> for my containers (and without relying on <code>podman-compose<\/code>). I ask that you keep those requirements in mind; I'm aware that this is an odd situation.<\/p>\n<p>What I did before this update (for MLFlow 1.22) was to create a kubernetes play yaml config, and I was successfully able to issue a <code>podman play kube ...<\/code> command to start a pod and from a different machine successfully run an experiment and save artifacts after setting the appropriate four env variables. I've been struggling with getting things working with the newest version.<\/p>\n<p>I am following the <code>docker-compose<\/code> example provided <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/mlflow_artifacts\" rel=\"nofollow noreferrer\">here<\/a>. I am trying a (hopefully) simpler approach. The following is my kubernetes play file defining a pod.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: &quot;2022-01-14T19:07:15Z&quot;\n  labels:\n    app: mlflowpod\n  name: mlflowpod\nspec:\n  containers:\n  - name: minio\n    image: quay.io\/minio\/minio:latest\n    ports:\n    - containerPort: 9001\n      hostPort: 9001\n    - containerPort: 9000\n      hostPort: 9000\n    resources: {}\n    tty: true\n    volumeMounts:\n    - mountPath: \/data\n      name: minio-data\n    args:\n    - server\n    - \/data\n    - --console-address\n    - :9001\n\n  - name: mlflow-tracking\n    image: localhost\/mlflow:latest\n    ports:\n    - containerPort: 80\n      hostPort: 8090\n    resources: {}\n    tty: true\n    env:\n      - name: MLFLOW_S3_ENDPOINT_URL\n        value: http:\/\/127.0.0.1:9000\n      - name: AWS_ACCESS_KEY_ID\n        value: minioadmin\n      - name: AWS_SECRET_ACCESS_KEY\n        value: minioadmin\n    command: [&quot;mlflow&quot;]\n    args:\n      - server\n      - -p \n      - 80\n      - --host \n      - 0.0.0.0\n      - --backend-store-uri \n      - sqlite:\/\/\/root\/store.db\n      - --serve-artifacts\n      - --artifacts-destination \n      - s3:\/\/mlflow\n      - --default-artifact-root \n      - mlflow-artifacts:\/\n#      - http:\/\/127.0.0.1:80\/api\/2.0\/mlflow-artifacts\/artifacts\/experiments\n      - --gunicorn-opts \n      - &quot;--log-level debug&quot;\n    volumeMounts:\n    - mountPath: \/root\n      name: mlflow-data  \n\n  volumes:\n  - hostPath:\n      path: .\/minio\n      type: Directory\n    name: minio-data\n  - hostPath:\n      path: .\/mlflow\n      type: Directory\n    name: mlflow-data\nstatus: {}\n<\/code><\/pre>\n<p>I start this with <code>podman play kube mlflowpod.yaml<\/code>. On the same machine (or a different one, it doesn't matter), I have cloned and installed <code>mlflow<\/code> into a virtual environment. From that virtual environment, I set an environmental variable <code>MLFLOW_TRACKING_URI<\/code> to <code>&lt;name-of-server&gt;:8090<\/code>. I then run the <code>example.py<\/code> file in the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/mlflow_artifacts\" rel=\"nofollow noreferrer\"><code>mlflow_artifacts<\/code><\/a> example directory. I get the following response:<\/p>\n<pre><code>....\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>Which seems like the client needs the server credentials to minIO, which I thought the proxy was supposed to take care of.<\/p>\n<p>If I also provide the env variables<\/p>\n<pre><code>$env:MLFLOW_S3_ENDPOINT_URL=&quot;http:\/\/&lt;name-of-server&gt;:9000\/&quot; \n$env:AWS_ACCESS_KEY_ID=&quot;minioadmin&quot;\n$env:AWS_SECRET_ACCESS_KEY=&quot;minioadmin&quot;\n<\/code><\/pre>\n<p>Then things work. But that kind of defeats the purpose of the proxy...<\/p>\n<p>What is it about the proxy setup via kubernates play yaml and podman that is going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642709711853,
        "Question_score":1,
        "Question_tags":"kubernetes|mlflow|podman",
        "Question_view_count":773,
        "Owner_creation_time":1321286030420,
        "Owner_last_access_time":1642791786807,
        "Owner_location":null,
        "Owner_reputation":428,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1642710219863,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70792868",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56088195,
        "Question_title":"mlflow can't find .py file",
        "Question_body":"<p>I'm trying to learn to use <code>mlflow<\/code> by creating a very simple project and log it.<\/p>\n\n<p>I've tried following <code>mlflow<\/code>'s example and my code runs properly when running the main.py as a normal bash command.<\/p>\n\n<p>I couldn't make it run using the <code>mlflow<\/code> CLI using project and a simple file.\nI got the following error.<\/p>\n\n<pre><code>(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run src\/main.py \n2019\/05\/11 10:21:41 ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run .\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Created directory \/tmp\/tmpe26oernf for downloading remote URIs passed to arguments of type 'path' ===\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Running command 'source activate mlflow-21497056aed7961402b515847613ed9f950fa9fc &amp;&amp; python src\/main.py 1.0' in run with ID 'ed51446de4c44903ab891d09cfe10e49' === \nbash: activate: No such file or directory\n2019\/05\/11 10:40:25 ERROR mlflow.cli: === Run (ID 'ed51446de4c44903ab891d09cfe10e49') failed ===\n\n<\/code><\/pre>\n\n<p>Needless to say my main has a <code>.py<\/code> suffix.<\/p>\n\n<p>Is there anything wrong that causes this issue?<\/p>\n\n<p>My main.py is:<\/p>\n\n<pre><code>import sys\n\nimport gym\nimport mlflow\n\n\nif __name__ == '__main__':\n    env = gym.make(\"CartPole-v0\")\n    right_percent = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 1.0\n    with mlflow.start_run():\n        obs = env.reset()\n        print(env.action_space)\n        action = 1  # accelerate right\n        print(obs)\n        mlflow.log_param(\"right percent\", right_percent)\n        mlflow.log_metric(\"mean score\", 1)\n        mlflow.log_metric(\"std score\", 0)\n<\/code><\/pre>\n\n<p>conda_env.yaml<\/p>\n\n<pre><code>name: rlearning\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - numpy\n  - pandas\n  - tensorflow-gpu\n  - pip:\n      - mlflow\n      - gym\n<\/code><\/pre>\n\n<p>MLproject<\/p>\n\n<pre><code>name: reinforced learning\n\nconda_env: files\/config\/conda_environment.yaml\n\nentry_points:\n  main:\n    parameters:\n      right_percent: {type: float, default: 1.0}\n    command: \"python src\/main.py {right_percent}\"\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557559963290,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1263,
        "Owner_creation_time":1435222208620,
        "Owner_last_access_time":1662621569283,
        "Owner_location":"London, UK",
        "Owner_reputation":91,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1557560675392,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56088195",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58917918,
        "Question_title":"How to make predictions using a model that requires an input shape with more than two dimensions using MLflow?",
        "Question_body":"<p>I'm trying to implement a tensorflow (keras) based model into mlflow while learning how it works and if it suite our needs. I'm trying to implement the Fashion MNIST example from tensorflow website <a href=\"https:\/\/www.tensorflow.org\/tutorials\/keras\/classification?hl=it\" rel=\"nofollow noreferrer\">Here the link<\/a><\/p>\n\n<p>I was able to train and to log the model successfully into mlflow using this code:<\/p>\n\n<pre><code>import mlflow\nimport mlflow.tensorflow\nimport mlflow.keras\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images \/ 255.0\n\ntest_images = test_images \/ 255.0\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])\n\nif __name__ == \"__main__\":\n\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n    print('\\nTest accuracy:', test_acc)\n\n    mlflow.log_metric(\"validation accuracy\", float(test_acc))\n    mlflow.log_metric(\"validation loss\", float(test_loss))\n    mlflow.keras.log_model(model, \n                        \"model\", \n                        registered_model_name = \"Fashion MNIST\")\n<\/code><\/pre>\n\n<p>Then I'm now serving it with the models serve subcommand<\/p>\n\n<pre><code>$ mlflow models serve -m [model_path_here] -p 1234\n<\/code><\/pre>\n\n<p>The problem is that I'm not able to make predictions:<\/p>\n\n<pre><code>fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0\nlabels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nurl = \"http:\/\/127.0.0.1:1234\/invocations\"\n\nto_predict = test_images[0]\n\ndata = {\n    \"data\": [to_predict.tolist()]\n}\nheaders = {'Content-type': 'application\/json', 'Accept': 'text\/plain'}\nr = requests.post(url, data=json.dumps(data), headers=headers)\nres = r.json()\n<\/code><\/pre>\n\n<p>I'm getting this error:<\/p>\n\n<pre><code>{'error_code': 'BAD_REQUEST', 'message': 'Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.', 'stack_trace': 'Traceback (most recent call last):\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/keras.py\", line 298, in predict\\n    predicted = pd.DataFrame(self.keras_model.predict(dataframe))\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 909, in predict\\n    use_multiprocessing=use_multiprocessing)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_arrays.py\", line 715, in predict\\n    x, check_steps=True, steps_name=\\'steps\\', steps=steps)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 2472, in _standardize_user_data\\n    exception_prefix=\\'input\\')\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_utils.py\", line 564, in standardize_input_data\\n    \\'with shape \\' + str(data_shape))\\nValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (1, 28)\\n'}\n<\/code><\/pre>\n\n<p>That code above worked fine with a one dimension model<\/p>\n\n<p>The error seems to me related to the fact that a pandas DataFrame is a two dimensional data structure and the model instead requires a three dimensional input.<\/p>\n\n<p>The latest words from the error \"...but got array with shape (1, 28)\". The input shape should be (1, 28, 28) instead<\/p>\n\n<p>There is a way to use this kind of models with mlflow? There is a way to serialize and send numpy arrays directly as input instead of pandas dataframes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574090895563,
        "Question_score":3,
        "Question_tags":"python|tensorflow|keras|mlflow",
        "Question_view_count":1221,
        "Owner_creation_time":1293720910407,
        "Owner_last_access_time":1641627502447,
        "Owner_location":null,
        "Owner_reputation":503,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58917918",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64354220,
        "Question_title":"Permission denied writing artifacts to an NFS-mounted PVC",
        "Question_body":"<p>I'm attempting to write MLflow artifacts to an NFS-mounted PVC. It's a new PVC mounting at <code>\/opt\/mlflow<\/code>, but MLflow seems to have permission writing to it. The specific error I'm getting is<\/p>\n<pre><code>PermissionError: [Errno 13] Permission denied: '\/opt\/mlflow'\n<\/code><\/pre>\n<p>I ran the same deployment with an S3-backed artifact store, and that worked just fine. That was on my home computer though, and I don't have the ability to do that at work. The MLflow documentation seems to indicate that I don't need any special syntax for NFS mounts.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1602681645387,
        "Question_score":1,
        "Question_tags":"kubernetes|mlflow|kubernetes-pvc",
        "Question_view_count":432,
        "Owner_creation_time":1586788742313,
        "Owner_last_access_time":1607609300903,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1607049416232,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64354220",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62865250,
        "Question_title":"How to serve AI models in a multi-tenant environment in scale?",
        "Question_body":"<p>I have a servers cluster, each server gets real-time authentication events as requests, and returns a risk score for the incoming event, based on AI models that sits in S3.<\/p>\n<p>This cluster serves multiple customers. Each customer has its own AI model in S3.\nEach AI model file in S3 size is ~50MB in size.<\/p>\n<p><strong>The problem:<\/strong><\/p>\n<p>Let's say this cluster consists of 10 servers, and it serves 20 customers. Respectively, there are 20 AI models in S3.<\/p>\n<p>In a naive solution, each server in the cluster might end up loading all the 20 models from S3 to the server memory.\n20(servers in the cluster)*50MB(model size in S3) = 1GB.\nIt takes long time to download the model and load it to memory, and the amount of memory is limited to the memory capacity of the server.\nAnd of course - these problems get bigger with scale.<\/p>\n<p>So what are my options?\nI know that there are out of the box products for model life cycle management, such as: MlFlow, KubeFlow, ...\nDo these products have a solution to the problem I raised?<\/p>\n<p>Maybe use Redis as a cache layer?<\/p>\n<p>Maybe use Redis as a cache layer in combination with MlFlow and KubeFlow?<\/p>\n<p>Any other solution?<\/p>\n<p><strong>Limitation:<\/strong>\nI can't have sticky session between the servers in that cluster, so I can't ensure all the requests of the same customer will end up in the same server.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594579872523,
        "Question_score":0,
        "Question_tags":"artificial-intelligence|scalability|mlflow|kubeflow",
        "Question_view_count":504,
        "Owner_creation_time":1289286902413,
        "Owner_last_access_time":1663763347283,
        "Owner_location":null,
        "Owner_reputation":1806,
        "Owner_up_votes":104,
        "Owner_down_votes":3,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62865250",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68501612,
        "Question_title":"MLflow saves models to relative place instead of tracking_uri",
        "Question_body":"<p>sorry if my question is too basic, but cannot solve it.\nI am experimenting with mlflow currently and facing the following issue:<\/p>\n<p>Even if I have set the <em>tracking_uri<\/em>, the mlflow artifacts are saved to the <em>.\/mlruns\/...<\/em> folder relative to the path from where I run <code>mlfow run path\/to\/train.py<\/code> (in command line). The mlflow server searches for the artifacts following the <em>tracking_uri<\/em> (<code>mlflow server --default-artifact-root here\/comes\/the\/same\/tracking_uri<\/code>).<\/p>\n<p>Through the following example it will be clear what I mean:<\/p>\n<p>I set the following in the training script before the <code>with mlflow.start_run() as run:<\/code><\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;file:\/\/\/home\/@myUser\/@SomeFolders\/mlflow_artifact_store\/mlruns\/&quot;)\n<\/code><\/pre>\n<p>My expectation would be that mlflow saves all the artifacts to the place I gave in the registry uri. Instead, it saves the artifacts relative to place from where I run <code>mlflow run path\/to\/train.py<\/code>, i.e. running the following<\/p>\n<pre><code>\/home\/@myUser\/ mlflow run path\/to\/train.py\n<\/code><\/pre>\n<p>creates the structure:<\/p>\n<pre><code>\/home\/@myUser\/mlruns\/@experimentID\/@runID\/artifacts\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/metrics\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/params\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/tags\n<\/code><\/pre>\n<p>and therefore it doesn't find the run artifacts in the tracking_uri, giving the error message:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;train.py&quot;, line 59, in &lt;module&gt;\n    with mlflow.start_run() as run:\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 204, in start_run\n    active_run_obj = client.get_run(existing_run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/client.py&quot;, line 151, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 57, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 524, in get_run\n    run_info = self._get_run_info(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 544, in _get_run_info\n    &quot;Run '%s' not found&quot; % run_uuid, databricks_pb2.RESOURCE_DOES_NOT_EXIST\nmlflow.exceptions.MlflowException: Run '788563758ece40f283bfbf8ba80ceca8' not found\n2021\/07\/23 16:54:16 ERROR mlflow.cli: === Run (ID '788563758ece40f283bfbf8ba80ceca8') failed ===\n<\/code><\/pre>\n<p>Why is that so? How can I change the place where the artifacts are stored, this directory structure is created? I have tried <code>mlflow run --storage-dir here\/comes\/the\/path<\/code>, setting the <em>tracking_uri<\/em>, <em>registry_uri<\/em>. If I run the <code>\/home\/path\/to\/tracking\/uri mlflow run path\/to\/train.py<\/code> it works, but I need to run the scripts remotely.<\/p>\n<p>My endgoal would be to change the artifact uri to an NFS drive, but even in my local computer I cannot do the trick.<\/p>\n<p>Thanks for reading it, even more thanks if you suggest a solution! :)\nHave a great day!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627054156107,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":785,
        "Owner_creation_time":1512572031753,
        "Owner_last_access_time":1655968570680,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68501612",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59827478,
        "Question_title":"How to get current run_id inside of mlflow.start_run()?",
        "Question_body":"<p><code>mlflow.active_run()<\/code> returns nothing so I can't just use\n<code>current_rui_id = mlflow.active_run().info.run_id<\/code><\/p>\n\n<p>I have to get <strong>run_id<\/strong> inside of this construction for being able to continue logging parameters,  metrics and artifacts inside of another block but for the same model:<\/p>\n\n<pre><code>with mlflow.start_run(run_name=\"test_ololo\"):\n\n    \"\"\" \n       fitting a model here ...\n    \"\"\"\n\n    for name, val in metrics:\n        mlflow.log_metric(name, np.float(val))\n\n    # Log our parameters into mlflow\n    for k, v in params.items():\n        mlflow.log_param(key=k, value=v)\n\n    pytorch.log_model(learn.model, f'model')\n    mlflow.log_artifact('.\/outputs\/fig.jpg')\n<\/code><\/pre>\n\n<p>I have to get current <strong>run_id<\/strong> to continue training inside the same run<\/p>\n\n<pre><code>with mlflow.start_run(run_id=\"215d3a71925a4709a9b694c45012988a\"):\n\n    \"\"\"\n       fit again\n       log_metrics\n    \"\"\"\n\n    pytorch.log_model(learn.model, f'model')\n    mlflow.log_artifact('.\/outputs\/fig2.jpg')\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1579537993127,
        "Question_score":4,
        "Question_tags":"python|mlflow",
        "Question_view_count":6042,
        "Owner_creation_time":1443627469260,
        "Owner_last_access_time":1663849445573,
        "Owner_location":null,
        "Owner_reputation":863,
        "Owner_up_votes":100,
        "Owner_down_votes":2,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59827478",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67677780,
        "Question_title":"MLFlow run: Pass parameters in a file instead of key\/value pairs",
        "Question_body":"<p>Usually when running an MLProject, I would use something similar to:<\/p>\n<pre><code>mlflow run . -P alpha=0.1 -P l1_ratio=0.9\n<\/code><\/pre>\n<p>Is it possible to pass a file containing the key\/value pairs instead ? so something like:<\/p>\n<pre><code>mlflow run . --file .\/parametrs\n<\/code><\/pre>\n<p>where .\/parameters contains the key\/value pairs (like an env file or something)<\/p>\n<p>One way I thought of is to make a seperate bash script that accept the file and extracts the key\/value pairs to be included in the run command, but I wonder if there's a way more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621883888327,
        "Question_score":1,
        "Question_tags":"python|machine-learning|mlflow|mlops",
        "Question_view_count":222,
        "Owner_creation_time":1540654775053,
        "Owner_last_access_time":1663859840653,
        "Owner_location":"Tunisia",
        "Owner_reputation":606,
        "Owner_up_votes":42,
        "Owner_down_votes":8,
        "Owner_views":69,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's not supported functionality according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, and <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/cli.py#L124\" rel=\"nofollow noreferrer\">source code<\/a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621931875960,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67677780",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65992591,
        "Question_title":"send post request using curl to mlflow api to multiple records",
        "Question_body":"<p>I have served an mlflow model and am sending POST requests in this format:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; \n--data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, \n&quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, \n&quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, \n6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' \nhttp:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>It is getting scored. However for my particular project, the input to rest api for scoring will always be multiple records in dataframe\/csv format instead of a single record. Can someone point me to how to achieve this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612181841393,
        "Question_score":1,
        "Question_tags":"curl|post|deployment|mlflow|serving",
        "Question_view_count":407,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65992591",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70005957,
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637158421443,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637158766987,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637159193836,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61427012,
        "Question_title":"can I use mlflow python API to register a spark UDF & then use the UDF in Spark scala code?",
        "Question_body":"<p>I'm trying to use mlflow to do the machine learning work. I register the ML model as UDF using the following python code. The question is how can I use the UDF(test_predict) in my scala code? The reason is that our main code is in Scala. The problem is that UDF created below is a temporary UDF and SparkSession scoped. thanks!<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport mlflow\nfrom mlflow import pyfunc\nimport numpy as np\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark import SQLContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\n\nsc=SparkContext()\nspark = SparkSession.builder.appName(\"Python UDF example\").getOrCreate()\npyfunc_udf=mlflow.pyfunc.spark_udf(spark=spark, model_uri=\".\/sk\",result_type=\"float\")\nspark.udf.register(\"test_predict\",pyfunc_udf)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1587824105457,
        "Question_score":2,
        "Question_tags":"python|apache-spark|pyspark|user-defined-functions|mlflow",
        "Question_view_count":617,
        "Owner_creation_time":1553506414607,
        "Owner_last_access_time":1614866413497,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":1588065231427,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61427012",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57980954,
        "Question_title":"when using python open() on mac, getting \"[Errno 2] no such file or directory: 'file:\/\/\/absolute\/path\/' \", path generated from MLflow",
        "Question_body":"<p>I imagine this has been answered, but I could not find.<\/p>\n\n<p>I am attempting to use:<\/p>\n\n<pre><code>with open('file:\/\/\/absolute\/path\/to\/file') as fn:\n     csv-stuff\n<\/code><\/pre>\n\n<p>however, I am getting Error : <\/p>\n\n<blockquote>\n  <p>\"[Errno 2] No such file or directory: 'file:\/\/\/absolute\/path\/to\/file'\"<\/p>\n<\/blockquote>\n\n<p>The absolute file path is being generated within the following commands from os and mlflow:<\/p>\n\n<pre><code>data_uri = os.path.join(run.info.artifact_uri, \"data\")\n<\/code><\/pre>\n\n<p>where \"data\" was logged without a artifact_path specified with:<\/p>\n\n<pre><code>mlflow.log_artifact(tempfile_path,\"data\")\n<\/code><\/pre>\n\n<p>I have been stuck on this for a few days and have not figured out the issue yet. Thanks for the help!<\/p>\n\n<p>P.S. This is my first post, feel free to tell me if I am doing something wrong. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1568748928353,
        "Question_score":1,
        "Question_tags":"python-3.x|macos|mlflow|nosuchfileexception",
        "Question_view_count":30,
        "Owner_creation_time":1551311642383,
        "Owner_last_access_time":1643120523147,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1568750990320,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57980954",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60616879,
        "Question_title":"Logging Artifacts from MlFlow on GCS Bucket",
        "Question_body":"<p>I have a running MlFlow server on GCS VM instance. I have created a bucket to log the artifacts.\nThis is the command I'm running to start the server and for specifying bucket path-<\/p>\n\n<pre><code>mlflow server --default-artifact-root gs:\/\/gcs_bucket\/artifacts --host x.x.x.x\n<\/code><\/pre>\n\n<p>But facing this error:<\/p>\n\n<pre><code>TypeError: stat: path should be string, bytes, os.PathLike or integer, not ElasticNet\n<\/code><\/pre>\n\n<p>Note- The mlflow server is running fine with the specified host alone. The problem is in the way when I'm specifying the storage bucket path.\nI have given permission of storage api by using these commands:<\/p>\n\n<pre><code>gcloud auth application-default login\ngcloud auth login\n<\/code><\/pre>\n\n<p>Also, on printing the artifact URI, this is what I'm getting:<\/p>\n\n<pre><code>mlflow.get_artifact_uri()\n<\/code><\/pre>\n\n<p>Output:<\/p>\n\n<pre><code>gs:\/\/gcs_bucket\/artifacts\/0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts\n<\/code><\/pre>\n\n<p>So in the above path from where this is coming <code>0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts<\/code> and why it's not getting auto-created at <code>gs:\/\/gcs_bucket\/artifacts<\/code><\/p>\n\n<p>After debugging more, why it's not able to get the local path from VM:\n<a href=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And this error I'm getting on VM:<\/p>\n\n<pre><code>ARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\nTraceback (most recent call last):\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 197, in list_experiments\n   experiment = self._get_experiment(exp_id, view_type)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 256, in _get_experiment\n   meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/utils\/file_utils.py\", line 160, in read_yaml\n   raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\nmlflow.exceptions.MissingConfigException: Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\n<\/code><\/pre>\n\n<p>Can I get a solution to this and what I'm missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1583840420677,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|mlflow",
        "Question_view_count":2153,
        "Owner_creation_time":1451124057623,
        "Owner_last_access_time":1664026764720,
        "Owner_location":"India",
        "Owner_reputation":736,
        "Owner_up_votes":69,
        "Owner_down_votes":2,
        "Owner_views":234,
        "Question_last_edit_time":1583922371776,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616879",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56791931,
        "Question_title":"Not able to save pyspark iforest model using pyspark",
        "Question_body":"<p>Using iforest as described here : <a href=\"https:\/\/github.com\/titicaca\/spark-iforest\" rel=\"nofollow noreferrer\">https:\/\/github.com\/titicaca\/spark-iforest<\/a>\nBut model.save() is throwing exception.<\/p>\n\n<p>Followed the code snippet mentioned under \"Python API\" section on mentioned git page.<\/p>\n\n<p>from pyspark.ml.feature import VectorAssembler\nimport os\nimport tempfile\nfrom pyspark_iforest.ml.iforest import *<\/p>\n\n<p>col_1:integer\ncol_2:integer\ncol_3:integer<\/p>\n\n<p>assembler = VectorAssembler(inputCols=in_cols, outputCol=\"features\")\nfeaturized = assembler.transform(df)<\/p>\n\n<p>iforest = IForest(contamination=0.5, maxDepth=2)\nmodel=iforest.fit(df)<\/p>\n\n<p>model.save(\"model_path\")\nException:\nscala.NotImplementedError: The default jsonEncode only supports string, vector and matrix. org.apache.spark.ml.param.Param must override jsonEncode for java.lang.Double.<\/p>\n\n<p>Below is the output dataframe I'm getting after executing \"model.transform(df)\". model.save() should be able to save model-files.\ncol_1:integer\ncol_2:integer\ncol_3:integer\nfeatures:udt\nanomalyScore:double\nprediction:double<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561641462687,
        "Question_score":0,
        "Question_tags":"machine-learning|pyspark|mlflow",
        "Question_view_count":372,
        "Owner_creation_time":1433235101097,
        "Owner_last_access_time":1663928002773,
        "Owner_location":"India",
        "Owner_reputation":779,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":132,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56791931",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73368271,
        "Question_title":"Pycaret MlFlow authentication",
        "Question_body":"<p>How can I use <code>log_environment = True<\/code> in Pycaret <code>setup<\/code> with<\/p>\n<p><code> import os import mlflow mlflow.set_tracking_uri(&quot;https:\/\/dagshub.com\/BexTuychiev\/pet_pawpularity.mlflow&quot;) os.environ[&quot;MLFLOW_TRACKING_USERNAME&quot;] = &quot;MLFLOW_TRACKING_USERNAME&quot; os.environ[&quot;MLFLOW_TRACKING_PASSWORD&quot;] = &quot;MLFLOW_TRACKING_PASSWORD&quot;<\/code><\/p>\n<p>Without getting  <code>RestException: INTERNAL_ERROR: Response: {'error': 'not found'} <\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660618090423,
        "Question_score":0,
        "Question_tags":"google-colaboratory|mlflow|pycaret|dagshub",
        "Question_view_count":35,
        "Owner_creation_time":1481730456640,
        "Owner_last_access_time":1663945777720,
        "Owner_location":"Kansas",
        "Owner_reputation":675,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":1816,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368271",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73643715,
        "Question_title":"ValueError: Enum ErrorCode has no value defined for name '403' in mlflow.set_experiment()",
        "Question_body":"<p>I am trying to run some code to train a model, while logging my results to MLflow on Databricks. I keep getting the following error when I try to make a call to <code>mlflow.set_experiment()<\/code>,<\/p>\n<pre><code>    raise ValueError('Enum {} has no value defined for name {!r}'.format(\nValueError: Enum ErrorCode has no value defined for name '403'\n<\/code><\/pre>\n<p>What exactly is going on here?<\/p>\n<p>I am using Databricks Connect to run my code and the section where the error pops up looks like this,<\/p>\n<pre><code>    # set remote tracking server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n\n    # create the MLflow client\n    client = MlflowClient(remote_server_uri)\n\n    # set experiment to log mlflow runs\n    mlflow.set_experiment(experiment_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662611126990,
        "Question_score":1,
        "Question_tags":"python|mlflow|databricks-connect",
        "Question_view_count":68,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":1662625313963,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73643715",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61871515,
        "Question_title":"MLflow Artifacts Storing But Not Listing In UI",
        "Question_body":"<p>I've run into an issue using MLflow server. When I first ran the command to start an mlflow server on an ec2 instance, everything worked fine. Now, although logs and artifacts are being stored to postgres and s3, the UI is not listing the artifacts. Instead, the artifact section of the UI shows:<\/p>\n\n<pre><code>Loading Artifacts Failed\nUnable to list artifacts stored under &lt;s3-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n\n<p>But when I check in s3, I see the artifact in the s3 location that the error shows. What could possibly have started causing this as it used to work not too long ago and nothing was changed on the ec2 that is hosting mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589810994053,
        "Question_score":9,
        "Question_tags":"amazon-s3|amazon-ec2|artifact|mlflow",
        "Question_view_count":3949,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61871515",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63640567,
        "Question_title":"Error using set MLFLOW_TRACKING_URI='http:\/\/0.0.0.0:5000' for serve models",
        "Question_body":"<p>Hi i need to run a command like this<\/p>\n<pre><code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow@localhost:5433\/mlflow --default-artifact-root file:D:\/artifact_root --host 0.0.0.0 --port 5000\n<\/code><\/pre>\n<p>for start my serve and i have not problem with this but when i try to run a example\nin the route of project from github python<\/p>\n<pre><code>mlflow\/examples\/sklearn_elasticnet_diabetes\/linux\/train_diabetes.py 0.1 0.9 \n<\/code><\/pre>\n<p>i get this error<\/p>\n<pre><code>  _model_registry_store_registry.register_entrypoints()\nElasticnet model (alpha=0.100000, l1_ratio=0.900000):\n  RMSE: 71.98302888908191\n  MAE: 60.5647520017933\n  R2: 0.21655161434654602\n&lt;function get_tracking_uri at 0x0000017F3AE885E8&gt;\nurl 'http:\/\/0.0.0.0:8001'\nurl2 'http|\/\/0.0.0.0|8001'\nTraceback (most recent call last):\n  File &quot;train_diabetes.py&quot;, line 90, in &lt;module&gt;\n    mlflow.log_param(&quot;alpha&quot;, alpha)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 210, in log_param\n    run_id = _get_or_start_run().info.run_id\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 508, in _get_or_start_run\n    return start_run()\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 148, in start_run\n    active_run_obj = MlflowClient().create_run(\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\client.py&quot;, line 44, in __init__\n    self._tracking_client = TrackingServiceClient(final_tracking_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py&quot;, line 32, in __init__       \n    self.store = utils._get_store(self.tracking_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py&quot;, line 126, in _get_store     \n    return _tracking_store_registry.get_store(store_uri, artifact_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\registry.py&quot;, line 37, in get_store    \n    return builder(store_uri=store_uri, artifact_uri=artifact_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py&quot;, line 81, in _get_file_store \n    return FileStore(store_uri, store_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py&quot;, line 100, in __init__\n    self.root_directory = local_file_uri_to_path(root_directory or _default_root_dir())\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py&quot;, line 387, in local_file_uri_to_path\n    return urllib.request.url2pathname(path)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\nturl2path.py&quot;, line 35, in url2pathname\n    raise OSError(error)\nOSError: Bad URL: 'http|\/\/0.0.0.0|8001'\n<\/code><\/pre>\n<p>before running the python code i run this command to set the env tracking uri for the execution set MLFLOW_TRACKING_URI='http:\/\/0.0.0.0:5000'<\/p>\n<p>i don\u00b4t know why mlflow replace the : for | i need help. Before this option worked but now it is failing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1598646868280,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":895,
        "Owner_creation_time":1598646307000,
        "Owner_last_access_time":1603930318483,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1598647185303,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63640567",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72516775,
        "Question_title":"I expose ports in docker but it does not connect",
        "Question_body":"<p>I have a simple docker image (I post the Dockerfile at the end) and I run it with<\/p>\n<pre><code>docker run -p 8888:8888 -p 5000:5000 -v $(pwd):\/workfolder -it --rm stockpred\n<\/code><\/pre>\n<p>I am expecting to expose the ports 8888 and 5000.<\/p>\n<p>Inside the container I do:<\/p>\n<pre><code>(base) root@41131b74043f:\/workfolder# mlflow ui\n[2022-06-06 10:59:24 +0000] [26] [INFO] Starting gunicorn 20.1.0\n[2022-06-06 10:59:24 +0000] [26] [INFO] Listening at: http:\/\/127.0.0.1:5000 (26)\n[2022-06-06 10:59:24 +0000] [26] [INFO] Using worker: sync\n[2022-06-06 10:59:24 +0000] [27] [INFO] Booting worker with pid: 27\n<\/code><\/pre>\n<p>so I go and open that address in my browser but I got<\/p>\n<p>The connection was reset<\/p>\n<blockquote>\n<p>The connection to the server was reset while the page was loading.<\/p>\n<pre><code>The site could be temporarily unavailable or too busy. Try again in a few moments.\nIf you are unable to load any pages, check your computer\u2019s network connection.\nIf your computer or network is protected by a firewall or proxy, make sure that Firefox is permitted to access the Web.\n<\/code><\/pre>\n<\/blockquote>\n<p>I thought that I could open the page externally. It must be something I am missing but what am I doing wrong?<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nRUN pip install mlflow&gt;=1.18.0 \\\n    &amp;&amp; pip install numpy \\\n    &amp;&amp; pip install scipy \\\n    &amp;&amp; pip install pandas \\\n    &amp;&amp; pip install scikit-learn \\\n    &amp;&amp; pip install cloudpickle \\\n    &amp;&amp; pip install pandas_datareader==0.10.0 \\\n    &amp;&amp; pip install yfinance\n<\/code><\/pre>\n<p>EDIT:<\/p>\n<p>It worked when I did<\/p>\n<pre><code>docker run --network=&quot;host&quot; -p 8888:8888 -p 5000:5000 -v $(pwd):\/workfolder -it --rm stockpred\n<\/code><\/pre>\n<p>Notice that I did not expose the ports in the Dockerfile.<\/p>\n<p>Can someone explain me why this is working like this?<\/p>\n<p>(I also tried exposing the ports in the Dockerfile and running like originally but it didn't work)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1654513786370,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":121,
        "Owner_creation_time":1421198269333,
        "Owner_last_access_time":1664010554427,
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Question_last_edit_time":1654516610190,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72516775",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57033896,
        "Question_title":"How to use MLfLow with private git repositories?",
        "Question_body":"<p>I tested <code>MLflow<\/code> experiment when the source code is stored in public a git repository. Example command looks like this<\/p>\n\n<pre><code>mlflow run  https:\/\/github.com\/amesar\/mlflow-fun.git#examples\/hello_world \\\n --experiment-id=2019 \\\n -Palpha=100 -Prun_origin=GitRun -Plog_artifact=True\n<\/code><\/pre>\n\n<p>However, when I provide an internal (private) git repository link instead of public- MLflow redirects to login url, and then execution fails like this.<\/p>\n\n<pre><code>git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\ncmdline: git fetch -v origin\nstderr: 'fatal: unable to update url base from redirection:\nasked for: https:\/\/gitlab-master.companyname.com\/myusername\/project_name\n\/tree\/master\/models\/myclassifier\/info\/refs?service=git-upload-pack\nredirect: https:\/\/gitlab-master.company.com\/users\/sign_in'\n<\/code><\/pre>\n\n<p>Is there any way to commmunicate credentials of git account to MLflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563170204117,
        "Question_score":2,
        "Question_tags":"git|gitlab|databricks|mlflow",
        "Question_view_count":1278,
        "Owner_creation_time":1401104228227,
        "Owner_last_access_time":1663944731387,
        "Owner_location":"Santa Clara, CA, USA",
        "Owner_reputation":4031,
        "Owner_up_votes":79,
        "Owner_down_votes":6,
        "Owner_views":244,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57033896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62686181,
        "Question_title":"MLflow - How can I run python code using a REST API",
        "Question_body":"<p>I'am a newbie on MachineLearning. Just a simple question, how can I run python code using REST API?\nHere is the documentation\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/rest-api.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/rest-api.html<\/a>\nBut there are no examples for REST API.\nI just created an experiment, but I cant create python run?\nAny examples like this? &quot;This is create just a experiment&quot;\ncurl -X POST http:\/\/localhost:5000\/api\/2.0\/preview\/mlflow\/experiments\/create -d '{&quot;name&quot;:&quot;TEST&quot;}'<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1593642122807,
        "Question_score":2,
        "Question_tags":"machine-learning|artificial-intelligence|mlflow",
        "Question_view_count":494,
        "Owner_creation_time":1593641319837,
        "Owner_last_access_time":1663184903993,
        "Owner_location":"Turkey",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1593642521280,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62686181",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56604989,
        "Question_title":"How to install mlflow using pip install",
        "Question_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1560545573400,
        "Question_score":1,
        "Question_tags":"pip|mlflow",
        "Question_view_count":365,
        "Owner_creation_time":1355343131933,
        "Owner_last_access_time":1649125560750,
        "Owner_location":null,
        "Owner_reputation":5655,
        "Owner_up_votes":73,
        "Owner_down_votes":3,
        "Owner_views":629,
        "Question_last_edit_time":1560799785056,
        "Answer_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1560545869140,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65744496,
        "Question_title":"MLFlow Model Registry ENDPOINT_NOT_FOUND: No API found for ERROR",
        "Question_body":"<p>I'm currently using MLFlow in Azure Databricks and trying to load a model from the Model Registry. Currently referencing the version, but will want to reference the stage 'Production' (I get the same error when referencing the stage as well)<\/p>\n<p>I keep encountering an error:<\/p>\n<pre><code>ENDPOINT_NOT_FOUND: No API found for 'POST \/mlflow\/model-versions\/get-download-uri'\n<\/code><\/pre>\n<p>My artifacts are stored in the dbfs filestore.<\/p>\n<p>I have not been able to identify why this is happening.<\/p>\n<p>Code:<\/p>\n<pre><code>from mlflow.tracking.client import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n\nimport mlflow.pyfunc\n\nmodel_name = &quot;model_name&quot;\n\nmodel_version_uri = &quot;models:\/{model_name}\/4&quot;.format(model_name=model_name)\n\nprint(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_version_uri))\nmodel_version_4 = mlflow.pyfunc.load_model(model_version_uri)\n\nmodel_production_uri = &quot;models:\/{model_name}\/production&quot;.format(model_name=model_name)\n\nprint(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_production_uri))\nmodel_production = mlflow.pyfunc.load_model(model_production_uri)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1610750609033,
        "Question_score":1,
        "Question_tags":"azure|databricks|mlflow",
        "Question_view_count":387,
        "Owner_creation_time":1581292138330,
        "Owner_last_access_time":1614304868413,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1610964132683,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65744496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70383800,
        "Question_title":"How can I run Tensorboard with MLFlow's logs?",
        "Question_body":"<p>I use MLFlow with autolog to keep track of my Tensorflow models:<\/p>\n<pre><code>mlflow.tensorflow.autolog(every_n_iter=1)\nwith mlflow.start_run():\n  model = ...\n  model.compile(...)\n  model.fit(...)\n<\/code><\/pre>\n<p>and then I want to use my tensorboard logs located in the artifacts.\nBut when I run:<\/p>\n<pre><code>%tensorboard --logdir=&lt;logs_path&gt;\n<\/code><\/pre>\n<p>I have the error message:\n&quot;No dashboards are active for the current data set.\nProbable causes:<\/p>\n<p>You haven\u2019t written any data to your event files.\nTensorBoard can\u2019t find your event files.&quot;<\/p>\n<p>I work on Databricks, so log_path is something like:<\/p>\n<pre><code>\/dbfs\/databricks\/mlflow-tracking\/..\n<\/code><\/pre>\n<p>Any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1639678545067,
        "Question_score":1,
        "Question_tags":"tensorflow|databricks|tensorboard|mlflow",
        "Question_view_count":501,
        "Owner_creation_time":1639677790923,
        "Owner_last_access_time":1663854761243,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70383800",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70296350,
        "Question_title":"Mlflow log_model, not able to predict with spark_udf but with python works",
        "Question_body":"<p>I was wondering to log a model on mlflow, once I do it, I'm able to predict probabilities with python loaded model but not with spark_udf. The thing is, I still need to have a preprocessing function inside the model. Here is a toy reproductible example for you to see when it fails:<\/p>\n<pre><code>import mlflow\nfrom mlflow.models.signature import infer_signature\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_classes=2, shuffle=True, random_state=1995)\nX, y = pd.DataFrame(X), pd.DataFrame(y,columns=[&quot;target&quot;])\n# geerate column names\nX.columns = [f&quot;col_{idx}&quot; for idx in range(len(X.columns))]\nX[&quot;categorical_column&quot;] = np.random.choice([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], size=len(X) )\n\n\ndef encode_catcolumn(X):\n  X = X.copy()\n  # replace cat values [a,b,c] for [-10,0,35] respectively\n  X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) \n  return X\n\n# with catcolumn encoded; i need to use custom encoding , we'll do this within mlflow later\nX_encoded = encode_catcolumn(X)\n\n<\/code><\/pre>\n<p>Now let's create a wrapper for the model to encode the function within the model. Please see that the function encode_catcolumn within the class and the one outside the class presented before are the same.<\/p>\n<pre><code>class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n  def encode_catcolumn(self,X):\n    X = X.copy()\n    # replace cat values [a,b,c] for [-10,0,35] respectively\n    X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) \n    return X \n  def predict(self, context, model_input):\n    # encode catvariable\n    model_input = self.encode_catcolumn(model_input)\n    # predict probabilities\n    predictions = self.model.predict_proba(model_input)[:,1]\n    return predictions\n<\/code><\/pre>\n<p>Now let's log the model<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;reproductible_example&quot;) as run:\n  clf = RandomForestClassifier()\n  clf.fit(X_encoded,y)\n  # wrappmodel with pyfunc, does the encoding inside the class \n  wrappedModel = SklearnModelWrapper(clf)\n  # When the model is deployed, this signature will be used to validate inputs.\n  mlflow.pyfunc.log_model(&quot;reproductible_example_model&quot;, python_model=wrappedModel)\n\nmodel_uuid = run.info.run_uuid\nmodel_path = f'runs:\/{model_uuid}\/reproductible_example_model'\n<\/code><\/pre>\n<p>Do the inference without spark and works perfectly:<\/p>\n<pre><code>model_uuid = run.info.run_uuid\nmodel_path = f'runs:\/{model_uuid}\/reproductible_example_model'\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(model_path)\n# predictions without spark , encodes the variables INSIDE; this WORKS\nloaded_model.predict(X)\n<\/code><\/pre>\n<p>Now do the inference with spark_udf and get an error:<\/p>\n<pre><code># create spark dataframe to test it on spark\nX_spark = spark.createDataFrame(X)\n# Load model as a Spark UDF.\nloaded_model_spark = mlflow.pyfunc.spark_udf(spark, model_uri=model_path)\n\n# Predict on a Spark DataFrame.\ncolumns = list(X_spark.columns)\n# this does not work\nX_spark.withColumn('predictions', loaded_model_spark(*columns)).collect()\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>PythonException: An exception was thrown from a UDF: 'KeyError: 'categorical_column'', from &lt;command-908038&gt;, line 7. Full traceback below:\n<\/code><\/pre>\n<p>I need to some how encode the variables and preprocess within the class. Is there any solution to this or any workaround to make this code able to woork with spark?\nWhat I've tried so far:<\/p>\n<ol>\n<li>Incorporate the encode_catcolumn within a sklearn Pipeline (with a custom encoder sklearn) -&gt; Fails;<\/li>\n<li>Create a function within the sklearn wrapper class (this example) -&gt; Fails\n3 ) Use the log_model and then create a pandas_udf in order to do it with spark as well --&gt; works but that's not what I want. I would like to be able to run the model on spark with just calling .predict() method or something like that.<\/li>\n<li>When a remove the preprocessing function and do it outside the class --&gt;  this actually works but this is not what<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639081909030,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|scikit-learn|mlflow|mlops",
        "Question_view_count":730,
        "Owner_creation_time":1588098542797,
        "Owner_last_access_time":1664026048063,
        "Owner_location":"Buenos Aires, Argentina",
        "Owner_reputation":391,
        "Owner_up_votes":44,
        "Owner_down_votes":2,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70296350",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71531665,
        "Question_title":"How to get `run_id` when using MLflow Project",
        "Question_body":"<p>When using MLflow Projects (via an <code>MLproject<\/code> file) I get this message at starting time:<\/p>\n<pre><code>INFO mlflow.projects.backend.local: \n=== Running command 'source \/anaconda3\/bin\/..\/etc\/profile.d\/conda.sh &amp;&amp; \nconda activate mlflow-4736797b8261ec1b3ab764c5060cae268b4c8ffa 1&gt;&amp;2 &amp;&amp; \npython3 main.py' in run with ID 'e2f0e8c670114c5887963cd6a1ac30f9' === \n<\/code><\/pre>\n<p>I want to access the <code>run_id<\/code> shown above (<strong>e2f0e8c670114c5887963cd6a1ac30f9<\/strong>) from inside the main script.<\/p>\n<p>I expected a run to be active but:<\/p>\n<pre><code>mlflow.active_run()\n&gt; None\n<\/code><\/pre>\n<p>Initiating a run inside the main script does give me access the correct <code>run_id<\/code>, although any subsequent runs will have a different <code>run_id<\/code>.<\/p>\n<pre><code># first run inside the script - correct run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; e2f0e8c670114c5887963cd6a1ac30f9\n\n# second run inside the script - wrong run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; 417065241f1946b98a4abfdd920239b1\n<\/code><\/pre>\n<p>Seems like a strange behavior, and I was wondering if there's another way to access the <code>run_id<\/code> assigned at the beginning of the <code>MLproject<\/code> run?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647628142007,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":143,
        "Owner_creation_time":1527686643970,
        "Owner_last_access_time":1663364883530,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71531665",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73808171,
        "Question_title":"What is the difference between deploying models in MLflow and Sagemaker?",
        "Question_body":"<p>I could do\n<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda<\/code><\/p>\n<p>and<\/p>\n<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234<\/code><\/p>\n<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?<\/p>\n<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663806620887,
        "Question_score":0,
        "Question_tags":"rest|deployment|amazon-sagemaker|endpoint|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1487831504143,
        "Owner_last_access_time":1664005735500,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73808171",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72365934,
        "Question_title":"Running mlflow ui in AWS Sagemaker",
        "Question_body":"<p>I want to run mlflow UI in sagemaker but it simply does not work, When it outputs the http address going to it results in a &quot;this site cannot be reached&quot;<\/p>\n<p>Here is the code:<\/p>\n<pre><code>def mlflow_test(server_uri, experiment_name):\n    mlflow.set_tracking_uri(server_uri)\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run():\n        params = {\n            &quot;n-estimators&quot;: 100,\n            &quot;min-samples-leaf&quot;: 10,\n            &quot;features&quot;: 'feature_test'\n        }\n        mlflow.log_params(params)\n        mlflow.log_metric('foo', 5)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>running that code will return:<\/p>\n<pre><code>[2022-05-24 15:48:44 +0000] [27820] [INFO] Starting gunicorn 20.1.0\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Listening at: http:\/\/127.0.0.1:5000 (27820)\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Using worker: sync\n[2022-05-24 15:48:44 +0000] [27823] [INFO] Booting worker with pid: 27823\n<\/code><\/pre>\n<p>Going to the <a href=\"http:\/\/127.0.0.1:5000\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000<\/a> link won't work. Anyone know how to get mlflow ui running in sagemaker? There's not much info on this that's at an easy to understand level. I just want to log my metrics and params in sagemaker and view them using the mlflow ui<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653407649377,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1615994492347,
        "Owner_last_access_time":1657796021117,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72365934",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62754017,
        "Question_title":"Save customized function inside function in MLFlow log_model",
        "Question_body":"<p>I would like to do something with MLFlow but I do not find any solution on Internet. I am working with MLFlow and R, and I want to save a regression model. The thing is that by the time I want to predict the testing data, I want to do some transformation of that data. Then I have:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>data &lt;- #some data with numeric regressors and dependent variable called 'y'\n\n# Divide into train and test\nind &lt;- sample(nrow(data), 0.8*nrow(data), replace = FALSE)\ndataTrain &lt;- data[ind,]\ndataTest &lt;- data[-ind,]\n\n# Run model in the mlflow framework\nwith(mlflow_start_run(), {\n   model &lt;- lm(y ~ ., data = dataTrain)\n   \n   predict_fun &lt;- function(model, data_to_predict){\n       data_to_predict[,3] &lt;- data_to_predict[,3]\/2\n       data_to_predict[,4] &lt;- data_to_predict[,4] + 1\n\n       return(predict(model, data_to_predict))\n       }\n\n   predictor &lt;- crate(~predict_fun(model,dataTest),model)\n\n   ### Some code to use the predictor to get the predictions and measure the accuracy as a log_metric\n   ##################\n   ##################\n   ##################\n\n   mlflow_log_model(predictor,'model')\n}\n<\/code><\/pre>\n<p>As you can notice, my prediction function not only consists in predict the new data you are evaluating, but it also makes some transformations in the third and fourth columns. All examples I saw on the web use the function predict in the <em>crate<\/em> as the default function of R.<\/p>\n<p>Once I save this model, when I run it in another notebook with some Test data, I get the error: <em>&quot;predict_fun&quot; doesn't exist<\/em>. That is because my algorithm has not saved this specific function. Do you know what can I do to save and specific prediction function that I have created instead of the default functions that are in R?<\/p>\n<p>This is not the real example I am working with, but it is an approximation of it. The fact is that I want to save extra functions apart from the model itself.<\/p>\n<p>Thank you very much!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1594030946863,
        "Question_score":1,
        "Question_tags":"r|predict|mlflow",
        "Question_view_count":140,
        "Owner_creation_time":1568015757070,
        "Owner_last_access_time":1642423173287,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62754017",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72835208,
        "Question_title":"Can't access the blob folder but files inside it are able to download",
        "Question_body":"<p>I have azure storage where I am using containers to store blobs. I am trying to download the blob from this container. But either using python SDK or rest, I am getting error &quot;The specified blob does not exist.&quot; but when I giving the full path with the final file such as .txt or whatever instead of root folder, it is able to download it.<\/p>\n<p>For example:\nfollowing URL gives error <strong><a href=\"https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\" rel=\"nofollow noreferrer\">https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models<\/a><\/strong> The specified blob does not exist.<\/p>\n<p>but the URL <strong><a href=\"https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/conda.yaml\" rel=\"nofollow noreferrer\">https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/conda.yaml<\/a><\/strong> able to download the file.<\/p>\n<p>Same thing happens with the python SDK. But I want to download the whole folder rather than the files inside it.<\/p>\n<p>How can I achieve it.<\/p>\n<p>Below is the code I am using to access the blob using pytohn SDK<\/p>\n<pre><code>from azure.storage.blob import BlobServiceClient\n\nSTORAGEACCOUNTURL = &quot;https:\/\/mlflowsmodeltorage.blob.core.windows.net&quot;\nSTORAGEACCOUNTKEY = &quot;xxxxxxxxxxxxxx&quot;\nCONTAINERNAME = &quot;mlflow-test&quot;\nBLOBNAME = &quot;110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/&quot;\n\nblob_service_client_instance = BlobServiceClient(\n    account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY,\n\n)\n\nblob_client_instance = blob_service_client_instance.get_blob_client(\n    CONTAINERNAME, BLOBNAME, snapshot=None)\n\nblob_data = blob_client_instance.download_blob()\ndata = blob_data.readall()\nprint(data)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656715374267,
        "Question_score":0,
        "Question_tags":"azure-blob-storage|azure-storage|mlflow|azure-storage-account",
        "Question_view_count":141,
        "Owner_creation_time":1626523396117,
        "Owner_last_access_time":1664042694257,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1656886891087,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72835208",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62296590,
        "Question_title":"MLFlow and Hydra causing crash when used together",
        "Question_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591767515617,
        "Question_score":4,
        "Question_tags":"python|docker|exception|mlflow|fb-hydra",
        "Question_view_count":718,
        "Owner_creation_time":1583072555673,
        "Owner_last_access_time":1663970229957,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1591773861796,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1591782103620,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72569496,
        "Question_title":"In mlflow, is it possible to track an artifact without having mlflow store it separately?",
        "Question_body":"<p>I am new to mlflow. I am trying to track\/log some artifacts (a directory of images output by my model) such that they are affiliated with the run that generated them, and so that I can view them in the mlflow UI along with all the other tracked information.<\/p>\n<p>This directory of images is generated in a custom folder path location (with a unique identifier for each run). My goal is to point mlflow to this directory so that it can recognize that these images are artifacts to track.<\/p>\n<p>Is this possible? From my understanding, the mlflow.log_artifact() function will simply create a duplicate of this image and store it within mlflow's default artifact path (ie, something like <em>mydrive1\/mlflow\/0\/\/artifacts\/<\/em>). I do not want to create a duplicate; I want to keep these images where I originally saved them.<\/p>\n<p>Example of file tree:<br \/>\nmydrive1\/<br \/>\n--\/train.py<br \/>\n--\/mlflow\/<br \/>\n----\/0\/<br \/>\n------\/meta.yaml<br \/>\n------\/[random char sequence]<br \/>\n--------\/artifacts\/<br \/>\n--------\/meta.yaml<\/p>\n<p>mydrive2\/<br \/>\n--\/output\/<br \/>\n----\/my_experiment0\/<br \/>\n------\/images\/<br \/>\n--------\/image1.png<br \/>\n--------\/image2.png<\/p>\n<p>I have found that if I manually edit the <em>artifact_uri<\/em> variable (in the meta.yaml file of the relevant run) to point to the relevant directory of images (ie, <em>mydrive2\/my_experiment0\/images\/<\/em>), all those images will show up in the artifact viewer in the mlflow UI. Is there a way to edit the <em>artifact_uri<\/em> variable via the mlflow API (or some other principled way)?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654839245583,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":164,
        "Owner_creation_time":1560804109483,
        "Owner_last_access_time":1664065617013,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72569496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69227917,
        "Question_title":"Connect MLflow server to minio in local",
        "Question_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1631902626293,
        "Question_score":1,
        "Question_tags":"python|minio|mlflow",
        "Question_view_count":1136,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1634743751772,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69479488,
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633602279323,
        "Question_score":2,
        "Question_tags":"python|yaml|mlflow",
        "Question_view_count":418,
        "Owner_creation_time":1583491811220,
        "Owner_last_access_time":1663774319043,
        "Owner_location":"Baku, Azerbaijan",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633946464143,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72454747,
        "Question_title":"Problem when loading a xgboost model from mlflow registry",
        "Question_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654036554297,
        "Question_score":1,
        "Question_tags":"python|xgboost|mlflow",
        "Question_view_count":163,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655934198727,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68753631,
        "Question_title":"Combine MLflow projects with docker-compose",
        "Question_body":"<p>I face the following situation:<\/p>\n<p>We train our models within docker container, which is build by running a docker-compose file. I have implemented MLflow to work with docker-compose (by doing something similar to e.g. this post: <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039<\/a>), creating two more containers (one for the server and one for the postgresql backend).<\/p>\n<p>However, the story doesn't end here. Our goal is to implement a full ML pipeline, which includes data creation, preprocessing steps and so on. I know, that ML projects is something which helps to create such pipeline. I have seen that it is designed to work with docker images (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/projects.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/projects.html<\/a>), but I don't get it, how one could use it with docker-compose.<\/p>\n<p>Could you help me in that by giving any tipps, guidelines, documentations, etc?<\/p>\n<p>Or in general, any advice, how a full machine learning pipeline could be implemented using mlflow?<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1628755106943,
        "Question_score":1,
        "Question_tags":"machine-learning|docker-compose|pipeline|mlflow",
        "Question_view_count":1007,
        "Owner_creation_time":1512572031753,
        "Owner_last_access_time":1655968570680,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1628775110070,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68753631",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69689266,
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635000756820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":1047,
        "Owner_creation_time":1244984040077,
        "Owner_last_access_time":1663968051750,
        "Owner_location":"New York, NY",
        "Owner_reputation":13408,
        "Owner_up_votes":306,
        "Owner_down_votes":12,
        "Owner_views":687,
        "Question_last_edit_time":1635005765867,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635012053310,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663958792436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689266",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70030903,
        "Question_title":"Error while loading MLFlow model in python 3.7",
        "Question_body":"<p>I am saving the MLFlow model using databricks. Below are the details:<\/p>\n<pre><code>artifact_path: model\ndatabricks_runtime: 8.4.x-gpu-ml-scala2.12\nflavors:\n  python_function:\n    data: data\n    env: conda.yaml\n    loader_module: mlflow.pytorch\n    pickle_module_name: mlflow.pytorch.pickle_module\n    python_version: 3.8.8\n  pytorch:\n    model_data: data\n    pytorch_version: 1.9.0+cu102\n<\/code><\/pre>\n<p><strong>I am not able to locally load the model using Python 3.7<\/strong>, whereas it works well with Python 3.9.<\/p>\n<p>Any idea what could be the possible resolution to this?<\/p>\n<p>Error Trace:<\/p>\n<pre><code>  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 714, in load_model\n    return _load_model(path=torch_model_artifacts_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 626, in _load_model\n    return torch.load(model_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 607, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 882, in _load\n    result = unpickler.load()\nTypeError: code() takes at most 15 arguments (16 given)\n<\/code><\/pre>\n<p>I have tried specifying the pickle module name explicitly as <code>mlflow.pytorch.load_model(ML_MODEL_PATH,pickle_module=mlflow.pytorch.pickle_module)<\/code><\/p>\n<p>But still getting the same error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1637304792843,
        "Question_score":0,
        "Question_tags":"python-3.x|pickle|torch|mlflow",
        "Question_view_count":477,
        "Owner_creation_time":1637303950900,
        "Owner_last_access_time":1663585014760,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70030903",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69738859,
        "Question_title":"Multiple values for a single parameter in the mlflow run command",
        "Question_body":"<p>I just started learning mlflow and wanted to know how to pass multiple values to  each parameter in the mlflow run command.<\/p>\n<p>The objective is to pass a dictionary to GridSearchCV as a param_grid to perform cross validation.<\/p>\n<p>In my main code, I retrieve the command line parameters using argparse. And by adding nargs='+' in the add_argument(), I can write spaced values for each hyper parameter and then applying vars() to create the dictionary. See code below:<\/p>\n<pre><code>import argparse\n\n# Build the parameters for the command-line\nparam_names = list(RandomForestClassifier().get_params().keys())\n\n# Param types in the same order they appear in param_names by using get_params()\nparam_types = [bool, float, dict, str, int, float, int, float, float, float,\n               float, float, float, int, int, bool, int, int, bool]\n\n# Allow for only optional command-line arguments\nparser = argparse.ArgumentParser()\ngrid_group = parser.add_argument_group('param_grid_group')\nfor i, p in enumerate(param_names):\n    grid_group.add_argument(f'--{p}', type=param_types[i], nargs='+')\n#Create a param_grid to be passed to GridSearchCV\nparam_grid_unprocessed = vars(parser.parse_args())\n<\/code><\/pre>\n<p>This works well with the classic python command :<\/p>\n<pre><code>python my_code.py --max_depth 2 3 4 --n_estimators 400 600 1000\n<\/code><\/pre>\n<p>As I said, here I can just write spaced values for each hyper-parameter and the code above does the magic by grouping the values inside a list and returning the dictionary below that I can then pass to GridSearchCV :<\/p>\n<pre><code>{'max_depth':[2, 3, 4], 'n_estimators':[400, 600, 1000]}\n<\/code><\/pre>\n<p>However with the mlflow run command, I can't get it right so far as it only accepts one value for each parameter. Here's my MLproject file :<\/p>\n<pre><code>name: mlflow_project\n\nconda_env: conda.yml\n\nentry_points:\n\n  main:\n    parameters:\n      max_depth: int\n      n_estimators: int\n    command: &quot;python my_code.py --max_depth {max_depth} --n_estimators {n_estimators}&quot;\n<\/code><\/pre>\n<p>So this works :<\/p>\n<pre><code>mlflow run . -P max_depth=2 -P n_estimators=400\n<\/code><\/pre>\n<p>But not this :<\/p>\n<pre><code> mlflow run . -P max_depth=[2, 3, 4] -P n_estimators=[400, 600, 1000]\n<\/code><\/pre>\n<p>In the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, it seems that it's impossible to do it. So, is there is any hack to overcome this problem ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635338354137,
        "Question_score":2,
        "Question_tags":"python|gridsearchcv|mlflow",
        "Question_view_count":356,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69738859",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71568975,
        "Question_title":"Custom MLFlow scoring_server for model serving",
        "Question_body":"<p>I would like to know if MLflow currently does support any kind of customization of it's scoring_serving that would allow the ability to register new endpoints to the published Rest API.<\/p>\n<p>By default the scoring server provides \/ping and \/invocations endpoint, but i would like to include more endpoints in addition to those.<\/p>\n<p>I've seen some resources that allow that kind of behaviour using custom WSGI implementations but i would like to know if extension of the provided mlflow scoring_server is possible in any way, so the default supporty provided by mlflow generated docker images and the deployment management is not lost.<\/p>\n<p>I explored existing official and unofficial documentation, and explored existing github issues and the mlflow codebase in it's github repository.<\/p>\n<p>Also i've explored some alternatives such as using custom WSGI server configuration for starting the Rest API.<\/p>\n<p>Any kind of resource\/documentation is greatly appreciated.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1647937847000,
        "Question_score":2,
        "Question_tags":"rest|mlflow|serving|mlops",
        "Question_view_count":96,
        "Owner_creation_time":1370003358430,
        "Owner_last_access_time":1655743101593,
        "Owner_location":"A Coru\u00f1a",
        "Owner_reputation":303,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71568975",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65887231,
        "Question_title":"Use mlflow to serve a custom python model for scoring",
        "Question_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611586824463,
        "Question_score":4,
        "Question_tags":"python|deployment|mlflow|mlops",
        "Question_view_count":3026,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1611592914947,
        "Answer_score":9.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1634187940523,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73386272,
        "Question_title":"How to log metrics to Azure ML Metrics Tab",
        "Question_body":"<p>I have the following train.py file<\/p>\n<pre><code>import argparse\nimport os\nimport numpy as np\nimport glob\n# import joblib\nimport mlflow\nimport logging\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.dataset import Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\n\n# let user feed in 2 parameters, the dataset to mount or download,\n# and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;\n)\nargs = parser.parse_args()\n\ntablename = args.tablename\n\n\nsubscription_id = ''\nresource_group = 'mlplayground'\nworkspace_name = 'mlplayground'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name=tablename)\ndata = dataset.to_pandas_dataframe()\n\n# use mlflow autologging\nmlflow.autolog()\n\ndata.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)\none_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) \none_hot_city = pd.get_dummies(data.city_name, prefix='city')\n\n#removing categorical features \ndata.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  \n\n#Merging one hot encoded features with our dataset 'data' \ndata=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) \n\ndata['pricepersqm'] = data.price \/ data.house_area\n\nx=data.drop('price',axis=1) \ny=data.price \n\nX_df = DataFrame(x, columns= data.columns)\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)\n\n#Converting the data into proper LGB Dataset Format\nd_train=lgb.Dataset(X_train, label=y_train)\n\n\n#Declaring the parameters\nparams = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 10,\n    'learning_rate': 0.01,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}\n\nprint(&quot;Train a LightGBM Regression model&quot;)\nclf=lgb.train(params,d_train,1000)\n\n#model prediction on X_test\nprint(&quot;Predict the test set&quot;)\ny_pred=clf.predict(X_test)\n\n#using RMSE error metric\nmse =mean_squared_error(y_pred,y_test)\nprint(&quot;RMSE: &quot;, mse**0.5)\nmlflow.log_metric(&quot;RMSE&quot;, mse**0.5)\n<\/code><\/pre>\n<p>And then from a notebook file I use the following:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core import Experiment\n\n# connect to your workspace\nws = Workspace.from_config()\n\nexperiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;\nexp = Experiment(workspace=ws, name=experiment_name)\n\n\n\nfrom azureml.core.environment import Environment\n\n# use a curated environment that has already been built for you\n\nenv = Environment.get(workspace=ws, \n                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, \n                      version=1)\n\nfrom azureml.core import ScriptRunConfig\n\nargs = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;&quot;,\n    script=&quot;train.py&quot;,\n    arguments=args,\n    compute_target=&quot;local&quot;,\n    environment=env,\n)\n\nrun = exp.submit(config=src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.<\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660729490553,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|mlflow",
        "Question_view_count":40,
        "Owner_creation_time":1302030303093,
        "Owner_last_access_time":1663332147473,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73386272",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71001833,
        "Question_title":"Accessing Delta Lake Table in Databricks via Spark in MLflow project",
        "Question_body":"<p>I am currently accessing deltalake table from databricks notebook using spark. However now I need to access delta tables from MLflow project. MLflow spark api only allows logging and loading of SparkML models. Any idea on how can I accomplish this?<\/p>\n<p>Currently I am trying to access spark via this code in MLflow project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nspark = pyspark.sql.SparkSession._instantiatedSession\nif spark is None:\n  # NB: If there is no existing Spark context, create a new local one.\n  # NB: We're disabling caching on the new context since we do not need it and we want to\n  # avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker\n  # (e.g. as part of spark_udf).\n  spark = ( pyspark.sql.SparkSession.builder \\\n   .config(&quot;spark.python.worker.reuse&quot;, True)\n   .config(&quot;spark.databricks.io.cache.enabled&quot;, False)\n   # In Spark 3.1 and above, we need to set this conf explicitly to enable creating\n   # a SparkSession on the workers\n   .config(&quot;spark.executor.allowSparkContext&quot;, &quot;true&quot;)\n   .master(&quot;local[*]&quot;)\n   .appName(&quot;MLflow Project&quot;)\n   .getOrCreate()\n  )\n<\/code><\/pre>\n<p>But I am getting this error:<\/p>\n<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1644092503000,
        "Question_score":1,
        "Question_tags":"apache-spark|pyspark|databricks|delta-lake|mlflow",
        "Question_view_count":282,
        "Owner_creation_time":1477235384120,
        "Owner_last_access_time":1663896021600,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1644169913796,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71001833",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72425907,
        "Question_title":"How to log a tensorflow model with mlflow.tensorflow.log_model (error module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants')",
        "Question_body":"<p>I am trying to log a trained model with MLFlow using mlflow.tensorflow.log_model.<\/p>\n<p>After training a simple sequential tf model<\/p>\n<pre><code>history = binary_model.fit(train_ds, validation_data=val_ds, epochs=num_epochs)\n<\/code><\/pre>\n<p>I am trying to log it:<\/p>\n<pre><code>    from tensorflow.python.saved_model import signature_constants\n    tag=[tf.saved_model.tag_constants.SERVING]\n    key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n                                tf_meta_graph_tags=tag,\n                                tf_signature_def_key=key,\n                                artifact_path=&quot;tf-models&quot;,\n                                registered_model_name=model_name)\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>    AttributeError                            Traceback (most recent call last)\n    \/var\/folders\/2k\/g7p7j2gx6v54vkwv3v401h2m0000gn\/T\/ipykernel_73638\/562549064.py in &lt;module&gt;\n          1 from tensorflow.python.saved_model import signature_constants\n    ----&gt; 2 tag=[tf.saved_model.tag_constants.SERVING]\n          3 key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n          4 \n          5 mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n\n    AttributeError: module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'\n<\/code><\/pre>\n<p>Any idea how to get the tags and keys correctly from the model to log it in MLFlow?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653845789630,
        "Question_score":1,
        "Question_tags":"python|tensorflow|mlflow",
        "Question_view_count":319,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <code>tag_constants<\/code> is in <code>tf.compat.v1.saved_model<\/code>.<\/p>\n<p>To resolve the error replace this line<\/p>\n<pre><code>tag=[tf.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>with this<\/p>\n<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>Please refer <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/saved_model\/tag_constants\" rel=\"nofollow noreferrer\">this<\/a> for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655094569432,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72425907",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62271624,
        "Question_title":"Access databricks secrets in pyspark\/python job",
        "Question_body":"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark\/python jobs, especially if they are run using mlflow.<\/p>\n\n<p>I have already tried <a href=\"https:\/\/stackoverflow.com\/questions\/51885332\/how-to-load-databricks-package-dbutils-in-pyspark?rq=1\">How to load databricks package dbutils in pyspark<\/a><\/p>\n\n<p>which does not work for remote jobs or mlflow project runs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591652446997,
        "Question_score":1,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":1369,
        "Owner_creation_time":1497525776727,
        "Owner_last_access_time":1615138462217,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":481,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62271624",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62720044,
        "Question_title":"No Artifacts Recorded MLFlow",
        "Question_body":"<p>I am unable to store, view, and retrieve the artifacts in MLFlow. The artifact folder is empty irrespective of creating a new experiment and assign proper experiment name and location.<\/p>\n<p>Server: mlflow server --backend-store-uri mlruns\/ --default-artifact-root mlruns\/ --host 0.0.0.0 --port 5000<\/p>\n<p>Create an Experiment: mlflow.create_experiment(exp_name, artifact_location='mlruns\/')<\/p>\n<pre><code>with mlflow.start_run():\n    mlflow.log_metric(&quot;mse&quot;, float(binary))\n    mlflow.log_artifact(data_path, &quot;data&quot;)\n    # log model\n    mlflow.keras.log_model(model, &quot;models&quot;)\n<\/code><\/pre>\n<p>The code compiles and runs but does not have any artifacts recorded. It has mlflow.log-model.history file but not the model.h5<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1593796777153,
        "Question_score":4,
        "Question_tags":"python-3.x|machine-learning|keras|mlflow",
        "Question_view_count":2676,
        "Owner_creation_time":1536570133123,
        "Owner_last_access_time":1601678349240,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":89,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62720044",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":54773795,
        "Question_title":"Accessing MLFlow UI with a folder name different than mlruns",
        "Question_body":"<p>I set the <code>tracking_uri<\/code> to a folder name different than <code>mlruns<\/code>. <\/p>\n\n<p>Is there a way I can open the <strong>MLFlow UI<\/strong> pointing to the new folder name for mlruns? <\/p>\n\n<p>I know I can rename the folder back to <code>mlruns<\/code>, which gets me access to all of my metrics and parameters for each experiment, but the artifacts are not accessible, since they were logged to a different folder name than mlruns. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1550605267453,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":1521,
        "Owner_creation_time":1550605103667,
        "Owner_last_access_time":1575604038253,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1550609420312,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54773795",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60641337,
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583947052940,
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_time":1539211301843,
        "Owner_last_access_time":1663982305137,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1584403666972,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1584554585176,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1624202175903,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69703225,
        "Question_title":"Can I change the port of my MLflow tracking server?",
        "Question_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635139632427,
        "Question_score":1,
        "Question_tags":"http|port|mlflow",
        "Question_view_count":540,
        "Owner_creation_time":1561106497313,
        "Owner_last_access_time":1661059074150,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635152056288,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703225",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56647549,
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|mlflow",
        "Question_view_count":1840,
        "Owner_creation_time":1504001058090,
        "Owner_last_access_time":1630952509900,
        "Owner_location":null,
        "Owner_reputation":2101,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561730574528,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72206086,
        "Question_title":"Can't log MLflow artifacts to S3 with docker-based tracking server",
        "Question_body":"<p>I'm trying to set up a simple MLflow tracking server with docker that uses a mysql backend store and S3 bucket for artifact storage.  I'm using a simple docker-compose file to set this up on a server and supplying all of the credentials through a .env file.  When I try to run the sklearn_elasticnet_wine example from the mlflow repo here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine<\/a> using<code>TRACKING_URI = &quot;http:\/\/localhost:5005<\/code> from the machine hosting my tracking server, the run fails with the following error: <code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code>.  I've verified that my environment variables are correct and available in my mlflow_server container. The runs show up in my backend store so the run only seems to be failing at the artifact logging step.  I'm not sure why this isn't working.  I've seen a examples of how to set up a tracking server online, including: <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039<\/a>.  Some use minio also but others just specify their s3 location as I have.  I'm not sure what I'm doing wrong at this point. Do I need to explicitly set the ARTIFACT_URI as well?  Should I be using Minio?  Eventually, I'll be logging runs to the server from another machine, hence the nginx container.  I'm pretty new to all of this so I'm hoping it's something really obvious and easy to fix but so far the Google has failed me.  TIA.<\/p>\n<pre><code>version: '3'\n\nservices:\n  app: \n    restart: always\n    build: .\/mlflow\n    image: mlflow_server\n    container_name: mlflow_server\n    expose:\n      - 5001\n    ports:\n      - &quot;5001:5001&quot;\n    networks:\n      - internal \n    environment:\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n      - AWS_S3_BUCKET=${AWS_S3_BUCKET}\n      - DB_USER=${DB_USER}\n      - DB_PASSWORD=${DB_PASSWORD}\n      - DB_PORT=${DB_PORT}\n      - DB_NAME=${DB_NAME}\n    command: &gt;\n      mlflow server \n      --backend-store-uri mysql+pymysql:\/\/${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}\/${DB_NAME} \n      --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/mlruns\/\n      --host 0.0.0.0 \n      --port 5001\n\n  nginx: \n    restart: always\n    build: .\/nginx\n    image: mlflow_nginx\n    container_name: mlflow_nginx\n    ports:\n      - &quot;5005:80&quot; \n    networks:\n      - internal \n    depends_on:\n      - app\n\nnetworks:\n  internal:\n    driver: bridge\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652294436513,
        "Question_score":2,
        "Question_tags":"docker|amazon-s3|docker-compose|boto3|mlflow",
        "Question_view_count":620,
        "Owner_creation_time":1639614248310,
        "Owner_last_access_time":1663891212383,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1652296959876,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72206086",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72688726,
        "Question_title":"Can you edit the tags of a registered model version after the fact using mlflow api?",
        "Question_body":"<p>I am trying to use the mlflow model registry. My plan is to have a monthly scheduled retraining pipeline. I know from reading the documentation that so long as I set the model name to the same string if I save a model\/use create_model_version it will create a new version of the model. I also saw from the documentation that I can set the tags associated with a version using create_model_version as well. I want to set tags for valid_to_date and valid_from_date for each version so that if I want to go back in time and backfill predictions with the correct version <em>as of<\/em> when the data is from I am using the correct model. My initial thought was every time I create a new model version I set the valid_from_date as the date of that model version creation and the valid_to_date as 1-1-2099. Then when I train a new version edit the tag of the previous version valid_to_date from 1-1-2099 to the date it was superceded by a new version. Is that something that can be done using the mlflow python api?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655735663570,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":108,
        "Owner_creation_time":1333388796950,
        "Owner_last_access_time":1663959988533,
        "Owner_location":null,
        "Owner_reputation":335,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72688726",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59003752,
        "Question_title":"adbazureml not supported by mlflow",
        "Question_body":"<p>We've been following the latest Microsoft webinar about deploying the ML model from Azure Databricks to Azure ML using the MLFlow, and we get the following error when trying to run the experiment from Databricks notebook using the following code:<\/p>\n\n<pre><code>experimentName=\"someExperimentName\"\nmlflow.set_experiment(experimentName)\n<\/code><\/pre>\n\n<p>the error message:<\/p>\n\n<blockquote>\n  <p>UnsupportedModelRegistryStoreURIException: Unsupported URI\n  'adbazureml:\/\/westus.experiments.azureml.net\/history\/v1.0\/subscriptions\/cemrecdsap-t10us-20180830\/resourceGroups\/2f5a718e-7c56-4dd3-aa7b-03a19b70667\/providers\/Microsoft.MachineLearningServices\/workspaces\/cemrecdsap-mlservice'\n  for model registry store. Supported schemes are: ['', 'file',\n  'sqlite', 'https', 'databricks', 'postgresql', 'mysql', 'http',\n  'mssql']<\/p>\n<\/blockquote>\n\n<p>Init script we use as suggested in Microsoft MLflow webinar:\n(it was available here, but now it's removed - <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/azure-databricks\/linking\/azureml-cluster-init.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/azure-databricks\/linking\/azureml-cluster-init.sh<\/a>)<\/p>\n\n<pre><code>#!\/bin\/bash\n\n############## START CONFIGURATION #################\n# Provide the required *AzureML* workspace information\nregion=\"westus\" \nsubscriptionId=\"bcb65f42-f234-4bff-91cf-9ef816cd9936\" \nresourceGroupName=\"dev-rg\"\nworkspaceName=\"myazuremlws\"\n# Optional config directory\nconfigLocation=\"\/databricks\/config.json\"\n############### END CONFIGURATION #################\n\n# Drop the workspace configuration on the cluster\nsudo touch $configLocation\nsudo echo {\\\\\"subscription_id\\\\\": \\\\\"${subscriptionId}\\\\\", \\\\\"resource_group\\\\\": \\\\\"${resourceGroupName}\\\\\", \\\\\"workspace_name\\\\\": \\\\\"${workspaceName}\\\\\"} &gt; $configLocation\n\n# Set the MLflow Tracking URI\ntrackingUri=\"adbazureml:\/\/${region}.experiments.azureml.net\/history\/v1.0\/subscriptions\/${subscriptionId}\/resourceGroups\/${resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/${workspaceName}\"\nsudo echo export MLFLOW_TRACKING_URI=${trackingUri} &gt;&gt; \/databricks\/spark\/conf\/spark-env.sh \n<\/code><\/pre>\n\n<p>We use the latest MLFlow version, 1.4<\/p>\n\n<p>Is there a chance that the <strong>adbazureml<\/strong> protocol that was used in the webinar is not supported yet by MLFlow?\nOr we missed something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1574472436810,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":430,
        "Owner_creation_time":1502581315743,
        "Owner_last_access_time":1576191685887,
        "Owner_location":"Israel",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59003752",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69944447,
        "Question_title":"How to change the directory of mlflow logs?",
        "Question_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636727119830,
        "Question_score":0,
        "Question_tags":"machine-learning|deep-learning|pytorch|mlflow",
        "Question_view_count":436,
        "Owner_creation_time":1410333105327,
        "Owner_last_access_time":1662489968593,
        "Owner_location":"Turin, Metropolitan City of Turin, Italy",
        "Owner_reputation":477,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1636755379243,
        "Answer_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636732235590,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636755540676,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67959892,
        "Question_title":"Log experiments with MLflow to DAGsHub tracking server",
        "Question_body":"<p>I'm trying to use MLflow and log my experiments to DAGsHub's remote-tracking server but I get this error message:<\/p>\n<p><code>WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'NoneType' object has no attribute 'info' <\/code> with a lot of HTML text.<\/p>\n<p>When I check my DAGsHub repo, no new experiment is created.<\/p>\n<p>What am I'm doing wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1623598028067,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":95,
        "Owner_creation_time":1620133604910,
        "Owner_last_access_time":1641927869990,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":53,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67959892",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72663925,
        "Question_title":"Failure on importing mlflow to Azure Databricks 7.3 LTS ML Runtime",
        "Question_body":"<p>I am having trouble trying to import mlflow to Azure databricks. I'm currently using 7.3 LTS ML Runtime, which already have mlflow==1.11.0. I am a developing data scientist and I have no clue how to solve this issue. Have already tried to reinstall and didn't suceed. Any thoughts?<\/p>\n<p>This is the error message:<\/p>\n<pre><code>Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (msrest 0.6.18 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('msrest&gt;=0.6.21'), {'azure-mgmt-containerregistry'}).\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'\n  _tracking_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _tracking_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_model_registry\/utils.py:106: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _model_registry_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'\n  _artifact_repository_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _artifact_repository_registry.register_entrypoints()\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  class IteratorBase(collections.Iterator, trackable.Trackable,\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/tensorflow\/python\/autograph\/utils\/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n\npip list\n\nazure-common              1.1.28           \nazure-core                1.24.1           \nazure-graphrbac           0.61.1           \nazure-identity            1.4.1            \nazure-mgmt-authorization  0.61.0           \nazure-mgmt-containerregistry 10.0.0        \nazure-mgmt-core           1.3.1            \nazure-mgmt-keyvault       2.2.0            \nazure-mgmt-network        10.2.0           \nazure-mgmt-resource       11.0.0           \nazure-mgmt-storage        11.2.0           \nazure-storage-blob        12.4.0           \nazureml-automl-core       1.19.0           \nazureml-core              1.8.0.post1      \nazureml-dataprep          2.6.6            \nazureml-dataprep-native   26.0.0           \nazureml-dataprep-rslex    1.4.0            \nazureml-dataset-runtime   1.19.0.post1     \nazureml-mlflow            1.8.0            \nazureml-pipeline          1.19.0           \nazureml-pipeline-core     1.19.0           \nazureml-pipeline-steps    1.19.0           \nazureml-sdk               1.19.0           \nazureml-telemetry         1.19.0           \nazureml-train             1.19.0           \nazureml-train-automl-client 1.19.0         \nazureml-train-core        1.19.0           \nazureml-train-restclients-hyperdrive 1.19.0\nmlflow                    1.11.0\npip                       20.0.2\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655493728417,
        "Question_score":0,
        "Question_tags":"python|azure-databricks|mlflow",
        "Question_view_count":102,
        "Owner_creation_time":1647266372287,
        "Owner_last_access_time":1655579594010,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663925",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72053762,
        "Question_title":"How to create seperate mlflow custom models for training and prediction?",
        "Question_body":"<p>My requirement is to create separate Mlflow custom models for training and prediction.\nI want to create training model and use those training model in prediction model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651214518370,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":45,
        "Owner_creation_time":1651212627497,
        "Owner_last_access_time":1663746029710,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72053762",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71408963,
        "Question_title":"Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker",
        "Question_body":"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()<\/code>.<\/p>\n<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))<\/code> to infer input and output signatures.<\/p>\n<p>Model is deployed successfully. When trying to query the model I get <code>ModelError<\/code> (full traceback below).<\/p>\n<p>To query the model, I am using precisely the same <code>test_example<\/code> that I used for <code>infer_signature()<\/code>:<\/p>\n<p><code>test_example = [['This is the subject', 'This is the body']]<\/code><\/p>\n<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()<\/code> as that is not <code>json<\/code>-serializeable.<\/p>\n<p>To query the model I tried two different approaches:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nSAGEMAKER_REGION = 'us-west-2'\nMODEL_NAME = '...'\n\nclient = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)\n\n# Approach 1\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=json.dumps(test_example),\n                ContentType=&quot;application\/json&quot;,\n            )\n\n# Approach 2\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),\n                ContentType=&quot;application\/json; format=pandas-split&quot;,\n            )\n<\/code><\/pre>\n<p>but they result in the same error.<\/p>\n<p>Will be grateful for your suggestions.<\/p>\n<p>Thank you!<\/p>\n<p>Note: I am using Python 3 and all <strong>strings are unicode<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;\n      2                 EndpointName=MODEL_NAME,\n      3                 Body=test_example,\n----&gt; 4                 ContentType=&quot;application\/json; format=pandas-split&quot;,\n      5             )\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    394             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 395             return self._make_api_call(operation_name, kwargs)\n    396 \n    397         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    724             error_class = self.exceptions.from_code(error_code)\n--&gt; 725             raise error_class(parsed_response, operation_name)\n    726         else:\n    727             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\n<\/code><\/pre>\n<p>Environment info:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],\n 'dependencies': ['python=3.6.10',\n  'pip==21.3.1',\n  'pytorch=1.10.2',\n  'cudatoolkit=10.2',\n  {'pip': ['mlflow==1.22.0',\n    'transformers==4.17.0',\n    'datasets==1.18.4',\n    'cloudpickle==1.3.0']}],\n 'name': 'bert_bec_test_env'}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646826989870,
        "Question_score":0,
        "Question_tags":"amazon-web-services|nlp|amazon-sagemaker|mlflow",
        "Question_view_count":61,
        "Owner_creation_time":1490275561927,
        "Owner_last_access_time":1663878457720,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":83,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":1646837087963,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71408963",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70445997,
        "Question_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Question_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1640160788793,
        "Question_score":1,
        "Question_tags":"docker|mlflow|docker-in-docker",
        "Question_view_count":779,
        "Owner_creation_time":1546431264350,
        "Owner_last_access_time":1663848875057,
        "Owner_location":"Norway",
        "Owner_reputation":31,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1640184686952,
        "Answer_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640385689187,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73591190,
        "Question_title":"MLFlow & Conda: store envs in project dir instead of ~\/opt\/anaconda3\/envs",
        "Question_body":"<p>As per conda's <a href=\"https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/user-guide\/tasks\/manage-environments.html#id3\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<blockquote>\n<p>You can control where a conda environment lives by providing a path to a target directory when creating the environment. [...]:\n<code>conda create --prefix .\/envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21<\/code><\/p>\n<\/blockquote>\n<p>Is it possible to modify how mlflow invoques <code>conda create<\/code> when generating all the components' environments, in order save those at the root of the project instead of the default <code>...\/anaconda3\/envs<\/code> ?<\/p>\n<p>Many thanks in advance for your help,<br \/>\nKind regards<br \/>\nMarc<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662198074837,
        "Question_score":0,
        "Question_tags":"conda|mlflow|anaconda3",
        "Question_view_count":18,
        "Owner_creation_time":1598781108953,
        "Owner_last_access_time":1664084623343,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1662198174732,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73591190",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66282143,
        "Question_title":"Sagemaker Train Job can't connect to ec2 instance",
        "Question_body":"<p>I have MLFlow server running on ec2 instance, port 5000.<\/p>\n<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.<\/p>\n<p>ec2 instance inbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/VXwid.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VXwid.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>SageMaker outbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>These 2 security groups are in the same VPC<\/p>\n<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=\"https:\/\/stackoverflow.com\/questions\/45416882\/aws-security-group-include-another-security-group\">here<\/a>), but connection fails<\/p>\n<p>SageMaker job init:<\/p>\n<pre><code>   role = &quot;ml_sagemaker&quot;\n   security_group_ids = ['sg-04868acca16e81183']\n   bucket = sagemaker_session.default_bucket()  \n   out_path = f&quot;s3:\/\/{bucket}\/{project_name}&quot;\n\n   estimator = PyTorch(entry_point='run_train.py',\n                       source_dir='.',\n                       sagemaker_session=sagemaker_session,\n                       instance_type=instance_type,\n                       instance_count=1,\n                       framework_version='1.5.0',\n                       py_version='py3',\n                       role=role,\n                       security_group_ids=security_group_ids,\n                       hyperparameters={},\n                       )\n   ....\n\n<\/code><\/pre>\n<p>Inside <code>run_train.py<\/code>:<\/p>\n<pre><code>import mlflow\ntracking_uri = &quot;http:\/\/172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP\nmlflow.set_tracking_uri(tracking_uri)\nmlflow.log_param(&quot;test_param&quot;, 3)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n<\/code><\/pre>\n<p><strong>However<\/strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here is SageMaker Notebook configurations:<\/p>\n<img src=\"https:\/\/i.stack.imgur.com\/bslu8.png\" width=\"300\" \/>\n<p>How can I connect to ec2 instance from SageMaker Training Job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613755129957,
        "Question_score":1,
        "Question_tags":"amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow",
        "Question_view_count":608,
        "Owner_creation_time":1438098365740,
        "Owner_last_access_time":1663853069297,
        "Owner_location":null,
        "Owner_reputation":653,
        "Owner_up_votes":216,
        "Owner_down_votes":1,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66282143",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57237388,
        "Question_title":"MLFlow project run fails during conda env creation",
        "Question_body":"<p>I am trying to get mlflow mlproject working.<\/p>\n\n<p>When i run the mlflow run with repo name<\/p>\n\n<pre><code>mlflow run  git@gitlabe2.xx.yy.zz:name\/mlflow-example.git\n<\/code><\/pre>\n\n<p>The execution fails with the below error<\/p>\n\n<pre><code>File \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 265, in run\nuse_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 144, in _run\nconda_env_name = _get_or_create_conda_env(project.conda_env_path)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 144, in _run\nconda_env_name = _get_or_create_conda_env(project.conda_env_path)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 498, in _get_or_create_conda_env\nconda_env_path], stream_output=True)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/utils\/process.py\", line 38, in exec_cmd\nraise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\n<\/code><\/pre>\n\n<p>Any pointers on where I should look ?<\/p>\n\n<p>The suspect the conda.yaml file has some issues especially the conda env name.\nI have different names for the environment where the project is created and where the project is being run. Does it matter ?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1564278857600,
        "Question_score":1,
        "Question_tags":"conda|mlflow",
        "Question_view_count":917,
        "Owner_creation_time":1466129156870,
        "Owner_last_access_time":1664082712907,
        "Owner_location":"Chengdu, Sichuan, China",
        "Owner_reputation":2087,
        "Owner_up_votes":31,
        "Owner_down_votes":1,
        "Owner_views":87,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57237388",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68718719,
        "Question_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Question_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628544411170,
        "Question_score":1,
        "Question_tags":"python|azure|databricks|datastore|mlflow",
        "Question_view_count":3081,
        "Owner_creation_time":1522076554450,
        "Owner_last_access_time":1663901377960,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":96,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629748421287,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643054905512,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56120016,
        "Question_title":"Is it possible to specify MLflow project Environment through a Dockerfile (instead of an image)?",
        "Question_body":"<p>To my understanding, currently (May 2019) mlflow support running project in docker environment; however, it needs the docker image already been built. This leaves the docker image building to be a separate workflow. What is the suggested way to run a mlflow project from Dockerfile? <\/p>\n\n<p>Is there plans to support targeting Dockerfile natively in mlflow? What are the considerations about using image vs Dockerfile? Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557782434943,
        "Question_score":2,
        "Question_tags":"docker|machine-learning|artificial-intelligence|databricks|mlflow",
        "Question_view_count":631,
        "Owner_creation_time":1386735502123,
        "Owner_last_access_time":1663974468663,
        "Owner_location":null,
        "Owner_reputation":3405,
        "Owner_up_votes":641,
        "Owner_down_votes":8,
        "Owner_views":1094,
        "Question_last_edit_time":1557791232507,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56120016",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73399640,
        "Question_title":"Track input transformation for keras-flavored mlflow models",
        "Question_body":"<p><strong>TL\/DR:<\/strong> How to track and serve the input transformation for keras-flavored mlflow models?<\/p>\n<p>Neural network training usually involves preprocessing steps in which<\/p>\n<ul>\n<li>continuous variables are scaled and shifted to have unit width and zero mean,<\/li>\n<li>categorical variables (integer or string) are transformed to one-hot encoding.<\/li>\n<\/ul>\n<p>When the model is applied to new data, the scaling weights and the category-to-index association needs to be known.<\/p>\n<p>In keras there are generally two options to perform preprocessing:<\/p>\n<ul>\n<li><strong>Option 1:<\/strong> Using <a href=\"https:\/\/keras.io\/guides\/preprocessing_layers\/\" rel=\"nofollow noreferrer\">preprocessing layers<\/a>, or<\/li>\n<li><strong>Option 2:<\/strong> perform the transformation before the training when the dataset is loaded.<\/li>\n<\/ul>\n<p>With <strong>Option 1<\/strong>, the transformation is part of the model and will be automatically applied when the network is used and served as a <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html\" rel=\"nofollow noreferrer\">mlflow model<\/a>.<\/p>\n<p>My question concerns <strong>Option 2<\/strong>: What is the recommended way<\/p>\n<ul>\n<li>to keep track of the input transformation in mlflow for different experiments,<\/li>\n<li>and how to apply the same transformations when the model is served, e.g. with <code>mlflow model serve<\/code>?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660810680010,
        "Question_score":1,
        "Question_tags":"keras|mlflow",
        "Question_view_count":49,
        "Owner_creation_time":1302064833600,
        "Owner_last_access_time":1663871200727,
        "Owner_location":null,
        "Owner_reputation":3635,
        "Owner_up_votes":460,
        "Owner_down_votes":7,
        "Owner_views":424,
        "Question_last_edit_time":1660979496128,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73399640",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57078147,
        "Question_title":"How should I mount docker volumes in mlflow project?",
        "Question_body":"<p>I use <code>mlflow<\/code> in a docker environment as described in this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">example<\/a> and I start my runs with <code>mlflow run .<\/code>.<\/p>\n\n<p>I get output like this<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>2019\/07\/17 16:08:16 INFO mlflow.projects: === Building docker image mlflow-myproject-ab8e0e4 ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Created directory \/var\/folders\/93\/xt2vz36s7jd1fh9bkhkk9sgc0000gn\/T\/tmp1lxyqqw9 for downloading remote URIs passed to arguments of type 'path' ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Running command 'docker run \n--rm -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e \nMLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py' in run with ID 'ef21de61d8a6436b97b643e5cee64ae1' ===\n<\/code><\/pre>\n\n<p>I would like to mount a docker volume named <code>my_docker_volume<\/code> to the container\n at \nthe path <code>\/data<\/code>. So instead of the <code>docker run<\/code> shown above, I would like to\n use<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>docker run --rm --mount source=my_docker_volume,target=\/data -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e MLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py\n<\/code><\/pre>\n\n<p>I see that I could in principle run it once without mounted volume and then \ncopy the <code>docker run ...<\/code> and add <code>--mount source=my_volume,target=\/data<\/code> but\n I'd rather use something like<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>mlflow run --mount source=my_docker_volume,target=\/data .\n<\/code><\/pre>\n\n<p>but this obviously doesn't work because --mount is not a parameter for \n<code>mlflow run<\/code>.\nWhat's the recommened way of mounting a docker volume then?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1563373349400,
        "Question_score":4,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1301,
        "Owner_creation_time":1375955724343,
        "Owner_last_access_time":1664025245257,
        "Owner_location":"Freiburg im Breisgau, Germany",
        "Owner_reputation":191,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1564754919900,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57078147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69551533,
        "Question_title":"Error can't get attribute Net when saving PyTorch model with MLFlow",
        "Question_body":"<p>After installing MLFlow using <a href=\"https:\/\/github.com\/artefactory\/one-click-mlflow\" rel=\"nofollow noreferrer\">one-click-mlflow<\/a> I save a pytorch model using the default command that I found in the user guide. You can find the command bellow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pytorch.log_model(net, artifact_path=&quot;model&quot;, pickle_module=pickle)\n<\/code><\/pre>\n<p>The neural network saved is very simple, this is basically a two layer neural network with Xavier initialization and hyperbolic tangent as activation function.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Net(T.nn.Module):\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.hid1 = T.nn.Linear(n_features, 10)\n        self.hid2 = T.nn.Linear(10, 10)\n        self.oupt = T.nn.Linear(10, 1)\n        T.nn.init.xavier_uniform_(self.hid1.weight) \n        T.nn.init.zeros_(self.hid1.bias)\n        T.nn.init.xavier_uniform_(self.hid2.weight)\n        T.nn.init.zeros_(self.hid2.bias)\n        T.nn.init.xavier_uniform_(self.oupt.weight)\n        T.nn.init.zeros_(self.oupt.bias)\n        \n    def forward(self, x):\n        z = T.tanh(self.hid1(x))\n        z = T.tanh(self.hid2(z))\n        z = self.oupt(z)\n        return z\n<\/code><\/pre>\n<p>Every things is runing fine in the Jupyter Notebook. I can log metrics and other artifact but when I save the model I got the following error message:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>2021\/10\/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torch version (1.9.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torch==1.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2021\/10\/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.10.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torchvision==0.10.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2021\/10\/13 09:21:01 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: \/tmp\/tmpnl9dsoye\/model\/data, flavor: pytorch)\nTraceback (most recent call last):\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/environment.py&quot;, line 212, in infer_pip_requirements\n    return _infer_requirements(model_uri, flavor)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 263, in _infer_requirements\n    modules = _capture_imported_modules(model_uri, flavor)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 221, in _capture_imported_modules\n    _run_command(\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 173, in _run_command\n    raise MlflowException(msg)\nmlflow.exceptions.MlflowException: Encountered an unexpected error while running ['\/home\/ucsky\/.virtualenv\/mymodel\/bin\/python', '\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py', '--model-path', '\/tmp\/tmpnl9dsoye\/model\/data', '--flavor', 'pytorch', '--output-file', '\/tmp\/tmplyj0w2fr\/imported_modules.txt', '--sys-path', '[&quot;\/home\/ucsky\/project\/ofi-ds-research\/incubator\/ofi-pe-fr\/notebook\/guillaume-simon\/06-modelisation-pytorch&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/git\/ext\/gitdb&quot;, &quot;\/usr\/lib\/python39.zip&quot;, &quot;\/usr\/lib\/python3.9&quot;, &quot;\/usr\/lib\/python3.9\/lib-dynload&quot;, &quot;&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/IPython\/extensions&quot;, &quot;\/home\/ucsky\/.ipython&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/gitdb\/ext\/smmap&quot;]']\nexit status: 1\nstdout: \nstderr: Traceback (most recent call last):\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py&quot;, line 125, in &lt;module&gt;\n    main()\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py&quot;, line 118, in main\n    importlib.import_module(f&quot;mlflow.{flavor}&quot;)._load_pyfunc(model_path)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 723, in _load_pyfunc\n    return _PyTorchWrapper(_load_model(path, **kwargs))\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 626, in _load_model\n    return torch.load(model_path, **kwargs)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 607, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 882, in _load\n    result = unpickler.load()\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 875, in find_class\n    return super().find_class(mod_name, name)\nAttributeError: Can't get attribute 'Net' on &lt;module '__main__' from '\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py'&gt;\n<\/code><\/pre>\n<p>Can somebody explain me what is wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634111089723,
        "Question_score":2,
        "Question_tags":"python|pytorch|virtualenv|mlflow|mlops",
        "Question_view_count":343,
        "Owner_creation_time":1263142631610,
        "Owner_last_access_time":1661432787853,
        "Owner_location":null,
        "Owner_reputation":382,
        "Owner_up_votes":248,
        "Owner_down_votes":3,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69551533",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64099250,
        "Question_title":"MLFlow in docker - unable to store artifacts in sftp server (atmoz)",
        "Question_body":"<p>I would like to run MLflow &quot;entirely offline&quot; using docker (i.e. no cloud storage like S3 or blob). So I followed <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this guide<\/a> and tried to set the artifact store to the <a href=\"https:\/\/github.com\/atmoz\/sftp\" rel=\"nofollow noreferrer\">atmoz<\/a> sftp server running inside another docker container. As suggested in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#sftp-server\" rel=\"nofollow noreferrer\">MLFlow docs<\/a>, I try to auth with host keys, however, when I try to register my artifact I receive the following error <code>pysftp.exceptions.CredentialException: No password or key specified.<\/code><\/p>\n<p>I guess, there's something wrong with my hostkey setup. I also tried to follow <a href=\"https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">this guide<\/a> (mentioned in <a href=\"https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">this question<\/a>), but unfortunately it didn't have enough details for my - limited - knowledge of containers, sftp servers and pub-priv-key setups. My docker-compose looks like this...<\/p>\n<pre><code>services:\ndb:\n    restart: always\n    image: mysql\/mysql-server:5.7.28\n    container_name: mlflow_db\n    expose:\n        - &quot;3306&quot;\n    networks:\n        - backend\n    environment:\n        - MYSQL_DATABASE=${MYSQL_DATABASE}\n        - MYSQL_USER=${MYSQL_USER}\n        - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n        - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n    volumes:\n        - dbdata:\/var\/lib\/mysql\n\nmlflow-sftp:\n    image: atmoz\/sftp\n    container_name: mlflow-sftp\n    ports:\n        - &quot;2222:22&quot;\n    volumes:\n        - .\/storage\/sftp:\/home\/foo\/storage\n        - .\/ssh_host_ed25519_key:\/home\/foo\/.ssh\/ssh_host_ed25519_key.pub:ro\n        - .\/ssh_host_rsa_key:\/home\/foo\/.ssh\/ssh_host_rsa_key.pub:ro\n    command: foo::1001\n    networks:\n        - backend\n    \nweb:\n    restart: always\n    build: .\/mlflow\n    depends_on:\n        - mlflow-sftp\n    image: mlflow_server\n    container_name: mlflow_server\n    expose:\n        - &quot;5000&quot;\n    networks:\n        - frontend\n        - backend\n    volumes:\n        - .\/ssh_host_ed25519_key:\/root\/.ssh\/ssh_host_ed25519_key:ro\n        - .\/ssh_host_rsa_key:\/root\/.ssh\/ssh_host_rsa_key:ro\n    command: &gt;\n        bash -c &quot;sleep 3\n        &amp;&amp; ssh-keyscan -H mlflow-sftp &gt;&gt; ~\/.ssh\/known_hosts\n        &amp;&amp; mlflow server --backend-store-uri mysql+pymysql:\/\/${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306\/${MYSQL_DATABASE} --default-artifact-root sftp:\/\/foo@localhost:2222\/storage --host 0.0.0.0&quot;\n    \nnginx:\n    restart: always\n    build: .\/nginx\n    image: mlflow_nginx\n    container_name: mlflow_nginx\n    ports:\n        - &quot;80:80&quot;\n    networks:\n        - frontend\n    depends_on:\n        - web\n<\/code><\/pre>\n<p>networks:\nfrontend:\ndriver: bridge\nbackend:\ndriver: bridge<\/p>\n<p>volumes:\ndbdata:<\/p>\n<p>... and in my python script I create a new mlflow experiment as follows.<\/p>\n<pre><code>remote_server_uri = &quot;http:\/\/localhost:80&quot; \nmlflow.set_tracking_uri(remote_server_uri)\nEXPERIMENT_NAME = &quot;test43&quot;\nmlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\nEXPERIMENT_NAME = &quot;test43&quot;\nmlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\nwith mlflow.start_run():\n    print(mlflow.get_artifact_uri())\n    print(mlflow.get_registry_uri())\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    predicted_qualities = lr.predict(test_x)\n\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n\n    mlflow.log_param(&quot;alpha&quot;, alpha)\n    mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n\n    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n\n    if tracking_url_type_store != &quot;file&quot;:\n        mlflow.sklearn.log_model(lr, &quot;model&quot;, registered_model_name=&quot;ElasticnetWineModel&quot;)\n    else:\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>I haven't modified the dockerfiles of the first mentioned guide i.e. you'll be able to see them <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">here<\/a>. My guess is that I messed something up with the host keys, maybe put them in a wrong directory, but after several hours of brute-force experimenting I hope someone can help me with a pointer in the right direction. Let me know if there's anything missing to reproduce the error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1601284508343,
        "Question_score":4,
        "Question_tags":"python|docker|public-key|mlflow",
        "Question_view_count":874,
        "Owner_creation_time":1517932915310,
        "Owner_last_access_time":1662728503583,
        "Owner_location":"Germany",
        "Owner_reputation":41,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64099250",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64462918,
        "Question_title":"How to save models in MLFlow with R and get Stages of them in Azure Databricks?",
        "Question_body":"<p>I would like to save a model in MLFlow with Azure Databricks. In Python, I can use the following code to save a model with a name automatically:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.spark.log_model(\n        model,\n        artifact_path = 'model_prueba',\n        registered_model_name = 'model_prueba'\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" alt=\"Registered models window\" \/><\/a><\/p>\n<p>But I am trying to do the same with <strong>R<\/strong> with the following code:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>mlflow_log_model(\n          model,\n          artifact_path = 'model_prueba_R',\n          registered_model_name = 'model_prueba_R'\n    )\n<\/code><\/pre>\n<p>But it does not register any model in the Models section. It only saves the model with the artifact path in the run section.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zfAza.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zfAza.png\" alt=\"Artifact location\" \/><\/a><\/p>\n<p>Anyone could tell me the way to save the model for staging automatically with code in R?<\/p>\n<p>Thank you very much!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1603281092693,
        "Question_score":1,
        "Question_tags":"python|r|model|azure-databricks|mlflow",
        "Question_view_count":201,
        "Owner_creation_time":1568015757070,
        "Owner_last_access_time":1642423173287,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64462918",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59881297,
        "Question_title":"How to serve custom MLflow model with Docker?",
        "Question_body":"<p>We have a project following essentially this\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">docker example<\/a> with the only difference that we created a custom model similar to <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">this<\/a> whose code lies in a directory called <code>forecast<\/code>. We succeeded in running the model with <code>mlflow run<\/code>. The problem arises when we try to serve the model. After doing <\/p>\n\n<pre><code>mlflow models build-docker -m \"runs:\/my-run-id\/my-model\" -n \"my-image-name\"\n<\/code><\/pre>\n\n<p>we fail running the container with<\/p>\n\n<pre><code>docker run -p 5001:8080 \"my-image-name\"\n<\/code><\/pre>\n\n<p>with the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'forecast'\n<\/code><\/pre>\n\n<p>It seems that the docker image is not aware of the source code defining our custom model class.\nWith Conda environnement the problem does not arise thanks to the <code>code_path<\/code> argument in <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n\n<p>Our Dockerfile is very basic, with just <code>FROM continuumio\/miniconda3:4.7.12, RUN pip install {model_dependencies}<\/code>.<\/p>\n\n<p>How to let the docker image know about the source code for deserialising the model and run it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579791133580,
        "Question_score":3,
        "Question_tags":"docker|mlflow",
        "Question_view_count":2105,
        "Owner_creation_time":1429204620943,
        "Owner_last_access_time":1662844490857,
        "Owner_location":"Paris, France",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59881297",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73570230,
        "Question_title":"How to access Mlflow running on fargate (ECS) with only VPN in\/outbound rules from sagemaker notebook instance?",
        "Question_body":"<p><strong>Context:<\/strong><\/p>\n<p>I have deployed Mlflow on ECS(Fargate) using terraform using this public <a href=\"https:\/\/github.com\/Glovo\/terraform-aws-mlflow\" rel=\"nofollow noreferrer\">git-repo<\/a>. After deploying Mlflow which was publicly accessible using the link, I made some changes in the security group and changed in\/outbound rule to the only company VPN ips, now that link is only accessible under the VPN.<\/p>\n<p><strong>Question:<\/strong><\/p>\n<p>Now I have Sagemake notebook instance and want to access that link inside the notebook and the notebook is running on AWS internet(outside Company-VPN) and I'm not able to access that link. What could be the possible solution?<\/p>\n<p>I don't want to open access of Mlflow-link publicaly to accessible form anywhere on the internet.<\/p>\n<p><strong>Running this code on notebook:<\/strong><\/p>\n<pre><code>!pip install mlflow\nimport mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/mlflow-mlp-xyz-xyz.eu-west-1.elb.amazonaws.com\/&quot;)\nmlflow.get_experiment_by_name('mlpmlflowlogger')\ncurrent_experiment=dict(mlflow.get_experiment_by_name('mlpmlflowlogger'))\nprint(current_experiment)\nexperiment_id=current_experiment['experiment_id']\nprint(experiment_id)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1662039083460,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ecs|amazon-vpc|aws-security-group|mlflow",
        "Question_view_count":31,
        "Owner_creation_time":1511960637980,
        "Owner_last_access_time":1663681247487,
        "Owner_location":"Deggendorf, Germany",
        "Owner_reputation":813,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":115,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73570230",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":44053141,
        "Question_title":"Can I make Neptune talk to git?",
        "Question_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1495124614783,
        "Question_score":0,
        "Question_tags":"git|github|machine-learning|neptune",
        "Question_view_count":140,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":1503919276067,
        "Answer_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1503919410423,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":72143195,
        "Question_title":"Running Neptune in Jupyter Notebook gives NameError that Neptune is not defined",
        "Question_body":"<pre><code># Create run in project (Neptune)\nrun = neptune.init(project='ssraghuvanshi1989\/GCI-01-Lung-CT-Segmentation-20220506')\n<\/code><\/pre>\n<hr \/>\n<pre><code>NameError                    Traceback (most recent call last)\nC:\\Users\\SAURAB~1\\AppData\\Local\\Temp\/ipykernel_27104\/3392926562.py in &lt;module&gt;\n      1 # Create run in project\n----&gt; 2 run = neptune.init(project='ssraghuvanshi1989\/GCI-01-Lung-CT-Segmentation-20220506')\n\nNameError: name 'neptune' is not defined\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1651847578783,
        "Question_score":0,
        "Question_tags":"python|google-colaboratory|nameerror|mlops|neptune",
        "Question_view_count":67,
        "Owner_creation_time":1423107401473,
        "Owner_last_access_time":1653027951467,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1651848652763,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72143195",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":73833935,
        "Question_title":"Running Neptune.ai in a loop",
        "Question_body":"<p>so i created a for loop so I can run various batch sizes, where each loop will open and close a neptune run. The first time runs fine, but the following runs, the accuracy doesn't record into neptune, and python does not throw an error? Can anyone think what the problem may be?<\/p>\n<pre><code>for i in range(len(percentage)):\n\n    run = neptune.init(\n        project=&quot;xxx&quot;,\n        api_token=&quot;xxx&quot;,\n    )\n\n    epochs = 600\n    batch_perc = percentage[i]\n    lr = 0.001\n    sb = 64 #round((43249*batch_perc)*0.00185)\n    params = {\n        'lr': lr,\n        'bs': sb,\n        'epochs': epochs,\n        'batch %': batch_perc\n    }\n    run['parameters'] = params\n\n    torch.manual_seed(12345)\n    td = 43249 * batch_perc\n    vd = 0.1*(43249 - td) + td\n\n    train_dataset = dataset[:round(td)]\n    val_dataset = dataset[round(td):round(vd)]\n    test_dataset = dataset[round(vd):]\n\n    print(f'Number of training graphs: {len(train_dataset)}')\n    run['train'] = len(train_dataset)\n    print(f'Number of validation graphs: {len(val_dataset)}')\n    run['val'] = len(val_dataset)\n    print(f'Number of test graphs: {len(test_dataset)}')\n    run['test'] = len(test_dataset)\n\n    train_loader = DataLoader(train_dataset, batch_size=sb, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=sb, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    model = GCN(hidden_channels=64).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(1, epochs):\n        train()\n        train_acc = test(train_loader)\n        run['training\/batch\/acc'].log(train_acc)\n        val_acc = test(val_loader)\n        run['training\/batch\/val'].log(val_acc)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663978074247,
        "Question_score":0,
        "Question_tags":"python|neptune",
        "Question_view_count":27,
        "Owner_creation_time":1648215317920,
        "Owner_last_access_time":1664077689273,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73833935",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":64713492,
        "Question_title":"Is there a way to log the keras model summary to neptune?",
        "Question_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604660980577,
        "Question_score":1,
        "Question_tags":"python|keras|deep-learning|neptune",
        "Question_view_count":285,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1660057709880,
        "Answer_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1604748950752,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":71857090,
        "Question_title":"jsonschema 4.4.0 does not provide the extra 'isoduration'",
        "Question_body":"<p>So I'm trying to run some piece of code and keep getting the following error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py&quot;, line 770, in resolve\n    raise DistributionNotFound(req, requirers)\npkg_resources.DistributionNotFound: The 'isoduration; extra == &quot;format&quot;' distribution was not found and is required by jsonschema\n<\/code><\/pre>\n<p>However, after running<\/p>\n<pre><code>pip uninstall -y jsonschema &amp;&amp; pip install -U jsonschema &amp;&amp; pip install jsonschema[isoduration]\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>Requirement already satisfied: jsonschema[isoduration] in \/opt\/conda\/lib\/python3.8\/site-packages (4.4.0)\n  WARNING: jsonschema 4.4.0 does not provide the extra 'isoduration'\n<\/code><\/pre>\n<p>and surely, running my code again I get the same error message as before.<\/p>\n<p><strong>I tried:<\/strong><\/p>\n<ul>\n<li><code>pip install isoduration<\/code>, but different format showed up as\nmissing<\/li>\n<li>hard removing <code>jsonschema<\/code> with <code>rm -rf ...<\/code><\/li>\n<li>installing <code>jsonschema==3.2.0<\/code> as it supposedly worked for a friend of mine<\/li>\n<\/ul>\n<p>I'm very confused with what's going on here, any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649850905623,
        "Question_score":1,
        "Question_tags":"python|jsonschema|kedro|neptune",
        "Question_view_count":74,
        "Owner_creation_time":1532254746377,
        "Owner_last_access_time":1656938822310,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1649853689192,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71857090",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":44053028,
        "Question_title":"Setting job tags for Neptune",
        "Question_body":"<p>Recently I started using Neptune (via <a href=\"https:\/\/go.neptune.deepsense.io\/\" rel=\"nofollow noreferrer\">Neptune Go<\/a>) and want to have a well-organised history of experiments. How to set tags to a given experiment? (Do I do it before running it, or after?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1495124248987,
        "Question_score":1,
        "Question_tags":"tags|neptune",
        "Question_view_count":57,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are four ways to set tags to your experiment:<\/p>\n\n<ol>\n<li>In the <code>run\/enqueue\/exec<\/code> command, i.e:<\/li>\n<\/ol>\n\n<p><code>neptune run --tags tag1 tag2 tag3 tag4<\/code><\/p>\n\n<ol start=\"2\">\n<li>In the configuration file:<\/li>\n<\/ol>\n\n<p><code>tags: [tag1, tag2, tag3, tag4]<\/code><\/p>\n\n<ol start=\"3\">\n<li>In your code:<\/li>\n<\/ol>\n\n<p><code>ctx.job.tags.append('new-tag')<\/code><\/p>\n\n<ol start=\"4\">\n<li>In the Web UI. In the experiment dashboard you have to click on \"Job Properties\" in the top left corner of the screen. Side panel will appear where you can modify job properties.<\/li>\n<\/ol>\n\n<p>So you can change tags of your experiment in every phase of your experiment execution.<\/p>\n\n<p>Sources: <\/p>\n\n<ul>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags<\/a><\/p><\/li>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags<\/a> <\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1495176106327,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053028",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":70322291,
        "Question_title":"Error notification not working in background job Neptune Software",
        "Question_body":"<p>For a particular server script, we are adding a background job.\nIn that ,it has an option to add error notification emails ,which is not working.  There is error in my script, which I can see in job log but not getting any notifications on email.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1639300561957,
        "Question_score":0,
        "Question_tags":"background|amazon-neptune|aws-neptune|neptune",
        "Question_view_count":36,
        "Owner_creation_time":1583493572323,
        "Owner_last_access_time":1660792659480,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1649766671956,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70322291",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":72606870,
        "Question_title":"Pass NEPTUNE_API_TOKEN environment variable via docker run command",
        "Question_body":"<p>Using the <code>docker run<\/code> command, I'm trying to pass my NEPTUNE_API_TOKEN to my container.<\/p>\n<p>My understanding is that I should use the <code>-e<\/code> flag as follows: <code>-e ENV_VAR='env_var_value'<\/code> and that might work.\nI wish, however, to use the value existing in the already-running session, as follows:<\/p>\n<pre><code>docker run -e NEPTUNE_API_TOKEN=$(NEPTUNE_API_TOKEN) &lt;my_image&gt;\n<\/code><\/pre>\n<p>However, after doing so, NEPTUNE_API_TOKEN is set to empty when checking the value inside the container.\nMy question is whether I'm doing something wrong or if this is not possible and I must provide an explicit Neptune API token as a string.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1655141431553,
        "Question_score":0,
        "Question_tags":"docker|neptune",
        "Question_view_count":42,
        "Owner_creation_time":1538410701937,
        "Owner_last_access_time":1663830248157,
        "Owner_location":"Israel",
        "Owner_reputation":735,
        "Owner_up_votes":211,
        "Owner_down_votes":5,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72606870",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":56881619,
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562220779043,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_time":1562220403123,
        "Owner_last_access_time":1563409319090,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881619",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":56308560,
        "Question_title":"Nodejs example for pachyderm",
        "Question_body":"<p>I am new to Pachyderm.<\/p>\n\n<p>I have a pipeline to extract, transform and then save in the db.\nEverything is already written in nodejs, docekrized.\nNow, I would like to move and use pachyderm.<\/p>\n\n<p>I tried following the python examples they provided, but creating this new pipeline always fails and the job never starts.<\/p>\n\n<p>All my code does is take the <code>\/pfs\/data<\/code> and copy it to <code>\/pfs\/out<\/code>. <\/p>\n\n<p>Here is my pipeline definition<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"copy\"\n    },\n    \"transform\": {\n        \"cmd\": [\"npm\", \"start\"],\n        \"image\": \"simple-node-docker\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"data\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>All that happens is that the pipeline fails and the job never starts.<\/p>\n\n<p>Is there a way to debug on why the pipeline is failing?\nIs there something special about my docker image that needs to happen?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558817113660,
        "Question_score":0,
        "Question_tags":"node.js|pachyderm",
        "Question_view_count":80,
        "Owner_creation_time":1401628725057,
        "Owner_last_access_time":1664053631220,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56308560",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53987937,
        "Question_title":"Pachyderm pipeline does not start a job and launches an empty repo",
        "Question_body":"<p>I have a JSON configuration for my pipeline in Pachyderm:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/pclean_phlc9h6grzqdhm6sc0zrxjne_UdOgg.py \/pfs\/mopng_beneficiary_v2\/euoEQHIwIQTe1wXtg46fFYok.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv \/pfs\/mopng_beneficiary_v2\/Qc.csv\"],\n        \"image\": \"mopng-beneficiary-v2-image\"\n    }\n}\n<\/code><\/pre>\n\n<p>And my docker file is as follows:<\/p>\n\n<pre><code>FROM ubuntu:14.04\n\n# Install opencv and matplotlib.\nRUN apt-get update \\\n    &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get install -y unzip wget build-essential \\\n        cmake git pkg-config libswscale-dev \\\n        python3-dev python3-numpy python3-tk \\\n        libtbb2 libtbb-dev libjpeg-dev \\\n        libpng-dev libtiff-dev libjasper-dev \\\n        bpython python3-pip libfreetype6-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\n\nRUN sudo pip3 install matplotlib\nRUN sudo pip3 install pandas\n\n# Add our own code.\nADD pclean.py \/pclean.py\n<\/code><\/pre>\n\n<p>However, when I run my command to create the pipeline:<\/p>\n\n<pre><code>pachctl create-pipeline -f https:\/\/raw.githubusercontent.com\/avisrivastava254084\/learning-pachyderm\/master\/pipeline.json\n<\/code><\/pre>\n\n<p>The files are existing in the pfs:<\/p>\n\n<pre><code>pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/pclean_phlc9h6grzqdhm6sc0zrxjne_UdOgg.py\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/euoEQHIwIQTe1wXtg46fFYok.csv\n<\/code><\/pre>\n\n<p>It should be worth to note that I am getting this from the logs command(<code>pachctl get-logs --pipeline=mopng-beneficiary-v2<\/code>):<\/p>\n\n<pre><code>container \"user\" in pod \"pipeline-mopng-beneficiary-v2-v1-lnbjh\" is waiting to start: trying and failing to pull image\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1546262146783,
        "Question_score":0,
        "Question_tags":"json|docker|pachyderm",
        "Question_view_count":232,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1563207497823,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53987937",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53991961,
        "Question_title":"File not found even after adding the file inside docker",
        "Question_body":"<p>I have written a docker file which adds my python script inside the container:\n<code>ADD test_pclean.py \/test_pclean.py<\/code><\/p>\n\n<p>My directory structure is:<\/p>\n\n<pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pipeline.json\n\u2514\u2500\u2500 test_pclean.py\n<\/code><\/pre>\n\n<p>My json file which acts as a configuration file for creating a pipeline in Pachyderm is as follows:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/test_pclean.py\"],\n        \"image\": \"avisrivastava254084\/mopng-beneficiary-v2-image-7\"\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>Even though I have copied the official documentation's example, I am facing an error:\n<code>python3: can't open file '\/test_pclean.py': [Errno 2] No such file or directory<\/code><\/p>\n\n<p>My dockerfile is:<\/p>\n\n<pre><code>FROM    debian:stretch\n\n# Install opencv and matplotlib.\nRUN apt-get update \\\n    &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get install -y unzip wget build-essential \\\n        cmake git pkg-config libswscale-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\n\nRUN apt update\nRUN apt-get -y install python3-pip\nRUN pip3 install matplotlib\nRUN pip3 install pandas\n\nADD test_pclean.py \/test_pclean.py\nENTRYPOINT [ \"\/bin\/bash\/\" ]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1546296533713,
        "Question_score":0,
        "Question_tags":"python|python-3.x|docker|pachyderm",
        "Question_view_count":430,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1546423579990,
        "Answer_body":"<p>I was not changing the commits to my docker images on each build and hence, Kubernetes was using the local docker file that it had(w\/o tags and commits, it doesn't acknowledge any change). Once I started using commit with each build, Kubernetes started downloading the intended docker image.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547720458432,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53991961",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":67937857,
        "Question_title":"Pachyderm deploy GCP - no such image",
        "Question_body":"<p>I'm deploying Pachyderm on GKE but when I deploy the pipeline (following the <a href=\"https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/\" rel=\"nofollow noreferrer\">https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/<\/a>) the Pod fails in ImagePullCrashLoopBack giving this error &quot;no such image&quot;.<\/p>\n<p>Here, the output of the command &quot;kubectl get pods&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/lpvj7.png\" rel=\"nofollow noreferrer\">screenshot<\/a><\/p>\n<p>How can I fix the deployment procedure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1623417969943,
        "Question_score":0,
        "Question_tags":"kubernetes|google-cloud-platform|pachyderm",
        "Question_view_count":48,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As mentioned in the Slack channel of Pachyderm community, adding the flag <code>--no-expose-docker-socket<\/code> to the deploy call should solve the issue.<\/p>\n<p><code>pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-expose-docker-socket<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1623418140676,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67937857",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":44269634,
        "Question_title":"How do you link Pachyderm with the correct Kubernetes context?",
        "Question_body":"<p>I have more than one Kubernetes context. When I change contexts, I have been using <code>kill -9<\/code>  to kill the port-forward in order to redo the <code>pachtctl port-forward &amp;<\/code> command. I wonder if this is the right way of doing it.<\/p>\n\n<p>In more detail:<\/p>\n\n<p>I start off being in a Kubernetes context, we'll call it context_x. I then want to change context to my local context, called minikube. I also want to see my repos for this minikube context, but when I use <code>pachctl list-repo<\/code>, it still shows context_x's Pachyderm repos. When I do <code>pachctl port-forward<\/code>, I then get an error message about the address being already in use. So I have to ps -a, then kill -9 on those port forward processes, and then do pachctl port-forward command again.<\/p>\n\n<p>An example of what I've been doing:<\/p>\n\n<pre><code>$ kubectl config use-context minikube\n$ pachctl list-repo #doesn't show minikube context's repos\n$ pachctl port-forward &amp;\n...several error messages along the lines of:\nUnable to create listener: Error listen tcp4 127.0.0.1:30650: bind: address already in use\n$ ps -a | grep forward\n33964 ttys002    0:00.51 kubectl port-forward dash-12345678-abcde 38080:8080\n33965 ttys002    0:00.51 kubectl port-forward dash-12345679-abcde 38081:8081\n37245 ttys002    0:00.12 pachctl port-forward &amp;\n37260 ttys002    0:00.20 kubectl port-forward pachd-4212312322-abcde 30650:650\n$ kill -9 37260\n$ pachctl port-forward &amp; #works as expected now\n<\/code><\/pre>\n\n<p>Also, kill -9 on the <code>pachctl port-forward<\/code> process 37245 doesn't work, it seems like I have to kill -9 on the <code>kubectl port-forward<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1496169756220,
        "Question_score":4,
        "Question_tags":"kubernetes|portforwarding|pachyderm",
        "Question_view_count":438,
        "Owner_creation_time":1327230207597,
        "Owner_last_access_time":1663187730303,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3322,
        "Owner_up_votes":319,
        "Owner_down_votes":5,
        "Owner_views":160,
        "Question_last_edit_time":1525391431087,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44269634",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53989099,
        "Question_title":"Pachyderm pipeline unable to pull docker image",
        "Question_body":"<p>I have successfully pushed my docker image to the registry:\n<code>my-username\/image-name<\/code><\/p>\n\n<p>However, pachyderm is still unable to pull docker image(from logs):<\/p>\n\n<pre><code>container \"user\" in pod \"pipeline-mopng-beneficiary-v2-v1-b6kln\" is waiting to start: trying and failing to pull image\n<\/code><\/pre>\n\n<p>Where should I specify the image so that pachyderm is able to pull one?\nThis is my config file:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/pclean.py \/pfs\/mopng_beneficiary_v2\/euoEQHIwIQTe1wXtg46fFYok.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv \/pfs\/mopng_beneficiary_v2\/Qc.csv\"],\n        \"image\": \"username\/my-image\"\n    }\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1546270692627,
        "Question_score":1,
        "Question_tags":"docker|pachyderm",
        "Question_view_count":158,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53989099",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645542424610,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_time":1440414980200,
        "Owner_last_access_time":1645630582560,
        "Owner_location":"Germany",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1645548706456,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645549016652,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71779667,
        "Question_title":"locked out of wandb local server - change user password",
        "Question_body":"<p>I am using a local weight and biases (wandb) instance running on a server with no internet connection.\nI have a user there and having no problems logging results from the server.<\/p>\n<p>However, when trying to see them in the UI it asked me to login again but unfortunately I forgot my password and reset password doesn't work with the message of <code>Error while trying to reset password<\/code>.<\/p>\n<p>I tried searching all over the documentation and found nothing to help with that.<\/p>\n<p>Any help for locally recovering my account will be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649323903133,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":304,
        "Owner_creation_time":1608753761683,
        "Owner_last_access_time":1663850083317,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1657484089700,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71779667",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67291062,
        "Question_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Question_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619559781187,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|wandb",
        "Question_view_count":385,
        "Owner_creation_time":1490778676137,
        "Owner_last_access_time":1664077488930,
        "Owner_location":null,
        "Owner_reputation":322,
        "Owner_up_votes":239,
        "Owner_down_votes":1,
        "Owner_views":109,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620162973716,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69142852,
        "Question_title":"Weights and Biases watch log causing CUDA out of memory",
        "Question_body":"<p>I am trying to use WandB gradient visualization to debug the gradient flow in my neural net on Google Colab. Without WandB logging, the training runs without error, taking up 11Gb\/16GB on the p100 gpu. However, adding this line <code>wandb.watch(model, log='all', log_freq=3)<\/code> causes a cuda out of memory error.<\/p>\n<p><strong>How does WandB logging create extra GPU memory overhead?<\/strong><\/p>\n<p><strong>Is there some way to reduce the overhead?<\/strong><\/p>\n<p>--adding training loop code--<\/p>\n<pre><code>learning_rate = 0.001\nnum_epochs = 50\n\ndevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\nmodel = MyModel()\n\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nwandb.watch(model, log='all', log_freq=3)\n\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_accuracy = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (name, output_array, input) in enumerate(trainloader):\n        \n        output_array = output_array.to(device)\n        input = input.to(device)\n        comb = torch.zeros(1,1,100,1632).to(device)\n\n        ## forward + backprop + loss\n        output = model(input, comb)\n\n        loss = my_loss(output, output_array)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n\n        temp = get_accuracy(output, output_array)\n\n        print('check 13')\n        !nvidia-smi | grep MiB | awk '{print $9 $10 $11}'\n\n        train_accuracy += temp     \n<\/code><\/pre>\n<p>-----edit-----<\/p>\n<p>I think WandB is creating an extra copy of the gradient during logging preprocessing. Here is the traceback:<\/p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-13de83557b55&gt; in &lt;module&gt;()\n     60         get_ipython().system(&quot;nvidia-smi | grep MiB | awk '{print $9 $10 $11}'&quot;)\n     61 \n---&gt; 62         loss.backward()\n     63 \n     64         print('check 10')\n\n4 frames\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    253                 create_graph=create_graph,\n    254                 inputs=inputs)\n--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    256 \n    257     def register_hook(self, hook):\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    147     Variable._execution_engine.run_backward(\n    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    150 \n    151 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in &lt;lambda&gt;(grad)\n    283             self.log_tensor_stats(grad.data, name)\n    284 \n--&gt; 285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n    286         self._hook_handles[name] = handle\n    287         return handle\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in _callback(grad, log_track)\n    281             if not log_track_update(log_track):\n    282                 return\n--&gt; 283             self.log_tensor_stats(grad.data, name)\n    284 \n    285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    219         # Remove nans from tensor. There's no good way to represent that in histograms.\n    220         flat = flat[~torch.isnan(flat)]\n--&gt; 221         flat = flat[~torch.isinf(flat)]\n    222         if flat.shape == torch.Size([0]):\n    223             # Often the whole tensor is nan or inf. Just don't log it in that case.\n\nRuntimeError: CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 15.90 GiB total capacity; 10.10 GiB already allocated; 717.75 MiB free; 14.27 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>---update----<\/p>\n<p>Indeed, commenting out the offending line <code>flat = flat[~torch.isinf(flat)]<\/code><\/p>\n<p>gets the logging step to just barely fit into the GPU memory.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1631361970417,
        "Question_score":2,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":405,
        "Owner_creation_time":1545588089587,
        "Owner_last_access_time":1657580031330,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1631485669880,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69142852",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73062370,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1658389162900,
        "Question_score":0,
        "Question_tags":"machine-learning|nlp|named-entity-recognition|wandb",
        "Question_view_count":58,
        "Owner_creation_time":1643710211767,
        "Owner_last_access_time":1663833597150,
        "Owner_location":null,
        "Owner_reputation":78,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1658393400156,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72590067,
        "Question_title":"jupyterLab Wandb does not iterative",
        "Question_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655016960480,
        "Question_score":0,
        "Question_tags":"jupyter-lab|wandb",
        "Question_view_count":40,
        "Owner_creation_time":1609152494583,
        "Owner_last_access_time":1663846781627,
        "Owner_location":"South Korea",
        "Owner_reputation":72,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655111602567,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71075704,
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_score":0,
        "Question_tags":"authentication|google-colaboratory|wandb",
        "Question_view_count":316,
        "Owner_creation_time":1644556763937,
        "Owner_last_access_time":1649221420000,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1644559750243,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64413753,
        "Question_title":"wandb - how to get it really silent (weights and biases)",
        "Question_body":"<p>Working with Anaconda-Spyder (python 3.7), I installed the latest release of wandb (0.10.7) and try to use it with tensorflow (2.1.0) and keras (2.3.1). Since then, my console is polluted with lengthy comments due to wandb. So far I am using config and logs (not yet sweeps). It worked well for several runs BUT I cannot handle the outcome of my code that disappear in a flow of messages.<\/p>\n<p>I'd like to get rid of these messages (or find an alternative to wandb...)\nThanks in advance for your help ;-)\nHere is code to import the necessary libraries :<\/p>\n<pre><code>import os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Dropout, Dense, LSTM, Flatten, Activation from tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras import models from tensorflow.keras.models\nimport Sequential from tensorflow.keras.optimizers import Adam import\nwandb os.environ[&quot;WANDB_SILENT&quot;] = &quot;true&quot;\nwandb.init(project=&quot;project_tsa&quot;)\n<\/code><\/pre>\n<p>Later on, I define the wandb config as follows:<\/p>\n<pre><code>wandb.init(config={&quot;project_name&quot;:&quot;project_tsa&quot;,\n                   &quot;architecture&quot;: &quot;ResNet&quot;,\n                   &quot;load_weights&quot;: load_weights,\n                   &quot;epochs&quot;: epochs,\n                   &quot;batch_size&quot;: batch_size,\n                   &quot;iterations&quot;: iterat,\n                   &quot;dropout&quot;: dropout,\n                   &quot;learning_rate&quot;: learning,\n                   &quot;features&quot;: n_feature_maps,\n                   &quot;sequence&quot;: seq_length})\n<\/code><\/pre>\n<p>Eventually, I define several logs :<\/p>\n<pre><code>wandb.log({&quot;precis_pos&quot;: precis})\nwandb.log({&quot;recall_pos&quot;: recall})\nwandb.log({&quot;sortino_pos&quot;: sharpe_all[4]})\nwandb.log({&quot;sortinogain_pos&quot;: (sharpe_all[4]-sharpe_all[3])})\n<\/code><\/pre>\n<p>As soon as wandb.init is present, I automatically get ten lines of warnings :<\/p>\n<pre><code>wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n<\/code><\/pre>\n<p>As soon as I define config (and even worse with the logs), the code ends with more than a hundred lines of warning.... Here are just a few as example :<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;D:\\Anaconda3\\lib\\threading.py&quot;, line 926, in _bootstrap_inner\n    self.run()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 40, in run\n    success = self.push()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 88, in push\n    _, upload_headers, result = self._api.upload_urls(project, [self.save_name])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 62, in wrapper\n    six.reraise(CommError, CommError(message, err), sys.exc_info()[2])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\six.py&quot;, line 702, in reraise\n    raise value.with_traceback(tb)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 24, in wrapper\n    return func(*args, **kwargs)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\internal\\internal_api.py&quot;, line 1039, in upload_urls\n    run = query_result[&quot;model&quot;][&quot;bucket&quot;]\nwandb.errors.error.CommError: 'NoneType' object is not subscriptable\n\n[SpyderKernelApp] WARNING | No such comm: 4562c8582bde4c50aedbd77151a94274\n[SpyderKernelApp] WARNING | No such comm: 197a5ff2d95849a0bd7d021f29e5f90e\n[SpyderKernelApp] WARNING | No such comm: c4ef2bdbcfb340e48c52099c8ac96dc1\n[SpyderKernelApp] WARNING | No such comm: 8898f986226043bfed836c08517299cb\n[SpyderKernelApp] WARNING | No such comm: 7fbb1deccf8c40629d95c57c6cbd2e6b\n[SpyderKernelApp] WARNING | No such comm: 1531c7eb303c4957e97426051d48441b\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603026773210,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":2929,
        "Owner_creation_time":1592919921400,
        "Owner_last_access_time":1663421488220,
        "Owner_location":"Brussels, Belgium",
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1661850129076,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64413753",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71502443,
        "Question_title":"Wandb login permission denied python virtual environment",
        "Question_body":"<p>I am using a remote Slurm cluster from my university and want to get access to my wandb profile. I run my py project in a virtual env with <code>python 3.7.4<\/code>, where I could install successfully <code>wandb<\/code>.<\/p>\n<p>However, when I try to login from command line <code>python -m wandb login<\/code> I get this error<\/p>\n<pre><code>(374_env1) [userX@peregrine ~]$ python -m wandb login\nTraceback (most recent call last):\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 193, in _run_module_as_main\n&quot;__main__&quot;, mod_spec)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 85, in _run_code\nexec(code, run_globals)\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/__main__.py&quot;, line 1, in &lt;module&gt;\nfrom wandb.cli import cli\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 53, in &lt;module&gt;\ndatefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1895, in basicConfig\nh = FileHandler(filename, mode)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1087, in __init__\nStreamHandler.__init__(self, self._open())\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1116, in _open\nreturn open(self.baseFilename, self.mode, encoding=self.encoding)\nPermissionError: [Errno 13] Permission denied: '\/local\/tmp\/debug-cli.log'\n<\/code><\/pre>\n<p>I tried also to login outside of the venv but still permission denied. Is it because as a user from the cluster I don't have enough <code>-m<\/code> permissions? I also tried <code>python -u wandb login<\/code> but still permission denied. ps. I don't have <code>sudo<\/code> permission.\nAny insights?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1647454802023,
        "Question_score":0,
        "Question_tags":"python-3.x|virtualenv|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1366555247053,
        "Owner_last_access_time":1661334085140,
        "Owner_location":null,
        "Owner_reputation":642,
        "Owner_up_votes":126,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Question_last_edit_time":1647455744270,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71502443",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70799787,
        "Question_title":"Can I use Wandb in Tracking my YOLOv4 Training?",
        "Question_body":"<p>I want to track my training in YOLOv4 using wandb but I can't see any tutorial on how to do it. I saw a youtube video but it was a training in YOLOv5.<\/p>\n<p>My wandb account is now logged in in the YOLOv4  training but I cant see no chart in the wandb page. It only displays this page <a href=\"https:\/\/i.stack.imgur.com\/AaAjB.png\" rel=\"nofollow noreferrer\">Wandb YOLOv4<\/a><\/p>\n<p>I want to know how exactly I can use wandb in my YOLOv4 training.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1642760709873,
        "Question_score":0,
        "Question_tags":"yolov4|wandb",
        "Question_view_count":122,
        "Owner_creation_time":1641512289450,
        "Owner_last_access_time":1655351990127,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1642761049736,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70799787",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916901,
        "Question_title":"WANDB Getting a run id based on tag",
        "Question_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650314726207,
        "Question_score":0,
        "Question_tags":"python|metadata|wandb",
        "Question_view_count":645,
        "Owner_creation_time":1444675970627,
        "Owner_last_access_time":1664044672477,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650458488356,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641753400317,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_time":1445719444550,
        "Owner_last_access_time":1664061059603,
        "Owner_location":"Iran, Canada",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641809697276,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70615413,
        "Question_title":"how to handle wandb Invalid filename characters exception on uploading image",
        "Question_body":"<p>I am using windows 10 &amp; venv &amp; python 3.9.7\nthis is my code to upload image to wandb<\/p>\n<pre><code>wandb_log[&quot;Image\/train_image&quot;] = wandb.Image('tmp.jpg')\nwandb.log(wandb_log, step)\n<\/code><\/pre>\n<p>the full directory of image is \u201cC:\\Users\\\uc774\uc900\ud601\\Documents\\Github\\terenz\\tmp.jpg\u201d\nHowever it creates this error<\/p>\n<pre><code>Media Image\/train_image is invalid. Please remove invalid filename characters\n<\/code><\/pre>\n<p>reinstalling wandb did not help to solve this problem.<br \/>\nAny suggestions? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1641517271160,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":94,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70615413",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916491,
        "Question_title":"WANDB run initialization",
        "Question_body":"<p>I wanted to try using wandb to log runs of my ML experiments for a project; but I am not able to initialize the run itself.\nI tried:<\/p>\n<p><code>run = wandb.init(project=&quot;name&quot;,entity=&quot;username&quot;,name=&quot;classification&quot;)<\/code><\/p>\n<p>This results in:\nwandb: W&amp;B API key is configured (use <code>wandb login --relogin<\/code> to force relogin)<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>What can I do to fix this? (I did login through the terminal before launching this cell idk what else to try)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1650311929900,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1650311491997,
        "Owner_last_access_time":1654197360160,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1650312801032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916491",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66541985,
        "Question_title":"'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file",
        "Question_body":"<p>I am trying to run the custom <a href=\"https:\/\/github.com\/ultralytics\/yolov5\" rel=\"nofollow noreferrer\">yolo model<\/a> on my data set in my local machine. I am following some reference code from the kaggle platform. Here first time I encounter the <code>wandb<\/code> frame work. while doing so I use the following to run the <code>train.py <\/code> file in my jupyter lab.<\/p>\n<pre><code>!WANDB_MODE=&quot;dryrun&quot; python train.py --img 640 --batch 16 --epochs 30 --data D:\/Anil\/Shawn_Research\/Iamge_DataSet\/VinBigData\/New_Direct\/vinbigdata.yaml --weights yolov5x.pt --cache\n<\/code><\/pre>\n<p>This work fine on the kaggle platform but in my local machine it shows following:<\/p>\n<pre><code>'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file.\n<\/code><\/pre>\n<p>While reading the similar thread I realized I might making mistake related to path variable or Environment variable.\nEven I tried to get solution from the <a href=\"https:\/\/docs.wandb.ai\/library\/environment-variables\" rel=\"nofollow noreferrer\">official document<\/a> but couldn't figure out.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615272052933,
        "Question_score":0,
        "Question_tags":"python-3.x|windows|pytorch|wandb",
        "Question_view_count":263,
        "Owner_creation_time":1490268584217,
        "Owner_last_access_time":1663920632157,
        "Owner_location":null,
        "Owner_reputation":37,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1661850709696,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66541985",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73335735,
        "Question_title":"Wandb automatically logged into the wrong user -- why?",
        "Question_body":"<p>I followed the usual instructions:<\/p>\n<pre><code>pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code>(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>fyi useufl command:<\/p>\n<pre><code>cat ~\/.netrc\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<hr \/>\n<p>seems my issue only happens in pycharm, when I run it in the terminal it works... :\/<\/p>\n<hr \/>\n<p>cross: <a href=\"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1660314751363,
        "Question_score":1,
        "Question_tags":"pycharm|wandb",
        "Question_view_count":157,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":1660666336796,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73335735",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67495547,
        "Question_title":"wandb logging PermissionError and OSError",
        "Question_body":"<p>Description:<\/p>\n<ul>\n<li><p>When running experiments using <code>Weights and Biases<\/code> (wandb), I\noccasionally get a <code>PermissionError<\/code> for Python's <code>logging<\/code> library\nand <code>OSError<\/code> for accessing the TLS CA cert.<\/p>\n<\/li>\n<li><p>I had the following stacktrace, repeated many times with different\ntypes of &quot;message&quot;. I can't discern the order of operations, but I'm\nguessing the cert can't be accessed and that causes the script to\ncrash, but I don't know why it only happens sometimes.<\/p>\n<\/li>\n<li><p>If it is relevant, I ran the experiments on an Ubuntu server, authenticated via Kerberos.<\/p>\n<\/li>\n<\/ul>\n<p>What I've tried:<\/p>\n<ul>\n<li>I have manually checked the CA cert, and more than half the time I can successfully run experiments. As such I don't think it's the same as <a href=\"https:\/\/stackoverflow.com\/questions\/49100986\/certbot-could-not-find-a-suitable-tls-ca-certificate-bundle-archlinux\">this<\/a> or <a href=\"https:\/\/stackoverflow.com\/questions\/46119901\/python-requests-cant-find-a-folder-with-a-certificate-when-converted-to-exe\">this<\/a>.<\/li>\n<\/ul>\n<p>Stacktrace<\/p>\n<pre><code>Message: 'handle_request: stop_status'                                                                                                                                      [854\/1967]Arguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1085, in emit\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1065, in flush\nPermissionError: [Errno 13] Permission denied\nCall stack:\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 890, in _bootstrap\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 932, in _bootstrap_inner\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 54, in run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 95, in _run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py&quot;, line 280, in _process\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 175, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 183, in send_request\nMessage: 'send_request: stop_status'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 24, in wrapper\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 681, in check_stop_requested\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 127, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 119, in post\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 61, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 416, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 227, in cert_verify\nOSError: Could not find a suitable TLS CA certificate bundle, invalid path: \/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/certifi\/cacert.pem\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1620775947520,
        "Question_score":0,
        "Question_tags":"python-requests|ssl-certificate|kerberos|wandb",
        "Question_view_count":468,
        "Owner_creation_time":1519031032163,
        "Owner_last_access_time":1663673544740,
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":47,
        "Owner_down_votes":5,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67495547",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71469200,
        "Question_title":"Weights and Biases - cumulative max (highwater mark) for distributed training and sweep",
        "Question_body":"<p>I have an algorithm that I run 10 times, and return the best run by a cumulative maximum - So for each run, I return the highest validation score of the entire run. For example, this graph:\n<a href=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" alt=\"actual validation\" \/><\/a><\/p>\n<p>turns to this graph:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/csUOd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/csUOd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran 7 of these, and grouped them together aggregating with maximum. However, since each experiment validates at different timestep, the resulting graph is not a cumulative maximum of the entire 7 runs. That happens because at each validation point, not all runs are present:\n<a href=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What I would like to have is something like this:\n<a href=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol>\n<li>Is this achievable?<\/li>\n<li>How can I set a sweep that uses the cumulative validation of the entire experiment (not not a single trial)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647267422323,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":101,
        "Owner_creation_time":1553508349353,
        "Owner_last_access_time":1648824868747,
        "Owner_location":"Israel",
        "Owner_reputation":86,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71469200",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73617230,
        "Question_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Question_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662443319723,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":35,
        "Owner_creation_time":1352429442633,
        "Owner_last_access_time":1664036259503,
        "Owner_location":null,
        "Owner_reputation":740,
        "Owner_up_votes":46,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662569956687,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69870886,
        "Question_title":"How to exit the login of wandb?Two people have two accounts on one machine...Anyway other than relogin?",
        "Question_body":"<p>There are two people using wandb api on one machine. If one forgets to relogin before running, the other one's run will be shown in the prevois one's account. I wonder if there is a way to exit the login state and then the other one will know that he\/ she hasn't login before he\/her starts running programs. IN A HURRY for help about this.\nThe senoir students in my lab ask me to try to do so but I simply don't know how and I couldn't find a answer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636274174763,
        "Question_score":0,
        "Question_tags":"authentication|exit|conflict|wandb",
        "Question_view_count":700,
        "Owner_creation_time":1627886181520,
        "Owner_last_access_time":1642858256430,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69870886",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64093979,
        "Question_title":"Weights and Biases: Login and network errors",
        "Question_body":"<p>I recently installed Weights and Biases (wandb) for recording the metrics of my machine learning projects. Everything worked fine when connected to wandb cloud instance or when I used a local docker image. Now, when I tried to access my local wandb instance from over the network, I started to get API error messages. However, I also noticed that wandb was trying to access my server using port 80, instead of 8080. I installed wandb client on a new cloud server and tried to access my server from there. Still, same error message shown below.<\/p>\n<p>This error happens when I use the command: <code>wandb login host=https:\/\/api.wandb.ai<\/code>\nI have tried to delete the .netrc file where the api settings are stored and re-installed wandb. Still same error. Using wandb version 0.10.2 on Ubuntu 18.04; Also, tried downgrading to version 0.8.36, no change.\nIf I try the command: <code>wandb login --relogin<\/code>, I get the same error.<\/p>\n<p>Is there some way to reset wandb so it forgets all these settings, or to resolve this issue directly?<\/p>\n<p>Many thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>\n<pre><code>Retry attempt failed:\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 160, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 84, in create_connection\n    raise err\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 677, in urlopen\n    chunked=chunked,\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 392, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1277, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1323, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1272, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1032, in _send_output\n    self.send(msg)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 972, in send\n    self.connect()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 187, in connect\n    conn = self._new_conn()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 172, in _new_conn\n    self, &quot;Failed to establish a new connection: %s&quot; % e\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 449, in send\n    timeout=timeout\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 727, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/retry.py&quot;, line 439, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/old\/retry.py&quot;, line 96, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/internal\/internal_api.py&quot;, line 128, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 119, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\nwandb: Network error (ConnectionError), entering retry loop. See wandb\/debug-internal.log for full traceback.\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/bin\/wandb&quot;, line 8, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 72, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 212, in login\n    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 29, in login\n    anonymous=anonymous, key=key, relogin=relogin, host=host, force=force\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 128, in _login\n    apikey.write_key(settings, key)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/lib\/apikey.py&quot;, line 223, in write_key\n    raise ValueError(&quot;API key must be 40 characters long, yours was %s&quot; % len(key))\nValueError: API key must be 40 characters long, yours was 26\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601246511380,
        "Question_score":4,
        "Question_tags":"python|wandb",
        "Question_view_count":6277,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1661849922332,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64093979",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_time":1528765704783,
        "Owner_last_access_time":1635485916997,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1634729204572,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72951797,
        "Question_title":"Can you pass agent-specific parameters to a wandb sweep?",
        "Question_body":"<p>I would like to be able to pass a particular parameter with a different value on each agent in my wandb sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657626404050,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":42,
        "Owner_creation_time":1522603483940,
        "Owner_last_access_time":1663947359087,
        "Owner_location":"Paris, France",
        "Owner_reputation":66,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72951797",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67696204,
        "Question_title":"AttributeError: 'NoneType' object has no attribute '_global_run_stack'",
        "Question_body":"<p><strong>Description<\/strong><\/p>\n<p>I am using PTAN library with an A3C model and I am trying to work with <strong>wandb sweep<\/strong> but I've encountered some weird problems, and I am not sure if it's a bug regarding sweep (because if I am going to use just a simple model without any threads involving is going to work properly) or I am doing something wrong.<\/p>\n<p><strong>How to reproduce<\/strong><\/p>\n<p><em><strong>training function:<\/strong><\/em><\/p>\n<pre><code>def train(conf):\n    batch = []\n    step_idx = 0\n    epoch = conf['epochs']\n    try:\n        with commune.RewardTracker(writer, stop_reward=conf['reward_bound']) as tracker:\n            with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n                while True:\n                    if step_idx == epoch:\n                        break\n                    train_entry = train_queue.get()\n                    if isinstance(train_entry, TotalReward):\n                        if tracker.reward(train_entry.reward, step_idx):\n                            break\n                        continue\n                    if isinstance(train_entry, TotalProfit):\n                        tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n                        continue\n                    step_idx += 1\n                    if step_idx % 100 == 0:\n                        torch.save(net.state_dict(), os.path.join(SAVING_FOLDER, PROJECT_NAME))\n\n                    batch.append(train_entry)\n                    if len(batch) &lt; conf['batch_size']:\n                        continue\n\n                    states_v, actions_t, vals_ref_v = commune.unpack_batch(batch, net,\n                                                                           last_val_gamma=conf['gamma'] ** conf['reward_steps'],\n                                                                           device=device)\n                    batch.clear()\n\n                    optimizer.zero_grad()\n                    logits_v, value_v = net(states_v)\n\n                    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n\n                    log_prob_v = F.log_softmax(logits_v, dim=1)\n                    adv_v = vals_ref_v - value_v.detach()\n                    log_prob_actions_v = adv_v * log_prob_v[range(conf['batch_size']), actions_t]\n                    loss_policy_v = -log_prob_actions_v.mean()\n\n                    prob_v = F.softmax(logits_v, dim=1)\n                    entropy_loss_v = conf['entropy_beta'] * (prob_v * log_prob_v).sum(dim=1).mean()\n\n                    loss_v = entropy_loss_v + loss_value_v + loss_policy_v\n                    loss_v.backward()\n                    nn_utils.clip_grad_norm_(net.parameters(), conf['clip_grad'])\n                    optimizer.step()\n\n                    tb_tracker.track(&quot;advantage&quot;, adv_v, step_idx)\n                    tb_tracker.track(&quot;values&quot;, value_v, step_idx)\n                    tb_tracker.track(&quot;batch_rewards&quot;, vals_ref_v, step_idx)\n                    tb_tracker.track(&quot;loss_entropy&quot;, entropy_loss_v, step_idx)\n                    tb_tracker.track(&quot;loss_policy&quot;, loss_policy_v, step_idx)\n                    tb_tracker.track(&quot;loss_value&quot;, loss_value_v, step_idx)\n                    tb_tracker.track(&quot;loss_total&quot;, loss_v, step_idx)\n    finally:\n        for p in data_proc_list:\n            p.terminate()\n            p.join()\n<\/code><\/pre>\n<p><strong>main function:<\/strong><\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    mp.set_start_method('fork')\n    device = torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)\n\n    with open(r'sweep_config.yaml') as file:\n        sweep_config = yaml.load(file, Loader=yaml.FullLoader)\n\n    logs_dir_name = &quot;a3c_stock&quot;\n    wandb.tensorboard.patch(root_logdir=logs_dir_name)\n\n    sweep_id = wandb.sweep(sweep_config, project=&quot;sweep_project&quot;, entity=&quot;vildnex&quot;)\n    wandb.init(config=config_default)\n\n    config = wandb.config\n\n    writer = SummaryWriter(comment=logs_dir_name)\n\n    env = make_env(config)\n    net = commune.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n    net.share_memory()\n\n    if not os.path.isdir(SAVING_FOLDER):\n        os.mkdir(SAVING_FOLDER)\n\n    if os.path.isfile(os.path.join(SAVING_FOLDER, PROJECT_NAME)):\n        net.load_state_dict(torch.load(os.path.join(SAVING_FOLDER, PROJECT_NAME), map_location=device))\n\n    optimizer = optim.RMSprop(net.parameters(), lr=config.learning_rate, eps=1e-3)\n\n    train_queue = mp.Queue(maxsize=config.processes_count)\n    data_proc_list = []\n    dict_conf = dict(config)\n    for _ in range(config.processes_count):\n        data_proc = mp.Process(target=data_func, args=(net, device, train_queue, dict_conf))\n        data_proc.start()\n        data_proc_list.append(data_proc)\n\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>Exception in thread Thread-6:\nTraceback (most recent call last):\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 303, in _run_job\n    self._function()\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 191, in &lt;lambda&gt;\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 105, in train\n    tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/commune.py&quot;, line 118, in profits\n    self.writer.add_scalar(&quot;total_profit&quot;, total_profit, frame)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 344, in add_scalar\n    self._get_file_writer().add_summary(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 250, in _get_file_writer\n    self.file_writer = FileWriter(self.log_dir, self.max_queue,\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 60, in __init__\n    self.event_writer = EventFileWriter(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 157, in __init__\n    _notify_tensorboard_logdir(logdir, save=save, root_logdir=root_logdir_arg)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 167, in _notify_tensorboard_logdir\n    wandb.run._tensorboard_callback(logdir, save=save, root_logdir=root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 804, in _tensorboard_callback\n    self._backend.interface.publish_tbdata(logdir, save, root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 202, in publish_tbdata\n    self._publish(rec)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 518, in _publish\n    raise Exception(&quot;The wandb backend process has shutdown&quot;)\nException: The wandb backend process has shutdown\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 954, in _bootstrap_inner\n    self.run()\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 892, in run\n    self._target(*self._args, **self._kwargs)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 308, in _run_job\n    wandb.finish(exit_code=1)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 2374, in finish\n    wandb.run.finish(exit_code=exit_code)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 1144, in finish\n    if self._wl and len(self._wl._global_run_stack) &gt; 0:\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py&quot;, line 234, in __getattr__\n    return getattr(self._instance, name)\nAttributeError: 'NoneType' object has no attribute '_global_run_stack'\n<\/code><\/pre>\n<p><strong>Environment<\/strong><\/p>\n<ul>\n<li>OS: Manjaro 5.21.5<\/li>\n<li>Environment: PyCharm Local<\/li>\n<li>Python Version: 3.9<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1621980760977,
        "Question_score":0,
        "Question_tags":"python|pytorch|sweeper|wandb",
        "Question_view_count":1192,
        "Owner_creation_time":1479249043857,
        "Owner_last_access_time":1664055565850,
        "Owner_location":null,
        "Owner_reputation":1383,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":177,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67696204",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71617315,
        "Question_title":"Wandb line plots only show bar charts after refresh",
        "Question_body":"<p>My weights and biases (wandb) panels (e.g. for loss) shortly show line plots (x: steps, y: loss), then refresh (showing a spinner for some time) and then only show bar charts.<\/p>\n<p>Editing such panels either shows (a) &quot;Select runs that logged eval\/loss\nto visualize data in this line chart.&quot; on the left or (b) &quot;Showing a bar chart instead of a line chart because all logged values are length one.&quot; on the right.<\/p>\n<p>Does that mean that (a) a value <code>eval\/loss<\/code> is not found for the runs, or (b) only one value is given per run? How can I change this? But why was there a real line plot shown for about a second, before the panel refreshes? Where there values dropped? Why?<\/p>\n<p>Panels shortly show line plots: <a href=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Then panels refresh and only show bar charts: <a href=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Code<\/h2>\n<p>I am using huggingface transformers <code>TrainingArguments<\/code> with argument <code>report_to=&quot;wandb&quot;<\/code> (but with the default <code>logging_steps<\/code> of 500). I am doing 10-fold cross validation without any explicit <code>wandb.log<\/code> call within the cross validation loop. I do all of this in the <code>train()<\/code> function, which has as last command <code>wandb.finish()<\/code>. <code>train()<\/code> is called via <code>wandb.agent(sweep_id, train)<\/code> as I am using all of this within a large sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648212833057,
        "Question_score":0,
        "Question_tags":"machine-learning|pytorch|huggingface-transformers|wandb",
        "Question_view_count":231,
        "Owner_creation_time":1305196069933,
        "Owner_last_access_time":1663753090530,
        "Owner_location":"Karlsruhe, Germany",
        "Owner_reputation":6763,
        "Owner_up_votes":937,
        "Owner_down_votes":28,
        "Owner_views":709,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71617315",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71312243,
        "Question_title":"Log metrics with configuation in Pytorch Lightning using w&b",
        "Question_body":"<p>I am using PyTorch Lightning together with w&amp;b and trying associate metrics with a finite set of configurations. In the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/common\/lightning_module.html\" rel=\"nofollow noreferrer\"><code>LightningModule<\/code><\/a> class I have defined the <code>test_step<\/code> as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Assuming (for simplicity) that the batch size is 1, this will log the accuracy for 1 sample and it will be displayed as a chart in the w&amp;b dashboard.<\/p>\n<p>I would like to associate this accuracy with some configuration of the experimental environment. This configuration might include BDP factor, bandwith delay, queue_size, location, etc. I don't want to plot the configurations I just want to be able to filter or group the accuracy by some configuration value.<\/p>\n<p>The only solution I can come up with is to add these configurations as a querystring:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  # read values in config file\n  # ...\n\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/BDP=2&amp;delay=10ms&amp;queue_size=10&amp;topology=single\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Is there a better solution for this that integrates my desired functionality of being able to group and filter by values like BDP?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646154863157,
        "Question_score":2,
        "Question_tags":"pytorch|metrics|pytorch-lightning|wandb",
        "Question_view_count":315,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":1646156240832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71312243",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72555212,
        "Question_title":"wandb pytorch: top1 accuracy per class",
        "Question_body":"<p>I have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using wandb . I have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. I am unable to achieve, are there any way to achieve it?<\/p>\n<p><strong>Validation Loader<\/strong><\/p>\n<pre><code> val_loaders = []\n    for nuisance in val_nuisances:\n        val_loaders.append((nuisance, torch.utils.data.DataLoader(\n            datasets.ImageFolder(os.path.join(valdir, nuisance), transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n            batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True,\n        )))\n\n\nval_nuisances = ['shape', 'pose', 'texture', 'context', 'weather']\n<\/code><\/pre>\n<p><strong>Validation Loop<\/strong><\/p>\n<pre><code>def validate(val_loaders, model, criterion, args):\n    overall_top1 = 0\n    for nuisance, val_loader in val_loaders:\n        batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n        losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n        top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n        top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n        progress = ProgressMeter(\n            len(val_loader),\n            [batch_time, losses, top1, top5],\n            prefix=f'Test {nuisance}: ')\n\n        # switch to evaluate mode\n        model.eval()\n\n        with torch.no_grad():\n            end = time.time()\n            for i, (images, target) in enumerate(val_loader):\n                if args.gpu is not None:\n                    images = images.cuda(args.gpu, non_blocking=True)\n                if torch.cuda.is_available():\n                    target = target.cuda(args.gpu, non_blocking=True)\n\n                # compute output\n                output = model(images)\n                loss = criterion(output, target)\n\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n                losses.update(loss.item(), images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % args.print_freq == 0:\n                    progress.display(i)\n\n            progress.display_summary()\n        overall_top1 += top1.avg\n    overall_top1 \/= len(val_loaders)\n    return top1.avg\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654753323610,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":71,
        "Owner_creation_time":1420474938783,
        "Owner_last_access_time":1663917040323,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":2201,
        "Owner_up_votes":186,
        "Owner_down_votes":3,
        "Owner_views":556,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72555212",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73109071,
        "Question_title":"How do I log a confusion matrix into Wanddb?",
        "Question_body":"<p>I'm using pytorch lightning, and at the end of each epoch, I create a confusion matrix from torchmetrics.ConfusionMatrix (see code below). I would like to log this into Wandb, but the Wandb confusion matrix logger only accepts y_targets and y_predictions. Does anyone know how to extract the updated confusion matrix y_targets and y_predictions from a confusion matrix, or alternatively give Wandb my updated confusion matrix in a way that it can be processed into eg a heatmap within wandb?<\/p>\n<pre><code>class ClassificationTask(pl.LightningModule):\n    def __init__(self, model, lr=1e-4, augmentor=augmentor):\n        super().__init__()\n        self.model = model\n        self.lr = lr\n        self.save_hyperparameters() #not being used at the moment, good to have ther in the future\n        self.augmentor=augmentor\n        \n        self.matrix = torchmetrics.ConfusionMatrix(num_classes=9)\n        \n        self.y_trues=[]\n        self.y_preds=[]\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x=self.augmentor(x)#.to('cuda')\n        y_pred = self.model(x)\n        loss = F.cross_entropy(y_pred, y,)  #weights=class_weights_tensor\n        \n\n        acc = accuracy(y_pred, y)\n        metrics = {&quot;train_acc&quot;: acc, &quot;train_loss&quot;: loss}\n        self.log_dict(metrics)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {&quot;val_acc&quot;: acc, &quot;val_loss&quot;: loss, }\n        self.log_dict(metrics)\n        return metrics\n    \n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        self.matrix.update(y_hat,y)\n        return loss, acc\n    \n    def validation_epoch_end(self, outputs):\n        confusion_matrix = self.matrix.compute()\n        wandb.log({&quot;my_conf_mat_id&quot; : confusion_matrix})\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam((self.model.parameters()), lr=self.lr)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658751297303,
        "Question_score":2,
        "Question_tags":"pytorch|confusion-matrix|pytorch-lightning|wandb",
        "Question_view_count":109,
        "Owner_creation_time":1626958179237,
        "Owner_last_access_time":1663922047490,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73109071",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68903269,
        "Question_title":"logging learning rate schedule in keras via weights and biases",
        "Question_body":"<p>I am training a keras model and using a custom learning rate scheduler for the optimizer (of type tf.keras.optimizers.schedules.LearningRateSchedule), and i want to log the learning rate change via the weights&amp;biases framework.\ni couldn't find how to pass it to the WandbCallback object or log it in any way<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1629789446037,
        "Question_score":5,
        "Question_tags":"python|tensorflow|keras|deep-learning|wandb",
        "Question_view_count":968,
        "Owner_creation_time":1541621385633,
        "Owner_last_access_time":1644833613247,
        "Owner_location":"Israel",
        "Owner_reputation":59,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1629798930663,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68903269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659671305703,
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_time":1587186468850,
        "Owner_last_access_time":1663918152847,
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1660033763876,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1659781694627,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66606294,
        "Question_title":"PyTorch Lightning wants create a folder on import due to usage of wandb, which raises error on AWS Lambda",
        "Question_body":"<p>So I want to build a Docker image with PyTorch Lightning that can be used with AWS lambda. However, when the function is invoked it raises an OS Error, that claims it uses a Read-only file system and wandb.py wants to write something.<\/p>\n<p>I tried these things:<\/p>\n<ol>\n<li>Overwrite the wandb.py file of pytroch lightning, that it does not init wandb --&gt; Raises error<\/li>\n<li>Execute a python script in Dockerfile, that the files are created on docker build and exist, when invoking the lambda function --&gt; Same OS error<\/li>\n<\/ol>\n<p>Does someone know a way to skip the wandb.py?<\/p>\n<p>This is the error message:<\/p>\n<pre><code>START RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772 Version: $LATEST\nOpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n[ERROR] OSError: [Errno 30] Read-only file system: '\/home\/sbx_user1051'\nTraceback (most recent call last):\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 234, in load_module\n    return load_source(name, filename, file)\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 171, in load_source\n    module = _load(spec)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 702, in _load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/task\/inference.py&quot;, line 5, in &lt;module&gt;\n    import pytorch_lightning as pl\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/__init__.py&quot;, line 63, in &lt;module&gt;\n    from pytorch_lightning.callbacks import Callback\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/__init__.py&quot;, line 25, in &lt;module&gt;\n    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/swa.py&quot;, line 26, in &lt;module&gt;\n    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/__init__.py&quot;, line 18, in &lt;module&gt;\n    from pytorch_lightning.trainer.trainer import Trainer\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py&quot;, line 30, in &lt;module&gt;\n    from pytorch_lightning.loggers import LightningLoggerBase\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/__init__.py&quot;, line 31, in &lt;module&gt;\n    from pytorch_lightning.loggers.wandb import _WANDB_AVAILABLE, WandbLogger  # noqa: F401\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py&quot;, line 34, in &lt;module&gt;\n    import wandb\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/__init__.py&quot;, line 131, in &lt;module&gt;\n    api = InternalApi()\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/apis\/internal.py&quot;, line 17, in __init__\n    self.api = InternalApi(*args, **kwargs)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 73, in __init__\n    self._settings = Settings(\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 25, in __init__\n    self._global_settings.read([Settings._global_path()])\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 105, in _global_path\n    util.mkdir_exists_ok(config_dir)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 687, in mkdir_exists_ok\n    os.makedirs(path)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 223, in makedirs\n    mkdir(name, mode)\nEND RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772\nREPORT RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772  Duration: 27000.33 ms   Billed Duration: 27001 ms   Memory Size: 10240 MB   Max Memory Used: 241 MB \nUnknown application error occurred\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615578448627,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|pytorch|pytorch-lightning|wandb",
        "Question_view_count":202,
        "Owner_creation_time":1510177189697,
        "Owner_last_access_time":1664037019800,
        "Owner_location":"Dortmund, Deutschland",
        "Owner_reputation":1194,
        "Owner_up_votes":149,
        "Owner_down_votes":11,
        "Owner_views":119,
        "Question_last_edit_time":1661850419560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66606294",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69425994,
        "Question_title":"is there any way to scale axis of plots in wandb?",
        "Question_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633272788153,
        "Question_score":1,
        "Question_tags":"wandb",
        "Question_view_count":604,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633345375676,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69425994",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69147788,
        "Question_title":"Weights & Biases with Transformers and PyTorch?",
        "Question_body":"<p>I'm training an NLP model at work (e-commerce SEO) applying a <code>BERT<\/code> variation for portuguese language (<code>BERTimbau<\/code>) through <code>Transformers<\/code> by Hugging Face.<\/p>\n<p>I didn't used the <code>Trainer<\/code> from Transformers API. I used <code>PyTorch<\/code> to set all parameters through <code>DataLoader.utils<\/code> and <code>adamW<\/code>. I trained my model using <code>run_glue.py<\/code>.<\/p>\n<p><strong>I'm training with a VM on GCP using Jupyterlab<\/strong>. I know that I can use Weights &amp; Biases both for PyTorch and Transformers. But I don't know exactly how to set it using <code>run_glue.py<\/code>. It's my first time using Weights &amp; Biases.<\/p>\n<p><em>After preprocessing and splitting train and test through Sklearn, my code is as it follows<\/em>:<\/p>\n<pre><code>from transformers import BertTokenizer\nimport torch\n#import torchvision\nfrom torch.utils.data import Dataset, TensorDataset\nimport collections.abc as container_abcs\n\n# To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n# Constructs a BERT tokenizer. Based on WordPiece. \n# The tokenization must be performed by the tokenizer included with BERT\ntokenizer = BertTokenizer.from_pretrained('neuralmind\/bert-large-portuguese-cased', \n                                          do_lower_case=True)\n\n# Tokenize all of the sentences and map the tokens to thier word IDs. To convert all the titles from text into encoded form.\n# We will use padding and truncation because the training routine expects all tensors within a batch to have the same dimensions.\nencoded_data_train = tokenizer.batch_encode_plus(\n    df[df.data_type=='train'].text.values, \n    add_special_tokens=True,                            # Add '[CLS]' and '[SEP]'. Sequences encoded with special tokens relative to their model\n    return_attention_mask=True,                         # Return mask according to specific tokenizer defined by max_length\n    pad_to_max_length=True,                             # Pad &amp; truncate all sentences. Pad all titles to certain maximum length                  \n    max_length=128,                                     # Do not need to set max_length=256\n    return_tensors='pt'                                 # Set to use PyTorch tensors\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df.data_type=='val'].text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=128, \n    return_tensors='pt'\n)\n\n# Split the data into input_ids, attention_masks and labels. \n# Converting the input data to the tensor , which can be feeded to the model\ninput_ids_train = encoded_data_train['input_ids']                    # Add the encoded sentence to the list.\nattention_masks_train = encoded_data_train['attention_mask']         # And its attention mask (simply differentiates padding from non-padding).\nlabels_train = torch.tensor(df[df.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type=='val'].label.values)\n\n# Create training data and validation data\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n<\/code><\/pre>\n<pre><code>from transformers import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained(&quot;neuralmind\/bert-large-portuguese-cased&quot;,  # Select your pretrained Model\n                                                      num_labels=len(label_dict),                # Labels tp predict\n                                                      output_attentions=False,                   # Whether the model returns attentions weights. We don\u2019t really care about output_attentions. \n                                                      output_hidden_states=False)                # Whether the model returns all hidden-states. We also don\u2019t need output_hidden_states\n<\/code><\/pre>\n<pre><code>from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32                                                              # Set your batch size according to your GPU memory      \n\ndataloader_train = DataLoader(dataset_train                                  # Use DataLoader to Optimize your model\n                              ,sampler=RandomSampler(dataset_train)          # Random Sampler from your dataset\n                              ,batch_size=batch_size)                        # If your batch_size is too high you will get a warning when you run the model \n                              #,num_workers=4                                # Number of cores\n                              #,pin_memory=True)                             # Use GPU to send your batch \n\ndataloader_validation = DataLoader(dataset_val \n                                   ,sampler=SequentialSampler(dataset_val)   # For validation the order doesn't matter. Sequential Sampler consumes less GPU.\n                                   ,batch_size=batch_size) \n                                   #,num_workers=4\n                                   #,pin_memory=True)\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n# hyperparameters\n# To construct an optimizer, we have to give it an iterable containing the parameters to optimize. \n# Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.\noptimizer = AdamW(model.parameters(),  # AdamW is a class from the huggingface library (as opposed to pytorch) \n                  lr=2e-5,             # args.learning_rate - default is 5e-5\n                  eps=1e-8)            # args.adam_epsilon  - default is 1e-8\n\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \nepochs = 2\n\n# Create the learning rate scheduler that decreases linearly from the initial learning rate set in the optimizer to 0, \n# after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,                               # Default value in run_glue.py                           \n                                            num_training_steps=len(dataloader_train)*epochs)  # Total number of training steps is [number of batches] x [number of epochs]. \n                                                                                              # Note that this is not the same as the number of training samples).\n<\/code><\/pre>\n<pre><code>from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')\n\n<\/code><\/pre>\n<p>And here follows <code>run_glue.py<\/code>:<\/p>\n<pre><code>import random\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\n# from tqdm.notebook import trange, tqdm\n\n'''\nThis training code is based on the 'run_glue.py' script here:\nhttps:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n'''\n\n# Just right before the actual usage select your hardware\ndevice = torch.device('cuda') # or cpu\nmodel = model.to(device)      # send your model to your hardware\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, validation and timings.\ndef evaluate(dataloader_val):\n    '''\n    Put the model in evaluation mode--the dropout layers behave differently\n    during evaluation.\n    '''\n    \n    model.eval()\n    \n    loss_val_total = 0 # Tracking variables \n\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        '''\n         Unpack this training batch from our dataloader.         \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels \n        '''\n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n        \n        '''\n        Tell pytorch not to bother with constructing the compute graph during\n        the forward pass, since this is only needed for backprop (training).\n        '''\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        '''\n        Perform a forward pass (evaluate the model on this training batch).\n        The documentation for this `model` function is here: \n        https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        This will return the loss (rather than the model output) \n        because we have provided the `labels`.\n        It returns different numbers of parameters depending on what arguments\n        arge given and what flags are set. For our useage here, it returns\n        the loss (because we provided labels) and the &quot;logits&quot;--the model\n        outputs prior to activation.\n        '''\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item() # Accumulate the validation loss.\n\n                \n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) # Calculate the average loss over all of the batches.\n\n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n\n    # ========================================\n    #               Training\n    # ========================================\n\n# For each epoch...\nfor epoch in tqdm(range(1, epochs+1)):    \n    '''\n    Put the model into training mode. Don't be mislead--the call to \n    `train` just changes the *mode*, it doesn't *perform* the training.\n    `dropout` and `batchnorm` layers behave differently during training\n    vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    '''\n    \n    model.train()        # Put the model into training mode.\n\n    \n    loss_train_total = 0 # Reset the total loss for this epoch.\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()        \n        '''\n        Always clear any previously calculated gradients before performing a\n        backward pass. PyTorch doesn't do this automatically because \n        accumulating the gradients is &quot;convenient while training RNNs&quot;. \n        (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        '''\n        \n        batch = tuple(b.to(device) for b in batch)\n        '''\n        Unpack this training batch from our dataloader. \n        \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels\n         '''\n        \n        inputs = {'input_ids':      batch[0], #.to(device)\n                  'attention_mask': batch[1], #.to(device)\n                  'labels':         batch[2], #.to(device)\n                 }       \n\n        outputs = model(**inputs)\n        \n\n        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n        loss_train_total += loss.item()         # Accumulate the training loss over all of the batches so that we can\n                                                # calculate the average loss at the end. `loss` is a Tensor containing a\n                                                # single value; the `.item()` function just returns the Python value \n                                                # from the tensor.\n        loss.backward() # Perform a backward pass to calculate the gradients.\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Clip the norm of the gradients to 1.0.\n                                                                   # This is to help prevent the &quot;exploding gradients&quot; problem.\n                                                                   # modified based on their gradients, the learning rate, etc.\n                \n        optimizer.step()        # Update parameters and take a step using the computed gradient.\n                                # The optimizer dictates the &quot;update rule&quot;--how the parameters are\n                                # modified based on their gradients, the learning rate, etc.\n                \n        scheduler.step()        # Update the learning rate.\n\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model') # Save Model\n        \n    tqdm.write(f'\\nEpoch {epoch}') # Show running epoch\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train) # Calculate the average loss over all of the batches.\n           \n    tqdm.write(f'Training loss: {loss_train_avg}') # Show loss average\n    \n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n    \n    # Record all statistics from this epoch.\n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631409889517,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|pytorch|huggingface-transformers|wandb",
        "Question_view_count":427,
        "Owner_creation_time":1618668550253,
        "Owner_last_access_time":1663440294737,
        "Owner_location":"S\u00e3o Paulo - Brazil",
        "Owner_reputation":113,
        "Owner_up_votes":99,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1661850382607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69147788",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67202711,
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619035214083,
        "Question_score":1,
        "Question_tags":"python|pytorch|wandb",
        "Question_view_count":840,
        "Owner_creation_time":1577734207070,
        "Owner_last_access_time":1663602351910,
        "Owner_location":null,
        "Owner_reputation":422,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620426900467,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71990430,
        "Question_title":"Wandb throws Permission denied error although I am logged in",
        "Question_body":"<p>I am using the cleanrl library, in particular the script <a href=\"https:\/\/github.com\/vwxyzjn\/cleanrl\/blob\/master\/cleanrl\/dqn_atari.py\" rel=\"nofollow noreferrer\">dqn_atari.py<\/a>  dqn_atari.py where  I followed the  <a href=\"https:\/\/docs.cleanrl.dev\/advanced\/resume-training\/\" rel=\"nofollow noreferrer\">instructions<\/a> in order to save and load the target and Q-network.<\/p>\n<p>I am running it locally within a conda environment.<\/p>\n<p>I haven't loaded something before, so the error may be due to my wandb configuration. The error is &quot;wandb: ERROR Permission denied to access wandb_entity\/wandb_project_name\/project_id&quot; and appears on line:<\/p>\n<pre><code>model = run.file(&quot;agent.pt&quot;)\n<\/code><\/pre>\n<p>The full  output is:<\/p>\n<pre><code>wandb: Currently logged in as: elena (use `wandb login --relogin` to force relogin)\n    wandb: Tracking run with wandb version 0.12.15\nwandb: Run data is saved locally in \/home\/elena\/workspace\/playground\/cleanrl\/wandb\/run-20220424_180429-2moec0qp\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Resuming run BreakoutNoFrameskip-v4__dqn-save__1__1650816268\nwandb: \u2b50 View project at https:\/\/wandb.ai\/elena\/test\nwandb:  View run at https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nA.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n[Powered by Stella]\n\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/stable_baselines3\/common\/buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 28.24GB &gt; 8.35GB\n  warnings.warn(\nwandb: ERROR Permission denied to access elena\/test\/2moec0qp\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/public.py&quot;, line 2428, in download\n    util.download_file_from_url(path, self.url, Api().api_key)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 1197, in download_file_from_url\n    response.raise_for_status()\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/requests\/models.py&quot;, line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/td_network.pt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;cleanrl\/dqn_atari_save.py&quot;, line 184, in &lt;module&gt;\n    model.download(f&quot;models\/{args.exp_name}\/&quot;)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 58, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\nwandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb:                                                                                \nwandb: \nwandb: Run history:\nwandb: global_step \u2581\nwandb: \nwandb: Run summary:\nwandb: charts\/episodic_return 0\nwandb:         charts\/epsilon 0.01\nwandb:          charts\/update 1969\nwandb:            global_step 0\nwandb: \nwandb: Synced BreakoutNoFrameskip-v4__dqn-save__1__1650816268: https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nwandb: Synced 3 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\nwandb: Find logs at: .\/wandb\/run-20220424_180429-2moec0qp\/logs\n<\/code><\/pre>\n<p>So as you can see I am logged in and I can see the files under &quot;https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp?workspace=user-elena&quot;. Something that drew my attention was &quot;requests.exceptions.HTTPError: 404 Client Error: Not Found for url: <a href=\"https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt%22\" rel=\"nofollow noreferrer\">https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt&quot;<\/a>. This path indeed looks different from the https path, but maybe this is not the issue?\nAny ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650818668313,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":596,
        "Owner_creation_time":1418505278743,
        "Owner_last_access_time":1663932736703,
        "Owner_location":null,
        "Owner_reputation":319,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990430",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71476873,
        "Question_title":"How to log additional single variable with wandb and huggingface transformers",
        "Question_body":"<p>I am using Huggingface's Transformers Trainer object and I really like the support it has for wandb.<\/p>\n<p>For my use case, I have subclassed the Trainer and, in addition to the values that are logged by default, I would like to keep track of one additional variable that gets updated at each training step.<\/p>\n<p>What is the easiest way to add tracking for a single variable with wandb?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647316549707,
        "Question_score":1,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":169,
        "Owner_creation_time":1484868229953,
        "Owner_last_access_time":1654795122843,
        "Owner_location":null,
        "Owner_reputation":507,
        "Owner_up_votes":61,
        "Owner_down_votes":0,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71476873",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68087139,
        "Question_title":"trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests",
        "Question_body":"<p>When saving a version in Kaggle, I get <strong>StdinNotImplementedError: getpass was called, but this frontend does not support input requests<\/strong> whenever I use the Transformers.Trainer class. The general code I use:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments\ntraining_args = TrainingArguments(params)\ntrainer = Trainer(params)\ntrainer.train()\n<\/code><\/pre>\n<p>And the specific cell I am running now:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments,EarlyStoppingCallback\n\nearly_stopping = EarlyStoppingCallback()\n\ntraining_args = TrainingArguments(\n    output_dir=OUT_FINETUNED_MODEL_PATH,          \n    num_train_epochs=20,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    warmup_steps=0,                \n    weight_decay=0.01,               \n    logging_dir='.\/logs',            \n    logging_steps=100,\n    evaluation_strategy=&quot;steps&quot;,\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=&quot;eval_loss&quot;,\n    greater_is_better=False\n    \n)\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,             \n    callbacks=[early_stopping]\n\n)\n\ntrainer.train()\n<\/code><\/pre>\n<p>When trainer.train() is called, I get the error below, which I do not get if I train with native PyTorch. I understood that the error arises since I am asked to input a password, but no password is asked when using native PyTorch code, nor when using the same code with trainer.train() on Google Colab.\nAny solution would be ok, like:<\/p>\n<ol>\n<li>Avoid being asked the password.<\/li>\n<li>Enable input requests when saving a notebook on Kaggle. After that, if I understood correctly, I would need to go to <a href=\"https:\/\/wandb.ai\/authorize\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/authorize<\/a> (after having created an account) and copy the generated key to console. However, I do not understand why wandb should be necessary since I never explicitly used it so far.<\/li>\n<\/ol>\n<pre><code>wandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 741, in init\n    wi.setup(kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 155, in setup\n    wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 210, in _login\n    wlogin.prompt_api_key()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 144, in prompt_api_key\n    no_create=self._settings.force,\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py&quot;, line 135, in prompt_api_key\n    key = input_callback(api_ask).strip()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py&quot;, line 825, in getpass\n    &quot;getpass was called, but this frontend does not support input requests.&quot;\nIPython.core.error.StdinNotImplementedError: getpass was called, but this frontend does not support input requests.\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    740         wi = _WandbInit()\n--&gt; 741         wi.setup(kwargs)\n    742         except_exit = wi.settings._except_exit\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in setup(self, kwargs)\n    154         if not settings._offline and not settings._noop:\n--&gt; 155             wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in _login(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)\n    209     if not key:\n--&gt; 210         wlogin.prompt_api_key()\n    211 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in prompt_api_key(self)\n    143             no_offline=self._settings.force,\n--&gt; 144             no_create=self._settings.force,\n    145         )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local)\n    134             )\n--&gt; 135             key = input_callback(api_ask).strip()\n    136         write_key(settings, key, api=api)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py in getpass(self, prompt, stream)\n    824             raise StdinNotImplementedError(\n--&gt; 825                 &quot;getpass was called, but this frontend does not support input requests.&quot;\n    826             )\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-82-4d1046ab80b8&gt; in &lt;module&gt;\n     42     )\n     43 \n---&gt; 44     trainer.train()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)\n   1067         model.zero_grad()\n   1068 \n-&gt; 1069         self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)\n   1070 \n   1071         # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in on_train_begin(self, args, state, control)\n    338     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n    339         control.should_training_stop = False\n--&gt; 340         return self.call_event(&quot;on_train_begin&quot;, args, state, control)\n    341 \n    342     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)\n    386                 train_dataloader=self.train_dataloader,\n    387                 eval_dataloader=self.eval_dataloader,\n--&gt; 388                 **kwargs,\n    389             )\n    390             # A Callback can skip the return of `control` if it doesn't change it.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in on_train_begin(self, args, state, control, model, **kwargs)\n    627             self._wandb.finish()\n    628         if not self._initialized:\n--&gt; 629             self.setup(args, state, model, **kwargs)\n    630 \n    631     def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in setup(self, args, state, model, **kwargs)\n    604                     project=os.getenv(&quot;WANDB_PROJECT&quot;, &quot;huggingface&quot;),\n    605                     name=run_name,\n--&gt; 606                     **init_args,\n    607                 )\n    608             # add config parameters (run may have been created manually)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    779             if except_exit:\n    780                 os._exit(-1)\n--&gt; 781             six.raise_from(Exception(&quot;problem&quot;), error_seen)\n    782     return run\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)\n\nException: problem\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624378186880,
        "Question_score":2,
        "Question_tags":"huggingface-transformers|kaggle|getpass|wandb",
        "Question_view_count":440,
        "Owner_creation_time":1624376028980,
        "Owner_last_access_time":1642694800790,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68087139",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69008133,
        "Question_title":"How to plot confidence intervals for different training samples",
        "Question_body":"<p>I am working on running training with different divisions of a training set. The plots that I get (using wandb) are fine, but not quite informative in my opinion and high in variance.\n<a href=\"https:\/\/i.stack.imgur.com\/kux5o.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kux5o.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is there a way to plot the mean of the plots, and then confidence intervals around it? Something similar to the picture below. Alternatively, is there a way to plot variance during training?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/veoWc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/veoWc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630473410597,
        "Question_score":1,
        "Question_tags":"python|visualization|confidence-interval|variance|wandb",
        "Question_view_count":232,
        "Owner_creation_time":1570797361193,
        "Owner_last_access_time":1652428368017,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69008133",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70637798,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641691804600,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|boto3|wandb",
        "Question_view_count":121,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1643119769043,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643119052816,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643119813376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72079835,
        "Question_title":"Pytorch lightning callback for switching dataloader_idx",
        "Question_body":"<p>Is there a callback or something similar used when incrementing the dataloader index? The reason is that I have defined multiple dataloaders and I would like to start a new run with weight and biases for each dataloader.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651433528283,
        "Question_score":0,
        "Question_tags":"pytorch-lightning|wandb",
        "Question_view_count":146,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72079835",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71944710,
        "Question_title":"Multiple lines on same plot with incremental logging - wandb",
        "Question_body":"<p>I am using weights &amp; biases (wandb). I want to group multiple plots into one while using incremental logging, any way to do that?<\/p>\n<p>Say we have 10 metrics, I can add them to the project incrementally, gradually building 10 graphs:<\/p>\n<pre><code>import wandb\nimport math\n\nN_STEPS = 100\n\nwandb.init(project=&quot;someProject&quot;, name=&quot;testMultipleLines&quot;)\nfor epoch in range(N_STEPS):\n    log = {}\n    log['main_metric'] = epoch \/ N_STEPS  # some main metric\n\n    # some other metrics I want to have all on 1 plot\n    other_metrics = {}\n    for j in range(10):\n        other_metrics[f'metric_{j}'] = math.sin(j * math.pi * (epoch \/ N_STEPS))\n    log['other_metrics'] = other_metrics\n\n    wandb.log(log)\n<\/code><\/pre>\n<p>This by default gets presented on the wandb interface as 11 different plots. How can they be grouped through the API (without using the web interface) such that <code>main_metric<\/code> is on one figure and all <code>other_metrics<\/code> are bunched together on a second figure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1650480171990,
        "Question_score":6,
        "Question_tags":"python|machine-learning|plot|wandb",
        "Question_view_count":275,
        "Owner_creation_time":1457253216340,
        "Owner_last_access_time":1663905208120,
        "Owner_location":null,
        "Owner_reputation":667,
        "Owner_up_votes":100,
        "Owner_down_votes":1,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71944710",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72972876,
        "Question_title":"YOLOv5 Evolution Results Not Reproducible wandb",
        "Question_body":"<p>I am running YOLOv5 in a sagemaker notebook.\nThe 10 epoch runs are using the following notebook script making use of the --evolve flag for hyperparameters.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;evolution&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=10\n--evolve=30\n<\/code><\/pre>\n<p>Evolution runs only output one point on the graph at the end of 10 epochs and the outputted hyperparameters do not show reproducible results when running in a 50 epoch run. The blue 50 epoch line showcases using the optimal hyperparameters which should intersect with the highest 10 epoch run, but it doesn't reach anywhere close.\n<a href=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>After finding the optimal hyperparameters I ran a 50 epoch run using those parameters using the following command.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;hyperparam&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--hyp=.\/deepsea-yolov5\/opt\/ml\/input\/data\/hyp.scratch-low.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=50\n<\/code><\/pre>\n<p>However as shown in the picture above, the runs do not intersect with the best-performing hyperparameter run.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657748030047,
        "Question_score":0,
        "Question_tags":"python|yolov5|wandb",
        "Question_view_count":63,
        "Owner_creation_time":1498005510443,
        "Owner_last_access_time":1663651022003,
        "Owner_location":"California, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72972876",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72001154,
        "Question_title":"How to prevent Weights & Biases from saving best model parameters",
        "Question_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650896835733,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|wandb",
        "Question_view_count":204,
        "Owner_creation_time":1514243890900,
        "Owner_last_access_time":1663708014960,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650898428827,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630064062593,
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1630357543656,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14.0,
        "Answer_creation_time":1630122133812,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68952727",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73517766,
        "Question_title":"How to recieve metrics for Object Detection while using pytorch and Weights & Biases?",
        "Question_body":"<p>I have been training and fine tuning few models for detection task on a custom dataset,\nI would like to plot relevant metrics such as mean Average Precision (taking into account the predicted bounding box location and the enclosed object's classification).<\/p>\n<p>I'm using Pytorch and have started using <code>Weights &amp; Biases<\/code> (<a href=\"https:\/\/youtu.be\/G7GH0SeNBMA?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk\" rel=\"nofollow noreferrer\">Weights &amp; Biases integrated with pytorch<\/a>)<\/p>\n<p>For avoiding inventing the wheel, I have used some files from here:<\/p>\n<p><a href=\"https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection<\/a><\/p>\n<p><code>engine.py<\/code> - holds the <strong>train_one_epoch()<\/strong> function<\/p>\n<p><code>cocoeval.py<\/code> - holds the <strong>summarize()<\/strong> function<\/p>\n<p>Now I would like to log those metrics to a <code>Weights &amp; Biases<\/code>,\nso I'll we able to get more clear view and intuition about the fine-tuning phase,but I'm not sure where is the proper place to put the logger invocation.\ncan somebody please assist me?<\/p>\n<pre><code>wandb.watch()\n<\/code><\/pre>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661681279513,
        "Question_score":0,
        "Question_tags":"image-processing|pytorch|torch|wandb",
        "Question_view_count":33,
        "Owner_creation_time":1287433738400,
        "Owner_last_access_time":1664033141413,
        "Owner_location":null,
        "Owner_reputation":610,
        "Owner_up_votes":289,
        "Owner_down_votes":1,
        "Owner_views":130,
        "Question_last_edit_time":1661681944403,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73517766",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71227518,
        "Question_title":"Connecting Julia to Weights & Biases over Python",
        "Question_body":"<p>I am trying to use weights&amp;biases for my models written in Julia. I am using <code>WeightsAndBiasLogger.jl<\/code> and try to test their demo code:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger\n\nargs = (n_epochs=1_000, lr=1e-3)\nlogger = WBLogger(project=&quot;sample-project&quot;)\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>I receive an error: <strong>&quot;ArgumentError: ref of NULL PyObject&quot;<\/strong> (considering the line: logger = WBLogger(project=&quot;sample-project&quot;)\n)<\/p>\n<p>Then I tried to fix this with the following command:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger, PyCall\n\nargs = (n_epochs=1_000, lr=1e-3)\n\nconst logger = PyNULL()\nfunction __init__()\n    copy!(logger, WBLogger(project=&quot;sample-project&quot;))\nend\n\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>It creates the <code>logger<\/code> object, but now the error is:<\/p>\n<p><strong>MethodError: no method matching config!(::PyObject, ::NamedTuple{(:n_epochs, :lr), Tuple{Int64, Float64}})\nClosest candidates are: config!(!Matched::WBLogger, ::Any; kwargs...)<\/strong> (this consider the line: config!()...<\/p>\n<p>So, does anyone know how to solve the issue? Obviously, I am new to Julia, thus I apologize if asking something very stupid. In addition, if you know a better solution to integrate Julia into W&amp;B or any good alternatives, I would be glad to hear it.<\/p>\n<p>PS: Julia ver 1.7.2<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645559620073,
        "Question_score":2,
        "Question_tags":"python|julia|pycall|wandb",
        "Question_view_count":102,
        "Owner_creation_time":1535938696457,
        "Owner_last_access_time":1664053164283,
        "Owner_location":"Roskilde, Denmark",
        "Owner_reputation":1443,
        "Owner_up_votes":339,
        "Owner_down_votes":26,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71227518",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72731861,
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655994282873,
        "Question_score":1,
        "Question_tags":"python|hyperparameters|mlops|wandb",
        "Question_view_count":182,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Question_last_edit_time":1656234648443,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656329528480,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73320449,
        "Question_title":"Plotting the confuison matrix into wandb (pytorch)",
        "Question_body":"<p>I'm training a model and I'm trying to add a confusion matrix, which would be displayed in my <code>wandb<\/code>, but I got lost a bit. Basically, the matrix works; I can print it, but it's not loaded into <code>wandb<\/code>. Everything should be OK, except it's not. Can you please help me? I'm new to all this. Thanks a lot!<\/p>\n<p><strong>the code<\/strong><\/p>\n<pre><code>    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                from sklearn.metrics import f1_score\n                f1_score = f1_score(labels.cpu().data, preds.cpu(), average=None)\n                wandb.log({'F1 score' : f1_score})\n\n                nb_classes = 7\n\n                confusion_matrix = torch.zeros(nb_classes, nb_classes)\n                with torch.no_grad():\n                    for i, (inputs, classes) in enumerate(dataloaders['val']):\n                        inputs = inputs.to(device)\n                        classes = classes.to(device)\n                        outputs = model_ft(inputs)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    for t, p in zip(classes.view(-1), preds.view(-1)):\n                        confusion_matrix[t.long(), p.long()] += 1\n              wandb.log({'matrix' : confusion_matrix})\n                           \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            wandb.log({'epoch loss': epoch_loss,\n                    'epoch acc': epoch_acc})\n            \n            data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n            table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n            wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n\n        \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc, f1_score))\n\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    print('f1_score: {}'.format(f1_score))\n   \n    model.load_state_dict(best_model_wts)\n    return model\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660219911593,
        "Question_score":0,
        "Question_tags":"python|pytorch|conv-neural-network|transfer-learning|wandb",
        "Question_view_count":69,
        "Owner_creation_time":1659890757880,
        "Owner_last_access_time":1660739315393,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660228199172,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70573652,
        "Question_title":"Model stopped training once I introduced << report_to = 'wandb' >> in TrainingArguments",
        "Question_body":"<p>I am downloading the model <a href=\"https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main<\/a> microsoft\/Multilingual-MiniLM-L12-H384 and then using it.<\/p>\n<p>Transformer Version: '4.11.3'<\/p>\n<p>I have written the below code:<\/p>\n<pre><code>import wandb\nwandb.login()\n%env WANDB_LOG_MODEL=true\n\nmodel = tr.BertForSequenceClassification.from_pretrained(&quot;\/home\/pc\/minilm_model&quot;,num_labels=2)\nmodel.to(device)\n\nprint(&quot;hello&quot;)\n\ntraining_args = tr.TrainingArguments(\nreport_to = 'wandb',\noutput_dir='\/home\/pc\/proj\/results2', # output directory\nnum_train_epochs=10, # total number of training epochs\nper_device_train_batch_size=16, # batch size per device during training\nper_device_eval_batch_size=32, # batch size for evaluation\nlearning_rate=2e-5,\nwarmup_steps=1000, # number of warmup steps for learning rate scheduler\nweight_decay=0.01, # strength of weight decay\nlogging_dir='.\/logs', # directory for storing logs\nlogging_steps=1000,\nevaluation_strategy=&quot;epoch&quot;,\nsave_strategy=&quot;no&quot;\n)\n\nprint(&quot;hello&quot;)\n\ntrainer = tr.Trainer(\nmodel=model, # the instantiated  Transformers model to be trained\nargs=training_args, # training arguments, defined above\ntrain_dataset=train_data, # training dataset\neval_dataset=val_data, # evaluation dataset\ncompute_metrics=compute_metrics\n)\n\n<\/code><\/pre>\n<p>After Executing this:<\/p>\n<p>The model stuck at this point:<\/p>\n<p>***** Running training *****<\/p>\n<pre><code>Num examples = 12981\n Num Epochs = 20\n Instantaneous batch size per device = 16\n Total train batch size (w. parallel, distributed &amp; accumulation) = 32\n Gradient Accumulation steps = 1\n Total optimization steps = 8120\nAutomatic Weights &amp; Biases logging enabled, to disable set os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;\n\n<\/code><\/pre>\n<p><strong>What could be the possible solution?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641265169473,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|wandb",
        "Question_view_count":171,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70573652",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67160576,
        "Question_title":"YoloV5 killed at first epoch",
        "Question_body":"<p>I'm using a virtual machine on Windows 10 with this config:<\/p>\n<pre><code>Memory 7.8 GiB\nProcessor Intel\u00ae Core\u2122 i5-6600K CPU @ 3.50GHz \u00d7 3\nGraphics llvmpipe (LLVM 11.0.0, 256 bits)\nDisk Capcity 80.5 GB\nOS Ubuntu 20.10 64 Bit\nVirtualization Oracle\n<\/code><\/pre>\n<p>I installed docker for Ubuntu as described in <a href=\"https:\/\/docs.docker.com\/engine\/install\/ubuntu\/\" rel=\"nofollow noreferrer\">the official documentation<\/a>.<br>\nI pulled the docker image as described on the <a href=\"https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Docker-Quickstart\" rel=\"nofollow noreferrer\">yolo github section for docker<\/a>.<br>\nSince I have no NVIDIA GPU I could not install a driver or CUDA.\nI pulled the aquarium from <a href=\"https:\/\/public.roboflow.com\/object-detection\/aquarium\" rel=\"nofollow noreferrer\">roboflow<\/a> and installed it on a folde aquarium.\nI ran this command to start the image and have my aquarium folder mounted<\/p>\n<pre><code>sudo docker run --ipc=host -it -v &quot;$(pwd)&quot;\/Desktop\/yolo\/aquarium:\/usr\/src\/app\/aquarium ultralytics\/yolov5:latest\n<\/code><\/pre>\n<p>And was greeted with this banner<\/p>\n<blockquote>\n<h1>=============\n== PyTorch ==<\/h1>\n<p>NVIDIA Release 21.03 (build 21060478) PyTorch Version 1.9.0a0+df837d0<\/p>\n<p>Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights\nreserved.<\/p>\n<p>Copyright (c) 2014-2021 Facebook Inc. Copyright (c) 2011-2014 Idiap\nResearch Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind\nTechnologies    (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC\nLaboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU<br \/>\n(Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America\n(Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright\n(c) 2006      Idiap Research Institute (Samy Bengio) Copyright (c)\n2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio,\nJohnny Mariethoz) Copyright (c) 2015      Google Inc. Copyright (c)\n2015      Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors\nAll rights reserved.<\/p>\n<p>NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA\nCORPORATION.  All rights reserved.<\/p>\n<p>Various files include modifications (c) NVIDIA CORPORATION.  All\nrights reserved.<\/p>\n<p>This container image and its contents are governed by the NVIDIA Deep\nLearning Container License. By pulling and using the container, you\naccept the terms and conditions of this license:\n<a href=\"https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license\" rel=\"nofollow noreferrer\">https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license<\/a><\/p>\n<p>WARNING: The NVIDIA Driver was not detected.  GPU functionality will\nnot be available.    Use 'nvidia-docker run' to start this container;\nsee    <a href=\"https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker<\/a> .<\/p>\n<p>NOTE: MOFED driver for multi-node communication was not detected.\nMulti-node communication performance may be reduced.<\/p>\n<\/blockquote>\n<p>So no error there.<br>\nI installed pip and with pip wandb I added wandb. I used <code>wandb login<\/code> and set my API key.<br><br>\nI ran following command:<\/p>\n<pre><code># python train.py --img 640 --batch 16 --epochs 10 --data .\/aquarium\/data.yaml --weights yolov5s.pt --project ip5 --name aquarium5 --nosave --cache\n<\/code><\/pre>\n<p>And received this output:<\/p>\n<pre><code>github: skipping check (Docker image)\nYOLOv5  v5.0-14-g238583b torch 1.9.0a0+df837d0 CPU\n\nNamespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='.\/aquarium\/data.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data\/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='aquarium5', noautoanchor=False, nosave=True, notest=False, project='ip5', quad=False, rect=False, resume=False, save_dir='ip5\/aquarium5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)\ntensorboard: Start with 'tensorboard --logdir ip5', view at http:\/\/localhost:6006\/\nhyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\nwandb: Currently logged in as: pebs (use `wandb login --relogin` to force relogin)\nwandb: Tracking run with wandb version 0.10.26\nwandb: Syncing run aquarium5\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/pebs\/ip5\nwandb:  View run at https:\/\/wandb.ai\/pebs\/ip5\/runs\/1c2j80ii\nwandb: Run data is saved locally in \/usr\/src\/app\/wandb\/run-20210419_102642-1c2j80ii\nwandb: Run `wandb offline` to turn off syncing.\n\nOverriding model.yaml nc=80 with nc=7\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\nModel Summary: 283 layers, 7079724 parameters, 7079724 gradients, 16.4 GFLOPS\n\nTransferred 356\/362 items from yolov5s.pt\nScaled weight_decay = 0.0005\nOptimizer groups: 62 .bias, 62 conv.weight, 59 other\ntrain: Scanning '\/usr\/src\/app\/aquarium\/train\/labels.cache' images and labels... 448 found, 0 missing, 1 empty, 0 corrupted: 100%|\u2588| 448\/448 [00:00&lt;?, ?\ntrain: Caching images (0.4GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 448\/448 [00:01&lt;00:00, 313.77it\/s]\nval: Scanning '\/usr\/src\/app\/aquarium\/valid\/labels.cache' images and labels... 127 found, 0 missing, 0 empty, 0 corrupted: 100%|\u2588| 127\/127 [00:00&lt;?, ?it\nval: Caching images (0.1GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127\/127 [00:00&lt;00:00, 141.31it\/s]\nPlotting labels... \n\nautoanchor: Analyzing anchors... anchors\/target = 5.17, Best Possible Recall (BPR) = 0.9997\nImage sizes 640 train, 640 test\nUsing 3 dataloader workers\nLogging results to ip5\/aquarium5\nStarting training for 10 epochs...\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n  0%|                                                                                                                           | 0\/28 [00:00&lt;?, ?it\/s]Killed\nroot@cf40a6498016:~# \/opt\/conda\/lib\/python3.8\/multiprocessing\/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n<\/code><\/pre>\n<p>From this output I would think that there were 0 epochs completed.<br>\nMy data.yaml contains this code:<\/p>\n<pre><code>train: \/usr\/src\/app\/aquarium\/train\/images\nval: \/usr\/src\/app\/aquarium\/valid\/images\n\nnc: 7\nnames: ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n<\/code><\/pre>\n<p><a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">wandb.ai<\/a> does not display any metrics, but I have the files config.yaml, requirements.txt, wandb-metadata.json and wandb-summary.json.<\/p>\n<p>Why am I not getting any output?<br>\nHas there in fact be no training at all?<br>\nIf there was a training, how can I use my model?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1618829350100,
        "Question_score":2,
        "Question_tags":"python|docker|pytorch|yolov5|wandb",
        "Question_view_count":1939,
        "Owner_creation_time":1430930915943,
        "Owner_last_access_time":1661487827213,
        "Owner_location":"Switzerland",
        "Owner_reputation":2006,
        "Owner_up_votes":255,
        "Owner_down_votes":4,
        "Owner_views":230,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67160576",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71191185,
        "Question_title":"Pip installations on Colab from local",
        "Question_body":"<p>I'd like to use wandb on Colab, and I've installed it through pip on the command line. However, the import isn't recognized on Colab, so I have to run <code>!pip install wandb<\/code> each time.<\/p>\n<p>How can I install <code>wandb<\/code> locally so that I don't have to install it on the Colab notebook each time?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1645328187297,
        "Question_score":0,
        "Question_tags":"python|pip|google-colaboratory|wandb",
        "Question_view_count":516,
        "Owner_creation_time":1558237713157,
        "Owner_last_access_time":1663947795227,
        "Owner_location":null,
        "Owner_reputation":421,
        "Owner_up_votes":20,
        "Owner_down_votes":5,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71191185",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73068169,
        "Question_title":"wandb : move runs from a blocked entity",
        "Question_body":"<p>I accidentally moved several runs from my own user account to a team entity.\nUnfortunatly, this team entity had a restriction on the quantity of experiments tracked, and it now appears as blocked. I have this error message :<\/p>\n<blockquote>\n<p>Your organization is over the limit of 250 tracked hours. Please upgrade your plan to keep using W&amp;B.<\/p>\n<\/blockquote>\n<p>I can't find a way to move back these runs to my free account, does anyone know how to do this ?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658414318473,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":19,
        "Owner_creation_time":1598425704360,
        "Owner_last_access_time":1658749438907,
        "Owner_location":"Toulouse, France",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73068169",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71432453,
        "Question_title":"getting aligned val_loss and train_loss plots for each epoch using WandB rather than separate plots",
        "Question_body":"<p>I have the following code for logging the train and val loss in each epoch using WandB API. I am not sure though why I am not getting val loss and train loss in the same epoch. Any idea how that could be fixed?<\/p>\n<pre><code>wandb.log({&quot;train loss&quot;: train_epoch_loss,\n           &quot;val loss&quot;: val_epoch_loss,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;train acc&quot;: train_epoch_acc,\n           &quot;val acc&quot;: val_epoch_acc,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;best val acc&quot;: best_acc, &quot;epoch&quot;: epoch})\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you see, val loss vs epochs and train loss vs epochs are two completely separate entities while I would like to have both of them in one plot in WandB.\n<a href=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646959611130,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|wandb",
        "Question_view_count":463,
        "Owner_creation_time":1369335377410,
        "Owner_last_access_time":1656440556490,
        "Owner_location":"Boston, MA, United States",
        "Owner_reputation":31183,
        "Owner_up_votes":4284,
        "Owner_down_votes":32,
        "Owner_views":18708,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432453",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69920078,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636577514790,
        "Question_score":0,
        "Question_tags":"tensorflow2.0|tf.keras|wandb",
        "Question_view_count":60,
        "Owner_creation_time":1600299419450,
        "Owner_last_access_time":1662833034530,
        "Owner_location":"Germany",
        "Owner_reputation":62,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636642289972,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71134673,
        "Question_title":"python ray tune unable to stop trial or experiment",
        "Question_body":"<p>I am trying to make ray tune with wandb stop the experiment under certain conditions.<\/p>\n<ul>\n<li>stop all experiment if any trial raises an Exception (so i can fix the code and resume)<\/li>\n<li>stop if my score gets -999<\/li>\n<li>stop if the variable <code>varcannotbezero<\/code> gets 0<\/li>\n<\/ul>\n<p><strong>The following things i tried all failed in achieving desired behavior:<\/strong><\/p>\n<ul>\n<li>stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0}<\/li>\n<li>max_failures=0<\/li>\n<li>defining a Stoper class did also not work<\/li>\n<\/ul>\n<pre><code>class RayStopper(Stopper):\n    def __init__(self):\n        self._start = time.time()\n        #self._deadline = 300\n    def __call__(self, trial_id, result):\n        self.score=result[&quot;score&quot;]\n        self.varcannotbezero=result[&quot;varcannotbezero&quot;]\n        return False\n    def stop_all(self):\n        if self.score==-999 or self.varcannotbezero==0:\n            return True\n        else:\n            return False\n<\/code><\/pre>\n<p>Ray tune just continues to run<\/p>\n<pre><code>    wandb_project=&quot;ABC&quot;\n    wandb_api_key=&quot;KEY&quot;\n    ray.init(configure_logging=False)\n\n    if current_best_params is None:\n        algo = HyperOptSearch()\n    else:\n        algo = HyperOptSearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points)\n    algo = ConcurrencyLimiter(algo, max_concurrent=1)\n\n    scheduler = AsyncHyperBandScheduler()\n    analysis = tune.run(\n        tune_obj,\n        name=&quot;Name&quot;,\n        resources_per_trial={&quot;cpu&quot;: 1},\n        search_alg=algo,\n        scheduler=scheduler,\n        metric=&quot;score&quot;,\n        mode=&quot;max&quot;,\n        num_samples=10,\n        stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0},\n        max_failures=0,\n        config=config,\n        callbacks=[WandbLoggerCallback(project=wandb_project,entity=&quot;mycompany&quot;,api_key=wandb_api_key,log_config=True)],\n        local_dir=local_dir,\n        resume=&quot;AUTO&quot;,\n        verbose=0\n    )\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1644967685327,
        "Question_score":0,
        "Question_tags":"optimization|hyperparameters|ray|ray-tune|wandb",
        "Question_view_count":284,
        "Owner_creation_time":1300741959900,
        "Owner_last_access_time":1664027459750,
        "Owner_location":null,
        "Owner_reputation":2390,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":308,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71134673",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73165796,
        "Question_title":"Wandb first run start time is delayed",
        "Question_body":"<p>I wanted to compare the execution speeds of three data types. The runs were organized in sequence of <code>Original<\/code>, <code>DictList<\/code>, <code>DataFrame<\/code>. So <code>Original<\/code> was the first run. The x-axis is set as <code>Relative Time (Process)<\/code><\/p>\n<p>Problem is that each runs starting time is all different! How can I make sure that they all start at time 0?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PotUf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PotUf.png\" alt=\"Loss Graph\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" alt=\"Elapsed Steps Graph\" \/><\/a><\/p>\n<p>I can't post the entire code. I'll post every point where <code>wandb<\/code> API is called.<\/p>\n<pre><code>import wandb\nif __name__ == &quot;__main__&quot;:\n    wandb.login()\n    datatypes = {&quot;Original&quot;: Original, &quot;DictList&quot;: DictList, &quot;DataFrame&quot;: DataFrame}\n    for type_name, datatype in datatypes.items():\n        wandb_run = wandb.init(project=&quot;Compare&quot;, name=type_name, reinit=True)\n        with wandb_run:\n            # Initialize RL training session\n            storage = datatype()\n            # Run RL training session\n            # Log (loss, elapsed_steps, etc.)    \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659093160443,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":13,
        "Owner_creation_time":1502879704843,
        "Owner_last_access_time":1664071330100,
        "Owner_location":"South Korea",
        "Owner_reputation":1248,
        "Owner_up_votes":800,
        "Owner_down_votes":23,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73165796",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71654404,
        "Question_title":"How to fix Wandb API giving error 400 when deleting artifacts?",
        "Question_body":"<p>I accidentally logged way too much to wandb and would like to delete some artifacts. I've tried the following script, but I get an error 400 whenever I run it:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\n\nwandb.login(key=KEY)\n\napi = wandb.Api(overrides={&quot;project&quot;: PROJECT, &quot;entity&quot;: ENTITY})\n\nfor run in api.runs():\n    files = sorted([f for f in run.logged_artifacts()], key=lambda f: f.updated_at)\n    print(&quot;Total files:&quot;, len(files))\n    print(&quot;Last file:&quot;, files[-1].name)\n    print(&quot;Last file date:&quot;, files[-1].updated_at)\n    for f in files[:-1]:\n        if &quot;.tar&quot; in f.name:\n            # also tried just f.delete()\n            a = api.artifact(f&quot;{PROJECT}\/{f.name}&quot;)\n            print(&quot;Deleting {}&quot;.format(f.name))\n            a.delete()\n<\/code><\/pre>\n<p>All I get is <code>requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648506655603,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":349,
        "Owner_creation_time":1349927421380,
        "Owner_last_access_time":1664055989120,
        "Owner_location":null,
        "Owner_reputation":2248,
        "Owner_up_votes":69,
        "Owner_down_votes":9,
        "Owner_views":403,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71654404",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73642527,
        "Question_title":"How to install wandb on a docker image for arm?",
        "Question_body":"<p>My docker building failed at the <code>RUN <\/code><\/p>\n<p>with:<\/p>\n<pre><code>(meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/Dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/\n\n[+] Building 184.7s (20\/28)\n =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s\n =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io\/continuumio\/miniconda3:latest                                                                           0.0s\n =&gt; [ 1\/24] FROM docker.io\/continuumio\/miniconda3                                                                                                  0.0s\n =&gt; https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main                                                                                     0.3s\n =&gt; CACHED [ 2\/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s\n =&gt; CACHED [ 3\/24] RUN useradd -m bot                                                                                                              0.0s\n =&gt; CACHED [ 4\/24] WORKDIR \/home\/bot                                                                                                               0.0s\n =&gt; CACHED [ 5\/24] ADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json                                                     0.0s\n =&gt; CACHED [ 6\/24] RUN opam init --disable-sandboxing                                                                                              0.0s\n =&gt; CACHED [ 7\/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s\n =&gt; CACHED [ 8\/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s\n =&gt; CACHED [ 9\/24] RUN eval $(opam env)                                                                                                            0.0s\n =&gt; CACHED [10\/24] RUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released                                                               0.0s\n =&gt; CACHED [11\/24] RUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released                                  0.0s\n =&gt; CACHED [12\/24] RUN opam update --all                                                                                                           0.0s\n =&gt; CACHED [13\/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s\n =&gt; [14\/24] RUN opam install -y coq-serapi                                                                                                       176.3s\n =&gt; [15\/24] RUN eval $(opam env)                                                                                                                   0.2s\n =&gt; ERROR [16\/24] RUN pip install wandb --upgrade                                                                                                  8.0s\n------\n &gt; [16\/24] RUN pip install wandb --upgrade:\n#20 0.351 Defaulting to user installation because normal site-packages is not writeable\n#20 0.637 Collecting wandb\n#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n#20 1.365 Requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (61.2.0)\n#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (1.16.0)\n#20 1.409 Collecting promise&lt;3,&gt;=2.0\n#20 1.472   Downloading promise-2.3.tar.gz (19 kB)\n#20 2.087 Collecting PyYAML\n#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)\n#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0\n#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)\n#20 2.648 Collecting setproctitle\n#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n#20 2.763 Collecting Click!=8.0.0,&gt;=7.0\n#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n#20 2.902 Collecting sentry-sdk&gt;=1.0.0\n#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n#20 3.112 Collecting psutil&gt;=5.0.0\n#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)\n#20 3.871 Collecting pathtools\n#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)\n#20 4.431 Collecting shortuuid&gt;=0.5.0\n#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (2.27.1)\n#20 4.568 Collecting docker-pycreds&gt;=0.4.0\n#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n#20 4.695 Collecting GitPython&gt;=1.0.0\n#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1\n#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1\n#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)\n#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)\n#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)\n#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)\n#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1\n#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n#20 5.172 Building wheels for collected packages: promise, psutil, pathtools\n#20 5.172   Building wheel for promise (setup.py): started\n#20 5.851   Building wheel for promise (setup.py): finished with status 'done'\n#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae\n#20 5.852   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n#20 5.854   Building wheel for psutil (setup.py): started\n#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'\n#20 6.226   ERROR: Command errored out with exit status 1:\n#20 6.226    command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb\n#20 6.226        cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 6.226   Complete output (45 lines):\n#20 6.226   running bdist_wheel\n#20 6.226   running build\n#20 6.226   running build_py\n#20 6.226   creating build\n#20 6.226   creating build\/lib.linux-aarch64-3.9\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   running build_ext\n#20 6.226   building 'psutil._psutil_linux' extension\n#20 6.226   creating build\/temp.linux-aarch64-3.9\n#20 6.226   creating build\/temp.linux-aarch64-3.9\/psutil\n#20 6.226   gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 6.226   ----------------------------------------\n#20 6.226   ERROR: Failed building wheel for psutil\n#20 6.226   Running setup.py clean for psutil\n#20 6.550   Building wheel for pathtools (setup.py): started\n#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'\n#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82\n#20 7.135   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n#20 7.136 Successfully built promise pathtools\n#20 7.136 Failed to build psutil\n#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb\n#20 7.262   WARNING: The script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on PATH.\n#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n#20 7.345     Running setup.py install for psutil: started\n#20 7.727     Running setup.py install for psutil: finished with status 'error'\n#20 7.727     ERROR: Command errored out with exit status 1:\n#20 7.727      command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil\n#20 7.727          cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 7.727     Complete output (47 lines):\n#20 7.727     running install\n#20 7.727     \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n#20 7.727       warnings.warn(\n#20 7.727     running build\n#20 7.727     running build_py\n#20 7.727     creating build\n#20 7.727     creating build\/lib.linux-aarch64-3.9\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     running build_ext\n#20 7.727     building 'psutil._psutil_linux' extension\n#20 7.727     creating build\/temp.linux-aarch64-3.9\n#20 7.727     creating build\/temp.linux-aarch64-3.9\/psutil\n#20 7.727     gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 7.727     ----------------------------------------\n#20 7.728 ERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil Check the logs for full command output.\n------\nexecutor failed running [\/bin\/sh -c pip install wandb --upgrade]: exit code: 1\n<\/code><\/pre>\n<p>why?<\/p>\n<p>Docker file so far:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace\n\nRUN useradd -m bot\nWORKDIR \/home\/bot\nUSER bot\n\n## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en\n#RUN wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh  \\\n#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/bin:${PATH}&quot;\n#RUN conda create -n pycoq python=3.9 -y\n## somehow this &quot;works&quot; but conda isn't fully aware of this. Fix later?\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${PATH}&quot;\n\nADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json\n\n# -- setup opam like VP's PyCoq\nRUN opam init --disable-sandboxing\n# compiler + '_' + coq_serapi + '.' + coq_serapi_pin\nRUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda\nRUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1\nRUN eval $(opam env)\n\nRUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released\n# RUN opam pin add -y coq 8.11.0\n# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released']\nRUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released\nRUN opam update --all\nRUN opam pin add -y coq 8.11.0\n\n#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1\nRUN opam install -y coq-serapi\n\nRUN eval $(opam env)\n\n# makes sure depedencies for pycoq are installed once already in the docker image\nENV WANDB_API_KEY=&quot;SECRET&quot;\nRUN pip install wandb --upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1662595635817,
        "Question_score":0,
        "Question_tags":"python|linux|docker|anaconda|wandb",
        "Question_view_count":49,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73642527",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71744288,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649110110203,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|huggingface-tokenizers|fine-tune|wandb|huggingface",
        "Question_view_count":289,
        "Owner_creation_time":1499867951607,
        "Owner_last_access_time":1661234383987,
        "Owner_location":null,
        "Owner_reputation":307,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649156563120,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72166449,
        "Question_title":"Weights and Biases error: The wandb backend process has shutdown",
        "Question_body":"<p>running the colab linked below, I get the following error:<\/p>\n<p>&quot;The wandb backend process has shutdown&quot;<\/p>\n<p>I see nothing suspicious in the way the colab uses wandb and I couldn't find anyone with the same problem. Any help is greatly appreciated. I am using the latest version of wandb in colab.<\/p>\n<p>This is where I set up wandb:<\/p>\n<pre><code>if WANDB:\n    wandb.login()\n<\/code><\/pre>\n<p>and this is the part where I get the error:<\/p>\n<pre><code>#setup wandb if we're using it\n\nif WANDB:\n    experiment_name = os.environ.get(&quot;EXPERIMENT_NAME&quot;)\n    group = experiment_name if experiment_name != &quot;none&quot; else wandb.util.generate_id()\n\ncv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f&quot;Fold {fold}\/{config.folds}&quot;, end=&quot;\\n&quot;*2)\n    fold_directory = os.path.join(config.output_directory, f&quot;fold_{fold}&quot;)    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, &quot;model.pth&quot;)\n    model_config_path = os.path.join(fold_directory, &quot;model_config.json&quot;)\n    checkpoints_directory = os.path.join(fold_directory, &quot;checkpoints\/&quot;)\n    make_directory(checkpoints_directory)\n    \n    #Data collators are objects that will form a batch by using a list of dataset elements as input.\n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[&quot;fold&quot;].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[&quot;anchor&quot;].values, \n                            pair_texts=train_fold[&quot;target&quot;].values,\n                            contexts=train_fold[&quot;title&quot;].values,\n                            targets=train_fold[&quot;score&quot;].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f&quot;Train samples: {len(train_dataset)}&quot;)\n    \n    validation_fold = train[train[&quot;fold&quot;].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[&quot;anchor&quot;].values, \n                                 pair_texts=validation_fold[&quot;target&quot;].values,\n                                 contexts=validation_fold[&quot;title&quot;].values,\n                                 targets=validation_fold[&quot;score&quot;].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=True, \n                                   drop_last=False)\n    \n    print(f&quot;Validation samples: {len(validation_dataset)}&quot;)\n\n\n    model = Model(**config.model)\n    \n    if not os.path.exists(model_config_path): \n        model.config.to_json_file(model_config_path)\n    \n    model_parameters = model.parameters()\n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    training_steps = len(train_loader) * config.epochs\n    \n    if &quot;scheduler&quot; in config:\n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(&quot;warmup&quot;, 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=&quot;min&quot;, \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=&quot;checkpoint.pth&quot;, \n                                       num_candidates=1)\n\n\n    if WANDB:\n        wandb.init()\n        #wandb.init(group=group, name=f&quot;fold_{fold}&quot;, config=config)\n    \n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=&quot;tqdm&quot;)\n    \n    if WANDB:\n        wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f&quot;Model's path: {model_path}&quot;)\n    \n    validation_fold[&quot;prediction&quot;] = validation_outputs.to(&quot;cpu&quot;).numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n    \n    cv_monitor_value = validation_loss if config.cv_monitor_value == &quot;loss&quot; else validation_metrics[config.cv_monitor_value]\n    cv_scores.append(cv_monitor_value)\n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=&quot;\\n&quot;*6)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652063587210,
        "Question_score":6,
        "Question_tags":"python|wandb",
        "Question_view_count":1610,
        "Owner_creation_time":1622105241727,
        "Owner_last_access_time":1661260334760,
        "Owner_location":"Berlin, Deutschland",
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72166449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73412851,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660891849347,
        "Question_score":0,
        "Question_tags":"python|wandb",
        "Question_view_count":61,
        "Owner_creation_time":1557474244067,
        "Owner_last_access_time":1663941664767,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":120,
        "Owner_down_votes":3,
        "Owner_views":140,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1660907847087,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":65392269,
        "Question_title":"access logged values during runtime",
        "Question_body":"<p>How can I retrieve a logged value from wandb before the run was finished?<\/p>\n<pre><code>import os\nimport wandb\nwandb.init(project='someproject')\n\n\ndef loss_a():\n    # do_stuff and log:\n    wandb.log({&quot;loss_a&quot;: 1.0})\n    \ndef loss_b():\n    # do_stuff and log:\n    wandb.log({&quot;loss_b&quot;: 2.0})\n\nfor epoch in range(2):\n    loss_a()\n    loss_b()\n    \n    # somehow retrieve loss_a and loss_b and print them here:\n    print(f'loss_a={??}, loss_b={??}')\n\n<\/code><\/pre>\n<p>After run was finished I can find it with <code>wandb.Api<\/code> to get <code>run.history<\/code>. But it seems that before run was fininshed, accessing <code>run.history<\/code> doesn't work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608551470203,
        "Question_score":0,
        "Question_tags":"machine-learning|wandb",
        "Question_view_count":302,
        "Owner_creation_time":1304429242790,
        "Owner_last_access_time":1663848455777,
        "Owner_location":null,
        "Owner_reputation":3201,
        "Owner_up_votes":946,
        "Owner_down_votes":3,
        "Owner_views":100,
        "Question_last_edit_time":1661850170528,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65392269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71195699,
        "Question_title":"wandb [SSL: WRONG_VERSION_NUMBER] error only thrown in Pycharm using python 3.8",
        "Question_body":"<p>I log the metrics of the training results on the wandb online server. This was working without any problems, till the beginning of this week. Since then i am suddenly unable to connect to the wandb online server and loggin the metrics isn't working anymore. I on Windows 10 using PyCharm with python version 3.8.<\/p>\n<p>The following exception is thrown:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 696, in urlopen\n    self._prepare_proxy(conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 964, in _prepare_proxy\n    conn.connect()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 364, in connect\n    conn = self._connect_tls_proxy(hostname, conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 501, in _connect_tls_proxy\n    socket = ssl_wrap_socket(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 453, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 495, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1040, in _create\n    self.do_handshake()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 439, in send\n    resp = conn.urlopen(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 755, in urlopen\n    retries = retries.increment(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\retry.py&quot;, line 574, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py&quot;, line 132, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\transport\\requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 117, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 655, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 514, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\nwandb: Network error (SSLError), entering retry loop.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: Network error (SSLError), entering retry loop.\n<\/code><\/pre>\n<p>Strange thing is, if i execute the run configuration used in pycharm as a command from it's built in terminal it works fine. Since the same virtual environment is used on the built in terminal i don't understand why this exceptions is thrown if using the run configuration in Pycharm. What am i missing here?<\/p>\n<p>Any help would be appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645369500377,
        "Question_score":1,
        "Question_tags":"python|python-3.x|ssl|pycharm|wandb",
        "Question_view_count":259,
        "Owner_creation_time":1644055951303,
        "Owner_last_access_time":1651943996327,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71195699",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71257152,
        "Question_title":"How to show wandb training progress from run folder",
        "Question_body":"<p>After training neural networks with wandb as the logger, I received a link to show the training results and a folder named &quot;run-...&quot;, I assume that is the logging of the training process. Now I don't have that link, how to show the wandb training process from run folder?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645730259610,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|pytorch|wandb",
        "Question_view_count":86,
        "Owner_creation_time":1504450759857,
        "Owner_last_access_time":1660667476990,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":83,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1645730901420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71257152",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":63469762,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597757510213,
        "Question_score":2,
        "Question_tags":"python|keras|k-fold|wandb",
        "Question_view_count":1043,
        "Owner_creation_time":1486549300030,
        "Owner_last_access_time":1620240688720,
        "Owner_location":"Germany",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1661849871016,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1598032113043,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1599772735680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Question_exclusive_tag":"Weights & Biases"
    }
]