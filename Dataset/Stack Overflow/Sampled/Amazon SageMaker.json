[
    {
        "Question_title":"Preprocessing data for Sagemaker Inference Pipeline with Blazingtext",
        "Question_body":"<p>I'm trying to figure out the best way to preprocess my input data for my inference endpoint for AWS Sagemaker. I'm using the BlazingText algorithm.<\/p>\n\n<p>I'm not really sure the best way forward and I would be thankful for any pointers.<\/p>\n\n<p>I currently train my model using a Jupyter notebook in Sagemaker and that works wonderfully, but the problem is that I use NLTK to clean my data (Swedish stopwords and stemming etc):<\/p>\n\n<pre><code>import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n<\/code><\/pre>\n\n<p>So the question is really, how do I get the same  pre-processing logic to the inference endpoint  ?<\/p>\n\n<p>I have a couple of thoughts about how to proceed:<\/p>\n\n<ul>\n<li><p>Build a docker container with the python libs &amp; data installed with the sole purpose of pre-processing the data. Then use this container in the inference pipeline. <\/p><\/li>\n<li><p>Supply the Python libs and Script to an existing container in the same way you can do for external lib an notebook <\/p><\/li>\n<li><p>Build a custom fastText container with the libs I need and run it outside of Sagemaker.<\/p><\/li>\n<li><p>Will probably work, but feels like a \"hack\": Build a Lambda function that has the proper Python libs&amp;data installed and calls the Sagemaker Endpoint. I'm worried about cold start delays as the prediction traffic volume will be low. <\/p><\/li>\n<\/ul>\n\n<p>I would like to go with the first option, but I'm struggling a bit to understand if there is a docker image that I could build from, and add my dependencies to, or if I need to build something from the ground up. For instance, would the image sagemaker-sparkml-serving:2.2 be a good candidate? <\/p>\n\n<p>But maybe there is a better way all around? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-07 09:18:33.373 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|inference",
        "Question_view_count":848,
        "Owner_creation_date":"2015-01-05 22:37:28.9 UTC",
        "Owner_last_access_date":"2022-09-24 06:04:55.29 UTC",
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Question_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-07 12:17:32.09 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-rds|amazon-sagemaker",
        "Question_view_count":865,
        "Owner_creation_date":"2017-11-01 18:06:37.047 UTC",
        "Owner_last_access_date":"2022-09-20 14:50:31.687 UTC",
        "Owner_location":null,
        "Owner_reputation":313,
        "Owner_up_votes":37,
        "Owner_down_votes":3,
        "Owner_views":34,
        "Answer_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-29 17:05:08.223 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Using SageMaker with Hydra",
        "Question_body":"<p>I have a question about SageMaker and Hydra.<\/p>\n<p><strong>TL;DR<\/strong>\nIs there a way to pass arguments from SageMaker estimator to a Hydra script? Currently it passes parameters in a very strict way.<\/p>\n<p><strong>Full Question<\/strong>\nI use Hydra in order to pass configs to my training script. I have many configs and it works good for my. For example, if I want to use a specific optimizer, I do:<\/p>\n<pre><code>python train.py optimizer=adam\n<\/code><\/pre>\n<p>This is my training script, for instance:<\/p>\n<pre><code>@hydra.main(version_base=None, config_path=&quot;configs\/&quot;, config_name=&quot;config&quot;)\ndef train(config: DictConfig):\n    logging.info(f&quot;Instantiating dataset &lt;{config.dataset._target_}&gt;&quot;)\n    train_ds, val_ds = hydra.utils.call(config.dataset)\n\n    logging.info(f&quot;Instantiating model &lt;{config.model._target_}&gt;&quot;)\n    model = hydra.utils.call(config.model)\n\n    logging.info(f&quot;Instantiating optimizer &lt;{config.optimizer._target_}&gt;&quot;)\n    optimizer = hydra.utils.instantiate(config.optimizer)\n\n    logging.info(f&quot;Instantiating loss &lt;{config.loss._target_}&gt;&quot;)\n    loss = hydra.utils.instantiate(config.loss)\n\n    callbacks = []\n    if &quot;callbacks&quot; in config:\n        for _, cb_conf in config.callbacks.items():\n            if &quot;_target_&quot; in cb_conf:\n                logging.info(f&quot;Instantiating callback &lt;{cb_conf._target_}&gt;&quot;)\n                callbacks.append(hydra.utils.instantiate(cb_conf))\n\n    metrics = []\n    if &quot;metrics&quot; in config:\n        for _, metric_conf in config.metrics.items():\n            if &quot;_target_&quot; in metric_conf:\n                logging.info(f&quot;Instantiating metric &lt;{metric_conf._target_}&gt;&quot;)\n                metrics.append(hydra.utils.instantiate(metric_conf))\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=config.epochs,\n        callbacks=callbacks,\n    )\n\n\nif __name__ == &quot;__main__&quot;:\n    train()\n<\/code><\/pre>\n<p>And I have a relevant <code>optimizer\/adam.yaml<\/code> file.<\/p>\n<p>Now, I started using SageMaker to run my experiments in the cloud and I noticed a problem.\nIt doesn't support the hydra syntax (<code>+optimizer=sgd<\/code>), stuff like that.<\/p>\n<p>Is there a way to make it play nicely with Hydra syntax? If not, do you have a suggestion for refactoring my training code so that it would work nicely with Hydra\/OmegaConf?<\/p>\n<p>I saw there is a similar question in SageMaker issues page, but it doesn't have any replies:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1837\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1837<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 07:33:39.553 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|yaml|amazon-sagemaker|fb-hydra|omegaconf",
        "Question_view_count":34,
        "Owner_creation_date":"2016-03-06 17:09:27.21 UTC",
        "Owner_last_access_date":"2022-09-22 12:24:12.473 UTC",
        "Owner_location":null,
        "Owner_reputation":769,
        "Owner_up_votes":20,
        "Owner_down_votes":6,
        "Owner_views":148,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to access text file from s3 bucket into sagemaker for training a model?",
        "Question_body":"<p>I am trying to train chatbot model using tensorflow and seq to seq architecture using sagemaker also I have completed coding in spyder but when \nI am trying to access cornel movie corpus dataset from s3 bucket into sagemaker it says no such file or directory even granting access to s3 bucket<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-25 06:17:51.797 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":576,
        "Owner_creation_date":"2020-04-25 06:07:24.533 UTC",
        "Owner_last_access_date":"2020-09-22 16:42:09.08 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-25 09:43:04.553 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Automatically Install OpenJDK into SageMaker Notebook",
        "Question_body":"<p>I have the following lines of code<\/p>\n<pre><code>import tabula\ntabula.environment_info()\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>    Python version:\n    3.7.10 (default, Jun  4 2021, 14:48:32) \n[GCC 7.5.0]\nJava version:\n    `java -version` faild. `java` command is not found from this Pythonprocess. Please ensure Java is installed and PATH is set for `java`\ntabula-py version: 2.3.0\nplatform: Linux-4.14.243-185.433.amzn2.x86_64-x86_64-with-debian-10.6\nuname:\n    uname_result(system='Linux', node='datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395', release='4.14.243-185.433.amzn2.x86_64', version='#1 SMP Mon Aug 9 05:55:52 UTC 2021', machine='x86_64', processor='')\nlinux_distribution: ('Debian GNU\/Linux', '10', 'buster')\nmac_ver: ('', ('', '', ''), '')\n<\/code><\/pre>\n<p>I am trying to figure out how to set up the appropriate cmd line statement to install OpenJDK from the SageMaker Notebook environment.<\/p>\n<p>I have opened up the terminal and inserted the following line of code and am still seeing the above error.<\/p>\n<pre><code>pip install install-jdk -t.\n<\/code><\/pre>\n<p>Overall, Before installing tabula-py, I need to ensure that I have Java runtime on my environment. How can I facilitate this in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-14 20:47:17.823 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"java|amazon-web-services|java-8|amazon-sagemaker",
        "Question_view_count":665,
        "Owner_creation_date":"2021-03-12 13:24:19.547 UTC",
        "Owner_last_access_date":"2022-05-25 02:11:47.037 UTC",
        "Owner_location":"United States",
        "Owner_reputation":141,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-15 02:10:14.857 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"A\/B Test feature in SageMaker: variant assignment is random?",
        "Question_body":"<p>A\/B test feature in SageMaker sounds so intriguing but the more I looked into, the more I am confused whether this is a useful feature. For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant.<\/p>\n\n<p>How is this assignment done? Is it purely random? Or am I supposed to pass some kind of ID (or hashed ID) which can indicate a person or a browser so that the same model is picked up for the same person.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-28 08:51:33.39 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":614,
        "Owner_creation_date":"2011-05-13 06:51:53.437 UTC",
        "Owner_last_access_date":"2022-09-25 04:56:44.01 UTC",
        "Owner_location":null,
        "Owner_reputation":10317,
        "Owner_up_votes":268,
        "Owner_down_votes":1,
        "Owner_views":595,
        "Answer_body":"<blockquote>\n  <p>For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant. <\/p>\n<\/blockquote>\n\n<p>The InvokeEndpoint response includes the \"InvokedProductionVariant\", in order to support the kind of analysis you describe. Details can be found in the API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax<\/a><\/p>\n\n<blockquote>\n  <p>How is this assignment done? Is it purely random? <\/p>\n<\/blockquote>\n\n<p>Traffic is distributed randomly while remaining proportional to the weight of the production variant.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-28 21:51:17.687 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Not able to read HDF5 file present in S3 in sagemaker notebook instance",
        "Question_body":"<p>My directory structure looks like this: <code>bucket-name\/training\/file.hdf5<\/code><\/p>\n<p>I tried reading this file in sagemaker notebook instance by this code cell:<\/p>\n<pre><code>bucket='bucket-name'\ndata_key = 'training\/file.hdf5'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nhf = h5py.File(data_location, 'r')\n<\/code><\/pre>\n<p>But it gives me error:<\/p>\n<pre><code>Unable to open file (unable to open file: name = 's3:\/\/bucket-name\/training\/file.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n<\/code><\/pre>\n<p>I have also tried <code>pd.read_hdf(data_location)<\/code> but was not succesfull.<\/p>\n<p>Trying to read a csv file into dataframe from same key doesnt throw error.<\/p>\n<p>Any help is appreciated. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-27 10:26:41.04 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|hdf5|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_date":"2021-12-27 10:18:22.04 UTC",
        "Owner_last_access_date":"2022-09-11 07:06:16.627 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Could not find model PipelineModel",
        "Question_body":"<p>When I try to build models to create a pipeline as follows,<\/p>\n<pre><code>    &lt;code for the preprocessor&gt;\n    preprocessor = sklearn_preprocessor.create_model() #successful\n\n    &lt;code for the estimator&gt;\n    xgb_model_step = xgb_model.create_model() #successful    \n    \n    sm_model = PipelineModel(name='model', role=role, models=[preprocessor, xgb_model_step]) #successful\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=end) &lt;--- failure!\n<\/code><\/pre>\n<p>The models are created successfully. In the deploy line I get an error as,<\/p>\n<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-east-1-1356784978535\/sagemaker-scikit-learn-2021-06-04-20-07-55-519\/output\/model.tar.gz.<\/code><\/p>\n<p>I am not sure how I can specify the path and make the deploy successful. Can somebody please help me with this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-06-04 20:30:35.93 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_date":"2016-09-09 06:10:10.977 UTC",
        "Owner_last_access_date":"2022-02-28 12:37:18.717 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Aws Sagemaker invoke-endpoint call and csv",
        "Question_body":"<p>i've created a clustering model on sagemaker and i'm invoking it via CLI with this command: \naws sagemaker-runtime invoke-endpoint --endpoint-name myendpoint --body  $mydata --content-type text\/csv output.json --region eu-west-1<\/p>\n\n<p>If my data starts with a negative number, i get an error\n\"usage: aws [options]   [ ...] [parameters]\nTo see help text, you can run:<\/p>\n\n<p>aws help\n  aws  help\n  aws   help\naws: error: argument --body: expected one argument\"<\/p>\n\n<p>While if it's a positive number, everything works. How can i escape the first minus of the data to make it work?\nThanks in advance<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-28 19:08:58.363 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1488,
        "Owner_creation_date":"2018-05-28 19:05:18.383 UTC",
        "Owner_last_access_date":"2022-09-23 12:36:42.723 UTC",
        "Owner_location":"Milan",
        "Owner_reputation":96,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker timing out for flask model deployment",
        "Question_body":"<p>Below is the predict.py in the ECR Container . Sagemaker endpoint gives &quot;Status:Failed&quot; output after retrying for 10-12 minutes. Both \/ping and \/invocations methods are available<\/p>\n<pre><code>\/opt\/ml\/code\/predict.py\n----------\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nclasspath =  &lt;.pkl file&gt; \nmodel = pickle.load(open(classpath, &quot;rb&quot;))\n\n\napp = flask.Flask(__name__)\nprint(app)\n\n@app.route(&quot;\/ping&quot;, methods=[&quot;GET&quot;]\ndef ping():\n    &quot;&quot;&quot;Determine if the container is working and healthy.&quot;&quot;&quot;\n    return flask.Response(response=&quot;Flask running&quot;, status=200, mimetype=&quot;application\/json&quot;)\n\n@app.route(&quot;\/invocations&quot;, methods=[&quot;POST&quot;])\n    &quot;&quot;InferenceCode&quot;&quot;\n    return flask.Response(response=&quot;Invocation Completed&quot;, status=200, \n    mimetype=&quot;application\/json&quot;)\n\nBelow snippet was both added and removed , however I still have the endpoint in failed status\n\n if __name__ == '__main__':\n     app.run(host='0.0.0.0',port=5000)\n\nError : \n&quot;The primary container for production variant &lt;modelname&gt; did not pass the ping health check. Please check CloudWatch logs for this endpoint.&quot;\n\n\nSagemaker endpoint Cloudwatch logs.\n[INFO] Starting gunicorn 20.1.0\n[INFO] Listening at: http:\/\/0.0.0.0:8000 (1)\n[INFO] Using worker: sync\n[INFO] Booting worker with pid: 11```\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-14 14:26:36.913 UTC",
        "Question_favorite_count":0.0,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker|amazon-ecr",
        "Question_view_count":363,
        "Owner_creation_date":"2021-07-14 14:16:56 UTC",
        "Owner_last_access_date":"2021-07-30 18:31:02.29 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-07-14 14:38:06.423 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can I use AWS Sagemaker without S3",
        "Question_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-29 22:49:55.25 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":826,
        "Owner_creation_date":"2018-06-08 23:29:22.963 UTC",
        "Owner_last_access_date":"2021-10-28 22:11:40.793 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-02 22:32:04.34 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Output model metrics to Cloudwatch",
        "Question_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-26 14:40:37.85 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-ec2|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_date":"2018-08-08 18:39:40.75 UTC",
        "Owner_last_access_date":"2022-09-23 13:21:10.983 UTC",
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":54,
        "Owner_down_votes":1,
        "Owner_views":63,
        "Answer_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-26 15:05:38.3 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"sagemaker inference container ModuleNotFoundError: No module named 'model_handler'",
        "Question_body":"<p>I am trying to deploy a model using my own custom inference container on sagemaker. I am following the documentation here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html<\/a><\/p>\n<p>I have an entrypoint file:<\/p>\n<pre><code>from sagemaker_inference import model_server\n#HANDLER_SERVICE = &quot;\/home\/model-server\/model_handler.py:handle&quot;\nHANDLER_SERVICE = &quot;model_handler.py&quot;\nmodel_server.start_model_server(handler_service=HANDLER_SERVICE)\n<\/code><\/pre>\n<p>I have a model_handler.py file:<\/p>\n<pre><code>from sagemaker_inference.default_handler_service import DefaultHandlerService\nfrom sagemaker_inference.transformer import Transformer\nfrom CustomHandler import CustomHandler\n\n\nclass ModelHandler(DefaultHandlerService):\n    def __init__(self):\n        transformer = Transformer(default_inference_handler=CustomHandler())\n        super(HandlerService, self).__init__(transformer=transformer)\n<\/code><\/pre>\n<p>And I have my CustomHandler.py file:<\/p>\n<pre><code>import os\nimport json\nimport pandas as pd\nfrom joblib import dump, load\nfrom sagemaker_inference import default_inference_handler, decoder, encoder, errors, utils, content_types\n\n\nclass CustomHandler(default_inference_handler.DefaultInferenceHandler):\n\n    def model_fn(self, model_dir: str) -&gt; str:\n        clf = load(os.path.join(model_dir, &quot;model.joblib&quot;))\n        return clf\n\n    def input_fn(self, request_body: str, content_type: str) -&gt; pd.DataFrame:\n        if content_type == &quot;application\/json&quot;:\n            items = json.loads(request_body)\n\n            for item in items:\n                processed_item1 = process_item1(items[&quot;item1&quot;])\n                processed_item2 = process_item2(items[&quot;item2])\n                all_item1 += [processed_item1]\n                all_item2 += [processed_item2]\n            return pd.DataFrame({&quot;item1&quot;: all_item1, &quot;comments&quot;: all_item2})\n\n    def predict_fn(self, input_data, model):\n        return model.predict(input_data)\n<\/code><\/pre>\n<p>Once I deploy the model to an endpoint with these files in the image, I get the following error: <code>ml.mms.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'model_handler'<\/code>.<\/p>\n<p>I am really stuck what to do here. I wish there was an example of how to do this in the above way end to end but I don't think there is. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-03 16:10:37.33 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":728,
        "Owner_creation_date":"2018-03-22 16:45:24.657 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747 UTC",
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":"<p>This is because of the path mismatch. The entrypoint is trying to look for &quot;model_handler.py&quot; in <code>WORKDIR<\/code> directory of the container.\nTo avoid this, always specify absolute path when working with containers.<\/p>\n<p>Moreover your code looks confusing. Please use this sample code as the reference:<\/p>\n<pre><code>import subprocess\nfrom subprocess import CalledProcessError\nimport model_handler\nfrom retrying import retry\nfrom sagemaker_inference import model_server\nimport os\n\n\ndef _retry_if_error(exception):\n    return isinstance(exception, CalledProcessError or OSError)\n\n\n@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\ndef _start_mms():\n    # by default the number of workers per model is 1, but we can configure it through the\n    # environment variable below if desired.\n    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n    print(&quot;Starting MMS -&gt; running &quot;, model_handler.__file__)\n    model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;)\n\n\ndef main():\n    _start_mms()\n    # prevent docker exit\n    subprocess.call([&quot;tail&quot;, &quot;-f&quot;, &quot;\/dev\/null&quot;])\n\nmain()\n<\/code><\/pre>\n<p>Further, notice this line - <code>model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;) <\/code>\nHere we are starting the server, and telling it to call <code>handle()<\/code> function in model_handler.py to invoke your custom logic for all incoming requests.<\/p>\n<p>Also remember that Sagemaker BYOC requires model_handler.py to implement another function <code>ping()<\/code><\/p>\n<p>So your &quot;model_handler.py&quot; should look like this -<\/p>\n<pre><code>custom_handler = CustomHandler()\n\n# define your own health check for the model over here\ndef ping():\n    return &quot;healthy&quot;\n\n\ndef handle(request, context): # context is necessary input otherwise Sagemaker will throw exception\n    if request is None:\n        return &quot;SOME DEFAULT OUTPUT&quot;\n    try:\n        response = custom_handler.predict_fn(request)\n        return [response] # Response must be a list otherwise Sagemaker will throw exception\n\n    except Exception as e:\n        logger.error('Prediction failed for request: {}. \\n'\n                     .format(request) + 'Error trace :: {} \\n'.format(str(e)))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-11 10:55:50.22 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-11-03 16:56:00.743 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Access .log files acc\\loss after training model (using .fit_generator) with multiple pauses",
        "Question_body":"<p>I'm using Amazon's SageMaker Studio Lab to train a model using a certain dataset.<br>\nThe code is as follow (which saves the History object in history variable):<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')  # Every run after runtime end, use the last saved model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n<\/code><\/pre>\n<p>I had to make several pauses due to ending runtime, and with each pause I saved the .log file. Now after appending those .log files, I'm trying to access them using the standard accuracy and loss plotting methods:<\/p>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>But the issue is I can't seem to manage to receive a working History object file.<br>\nAmong the things I tried:<br>\nI read about this possible method, of re-loading the model and trying to get it's history, but it didn't work, &quot;'NoneType' object has no attribute 'history'&quot;<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')\nhistory = model.history\n<\/code><\/pre>\n<p>Another try was using pandas package and loading the file, which generated an error &quot;'DataFrame' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = pd.read_csv('history.log', sep=',', engine='python')\n<\/code><\/pre>\n<p>And this try generated a CSVLogger object, &quot;'CSVLogger' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = CSVLogger('history.log')\n<\/code><\/pre>\n<p>Appreciate any help on how to recover the History object, so I can plot those results (if it's even possible?)...<br>\nThanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-03 14:04:28.913 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|matplotlib|keras|amazon-sagemaker",
        "Question_view_count":54,
        "Owner_creation_date":"2018-10-22 16:34:53.15 UTC",
        "Owner_last_access_date":"2022-09-24 16:47:21.423 UTC",
        "Owner_location":null,
        "Owner_reputation":119,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":"<p>Instead of recreating the History object, what I did was read the .log file using pandas package, <code>read_csv<\/code> method, and create a DataFrame data structure with the wanted columns and plot it. Code below:<\/p>\n<pre><code>history = pd.read_csv('history.log')\nhistory_acc = pd.DataFrame(history, columns=[&quot;accuracy&quot;, &quot;val_accuracy&quot;])\nhistory_loss = pd.DataFrame(history, columns=[&quot;loss&quot;, &quot;val_loss&quot;])\nplot_accuracy(history_acc,'plot title...')\nplot_loss(history_loss,'plot title...')\n<\/code><\/pre>\n<br>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>Hope this helps someone having the same issue as I did in the future.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-03 19:51:55.187 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How do you assign ressourses when using PyTorch on AWS Sagemaker?",
        "Question_body":"<p>I wish to use AWS Sagemaker in to train a PyTorch model. I am wondering how do I assign resources to the task? if I had my own computer I would use:<\/p>\n<pre><code>device = torch.cuda(&quot;cuda0&quot;)  \n<\/code><\/pre>\n<p>Is it the same for AWS Sagemaker<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-24 03:42:27.647 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"deep-learning|pytorch|amazon-sagemaker",
        "Question_view_count":51,
        "Owner_creation_date":"2020-08-12 00:09:17.843 UTC",
        "Owner_last_access_date":"2022-01-11 23:47:25.687 UTC",
        "Owner_location":"Cornwall, ON, Canada",
        "Owner_reputation":71,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-08-24 16:04:53.127 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"No space left on device [Amazon SageMaker]",
        "Question_body":"<p>I am working on training my model in P2.xlarge instance. When I download a dataset, I get the following error: \"Exception during downloading or extracting: [Errno 28] No space left on device\"\\\nI checked that P2.xlarge has 61GiB storage, translating to 64GB. I hardly have 5GB worth of data in my instance. Could you please let me know how to proceed?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_date":"2018-07-12 03:18:08.853 UTC",
        "Question_favorite_count":2.0,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":11293,
        "Owner_creation_date":"2015-10-29 09:52:03.787 UTC",
        "Owner_last_access_date":"2018-12-14 20:10:25.873 UTC",
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"import matplotlib failed while deploying my model in AWS sagemaker",
        "Question_body":"<p>I have deployed my AWS model successfully.<\/p>\n<p>but while testing i am getting runtime Error: &quot;<strong>import matplotlib.pyplot as plt<\/strong>&quot; . I think it is due to pytorch framework version i used(framework_version=1.2.0). I am facing the same issue when i use higher versions as well.<\/p>\n<pre><code>PyTorchModel(model_data=model_artifact,\n                 role = role,\n                 framework_version=1.2.0,\n                 entry_point='predict.py',\n                 predictor_cls=ImagePredictor)\n<\/code><\/pre>\n<p>I have other issue when i use version=1.0.0. i.e i am not able to import libraries from sub directories and deployment itself is failing.<\/p>\n<p>Eg: i have some code files in &quot;Code&quot; directory.<\/p>\n<pre><code> from Code.CTModel import NetWork  ---&gt; **this line will fail as &quot;No module named Code&quot; when i use version=1.0.0**\n<\/code><\/pre>\n<p><strong>Ultimately i want to how to use\/import libraries which are written under sub-directories.<\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-15 09:50:06.483 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|deep-learning|pytorch|amazon-sagemaker",
        "Question_view_count":356,
        "Owner_creation_date":"2020-07-15 09:17:15.23 UTC",
        "Owner_last_access_date":"2020-12-10 13:48:50.677 UTC",
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-07-15 10:08:54.26 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker RL with ray: ray.tune.error.TuneError: No trainable specified",
        "Question_body":"<p>I have a training script based on the AWS SageMaker RL example rl_network_compression_ray_custom but changed the env to make a basic gym env Asteroids-v0 (installing dependencies at main entrypoint to the training script). When I run the fit on the RLEstimator it gives the following error <code>ray.tune.error.TuneError: No trainable specified!<\/code> even if the run is specified in the training config as DQN.<\/p>\n\n<p>Does anyone know about this issue and how to solve it?<\/p>\n\n<p>Here is the longer log:<\/p>\n\n<pre><code>Running experiment with config {\n  \"training\": {\n    \"env\": \"Asteroids-v0\",\n    \"run\": \"DQN\",\n    \"stop\": {\n      \"training_iteration\": 1\n    },\n    \"local_dir\": \"\/opt\/ml\/output\/intermediate\",\n    \"checkpoint_freq\": 10,\n    \"config\": {\n      \"double_q\": false,\n      \"dueling\": false,\n      \"num_atoms\": 1,\n      \"noisy\": false,\n      \"prioritized_replay\": false,\n      \"n_step\": 1,\n      \"target_network_update_freq\": 8000,\n      \"lr\": 6.25e-05,\n      \"adam_epsilon\": 0.00015,\n      \"hiddens\": [\n        512\n      ],\n      \"learning_starts\": 20000,\n      \"buffer_size\": 1000000,\n      \"sample_batch_size\": 4,\n      \"train_batch_size\": 32,\n      \"schedule_max_timesteps\": 2000000,\n      \"exploration_final_eps\": 0.01,\n      \"exploration_fraction\": 0.1,\n      \"prioritized_replay_alpha\": 0.5,\n      \"beta_annealing_fraction\": 1.0,\n      \"final_prioritized_replay_beta\": 1.0,\n      \"num_gpus\": 0.2,\n      \"timesteps_per_iteration\": 10000\n    },\n    \"checkpoint_at_end\": true\n  },\n  \"trial_resources\": {\n    \"cpu\": 1,\n    \"extra_cpu\": 3\n  }\n}\nImportant! Ray with version &lt;=7.2 may report \"Did not find checkpoint file\" even if the experiment is actually restored successfully. If restoration is expected, please check \"training_iteration\" in the experiment info to confirm.\nTraceback (most recent call last):\n  File \"train-ray.py\", line 83, in &lt;module&gt;\n    MyLauncher().train_main()\n  File \"\/opt\/ml\/code\/sagemaker_rl\/ray_launcher.py\", line 332, in train_main\n    launcher.launch()\n  File \"\/opt\/ml\/code\/sagemaker_rl\/ray_launcher.py\", line 313, in launch\n    run_experiments(experiment_config)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/tune.py\", line 296, in run_experiments\n    experiments = convert_to_experiment_list(experiments)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/experiment.py\", line 199, in convert_to_experiment_list\n    for name, spec in experiments.items()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/experiment.py\", line 199, in &lt;listcomp&gt;\n    for name, spec in experiments.items()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/experiment.py\", line 122, in from_json\n    raise TuneError(\"No trainable specified!\")\nray.tune.error.TuneError: No trainable specified!\n2020-04-22 13:21:15,784 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand \"\/usr\/bin\/python train-ray.py --rl.training.checkpoint_freq 1 --rl.training.stop.training_iteration 1 --s3_bucket XXXXX\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-22 13:34:54.857 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|reinforcement-learning|amazon-sagemaker|ray|rllib",
        "Question_view_count":384,
        "Owner_creation_date":"2015-10-16 12:15:20.637 UTC",
        "Owner_last_access_date":"2022-09-14 15:22:35.597 UTC",
        "Owner_location":null,
        "Owner_reputation":396,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-22 15:20:00.6 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-21 17:56:05.813 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-23 13:48:10.417 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Question_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-19 10:13:08.283 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_date":"2022-02-16 03:15:56.94 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.55 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-19 23:22:51.72 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Model is trying to find an old docker image",
        "Question_body":"<p>I'm stuck in a situation where Sagemaker is looking for a docker image in ECS registry which I had to remove and I can't figure out how to make it forget about that.<\/p>\n<p>I had to rebuild a docker with torchserve for Sagemaker. I removed the old one (let's call it <code>torchserve-old-name<\/code>, and uploaded a new <code>torchserve:v2<\/code>. For some weird reason SM is still looking for the old one.<\/p>\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.predictor import Predictor\n\nimage = &quot;134244564256.dkr.ecr.us-west-2.amazonaws.com\/torchserve:v2&quot;\nmodel_data = &quot;s3:\/\/my-models\/torchserve\/my-model.tar.gz&quot;\nsm_model_name = 'my-model'\n\nprint(f&quot;docker image: {image}&quot;)\n# docker image: 134244564256.dkr.ecr.us-west-2.amazonaws.com\/torchserve:v2\n\ntorchserve_model = Model(model_data = model_data, \n                         image_uri = image,\n                         role  = role,\n                         predictor_cls=Predictor,\n                         name  = sm_model_name)\nendpoint_name = 'torchserve-endpoint-' + time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())\n\npredictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n                                    initial_instance_count=1,\n                                    endpoint_name = endpoint_name)\n<\/code><\/pre>\n<p>Results in:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-12-67fff0e32d2d&gt; in &lt;module&gt;\n      3 predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n      4                                     initial_instance_count=1,\n----&gt; 5                                     endpoint_name = endpoint_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\n    762             kms_key=kms_key,\n    763             wait=wait,\n--&gt; 764             data_capture_config_dict=data_capture_config_dict,\n    765         )\n    766 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\n   3454 \n   3455             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 3456         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   3457 \n   3458     def expand_role(self, role):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n   2957         )\n   2958         if wait:\n-&gt; 2959             self.wait_for_endpoint(endpoint_name)\n   2960         return endpoint_name\n   2961 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   3243                 ),\n   3244                 allowed_statuses=[&quot;InService&quot;],\n-&gt; 3245                 actual_status=status,\n   3246             )\n   3247         return desc\n UnexpectedStatusException: Error hosting endpoint torchserve-endpoint-2021-02-06-21-39-31: Failed. Reason: \n The repository 'torchserve-old-name' does not exist in the registry with id '134244564256'..\n<\/code><\/pre>\n<p>I am 100% sure that the old image 'torchserve-old-name' has not been mentioned in this code. I've restarted and re-ran notebook.<\/p>\n<p>Where is it cached, and how can I clear that cache? Sagemaker function documentation doesn't seem to mention it.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-02-07 04:47:16.417 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"docker|amazon-sagemaker",
        "Question_view_count":88,
        "Owner_creation_date":"2012-10-03 10:06:31.847 UTC",
        "Owner_last_access_date":"2022-09-25 03:37:17.877 UTC",
        "Owner_location":"Zurich, Switzerland",
        "Owner_reputation":12050,
        "Owner_up_votes":2598,
        "Owner_down_votes":4,
        "Owner_views":488,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-02-14 12:04:31.143 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"What is AWS SageMaker mAP referring to?",
        "Question_body":"<p>I'm running a model on AWS SageMaker, using their example object detection Jupyter notebook (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb<\/a>).  In the results it gives the following:<\/p>\n\n<p>validation mAP =(0.111078678154)<\/p>\n\n<p>I was wondering what this mAP score is referring to? <\/p>\n\n<p>I've used tensorflow, where it gives an averaged mAP(averages from .5IoU to .95IoU with .05 increments), mAP@.5IoU, mAP@.75IoU.  I've checked the documents on SageMaker, but cannot find anything referring to what the definition of mAP is.<\/p>\n\n<p>Is it safe to assume that the mAP score SageMaker reports is the \"averaged mAP(averages from .5IoU to .95IoU with .05 increments)\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-22 18:15:03.397 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":290,
        "Owner_creation_date":"2019-07-22 17:44:54.337 UTC",
        "Owner_last_access_date":"2019-07-30 22:50:49.533 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker ValidationException : Value '[]' at 'subnetIds' failed to satisfy constraint",
        "Question_body":"<hr \/>\n<h2>1 validation error detected\nValue '[]' at 'subnetIds' failed to satisfy constraint: Member must have length greater than or equal to 1<\/h2>\n<p>I want to make Sagemaker studio Domain in Ohio region, but I got \u2191 error.\nI also confirmed that vpc existed (no default) and one subnet existed.<\/p>\n<p>How can I fix the error? Please share your knowledge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-19 06:21:36.467 UTC",
        "Question_favorite_count":null,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":992,
        "Owner_creation_date":"2021-07-02 03:11:40.99 UTC",
        "Owner_last_access_date":"2022-06-14 08:38:59.98 UTC",
        "Owner_location":null,
        "Owner_reputation":87,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How do I parse in the arguments to my Python file when running the Sagemaker pipeline's ProcessingStep?",
        "Question_body":"<p>I read from this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep\" rel=\"nofollow noreferrer\">documentation<\/a> that the ProcessingStep can accept job arguments.<\/p>\n<p>I currently have a python script contaning a function to be executed via ProcessingStep that requires arguments to be parsed in. I am not sure how I can extract the arguments from the 'Job arguments' such that I can call the function in the python script with the arguments.<\/p>\n<p>Here is an example of code snippet from my python script:<\/p>\n<pre><code>def params(input_params):\n    details = {&quot;database&quot;: input_params[0],\n         &quot;table&quot;: input_params[1], \n         &quot;catalog&quot;: input_params[2], \n         &quot;earliestday&quot;: int(input_params[3]), \n         &quot;latestday&quot;: int(input_params[4]),\n         &quot;s3bucket&quot;: input_params[5], \n         &quot;bucketpath&quot;: input_params[6]}\n    return details\n\noutput_params = params(input_params) #this is where I'm not sure how I can extract the argument from the job arguments in the ProcessingStep to call my function here\n<\/code><\/pre>\n<p>Here's how my processingstep code looks like:<\/p>\n<pre><code>step_params = ProcessingStep(\n    name=&quot;StateParams&quot;,\n    processor=sklearn_processor, \n    outputs = [processing_output],\n    job_arguments = [&quot;ABC&quot;, &quot;SESSION_123&quot;, &quot;AwsDataCatalog&quot;, &quot;5&quot;, &quot;7&quot;, &quot;mybucket&quot;, &quot;bucket2\/tmp\/athena_sagemaker&quot;],   #This is the job argument I input which I hope will be parsed into my python file function\n    code = &quot;params.py&quot;,\n)\n<\/code><\/pre>\n<p>Would greatly appreciate if any of you can advice me on how I can go about using the job arguments in the ProcessingStep to successfully call the function in the python script, thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-11 08:04:05.253 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":551,
        "Owner_creation_date":"2021-06-09 07:10:46.127 UTC",
        "Owner_last_access_date":"2022-09-08 12:58:09.29 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Question_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-11-05 18:28:16.053 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|object-detection|amazon-sagemaker",
        "Question_view_count":256,
        "Owner_creation_date":"2017-10-13 07:37:10.153 UTC",
        "Owner_last_access_date":"2022-09-20 08:00:20.06 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":180,
        "Owner_up_votes":268,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-06 02:23:45.8 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-28 23:26:47.103 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-06-02 00:17:48.707 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Invoking Sagemaker MultiDataModel Endpoint throws \"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation\"",
        "Question_body":"<p>I'm trying to create a multi-model endpoint on sagemaker, using pre-trained tensorflow models  which were uploaded to s3 (tar.gz files). Creating a 'single-model' endpoint works fine with both of them.<\/p>\n<p>I followed a few blog posts for this task (<a href=\"https:\/\/dataintegration.info\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/towardsdatascience.com\/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f\" rel=\"nofollow noreferrer\">2<\/a>).<\/p>\n<p>I've successfully deployed a MultiDataModel endpoint on Sagemaker (code attached below the error), but when trying to invoke a model (any of them) I received the following error:<\/p>\n<pre><code>~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/model.py in predict(self, data, initial_args)\n    105                 args[&quot;CustomAttributes&quot;] = self._model_attributes\n    106 \n--&gt; 107         return super(TensorFlowPredictor, self).predict(data, args)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n    159             data, initial_args, target_model, target_variant, inference_id\n    160         )\n--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    162         return self._handle_response(response)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    414             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 415             return self._make_api_call(operation_name, kwargs)\n    416 \n    417         _api_call.__name__ = str(py_operation_name)\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    744             error_class = self.exceptions.from_code(error_code)\n--&gt; 745             raise error_class(parsed_response, operation_name)\n    746         else:\n    747             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: \nReceived server error (504) from model with message &quot;&lt;html&gt;\n&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;\/title&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;\/h1&gt;&lt;\/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx\/1.20.2&lt;\/center&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n&quot;. See https:\/\/eu-central-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-central- 1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/mme-tensorflow-2022-05-29-06-38-29 in \naccount ******** for more information.\n<\/code><\/pre>\n<p>Here is the code for creating and deploying the models and the endpoint:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nrating_model_archive = &quot;rating_model.tar.gz&quot;\nsim_users_model_archive = &quot;sim_users_model.tar.gz&quot;\ncurrent_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n\nsagemaker_model_rating = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{rating_model_archive}',\n                                         name = f'rating-model-{current_time}',\n                                         role = role,\n                                         framework_version = &quot;2.8&quot;, #tf.__version__,\n                                         entry_point = 'empty_train.py',\n                                         sagemaker_session=sagemaker_session)\n\nsagemaker_model_sim = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{sim_users_model_archive}',\n                                      name = f'similar-users-model-{current_time}',\n                                      role = role,\n                                      framework_version = &quot;2.8&quot;, #tf.__version__,\n                                      entry_point = 'empty_train.py',\n                                      sagemaker_session=sagemaker_session)\n\nmodel_data_prefix = f's3:\/\/{bucket_name}\/model\/'\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=sagemaker_model_rating,\n                     sagemaker_session=sagemaker_session)\n\ntf_predictor = mme.deploy(initial_instance_count=2,\n                          instance_type=&quot;ml.m4.xlarge&quot;,#'ml.t2.medium',\n                          endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>Up until here, as mentioned earlier, it works fine, and I have a running endpoint.\nWhen trying to invoke it with the following code, I get the aforementioned error:<\/p>\n<pre><code>input1 = {\n    &quot;instances&quot;: [\n        {&quot;user_id&quot;: [854],\n         &quot;item_id&quot;: [123]}\n                 ]\n}\n\ninput2 = {\n    &quot;instances&quot;: [12]\n}\n\ntf_predictor.predict(data=input2, initial_args={'TargetModel': sim_users_model_archive})\n# tf_predictor.predict(data=input1, initial_args={'TargetModel': rating_model_archive})\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-29 08:23:49.237 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_date":"2022-05-25 12:31:07.867 UTC",
        "Owner_last_access_date":"2022-08-08 20:26:46.947 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Automate data preprocessing on AWS sagemaker",
        "Question_body":"<p>Is there a way to Automate the data preprocessing on Sagemaker on weekly basis \nthe preprocessing can include many simple transformations over the data in MBs from S3. \n<p>My Idea of Automation is like Triggering a script or a notebook instance that could run weekly<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-26 18:09:46.887 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|automation|etl|amazon-sagemaker",
        "Question_view_count":326,
        "Owner_creation_date":"2018-06-05 14:58:09.65 UTC",
        "Owner_last_access_date":"2020-06-29 00:35:24.063 UTC",
        "Owner_location":"Jersey City, NJ, USA",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon sagemaker. SKlearn estimator vs Tensorflow estimator - why requirements_file is not present in one of them?",
        "Question_body":"<p>I am looking at definitions of two estimators SKLearn and Tensorflow in Amazon Sagemaker:<\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a><\/p>\n\n<pre><code>class sagemaker.sklearn.estimator.SKLearn(entry_point, framework_version='0.20.0', source_dir=None, hyperparameters=None, py_version='py3', image_name=None, **kwargs)\n\nclass sagemaker.tensorflow.estimator.TensorFlow(training_steps=None, evaluation_steps=None, checkpoint_path=None, py_version='py2', framework_version=None, model_dir=None, requirements_file='', image_name=None, script_mode=False, distributions=None, **kwargs)\n<\/code><\/pre>\n\n<p>Tensorflow has requirements_file parameter, while SKLearn does not. Is there reason why? How can I add <code>requirements.txt<\/code> to SKLearn estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-06 18:09:44.387 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|requirements|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_date":"2012-09-26 16:34:10.37 UTC",
        "Owner_last_access_date":"2022-09-25 01:21:08.267 UTC",
        "Owner_location":null,
        "Owner_reputation":6498,
        "Owner_up_votes":1266,
        "Owner_down_votes":2,
        "Owner_views":988,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to invoke a sagemaker endpoint from Grafana",
        "Question_body":"<p>I am trying with AWS sagemaker and created endpoint successfully. I want to visualize the data on Grafana now. Can anyone suggest an approach to this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 09:43:59 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|grafana|amazon-sagemaker",
        "Question_view_count":49,
        "Owner_creation_date":"2020-07-26 17:36:21.563 UTC",
        "Owner_last_access_date":"2022-09-15 14:20:33.707 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Which SageMaker server supports server-side batching and how to enable it?",
        "Question_body":"<p>MMS, TFServing and TorchServe support <strong>server-side batching<\/strong> (consequent requests can be locally batched in async fashion by the server while maintaining the illusion of synchronous batch-1 size to the client). How to enable those features on SageMaker endpoints?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 17:54:11.52 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":17,
        "Owner_creation_date":"2014-10-07 08:13:42.83 UTC",
        "Owner_last_access_date":"2022-09-23 14:45:05.23 UTC",
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-04-25 23:43:37.877 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_date":"2011-02-23 18:00:07.147 UTC",
        "Owner_last_access_date":"2022-09-24 18:44:13.9 UTC",
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":9271,
        "Owner_up_votes":2074,
        "Owner_down_votes":44,
        "Owner_views":1819,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-04-30 23:01:07.447 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Question_body":"<p>I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\nThe name of the inputs are:<\/p>\n\n<pre><code>1.    input_image\n2.    input_image_meta\n3.    input_anchors\n<\/code><\/pre>\n\n<p>and the name of outputs are:<\/p>\n\n<pre><code>1    output_detections\n2    output_mrcnn_class\n3    output_mrcnn_bbox\n4    output_mrcnn_mask\n5    output_rois\n<\/code><\/pre>\n\n<p>I have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting {'error': \"Missing 'inputs' or 'instances' key\"} in return.<\/p>\n\n<p>The sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\nOn the client side, I call the predictor using follwoing code:<\/p>\n\n<pre><code>request = {}\nrequest[\"img_link\"] = \"image.jpg\"\nresult = predictor.predict(request)\n<\/code><\/pre>\n\n<p>But when I print the result the following gets printed out, {'error': \"Missing 'inputs' or 'instances' key\"}\nAll the bucket connections for loading the image are in inference.py<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-08-30 17:28:20.26 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"json|tensorflow|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":798,
        "Owner_creation_date":"2019-08-30 17:20:07.19 UTC",
        "Owner_last_access_date":"2019-09-03 13:23:31.813 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"which one to use to process data for sagemaker batch inferencing pipeline - SKlearnEstimator or SKlearnProcessor",
        "Question_body":"<p>I'm building a Sagemaker batch inferencing pipeline and get confused about the options to process features (before inferencing) between using <code>sagemaker.sklearn.processing.SKLearnProcessor<\/code> and <code>sagemaker.sklearn.estimator.SKLearn<\/code>\nMy understanding of these two options are:<\/p>\n<p>There are docs from aws to use <code>sagemaker.sklearn.estimator.SKLearn<\/code> to do the batch transformation to process the data.\nThe pros of using this class and its <code>.create_model()<\/code> method is that I can incorporate the created model(to process the feature before inferencing) to <code>sagemaker.pipeline.PipelineModel<\/code> which's deployed on endpoint. so the whole pipeline is behind a single endpoint to be called when inference request input in. This detailed from:\n<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html<\/a>\n<em><strong>I don't know the specific cons, and that's the first question (1).<\/strong><\/em><\/p>\n<p>However, if it's only for data processing, I can also use <code>sagemaker.sklearn.processing.SKLearnProcessor<\/code> to create Sagemaker Processing jobs to process features, then dump to s3 for model to batch inferencing.\nThe pros to me is that it's making more sense to me to have a job that designed for processing, but cons is that it seems like I have to write a handler to pipeline the processing and inferencing myself, unlike the sagemaker.sklearn.estimator.SKLearn.\n<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.html<\/a>\n<em><strong>So, my next question (2) is there a way to involve SKLearnProcessor in the sagemaker.pipeline.PipelineModel? if not, the following up question (3) is that if SKLearnProcessor is not designed for using in inferencing, what's the use case of it.<\/strong><\/em><\/p>\n<p><em><strong>The final question (4) is that from efficiency perspective, what's pros and cons using each method in a Sagemaker batch inferencing pipeline?<\/strong><\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-25 04:38:25.463 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|amazon-sagemaker|data-processing",
        "Question_view_count":8,
        "Owner_creation_date":"2016-08-26 09:07:46.917 UTC",
        "Owner_last_access_date":"2022-09-25 04:38:07.563 UTC",
        "Owner_location":null,
        "Owner_reputation":505,
        "Owner_up_votes":53,
        "Owner_down_votes":19,
        "Owner_views":52,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Tensorflow Session management issue | When session managed by other TF module",
        "Question_body":"<p>I am  using an external module for tensorflow work, getting session management issue when try to use it in flask application from pickle file, can someone please help me<\/p>\n\n<blockquote>\n  <p>ValueError: Tensor(\"TensorSliceDataset:0\", shape=(), dtype=variant)\n  must be from the same graph as Tensor(\"Iterator:0\", shape=(),\n  dtype=resource).<\/p>\n<\/blockquote>\n\n<p>Sample code structure<\/p>\n\n<pre><code>from io import StringIO\nimport flask\nimport pandas as pd\nimport tensorrec as tr\nimport tensorflow as tf\n\n\nclass ScoringService(object):\n    model = None                \n    model_path = 'path for pickle file'\n    @classmethod\n    def get_model(cls):\n        if cls.model == None:\n            cls.model = tr.TensorRec.load_model(model_path)\n\n        return cls.model\n\n    @classmethod\n    def get_reco(cls, model,use_ft, ite_ft):\n            tf.reset_default_graph()\n            predictions = model.predict(use_ft, ite_ft)\n        return predictions\n\n    @classmethod\n    def predict(cls, input):\n        clf = cls.get_model()\n        n_reco_test = cls.get_reco(clf, input,use_ft, ite_ft)\n        return n_reco_test\n\n# The flask app for serving predictions\napp = flask.Flask(__name__)\n\n@app.route('\/invocations', methods=['POST'])\ndef transformation():\n    data = None\n\n    # Convert from CSV to pandas\n    if flask.request.content_type == 'text\/csv':\n        data = flask.request.data.decode('utf-8')\n        s = StringIO(data)\n        data = pd.read_csv(s, header=None)\n    else:\n        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text\/plain')\n    # Do the prediction\n    predictions = ScoringService.predict(data)\n    return flask.Response(response=predictions, status=200,mimetype='text\/csv')\n<\/code><\/pre>\n\n<p>more detail about error <\/p>\n\n<pre><code>\"POST \/invocations HTTP\/1.1\" 500 291 \"-\" \"AHC\/2.0\"\nFile \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/framework\/op_def_library.py\", line 350, in _apply_op_helper\ng = ops._get_graph_from_inputs(_Flatten(keywords.values()))\nFile \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/framework\/ops.py\", line 5637, in _get_graph_from_inputs\n_assert_same_graph(original_graph_element, graph_element)\nFile \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/framework\/ops.py\", line 5573, in _assert_same_graph\noriginal_item)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2018-12-06 04:42:53.607 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_date":"2017-07-30 08:26:08.107 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.87 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Question_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-22 21:37:03.92 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":1426,
        "Owner_creation_date":"2015-09-25 17:16:18.36 UTC",
        "Owner_last_access_date":"2022-05-26 17:59:10.97 UTC",
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-13 03:56:54.357 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2018-03-22 21:42:13.88 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Spark S3 access issue",
        "Question_body":"<p>I am new in AWS sagemaker. I created a notebook in a VPC with private subnet, kms default encrypted key, root access, no direct internet access. I have attached policy which have full access to Sagemaker and S3 in IAM as per documentations.  Now while one of data scientist trying to run his code in jupyter, getting below error. I can see jar files (\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/), I have even given access key and secret key in code, is there anything we are doing wrong here<\/p>\n\n<pre><code>import os\nimport boto3\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\nimport pyspark\n\nrole = get_execution_role()\nspark = SparkSession.builder \\\n            .appName(\"app_name2\") \\\n            .getOrCreate()\n\nsc=pyspark.SparkContext.getOrCreate()\nsc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n\nhadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", 'access_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", 'secret_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\nspark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\");\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\ndf= spark.read.csv(\"s3a:\/\/mybucket\/ConsolidatedData\/my.csv\",header=\"true\")\n\n\nPy4JJavaError: An error occurred while calling o579.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n    at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:709)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-09 19:19:10.823 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":933,
        "Owner_creation_date":"2017-12-21 19:07:16.66 UTC",
        "Owner_last_access_date":"2022-09-23 04:14:35.81 UTC",
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-03-10 05:25:07.99 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Connect Jupyter Notebook (locally) to AWS s3 without SageMaker",
        "Question_body":"<p>Is it possible to connect jupyter notebook that is running locally to one of the buckets on AWS S3 without using SageMaker and involving <strong>no or with<\/strong> access and secret keys?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-15 20:37:45.887 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|jupyter|amazon-sagemaker",
        "Question_view_count":7218,
        "Owner_creation_date":"2017-07-18 02:02:41.2 UTC",
        "Owner_last_access_date":"2022-07-30 14:44:42.427 UTC",
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-10-15 21:12:25.217 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Invalid .lst file in sagemaker",
        "Question_body":"<p>Folder structure for my S3 bucket is:<\/p>\n\n<pre><code>Bucket\n    -&gt;training-set\n           -&gt;medium\n                 -&gt;    img1.jpeg\n                 -&gt;    img2.jpeg\n                 -&gt;    img3.PNG\n<\/code><\/pre>\n\n<p>My training-set.lst file looks like this:<\/p>\n\n<pre><code>1  \\t 1  \\t medium\/img1.jpeg\n2  \\t 1  \\t medium\/img2.jpeg\n3  \\t 1  \\t medium\/img3.PNG\n<\/code><\/pre>\n\n<p>I created this using excel sheet.<\/p>\n\n<p>Error:\nTraining failed with the following error: ClientError: Invalid lst file: training-set.lst<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n          \"ChannelName\": \"train\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/training-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/test-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"train_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/training-set\/training-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/test-set\/test-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        }\n    ]\n<\/code><\/pre>\n\n<p>I am trying to use this in Amazon Sagemaker. But I'm unable to do that. Can someone please help?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-03 10:22:31.55 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|image-recognition|amazon-sagemaker",
        "Question_view_count":591,
        "Owner_creation_date":"2018-04-15 09:58:20.32 UTC",
        "Owner_last_access_date":"2022-09-22 03:17:07.68 UTC",
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":177,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2018-08-03 11:01:42.06 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Question_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-04-01 20:44:18.56 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|ubuntu|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":1398,
        "Owner_creation_date":"2018-05-14 09:05:19.837 UTC",
        "Owner_last_access_date":"2022-07-01 15:18:50.603 UTC",
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-04-01 21:13:30.733 UTC",
        "Answer_last_edit_date":"2021-04-01 21:20:29.39 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-04-02 17:15:27.407 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Inference with Redis",
        "Question_body":"<p>I have a custom inference container on Sagemaker that runs a Flask API to handle the incoming calls. Around this, I have another API with a Lambda that calls the respective Sagemaker endpoint. The underlying model is generating vector embeddings for incoming sentences.<\/p>\n<p>Now, I would like to enable caching and store already computed vectors with Redis.<\/p>\n<p>My question: Does it make more sense to enable Redis on the inference container or in the Lambda API wrapped around the endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-18 10:22:23.78 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|redis|amazon-sagemaker",
        "Question_view_count":109,
        "Owner_creation_date":"2014-03-13 09:33:37.223 UTC",
        "Owner_last_access_date":"2022-09-15 12:15:05.39 UTC",
        "Owner_location":"Cologne, Germany",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Evaluating payload provided when calling invoke endpoint operation",
        "Question_body":"<p>I'm working on a credit fraud dataset on sagemaker. I'm using a linear learner binary classification algorithm. I divided the data into training and test sets and got the results for test set. When I tried to evaluate model performance characteristics on training set, I'm getting the following error<\/p>\n\n<pre><code> An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error \n(400) from model with message \"unable to evaluate payload provided\".\n<\/code><\/pre>\n\n<p>I mentioned the code below<\/p>\n\n<pre><code>train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'train\/examples') #making train_data\n\n#making test_data\ntest_key = \"{}\/test\/examples\".format(prefix)\ns3.Bucket(bucket).download_file(test_key, 'test_data')\n\n#preparing train channels for training the data\ntrain_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\n\n#training the data\nlinear.fit(inputs=train_channel,  logs=True)\n\n#creating the endpoint\nlinear_predictor = linear.deploy(initial_instance_count=1,\n                             instance_type='ml.m4.xlarge')\n\n#getting the results on test_data\nl = []\nwith open('test_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    l.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(l[0:10])\n\n#getting the results on train_data\n#THE CODE BELOW IS THROWING THE ABOVE MENTIONED ERROR\nq =[]\nwith open('train_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    q.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(q[0:10])\n<\/code><\/pre>\n\n<p>I'm getting the results on test data. I stored it in list l. For getting the results on the training set, I followed the similar procedure, but I'm getting the above mentioned error. Can someone please offer a resolution for this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-11 13:04:57.977 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|aws-sdk|amazon-sagemaker",
        "Question_view_count":1769,
        "Owner_creation_date":"2019-09-12 20:07:41.627 UTC",
        "Owner_last_access_date":"2022-09-21 14:35:39.23 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p>\"unable to evaluate payload provided\" occurs only when the input data format is not compatible with the ML model you created. In this case, to get results on training set, we need to remove the last column(label column) before passing it to endpoint<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-19 11:18:40.51 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2019-11-12 08:41:42.69 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Is it possible to use the multiclass classifier of aws to recognize the given place of the text?",
        "Question_body":"<p>I'm using AWS SageMaker, and i want to create something that, with a given text, it recognize the place of that description. Is it possible?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-05 16:06:23.273 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|classification|amazon-sagemaker|multiclass-classification",
        "Question_view_count":103,
        "Owner_creation_date":"2018-04-16 15:34:34.82 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:41.44 UTC",
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Install spark 3.2 in SageMaker Notebook",
        "Question_body":"<p>SageMaker supports pyspark 2.4 and I want to install latest version of pyspark in Sagemaker. Can you please let me know how can I install latest version of pyspark in SageMaker notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-19 17:41:23.47 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_date":"2022-03-30 14:13:10.15 UTC",
        "Owner_last_access_date":"2022-09-19 23:04:55.96 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Run Sagemaker notebook instance and be able to close tab",
        "Question_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-03-10 14:31:23.383 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1154,
        "Owner_creation_date":"2019-10-27 20:29:51.12 UTC",
        "Owner_last_access_date":"2022-09-22 11:53:24.677 UTC",
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":19,
        "Owner_down_votes":4,
        "Owner_views":45,
        "Answer_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-07 07:53:48.663 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How send multiple request to AWS Sagemaker Endpoint on single invoke?",
        "Question_body":"<p>I've deployed a Deep learning model on SageMaker endpoint and can request\/get answer using <code>sagemaker_client.invoke_endpoint<\/code>. But each <code>invoke_endpoint<\/code> accepts single body. How can I send multiple body to get multiple result on single request?<\/p>\n\n<p>I've tried setting <code>body='{\"instances\": [myData1, myData2]}'<\/code> but It recognizes as single string.<\/p>\n\n<pre><code>def sagemaker_handler(doc):\n    data = doc.encode(\"UTF-8\")\n    response = sagemaker_client.invoke_endpoint(EndpointName='myEndpoint',\n                                                ContentType='application\/json',\n                                                Accept='application\/json', Body=data)\nreturn response\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2019-01-17 10:26:48.33 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4534,
        "Owner_creation_date":"2014-05-22 19:34:18.19 UTC",
        "Owner_last_access_date":"2022-07-18 18:31:30.377 UTC",
        "Owner_location":"Dhaka, Bangladesh",
        "Owner_reputation":46,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Issues with prediction time not proportional w.r.t. number of trees in RF",
        "Question_body":"<p>My estimation strategy consists on using a set of Random Forest models, each one concerns some\nsubset of data (ex : RF_A if feature == A). This has been said seek of completeness as I dont think this affects my issue.<\/p>\n\n<p>My deployment strategy:<\/p>\n\n<ul>\n<li>Fit: return a pickle that contains a dictionary of fitted sklearn Random\nForest models<\/li>\n<li>Deploy: load these dictionaries in memory.<\/li>\n<li>Inference:\n1) maps each observation to the correct model in the already loaded dictionary\n2) for each observation, computes predictions given by each tree in order to allow for elementary confidence interval computation\n<a href=\"http:\/\/blog.datadive.net\/prediction-intervals-for-random-forests\/\" rel=\"nofollow noreferrer\">http:\/\/blog.datadive.net\/prediction-intervals-for-random-forests\/<\/a>\nNote that this last operation is the most time consuming in the inference and the time is proportional to the number of trees in my RF (loop w.r.t. trees).<\/li>\n<\/ul>\n\n<p>My code (my custom code in lib) :<\/p>\n\n<pre><code>import argparse\nimport os\nimport sys\nimport pandas as pd\nfrom sklearn.externals import joblib\nmodule_path = os.path.abspath('\/opt\/ml\/code')\nif module_path not in sys.path:\n    sys.path.append(module_path)\nfrom lib import training, prediction\nfrom data.transactions import raw\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    args = parser.parse_args()\n    grid_models_dict =\\\n        training.train_models_in_dict(raw_training_data=raw)\n    joblib.dump(grid_models_dict, os.path.join(args.model_dir, \"model\"))\ndef model_fn(model_dir):\n    grid_models_dict = joblib.load(os.path.join(model_dir, \"model\"))\n    return grid_models_dict\ndef predict_fn(input_data, model):\n    predicted = prediction.predict(input_data, model)\n    return predicted\n<\/code><\/pre>\n\n<p>My problem :<\/p>\n\n<p>Say I have two deployments scenarios : one with 100 trees\/RF and one with 300 trees\/RF.<\/p>\n\n<p>Fit is performed without issues. On S3 : compressed 100 trees\/RF pickle is 261 Mo and compressed 300 trees\/RF is 784 Mo.<\/p>\n\n<p>Deploy is done with some issues : some timeout with some workers with the 300 trees\/RF already reported for example awslabs\/amazon-sagemaker-examples#556, but it deploy at the end.<\/p>\n\n<p>Prediction is performed :<\/p>\n\n<ul>\n<li>with the 100 trees\/RF in around 500 ms, always, with the same observation<\/li>\n<li><p>with the 300 trees\/RF: in paper, with the same observation, due my prediction nature which is a for loop w.r.t. trees, I am supposed to predict in less than 1 second.<\/p><\/li>\n<li><p>with the 300 trees\/RF, in practice, with the same observation :\n1) sometimes (33% of cases) in 700 ms,\n2) sometimes (33% of cases) in 40 to 50 seconds,\n3) and sometimes (33% of cases) I have a timeout error (inference timeout is limited to 60 seconds)<\/p><\/li>\n<\/ul>\n\n<p>This behavior remains when I deploy in a bigger\/recent machine. (ml.t2.xlarge to ml.c5.4xlarge)<\/p>\n\n<p>My guess is that there is a memory swapping mechanism or that the container's memory is not fully privately allocated to me after some threshold.<\/p>\n\n<p>Is there any solution to predict consistently with more than 100trees\/RF ?<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Issue also reported here :\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/681\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/681<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-04 13:45:40.937 UTC",
        "Question_favorite_count":0.0,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":64,
        "Owner_creation_date":"2017-02-06 13:27:54.053 UTC",
        "Owner_last_access_date":"2022-09-19 08:33:15.697 UTC",
        "Owner_location":null,
        "Owner_reputation":190,
        "Owner_up_votes":4,
        "Owner_down_votes":2,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-03-05 09:54:14.84 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Install packages from source on Amazon Sagemaker studio",
        "Question_body":"<p>I'm installing from source some packages <strong>cloning the Git repository<\/strong> on an <strong>Amazon Sagemaker Studio notebook<\/strong>.<\/p>\n<p>Out of Sagemaker I managed to install for example <code>neuralcoref<\/code> without any problem:<\/p>\n<pre><code>git clone https:\/\/github.com\/huggingface\/neuralcoref.git\ncd neuralcoref\npip install -r requirements.txt\npip install -e .\n<\/code><\/pre>\n<p>running the example in the README with success.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Load your usual SpaCy model (one of SpaCy English models)\nimport spacy\nnlp = spacy.load('en')\n\n# Add neural coref to SpaCy's pipe\nimport neuralcoref\nneuralcoref.add_to_pipe(nlp)\n\n# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\ndoc = nlp(u'My sister has a dog. She loves him.')\n\ndoc._.has_coref\ndoc._.coref_clusters\n<\/code><\/pre>\n<p>But if I do the same on an Amazon Sagemaker Studio notebook I get an error during the installation:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: spacy&lt;3.0.0,&gt;=2.1.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from -r ..\/neuralcoref\/requirements.txt (line 1)) (2.3.7)\nRequirement already satisfied: cython&gt;=0.25 in \/opt\/conda\/lib\/python3.7\/site-packages (from -r ..\/neuralcoref\/requirements.txt (line 2)) (0.29.15)\nRequirement already satisfied: pytest in \/opt\/conda\/lib\/python3.7\/site-packages (from -r ..\/neuralcoref\/requirements.txt (line 3)) (5.3.5)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.1.3)\nRequirement already satisfied: thinc&lt;7.5.0,&gt;=7.4.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (7.4.5)\nRequirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (56.2.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (3.0.5)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (2.0.5)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (2.25.1)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (4.42.1)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.0.0)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (0.7.4)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.0.5)\nRequirement already satisfied: numpy&gt;=1.15.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.20.0)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.0.5)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (0.8.2)\nRequirement already satisfied: importlib-metadata&gt;=0.20 in \/opt\/conda\/lib\/python3.7\/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.5.0)\nRequirement already satisfied: zipp&gt;=0.5 in \/opt\/conda\/lib\/python3.7\/site-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (3.5.0)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (2021.5.30)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;-r ..\/neuralcoref\/requirements.txt (line 1)) (2.8)\nRequirement already satisfied: py&gt;=1.5.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (1.10.0)\nRequirement already satisfied: packaging in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (21.0)\nRequirement already satisfied: attrs&gt;=17.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (19.3.0)\nRequirement already satisfied: more-itertools&gt;=4.0.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (8.2.0)\nRequirement already satisfied: pluggy&lt;1.0,&gt;=0.12 in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (0.13.1)\nRequirement already satisfied: wcwidth in \/opt\/conda\/lib\/python3.7\/site-packages (from pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (0.1.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from packaging-&gt;pytest-&gt;-r ..\/neuralcoref\/requirements.txt (line 3)) (2.4.6)\nWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https:\/\/pip.pypa.io\/warnings\/venv\nWARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\nYou should consider upgrading via the '\/opt\/conda\/bin\/python -m pip install --upgrade pip' command.\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nObtaining file:\/\/\/root\/experiments\/neuralcoref\nRequirement already satisfied: numpy&gt;=1.15.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from neuralcoref==4.0) (1.20.0)\nRequirement already satisfied: boto3 in \/opt\/conda\/lib\/python3.7\/site-packages (from neuralcoref==4.0) (1.17.74)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from neuralcoref==4.0) (2.25.1)\nRequirement already satisfied: spacy&lt;3.0.0,&gt;=2.1.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from neuralcoref==4.0) (2.3.7)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;neuralcoref==4.0) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;neuralcoref==4.0) (2021.5.30)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;neuralcoref==4.0) (1.26.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in \/opt\/conda\/lib\/python3.7\/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;neuralcoref==4.0) (2.8)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (1.1.3)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (3.0.5)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (2.0.5)\nRequirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (56.2.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (4.42.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (0.7.4)\nRequirement already satisfied: thinc&lt;7.5.0,&gt;=7.4.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (7.4.5)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (1.0.5)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (1.0.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (0.8.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (1.0.5)\nRequirement already satisfied: importlib-metadata&gt;=0.20 in \/opt\/conda\/lib\/python3.7\/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (1.5.0)\nRequirement already satisfied: zipp&gt;=0.5 in \/opt\/conda\/lib\/python3.7\/site-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;3.0.0,&gt;=2.1.0-&gt;neuralcoref==4.0) (3.5.0)\nRequirement already satisfied: s3transfer&lt;0.5.0,&gt;=0.4.0 in \/opt\/conda\/lib\/python3.7\/site-packages (from boto3-&gt;neuralcoref==4.0) (0.4.2)\nRequirement already satisfied: botocore&lt;1.21.0,&gt;=1.20.74 in \/opt\/conda\/lib\/python3.7\/site-packages (from boto3-&gt;neuralcoref==4.0) (1.20.74)\nRequirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from boto3-&gt;neuralcoref==4.0) (0.10.0)\nRequirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in \/opt\/conda\/lib\/python3.7\/site-packages (from botocore&lt;1.21.0,&gt;=1.20.74-&gt;boto3-&gt;neuralcoref==4.0) (2.8.1)\nRequirement already satisfied: six&gt;=1.5 in \/opt\/conda\/lib\/python3.7\/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.21.0,&gt;=1.20.74-&gt;boto3-&gt;neuralcoref==4.0) (1.14.0)\nInstalling collected packages: neuralcoref\n  Attempting uninstall: neuralcoref\n    Found existing installation: neuralcoref 4.0\n    Can't uninstall 'neuralcoref'. No files were found to uninstall.\n  Running setup.py develop for neuralcoref\n    ERROR: Command errored out with exit status 1:\n     command: \/opt\/conda\/bin\/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/root\/xperiments\/neuralcoref\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/root\/experiments\/neuralcoref\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' develop --no-deps\n         cwd: \/root\/experiments\/neuralcoref\/\n    Complete output (15 lines):\n    Cythonizing sources\n    running develop\n    running egg_info\n    writing neuralcoref.egg-info\/PKG-INFO\n    writing dependency_links to neuralcoref.egg-info\/dependency_links.txt\n    writing requirements to neuralcoref.egg-info\/requires.txt\n    writing top-level names to neuralcoref.egg-info\/top_level.txt\n    adding license file 'LICENSE.txt' (matched pattern 'LICEN[CS]E*')\n    reading manifest file 'neuralcoref.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'neuralcoref.egg-info\/SOURCES.txt'\n    running build_ext\n    building 'neuralcoref.neuralcoref' extension\n    gcc -pthread -B \/opt\/conda\/compiler_compat -Wl,--sysroot=\/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I\/opt\/conda\/include\/python3.7m -I\/root\/experiments\/neuralcoref\/include -I\/opt\/conda\/include\/python3.7m -c neuralcoref\/neuralcoref.cpp -o build\/temp.linux-x86_64-3.7\/neuralcoref\/neuralcoref.o -O2 -Wno-strict-prototypes -Wno-unused-function\n    error: command 'gcc' failed with exit status 1\n    ----------------------------------------\n  ERROR: Can't roll back neuralcoref; was not uninstalled\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/root\/experiments\/neuralcoref\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/root\/experiments\/neuralcoref\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' develop --no-deps Check the logs for full command output.\n<\/code><\/pre>\n<p>I tried to do: <code>conda develop .<\/code> instead of <code>pip install -e .<\/code> and it doesn't give error, but in the execution of the script it raises error. <code>neuralcoref<\/code> is only an example: I installed <code>fairseq<\/code> with the same operations and it gives me an error like this:<\/p>\n<pre><code>ImportError: Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`\n<\/code><\/pre>\n<p>It's like in Sagemaker I cannot install Cython dependencies with <code>pip install --editable .<\/code>. How can I do that?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-07-22 17:39:22.04 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|pip|jupyter-notebook|conda|amazon-sagemaker",
        "Question_view_count":1313,
        "Owner_creation_date":"2019-12-20 09:12:25.98 UTC",
        "Owner_last_access_date":"2022-09-23 14:56:14.933 UTC",
        "Owner_location":null,
        "Owner_reputation":465,
        "Owner_up_votes":62,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Running ML preprocessing job in AWS",
        "Question_body":"<p>My dear people,<\/p>\n\n<p>I am running some processing jobs in some files stored in S3, I just need regular computing power without GPU. I found I can use both <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn\/\" rel=\"nofollow noreferrer\">Sagemaker preprocessing jobs<\/a> using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Sagemaker SDK<\/a>; and also can do the exact same task using <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/task_definitions.html\" rel=\"nofollow noreferrer\">Fargate based AWS ECS Task<\/a> using the Python-based <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ecs.htm\" rel=\"nofollow noreferrer\">ECS SDK<\/a>. When I compare the procedure it seems like both of them are very similar which is <\/p>\n\n<ul>\n<li>Build a docker image<\/li>\n<li>Push the image to ECR<\/li>\n<li>Configure Fargate or ECS<\/li>\n<li>Run the task<\/li>\n<\/ul>\n\n<p>Also from the pricing model, it seems really close; so I am wondering why on the same platform there are two services doing a very similar thing. Can anyone explain the motivation behind it and which service to use when if I don't need GPU? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-03 21:49:11.053 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ecs|amazon-sagemaker|aws-fargate",
        "Question_view_count":38,
        "Owner_creation_date":"2013-06-03 03:45:11.26 UTC",
        "Owner_last_access_date":"2022-09-18 12:32:59.83 UTC",
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":5295,
        "Owner_up_votes":351,
        "Owner_down_votes":4,
        "Owner_views":455,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to serve daily precomputed predictions in aws sagemaker?",
        "Question_body":"<p>I'm trying to use Sagemaker to serve precomputed predictions. The predictions are in the following format in a python dictionary.<\/p>\n\n<pre><code>customer_group prediction\n1              50\n2              60\n3              25\n4              30\n...\n<\/code><\/pre>\n\n<p>Currently the docker <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">serve API code<\/a> goes to s3 and downloads the data daily.<\/p>\n\n<p>The problem is that downloading the data blocks the api from responding to the Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests\" rel=\"nofollow noreferrer\">health endpoint calls<\/a>.<\/p>\n\n<p>This a case study of <a href=\"https:\/\/aws.amazon.com\/solutions\/case-studies\/zappos-case-study\/\" rel=\"nofollow noreferrer\">how zappos did it<\/a> using Amazon DynamoDB. However, is there a way to do it in Sagemaker? <\/p>\n\n<p>Where and how can I add the s3 download function to avoid interrupting the health check?<\/p>\n\n<p>Could this work? -> <a href=\"https:\/\/github.com\/seomoz\/s3po\" rel=\"nofollow noreferrer\">https:\/\/github.com\/seomoz\/s3po<\/a>\n<a href=\"https:\/\/blog.miguelgrinberg.com\/post\/the-flask-mega-tutorial-part-x-email-support\" rel=\"nofollow noreferrer\">https:\/\/blog.miguelgrinberg.com\/post\/the-flask-mega-tutorial-part-x-email-support<\/a><\/p>\n\n<pre><code>app = flask.Flask(__name__)\n\n@app.route('\/ping', methods=['GET'])\ndef ping():\n    \"\"\"Determine if the container is working and healthy. In this sample container, we declare\n    it healthy if we can load the model successfully.\"\"\"\n    health = ScoringService.get_model() is not None  # You can insert a health check here\n\n    status = 200 if health else 404\n    return flask.Response(response='\\n', status=status, mimetype='application\/json')\n\n@app.route('\/invocations', methods=['POST'])\ndef transformation():\n    \"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n    just means one prediction per line, since there's a single column.\n    \"\"\"\n    data = None\n\n    # Convert from CSV to pandas\n    if flask.request.content_type == 'text\/csv':\n        data = flask.request.data.decode('utf-8')\n        s = StringIO.StringIO(data)\n        data = pd.read_csv(s, header=None)\n    else:\n        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text\/plain')\n\n    print('Invoked with {} records'.format(data.shape[0]))\n\n    # Do the prediction\n    predictions = ScoringService.predict(data)\n\n    # Convert from numpy back to CSV\n    out = StringIO.StringIO()\n    pd.DataFrame({'results':predictions}).to_csv(out, header=False, index=False)\n    result = out.getvalue()\n\n    return flask.Response(response=result, status=200, mimetype='text\/csv')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-02 21:53:43.127 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_date":"2016-07-14 00:43:23.417 UTC",
        "Owner_last_access_date":"2022-09-14 08:41:23.24 UTC",
        "Owner_location":null,
        "Owner_reputation":163,
        "Owner_up_votes":152,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-05-03 11:33:04.957 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Neo Compilation witn Dynamic Input Data Size",
        "Question_body":"<p>I'm trying to compile a <code>PyTorch<\/code> model to a raspberry pie 3. The model that I am using does not have any limitation in regards to the width and the height. Sagemaker Neo takes in a <code>Data input configuration<\/code> parameter which looks like it needs to be fixed from the documentation and the examples. <\/p>\n\n<pre><code>Data Input Configuration:  \nAmazon SageMaker needs to know the what the shape of the data matrix is\n{\"data\": [1, 3, 224, 224]}\n<\/code><\/pre>\n\n<p>The above example can be found <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-job-compilation-console.html\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>I need something like the following: <\/p>\n\n<pre><code>Data Input Configuration:  \nAmazon SageMaker needs to know the what the shape of the data matrix is\n{\"data\": [1, 3, *, *]}\n<\/code><\/pre>\n\n<p>entering the above yields the following error message:<\/p>\n\n<pre><code>JSON.parse: unexpected character at line 1 column 17 of the JSON data\n<\/code><\/pre>\n\n<p>and fixing the height and the width yields the following message:<\/p>\n\n<pre><code>ClientError: InputConfiguration: Invalid PyTorch model or input-shape mismatch. Make sure that inputs are lexically ordered and of the correct dimensionality.\n<\/code><\/pre>\n\n<p>Is there anyway to specify that I want 1 images, with 3 channels, and variable height and width?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-21 03:31:41.6 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_date":"2013-07-22 01:18:53.41 UTC",
        "Owner_last_access_date":"2022-09-25 03:42:45.95 UTC",
        "Owner_location":"Montreal, QC, Canada",
        "Owner_reputation":2306,
        "Owner_up_votes":1283,
        "Owner_down_votes":3,
        "Owner_views":182,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon S3 files access in Sagemaker instance",
        "Question_body":"<p>How exactly is file in my S3 bucket is accessible in my Sagemaker instance?\nGiven that I am not adding any access provision...what exactly is happening in the backend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-29 14:31:59.12 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1234,
        "Owner_creation_date":"2021-06-16 15:15:46.76 UTC",
        "Owner_last_access_date":"2021-07-05 02:44:15.657 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to load file in sagemaker custom deploy endpoint script",
        "Question_body":"<p>I am trying to deploy a sentiment analysis model on sagemaker to an endpoint to predict sentiment in real time of an input text. This model will take a single text String as input and return the sentiment.<\/p>\n\n<p>To train the xgboost model, I followed this <a href=\"https:\/\/github.com\/NadimKawwa\/sagemaker_ml\/blob\/master\/SageMaker_IMDB_highlevel.ipynb\" rel=\"nofollow noreferrer\"> notebook<\/a> upto step 23. \nThis uploaded model.tar.gz to s3 bucket. I additionally uploaded vocabulary_dict generated by sklearn's CountVectorizer(to create bag of words)to s3 bucket as well. <\/p>\n\n<p>To deploy this pre-trained model, I can use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-endpoints-from-model-data\" rel=\"nofollow noreferrer\">this method<\/a> and supply an entry point python file predict.py.<\/p>\n\n<pre><code>sklearn_model = SKLearnModel(model_data=\"s3:\/\/bucket\/model.tar.gz\", role=\"SageMakerRole\", entry_point=\"predict.py\")\n<\/code><\/pre>\n\n<p>Documentation says that I have to provide model.tar.gz only as argument and it will be loaded in model_fn. But if I am writing my own model_fn, how do I load the model then? If I put additional files in the same directory as of model.tar.gz in S3, can I load them as well?<\/p>\n\n<p>Now to do the classification, I will have to vectorize the input text before calling model.predict(bow_vector) in the method predict_fn. In order to do that, I need word_dict which I prepared during pre-processing training data and wrote to s3. <\/p>\n\n<p>My question is how do I get the word_dict inside the model_fn? Can I load it from s3? \nBelow is code for predict.py.<\/p>\n\n<pre><code>import os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom bs4 import BeautifulSoup\nimport sagemaker_containers\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\ndef model_fn(model_dir):\n\n    #TODO How to load the word_dict.\n    #TODO How to load the model.\n    return model, word_dict\n\ndef predict_fn(input_data, model):\n    print('Inferring sentiment of input data.')\n    trained_model, word_dict = model\n    if word_dict is None:\n        raise Exception('Model has not been loaded properly, no word_dict.')\n\n    #Process input_data so that it is ready to be sent to our model.\n\n    input_bow_csv = process_input_text(word_dict, input_data)\n    prediction = trained_model.predict(input_bow_csv)\n    return prediction\n\n\ndef process_input_text(word_dict, input_data):\n\n    words = text_to_words(input_data);\n    vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, word_dict)\n    bow_array = vectorizer.transform([words]).toarray()[0]\n    bow_csv = \",\".join(str(bit) for bit in bow_array)\n    return bow_csv\n\ndef text_to_words(text):\n    \"\"\"\n    Uses the Porter Stemmer to stem words in a review\n    \"\"\"\n    #instantiate stemmer\n    stemmer = PorterStemmer()\n    text_nohtml = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags\n    text_lower = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_nohtml.lower()) # Convert to lower case\n    words = text_lower.split() # Split string into words\n    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n    words = [PorterStemmer().stem(w) for w in words] # stem\n    return words\n\ndef input_fn(input_data, content_type):\n    return input_data;\n\ndef output_fn(prediction_output, accept):\n    return prediction_output;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-18 18:04:31.257 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|scikit-learn|aws-sdk|amazon-sagemaker",
        "Question_view_count":676,
        "Owner_creation_date":"2019-08-18 01:53:16.3 UTC",
        "Owner_last_access_date":"2019-09-25 20:27:58.817 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to specify a forced_bos_token_id when using Facebook's M2M-100 HuggingFace model through AWS SageMaker?",
        "Question_body":"<p>The <a href=\"https:\/\/huggingface.co\/facebook\/m2m100_1.2B\" rel=\"nofollow noreferrer\">model page<\/a> provides this snippet for how the model should be used:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\nhi_text = &quot;\u091c\u0940\u0935\u0928 \u090f\u0915 \u091a\u0949\u0915\u0932\u0947\u091f \u092c\u0949\u0915\u094d\u0938 \u0915\u0940 \u0924\u0930\u0939 \u0939\u0948\u0964&quot;\nchinese_text = &quot;\u751f\u6d3b\u5c31\u50cf\u4e00\u76d2\u5de7\u514b\u529b\u3002&quot;\n\nmodel = M2M100ForConditionalGeneration.from_pretrained(&quot;facebook\/m2m100_1.2B&quot;)\ntokenizer = M2M100Tokenizer.from_pretrained(&quot;facebook\/m2m100_1.2B&quot;)\n\n# translate Hindi to French\ntokenizer.src_lang = &quot;hi&quot;\nencoded_hi = tokenizer(hi_text, return_tensors=&quot;pt&quot;)\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(&quot;fr&quot;))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# =&gt; &quot;La vie est comme une bo\u00eete de chocolat.&quot;\n\n# translate Chinese to English\ntokenizer.src_lang = &quot;zh&quot;\nencoded_zh = tokenizer(chinese_text, return_tensors=&quot;pt&quot;)\ngenerated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(&quot;en&quot;))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# =&gt; &quot;Life is like a box of chocolate.&quot;\n<\/code><\/pre>\n<p>It also provides a snippet for how to deploy and use it with AWS SageMaker:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n    'HF_MODEL_ID':'facebook\/m2m100_1.2B',\n    'HF_TASK':'text2text-generation'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6.1',\n    pytorch_version='1.7.1',\n    py_version='py36',\n    env=hub,\n    role=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1, # number of instances\n    instance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n    'inputs': &quot;The answer to the universe is&quot;\n})\n<\/code><\/pre>\n<p>It is not clear, however, how to specify the source language or the target language with the AWS setup. I tried:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor.predict({\n    'inputs': &quot;The answer to the universe is&quot;,\n    'forced_bos_token_id': &quot;fr&quot;\n})\n<\/code><\/pre>\n<p>but my parameter was ignored.<\/p>\n<p>I haven't managed to find any documentation that would explain what the expect format is through this API.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-18 09:35:03.187 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|facebook|amazon-sagemaker|huggingface-transformers|machine-translation",
        "Question_view_count":187,
        "Owner_creation_date":"2013-03-10 11:22:30.047 UTC",
        "Owner_last_access_date":"2022-08-30 07:21:38.897 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":2791,
        "Owner_up_votes":163,
        "Owner_down_votes":13,
        "Owner_views":174,
        "Answer_body":"<p>The tokenizer needs to be installed and imported in any case:<\/p>\n<pre><code>pip install transformers\npip install sentencepiece\n<\/code><\/pre>\n<p>Then the tokenizer needs to be passed the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer = M2M100Tokenizer.from_pretrained(&quot;facebook\/m2m100_1.2B&quot;)\npredictor.predict({\n    'inputs': &quot;The answer to the universe is&quot;,\n    'parameters': {\n        'forced_bos_token_id': tokenizer.get_lang_id(&quot;it&quot;)\n    }\n})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-18 11:57:00.09 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker InvokeEndpoint: operation: Endpoint of account not found",
        "Question_body":"<p>I've been following this guide here: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/building-an-nlu-powered-search-application-with-amazon-sagemaker-and-the-amazon-es-knn-feature\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/building-an-nlu-powered-search-application-with-amazon-sagemaker-and-the-amazon-es-knn-feature\/<\/a><\/p>\n<p>I have successfully deployed the model from my notebook instance. I am also able to generate predictions by calling <code>predict()<\/code> method from <code>sagemaker.predictor<\/code>.<\/p>\n<p>This is how I created and deployed the model<\/p>\n<pre><code>class StringPredictor(Predictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text\/plain')\n\npytorch_model = PyTorchModel(model_data = inputs, \n                             role=role, \n                             entry_point ='inference.py',\n                             source_dir = '.\/code', \n                             framework_version = '1.3.1',\n                             py_version='py3',\n                             predictor_cls=StringPredictor)\n\npredictor = pytorch_model.deploy(instance_type='ml.m5.large', initial_instance_count=4)\n<\/code><\/pre>\n<p>From the SageMaker dashboard, I can even see that my endpoint and the status is &quot;in-service&quot;<\/p>\n<p>If I run <code>aws sagemaker list-endpoints<\/code> I can see my desired endpoint showing up correctly as well.\nMy issue is when I run this code (outside of sagemaker), I'm getting an error:<\/p>\n<pre><code>import boto3\nsm_runtime_client = boto3.client('sagemaker-runtime')\npayload = &quot;somestring that is used here&quot;\nresponse = sm_runtime_client.invoke_endpoint(EndpointName='pytorch-inference-xxxx',ContentType='text\/plain',Body=payload)\n<\/code><\/pre>\n<p>This is the error thrown<\/p>\n<pre><code>botocore.errorfactory.ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint pytorch-inference-xxxx of account xxxxxx not found.\n<\/code><\/pre>\n<p>This is quite strange as I'm able to see and run the endpoint just fine from sagemaker notebook and I am able to run the <code>predict()<\/code> method too.<\/p>\n<p>I have verified the region, endpoint name and the account number.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-13 18:54:01.387 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|python-3.x|amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":767,
        "Owner_creation_date":"2016-09-30 22:34:59.71 UTC",
        "Owner_last_access_date":"2022-09-22 13:53:27.543 UTC",
        "Owner_location":null,
        "Owner_reputation":586,
        "Owner_up_votes":83,
        "Owner_down_votes":21,
        "Owner_views":103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-14 03:36:20 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Tracking SageMaker Estimator with MLFlow",
        "Question_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-23 01:06:27.047 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":437,
        "Owner_creation_date":"2019-01-20 22:33:06.667 UTC",
        "Owner_last_access_date":"2022-09-18 13:59:17.783 UTC",
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-24 09:24:49.803 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2020-04-23 01:30:39.877 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to implement a beam search decoder in an SageMaker hosting endpoint?",
        "Question_body":"<p>I've created a SageMaker model for a Seq2Seq neural network, and then started a SageMaker endpoint:<\/p>\n\n<pre><code>create_endpoint_config_response = sage.create_endpoint_config(\n    EndpointConfigName = endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType':'ml.m4.xlarge', \n        'InitialInstanceCount':1,\n        'ModelName':model_name,\n        'VariantName':'AllTraffic'}])\n\ncreate_endpoint_response = sage.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>\n\n<p>This standard endpoint does not support beam search. What is the best approach for creating a SageMaker endpoint that supports beam search?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-07-31 03:05:11.5 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|beam-search",
        "Question_view_count":138,
        "Owner_creation_date":"2018-07-31 02:39:34.23 UTC",
        "Owner_last_access_date":"2019-12-13 11:46:03.2 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Question_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-03 06:50:07.823 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"boto3|amazon-sagemaker",
        "Question_view_count":599,
        "Owner_creation_date":"2013-04-02 09:58:26.347 UTC",
        "Owner_last_access_date":"2022-09-25 05:14:00.05 UTC",
        "Owner_location":"Noida, India",
        "Owner_reputation":6584,
        "Owner_up_votes":477,
        "Owner_down_votes":15,
        "Owner_views":962,
        "Answer_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-03 13:02:56.623 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2019-06-03 07:07:19.143 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to handle entrypoints nested in folders with amazon sagemaker pytorch estimator?",
        "Question_body":"<p>I am attempting to run a training job on amazon sagemaker using the python-sagemaker-sdk, estimator class.<\/p>\n\n<p>I have the following<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='training_scripts\/train_MSCOCO.py',\n                            source_dir='.\/',\n                            role=#dummy_role,\n                            train_instance_type='ml.p3.2xlarge',\n                            train_instance_count=1,\n                            framework_version='1.0.0',\n                            output_path=#dummy_output_path,\n                            hyperparameters={'lr': 0.001,\n                                             'batch_size': 32,\n                                             'num_workers': 4,\n                                             'description': description})\n<\/code><\/pre>\n\n<p>role and output_path hidden for privacy.<\/p>\n\n<p>I get the following error, \"No module named training_scripts\\train_MSCOCO\".<\/p>\n\n<p>When I run python -m training_scripts.train_MSCOCO the script runs fine. However when I pass <code>entry_point='training_script.train_MSCOCO.py<\/code> it will not run as, \"No file named \"training_scripts.train_MSCOCO.py\" was found in directory \".\/\"\".<\/p>\n\n<p>I am confused as to how to run a nested training script from the top level of my repository within AWS sagemaker, as they seem to have conflicting path needs, one in python module dot notation, the other in standard filepath slash notation.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-24 16:06:29.853 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2016-07-28 15:43:26.203 UTC",
        "Owner_last_access_date":"2020-12-01 16:51:05.67 UTC",
        "Owner_location":"New Jersey, United States",
        "Owner_reputation":65,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Emacs Tramp access to AWS SageMaker instance",
        "Question_body":"<p>I do machine learning code development on an AWS SageMaker instance, and would like to make use of Emacs\/Tramp<sup>\u2020<\/sup>. My question: how, if at all, could that be done? I strongly suspect that some knowledge about security protocols \/ IAM roles, etc., would be important, but I am a mere SageMaker end-user.<\/p>\n<p>A co-worker may be able to assist on some of these questions, but he is way over-subscribed. It is a big ask of him to indulge my personal preferences (however strong), so I start by posing the question, has anybody else has already solved this problem, or are there good starting points for consideration?<\/p>\n<p>Already seen:<\/p>\n<ul>\n<li>Martin Baillie's <a href=\"https:\/\/martin.baillie.id\/wrote\/emacs-tramp-over-aws-ssm-apis\/\" rel=\"nofollow noreferrer\">Emacs TRAMP over AWS SSM APIs<\/a>, which seems light on key details, and may not be applicable for SageMaker environments<\/li>\n<li>AWS's own <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint-tutorial-pycharm.html\" rel=\"nofollow noreferrer\">Tutorial: Set Up PyCharm Professional with a Development Endpoint<\/a>, which seems to be pretty specific to PyCharm, and which again may not be suitable for SageMaker environments.<\/li>\n<\/ul>\n<p><sup>\u2020<\/sup>Why? Because I have a long lifetime of using emacs key bindings and macros, etc., that improve my efficiency greatly. Why not emacs within a terminal running on a SageMaker instance? That's what I'm doing now, but it leaves out important flexibility compared with a local windowed emacs client; the latter can be as tall or wide as my pixels permit, can have multiple frames simultaneously, wouldn't have as many networking latencies, etc.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-20 22:18:44.183 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|emacs|amazon-sagemaker|tramp",
        "Question_view_count":54,
        "Owner_creation_date":"2014-07-31 21:11:09.307 UTC",
        "Owner_last_access_date":"2022-09-23 18:56:32.477 UTC",
        "Owner_location":null,
        "Owner_reputation":480,
        "Owner_up_votes":101,
        "Owner_down_votes":5,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-20 22:29:13.643 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Member must satisfy regular expression pattern: ^(https|s3):\/\/([^\/]+)\/?(.*)$",
        "Question_body":"<p>I am writing a notebook on amazon SageMaker studio. I follow the instructions given by the\ninstructor who is an amazon engineer. However, I am getting the following error.<\/p>\n<pre><code>%%sh\npip3 -q install --upgrade pip\npip3 -q install sagemaker awscli boto3 smdebug pandas matplotlib seaborn --upgrade\n\nfrom IPython.core.display import HTML\nHTML('&lt;script&gt;.jupyter.notebook.kernel.restart(&lt;\/script&gt;)')\nimport numpy as np\nimport pandas as pd\nimport sagemaker\nimport boto3, os\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n\n!wget -N https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00222\/bank-additional.zip'\n!unzip -o bank-additional.zip'\n\n\n!head .\/bank-additional\/bank-additional-full.csv\n\ndata = pd.read_csv('bank-additional\/bank-additional-full.csv', sep=';')\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\ndata[:10]\n\n\ndata['no_previous_contact'] = np.where(data['pdays'], 1, 0)\ndata.drop(data['pdays'])\ndata['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)\n\n\nmodel_data = pd.get_dummies(data)\nmodel_data[:10]\n\ntrain_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=123), \n                                                 [int(0.7 * len(model_data)), int(0.9*len(model_data))])  \n\npd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\npd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n\ntest_data.drop(['y_no', 'y_yes'], axis=1).to_csv('test.csv', index=False, header=False)\n\nbucket = sagemaker.Session().default_bucket()                     \nprefix = 'sagemaker\/DEMO-xgboost-dm'\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('train.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation\/validation.csv')).upload_file('validation.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('test.csv')\n\n\ns3_input_train = sagemaker.TrainingInput(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.TrainingInput(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\ns3_data = {'train': s3_input_train, 'validation': s3_input_validation}\n\nsess = sagemaker.Session()\nregion = boto3.Session().region_name\n\n\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, version='latest')\nsave_interval = '1'\n\nxgb = Estimator(  # The algorithm (XGBoost)\n                  # IAM Permissions for SageMaker\n                  # save the model here\n    container,\n    role=sagemaker.get_execution_role(),\n    sagemaker_session=sess,\n    input_mode='File',\n    output_path='s3\/\/{}\/{}\/output'.format(bucket, prefix),\n    instance_count=1,\n    instance_type='ml.m4.2xlarge',\n    use_spot_instances=True,\n    max_run=300,\n    max_weight=600,\n    debugger_hook_config=DebuggerHookConfig(s3_output_path='s3\/\/{}\/{}\/output'.format(bucket,\n            prefix), collection_configs=[CollectionConfig(name='metrics'\n            , parameters={'save_interval': str(save_interval)}),\n            CollectionConfig(name='predictions',\n            parameters={'save_interval': str(save_interval)})]),\n    rules=[Rule.sagemaker(rule_configs.class_imbalance(),\n            rule_parameters={'collection_names': 'metrics'})],\n    )\n\nxgb.set_hyperparameters(\nobjective='binary:logistic', \neval_metric='auc', \nnum_round=100, \nearly_stopping_rounds=10)\n\nxgb.fit(s3_data)\n\n<\/code><\/pre>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 3 validation errors detected: Value 's3\/\/sagemaker-us-west-2-034611191912\/sagemaker\/DEMO-xgboost-dm\/output' at 'profilerConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3):\/\/([^\/]+)\/?(.<em>)$; Value 's3\/\/sagemaker-us-west-2-034611191912\/sagemaker\/DEMO-xgboost-dm\/output' at 'debugHookConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3):\/\/([^\/]+)\/?(.<\/em>)$; Value 's3\/\/sagemaker-us-west-2-034611191912\/sagemaker\/DEMO-xgboost-dm\/output' at 'outputDataConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3):\/\/([^\/]+)\/?(.*)$<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-30 02:20:31.027 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":640,
        "Owner_creation_date":"2018-03-22 19:27:05.663 UTC",
        "Owner_last_access_date":"2022-05-18 15:32:39.297 UTC",
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Python binary data save as png",
        "Question_body":"<p>So I've been banging my head against this for hours and I can't quite get it to work.<\/p>\n\n<p>The file is sent as image\/png and comes across in what looks like binary data if I print it.<\/p>\n\n<p>The problem I'm getting is commented #PROBLEM HERE - essential PIL can't read the file back, it's 100% in the directory.<\/p>\n\n<pre><code>cannot identify image file '\/tmp\/image.jpg'\n<\/code><\/pre>\n\n<p>I tried JSON etc too and base64 but ended up in same circle, it's going to be something really silly.<\/p>\n\n<p>I'm using Postman to test Sagemaker endpoint integration - here's my code - I'm including the lot because hopefully you see whilst this is a dumb question I've put a LOT of work into this I'm relatively new to python.<\/p>\n\n<p>In order to make this work in Lambda I had to drag modules from both MXNet, Sagemaker, and numerous .so from the ubuntu file system and upload as part of the package so that it met the space requirements for Lambda.(End result is 100mb lambda package, which is better than the 300 I started with.)<\/p>\n\n<pre><code>sys.path.append('.\/libs')\nfrom record_pb2 import Record\nfrom recordio import MXRecordIO\nruntime= boto3.client('runtime.sagemaker')\n\n    # event, context\n    def handler(event, context):\n\n      print(event)\n      print('Body')\n      # tried 'w' here too.\n      with open('\/tmp\/image.jpg' , 'wb') as file:\n        # Tried without encode and no 'b' above.\n        file.write(event['body'].encode())\n\n      files = os.listdir('\/tmp')\n      print(files)\n\n      im = PIL.Image.open('\/tmp\/image.jpg')\n      im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n      im.save('\/tmp\/resized.jpg', \"JPEG\")\n      files = os.listdir('\/tmp')\n      print(files)\n\n\n      with open('\/tmp\/resized.jpg', 'r') as image:\n        img = image.read()\n        img = bytearray(img)\n\n      response = runtime.invoke_endpoint(EndpointName='ss-notebook-demo-2020-06-02-09-27-40-405',\n                                          ContentType='image\/jpeg',\n                                          Accept='application\/x-protobuf',\n                                          Body=img)\n\n      print(response)\n\n      result = response['Body'].read()\n\n      results_file = '\/temp\/results.rec'\n\n      with open(results_file, 'wb') as f:\n          f.write(result)\n\n      rec = Record()\n      recordio = MXRecordIO(results_file, 'r')\n      protobuf = rec.ParseFromString(recordio.read())\n\n      values = list(rec.features[\"target\"].float32_tensor.values)\n      shape = list(rec.features[\"shape\"].int32_tensor.values)\n      shape = np.squeeze(shape)\n      mask = np.reshape(np.array(values), shape)\n      mask = np.squeeze(mask, axis=0)\n\n      pred_map = np.argmax(mask, axis=0)\n      unique_elements, counts_elements = np.unique(pred_map[pred_map != 0], return_counts=True)\n      #print(unique_elements, counts_elements)\n      #print(pred_map[pred_map != 0].size)\n      #print(np.bincount(pred_map[pred_map != 0]).argmax())\n\n      return {\n        'classes': unique_elements,\n        'counts': counts_elements,\n        'top': np.bincount(pred_map[pred_map != 0]).argmax(),\n      }\n<\/code><\/pre>\n\n<p>For those that are interested that right there is how you get classes and pixel count PER class from a semantic segmentation recordio-protobuf in Sagemaker! :-)<\/p>\n\n<p>Update:<\/p>\n\n<p>Body sample, note this isn't the whole thing because it's huge.<\/p>\n\n<pre><code>'body': '\ufffd\ufffd\ufffd\ufffd\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\ufffd\ufffd\\x00C\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\ufffd\ufffd\\x00C\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\ufffd\ufffd\\x00\\x11\\x08\\x07\ufffd\\x05d\\x03\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\ufffd\ufffd\\x00\\x1f\\x00\\x00\\x01\\x05\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\ufffd\ufffd\\x00\ufffd\\x10\\x00\\x02\\x01\\x03\\x03\\x02\\x04\\x03\\x05\\x05\\x04\\x04\\x00\\x00\\x01}\\x01\\x02\\x03\\x00\\x04\\x11\\x05\\x12!1A\\x06\\x13Qa\\x07\"q\\x142\ufffd\ufffd\ufffd\\x08#B\ufffd\ufffd\\x15R\ufffd\ufffd$3br\ufffd\\t\\n\\x16\\x17\\x18\\x19\\x1a%&amp;\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\\x00\\x1f\\x01\\x00\\x03\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\ufffd\ufffd\\x00\ufffd\\x11\\x00\\x02\\x01\\x02\\x04\\x04\\x03\\x04\\x07\\x05\\x04\\x04\\x00\\x01\\x02w\\x00\\x01\\x02\\x03\\x11\\x04\\x05!1\\x06\\x12AQ\\x07aq\\x13\"2\ufffd\\x08\\x14B\ufffd\ufffd\ufffd\ufffd\\t#3R\ufffd\\x15br\ufffd\\n\\x16$4\ufffd%\ufffd\\x17\\x18\\x19\\x1a&amp;\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\ufffd*\\x03\ufffd\\n8\ufffd\\x06R\ufffd\ufffd\ufffd\ufffd\\x17\ufffdvs\ufffd\\x04\ufffd\ufffdg\\x14\ufffd)3\ufffd\ufffd2\ufffdk\ufffd7\ufffdK\\x029\ufffd\ufffd\\x1f\ufffd\ufffd4&lt;\ufffd|\ufffd\ufffdI\ufffd\\'heN\ufffd\ufffd\\x1f\ufffd^s\ufffd3\ufffdq\ufffd\\x16\\x17\ufffd\u0258\\'\ufffd3\\x1c\ufffd\u0651\ufffd~G\\x07\u061a\ufffd+\ufffd\ufffd\\x7f\\x17\ufffdG%\ufffd\u0513R\ufffd\ufffd\\x17\\x7f\ufffd),! \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2020-06-05 19:33:00.207 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|python-imaging-library|amazon-sagemaker",
        "Question_view_count":1023,
        "Owner_creation_date":"2012-01-29 01:50:08.98 UTC",
        "Owner_last_access_date":"2022-09-24 18:50:39.96 UTC",
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3841,
        "Owner_up_votes":141,
        "Owner_down_votes":1,
        "Owner_views":364,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-06-05 21:52:30.983 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Where do I store my model's training data, artifacts, etc?",
        "Question_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-06 15:15:46.377 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1022,
        "Owner_creation_date":"2014-05-05 00:56:45.187 UTC",
        "Owner_last_access_date":"2022-05-04 22:46:57.89 UTC",
        "Owner_location":null,
        "Owner_reputation":145,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-07 08:46:16.867 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"tensorflow.python.framework.errors_impl.NotFoundError: \/opt\/ml\/input\/data\/train\/label_map.pbtxt; No such file or directory",
        "Question_body":"<p>I've been following this tutorial on <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Training and deploying models using TensorFlow 2 with the Object Detection API on Amazon SageMaker<\/a> but keep on getting the above error when attempting to train the model using estimator.fit(inputs) in train_model.ipynb. All of the code for the tutorial is available at: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api<\/a><\/p>\n<p>The label_map.pbtxt, train.records and validation.records files were successfully created in my bucket (at s3:\/\/bucket\/data\/bees\/tfrecords), and I've adjusted my pipeline.config file to contain:<\/p>\n<pre><code>train_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/train.records&quot;\n  }\n}\n\neval_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/validation.records&quot;\n  }\n}\n<\/code><\/pre>\n<p>I'm completely new to Amazon Sagemaker and containers but have followed the walkthrough to a tee, so I'm lost as to why it's failing. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-20 07:46:39.277 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|object-detection|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_date":"2021-11-20 06:29:57.727 UTC",
        "Owner_last_access_date":"2022-06-06 11:44:46.37 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"No Module named \"Fastai\" when trying to deploy fastai model on sagemaker",
        "Question_body":"<p>I have trained and built a Fastai(v1) model and exported it as a .pkl file.\nNow i want to deploy this model for inference in Amazon Sagemaker<\/p>\n<p>Following the Sagemaker documentation for Pytorch model [https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#write-an-inference-script][1]<\/p>\n<p>Steps taken<br>\nFolder structure<\/p>\n<pre>\nSagemaker\/\n       export.pkl\n       code\/\n           inference.py\n           requirement.txt\n<\/pre>\n<pre> \nrequirement.txt\n\n    spacy==2.3.4\n    torch==1.4.0\n    torchvision==0.5.0\n    fastai==1.0.60\n    numpy\n\n<\/pre>\n<p>Command i used to create the zip file<\/p>\n<pre>\n    cd Sagemaker\/\n    tar -czvf \/tmp\/model.tar.gz .\/export.pkl .\/code\n<\/pre>\n<p>This would generate a model.tar.gz file and i upload it to S3 bucket<br><\/p>\n<p>To deploy this i used the python sagemaker SDK<\/p>\n<pre><code>\n    from sagemaker.pytorch import PyTorchModel\n        role = &quot;sagemaker-role-arn&quot;\n        model_path = &quot;s3 key for the model.tar.gz file that i created above&quot;\n        pytorch_model = PyTorchModel(model_data=model_path,role=role,`entry_point='inference.py',framework_version=&quot;1.4.0&quot;, py_version=&quot;py3&quot;)\n    \n        predictor = pytorch_model.deploy(instance_type='ml.c5.large', initial_instance_count=1)\n\n<\/code><\/pre>\n<p>After executing the above code i see that the model is created in sagemaker and deployed but i end up getting an error running the inference<\/p>\n<pre><code>\n    botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message &quot;No module named 'fastai'\n    Traceback (most recent call last):\n      File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 110, in transform\n        self.validate_and_initialize(model_dir=model_dir)\n      File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 157, in validate_and_initialize\n        self._validate_user_module_and_set_functions()\n      File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 170, in _validate_user_module_and_set_functions\n        user_module = importlib.import_module(user_module_name)\n      File &quot;\/opt\/conda\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n        return _bootstrap._gcd_import(name[level:], package, level)\n      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n      File &quot;\/opt\/ml\/model\/code\/inference.py&quot;, line 2, in &lt;module&gt;\n        from fastai.basic_train import load_learner, DatasetType, Path\n    ModuleNotFoundError: No module named 'fastai'\n\n<\/code><\/pre>\n<p>Clearly the fastai module doesn't get downloaded what is the cause for this and what am i doing wrong in this case<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-05 13:03:11.863 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker|fast-ai",
        "Question_view_count":125,
        "Owner_creation_date":"2021-02-19 13:15:39.987 UTC",
        "Owner_last_access_date":"2022-09-24 17:23:03.67 UTC",
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":375,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to know training status on aws sagemaker if I lost internet connection and returned on the kernel",
        "Question_body":"<p>I was training in aws Sagemaker Jupyter Lab( p3.2xlarge instance) and It was LSTM network and suddenly internet connection got lost and When I returned to the kernel when I got internet connection back..I could not see any training epochs updated.<\/p>\n<p>Does anyone know that can I access what is processing in the kernel by terminal or something so I will know the training status.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-07 14:48:49.227 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|lstm|amazon-sagemaker",
        "Question_view_count":368,
        "Owner_creation_date":"2018-07-26 07:55:25.75 UTC",
        "Owner_last_access_date":"2022-09-23 13:58:07.923 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":69,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-07-07 14:56:04.54 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Multiple Object Detection in Image Recognition \/ Classification",
        "Question_body":"<p>Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection \/ inference. <\/p>\n\n<p>Thus, can we: <\/p>\n\n<p><strong>a) train using multi-label images<\/strong> <\/p>\n\n<p>and\/or <\/p>\n\n<p><strong>b) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training \/ transfer learning).<\/strong><\/p>\n\n<p>Also, I know that the doc for SageMaker Image Classification Algorithm says \"takes an image as input and classifies it into <strong>one<\/strong> of multiple output categories\". <\/p>\n\n<p>Any recommendations are also welcome.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-05 03:54:14.69 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|computer-vision|amazon-rekognition|amazon-sagemaker",
        "Question_view_count":1764,
        "Owner_creation_date":"2011-03-24 04:39:38.957 UTC",
        "Owner_last_access_date":"2022-03-14 04:59:19.877 UTC",
        "Owner_location":"San Francisco Bay Area",
        "Owner_reputation":645,
        "Owner_up_votes":46,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p>There is a new built-in algorithm released with Amazon Sagemaker today for object detection. Based on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Amazon SageMaker Object Detection uses the Single Shot multibox Detector (SSD) algorithm. The response from the inference contains an array consists of a predicted class label of the object detected, associated confidence score and bounding box co-ordinates.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-07-13 04:30:53.52 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2018-08-30 09:14:29.95 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker Pipeline Issue - Pipeline variables do not support __str__ operation",
        "Question_body":"<p>I'm trying to build SageMaker Pipeline based on Tensorflow framework. I have only Training, Evaluating steps, and Register model. On the evaluation step I declared <code>MetricsSource<\/code> for <code>ModelMetrics<\/code> and received an error.<\/p>\n<p>Code is below:<\/p>\n<pre><code>pipeline_model = PipelineModel(\n    models=[tf_model],\n    role=role, \n    sagemaker_session=sagemaker_session\n)\n\neval_res = step_evaluate_model.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\nevaluation_s3_uri = f'{eval_res}\/evaluation.json'\n\nmodel_statistics=MetricsSource(\n        s3_uri=evaluation_s3_uri,\n        content_type='application\/json')\n\nmodel_metrics = ModelMetrics(model_statistics=model_statistics)\n\nstep_register_pipeline_model = pipeline_model.register(\n    content_types=['application\/json'],\n    response_types=['application\/json'],\n    inference_instances=['ml.m4.xlarge','ml.c5.2xlarge'],\n    transform_instances=['ml.c5.2xlarge'],\n    model_package_group_name=model_package_group_name,\n    model_metrics=model_metrics,\n    approval_status=model_approval_status.default_value,\n)\n\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>TypeError                                 Traceback (most recent call last)\nInput In [17], in &lt;cell line: 17&gt;()\n     14 model_metrics = ModelMetrics(model_statistics=model_statistics)\n     15 # print('\\n',pipeline_model)\n---&gt; 17 step_register_pipeline_model = pipeline_model.register(\n     18     content_types=['application\/json'],\n     19     response_types=['application\/json'],\n     20     inference_instances=['ml.m4.xlarge','ml.c5.2xlarge'],\n     21     transform_instances=['ml.c5.2xlarge'],\n     22     model_package_group_name=model_package_group_name,\n     23     model_metrics=model_metrics,\n     24     approval_status=model_approval_status.default_value,\n     25 )\nTypeError: Pipeline variables do not support __str__ operation. Please use `.to_string()` to convert it to string type in execution timeor use `.expr` to translate it to Json for display purpose in Python SDK.\n<\/code><\/pre>\n<p>Could you please help me to solve it? I'd appreciate for any idea. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-27 14:56:36.937 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|aws-pipeline",
        "Question_view_count":273,
        "Owner_creation_date":"2016-08-03 12:48:10.79 UTC",
        "Owner_last_access_date":"2022-09-16 16:25:26.56 UTC",
        "Owner_location":null,
        "Owner_reputation":504,
        "Owner_up_votes":758,
        "Owner_down_votes":3,
        "Owner_views":82,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Trigger Amazon Sagemaker Processing Jobs",
        "Question_body":"<p>I'm designing the architecture of my ETL for an ML related project.<\/p>\n<p>Generally when automating tasks in AWS I use <em>EventBridge<\/em>, <em>SQS<\/em>, <em>S3<\/em> as triggers if using Lambdas. In my case Lambdas doesn't fit my needs so I decided to go with Amazon Sagemaker Processing Jobs. These processors could scale with <em>instance type<\/em> and <em>number of instances<\/em>, also they have an expiration time of 24 hours, this are the requirements that Lambda couldn't achieve.<\/p>\n<p>The architecture that I generally use:<\/p>\n<ol>\n<li>Trigger: SQS | S3 | EventBridge<\/li>\n<li>Launch: Lambda<\/li>\n<li>Processor: Sagemaker Processing Job<\/li>\n<\/ol>\n<p>But as you can imaging that Lambda layer is only used for launching Sagemaker Processing Jobs so it is desirable to avoid it.<\/p>\n<p><strong>Questions<\/strong>:<\/p>\n<p><strong>Q1.<\/strong> There is a better architecture with AWS services in order to automate\/trigger\/schedule Sagemaker Processing Jobs?<\/p>\n<p><strong>Q2.<\/strong> Which kind of services could perform better this task?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-17 22:07:10.87 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|architecture|etl|amazon-sagemaker",
        "Question_view_count":467,
        "Owner_creation_date":"2018-02-24 22:32:25.083 UTC",
        "Owner_last_access_date":"2022-09-23 08:27:16.67 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-17 22:20:45.363 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Question_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-23 21:01:20.943 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker|tensorflow-hub",
        "Question_view_count":2621,
        "Owner_creation_date":"2018-07-17 15:14:49.147 UTC",
        "Owner_last_access_date":"2021-11-04 23:15:34.717 UTC",
        "Owner_location":"Berkeley, CA, USA",
        "Owner_reputation":425,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Answer_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-07-24 14:34:18.283 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":6.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon Sagemaker Failed: Please check if you have a directory that has same name as the git repo?",
        "Question_body":"<p>I was working in Sagemaker, and noticed that my notebook instance was behind my github repo as I had just pushed to it outside of working in Sagemaker. I couldn't seem to pull, so I deleted the directory within Jupyter and git cloned my updated repo. It worked fine, but once I was done working I haven't since been able to reinitialize the notebook. Sagemaker simpy says<\/p>\n<blockquote>\n<p><strong>Failure reason<\/strong><\/p>\n<p>Please check if you have a directory that has same name as the git repo.<\/p>\n<\/blockquote>\n<p>I cloned from the same repo, so I don't imagine that the directory name changed. Maybe the directory is in the wrong place? Either way, how do I go in and change things if I can't open the notebook? Not sure what to do about this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-01 10:12:09.64 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"github|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_date":"2017-10-24 13:47:52.37 UTC",
        "Owner_last_access_date":"2022-04-01 15:10:20.05 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":68,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 08:38:22.897 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-07-28 15:35:17.217 UTC",
        "Owner_last_access_date":"2022-03-28 15:52:24.487 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-14 22:27:30.467 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-09 07:26:08.407 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|keras|amazon-ec2|amazon-sagemaker",
        "Question_view_count":316,
        "Owner_creation_date":"2015-03-18 10:49:38.223 UTC",
        "Owner_last_access_date":"2022-09-24 14:02:18.917 UTC",
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-10 10:19:58.007 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2021-06-09 07:36:28.497 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"With AWS SageMaker, is it possible to deploy a pre-trained model using the sagemaker SDK?",
        "Question_body":"<p>I'm trying to avoid migrating an existing model training process to SageMaker and avoid creating a custom Docker container to host our trained model.<\/p>\n\n<p>My hope was to inject our existing, trained model into the pre-built scikit learn container that AWS provides via the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.sklearn.html#scikit-learn-estimator\" rel=\"noreferrer\">sagemaker-python-sdk<\/a>. All of the examples that I have found require training the model first which creates the model\/model configuration in SageMaker. This is then deployed with the <code>deploy<\/code> method.<\/p>\n\n<p>Is it possible to provide a trained model to the <code>deploy<\/code> method and have it hosted in the pre-built scikit learn container that AWS provides?<\/p>\n\n<p>For reference, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb\" rel=\"noreferrer\">the examples<\/a> I've seen follow this order of operations:<\/p>\n\n<ol>\n<li>Creating an instance of <code>sagemaker.sklearn.estimator.SKLearn<\/code> and providing a training script<\/li>\n<li>Call the <code>fit<\/code> method on it<\/li>\n<li>This creates the model\/model configuration in SageMaker<\/li>\n<li>Call the <code>deploy<\/code> method on the <code>SKLearn<\/code> instance which automagically takes the model created in step 2\/3 and deploys it in the pre-build scikit learn container as an HTTPS endpoint.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-28 01:07:50.157 UTC",
        "Question_favorite_count":4.0,
        "Question_score":8,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":1719,
        "Owner_creation_date":"2012-07-25 17:32:50.417 UTC",
        "Owner_last_access_date":"2022-08-24 18:33:32.027 UTC",
        "Owner_location":"California",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-02-28 17:34:27.053 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Using external libraries for model training in aws sagemaker",
        "Question_body":"<p>I am just getting started with aws sagemaker and realized it doesn't have a random forest classifier. I found this github tutorial on creating your own and deploying it in sagemaker: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a>.<\/p>\n<p>I am using the python sdk and was more or less curious to see if anyone actually uses this or any external libraries for training with sagemaker. It seems that if you aren't using the built in algorithms then it is very involved to create your own and the functionality of the model and ability to interpret it is very limited once you do get it trained.<\/p>\n<p>For example after deploying the model to an aws endpoint and pulling down the artifacts I could only call the <code>predict<\/code> method (no <code>predict_probab<\/code> as is possible in the actual <code>sklearn randomforestclassifier<\/code>). I also haven't been able to find anything like what you get in <code>sklearn.metrics<\/code> such as <code>accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix<\/code> etc so I'm assuming one would need to build equivalents to these from scratch to be able to interpret their model.<\/p>\n<p>I have spent a week or so researching this and trying to get this rigged up and it just seems that importing external libraries for model training in sagemaker is not very popular or well-documented online. Interested to know if I'm just unaware of more functionality or if there are alternatives that people prefer or if I should just stick with the built in xgboost classifier if I am looking for a tree-based option. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-17 19:29:58.873 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_date":"2020-05-13 02:56:15.433 UTC",
        "Owner_last_access_date":"2022-09-24 10:05:22.157 UTC",
        "Owner_location":null,
        "Owner_reputation":373,
        "Owner_up_votes":73,
        "Owner_down_votes":0,
        "Owner_views":110,
        "Answer_body":"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>\n<p>From a hosting perspective, you do have the ability to provide a custom entry_point \/ inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">link<\/a> for more information<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 23:31:09.133 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Question_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-18 12:50:56.067 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling",
        "Question_view_count":738,
        "Owner_creation_date":"2017-03-13 03:58:08.79 UTC",
        "Owner_last_access_date":"2021-03-15 17:36:41.343 UTC",
        "Owner_location":null,
        "Owner_reputation":437,
        "Owner_up_votes":18,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Answer_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-20 14:53:01.867 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Display tqdm in AWS Sagemaker's jupyterlab",
        "Question_body":"<p>Does anyone nows how can we have python tqdm progress bar working on a Sagemaker Jupyterlab Noteook ? The tqdm progress bar is never displayed, components are displayed as their code. <\/p>\n\n<p>Example :<\/p>\n\n<pre><code>HBox(children=(FloatProgress(value=0.0, max=5234.0), HTML(value='')))\n<\/code><\/pre>\n\n<p>I'm aware of the usual fix describe <a href=\"https:\/\/stackoverflow.com\/questions\/57343134\/jupyter-notebooks-not-displaying-progress-bars\">here<\/a>, but It does not work since trying to executing <code>jupyter lab build<\/code> will results in the issue describe <a href=\"https:\/\/github.com\/jupyter-widgets\/ipywidgets\/issues\/2061\" rel=\"noreferrer\">here<\/a> <\/p>\n\n<p>Many thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-12 15:15:14.66 UTC",
        "Question_favorite_count":1.0,
        "Question_score":9,
        "Question_tags":"amazon-sagemaker|jupyter-lab",
        "Question_view_count":4085,
        "Owner_creation_date":"2017-10-20 16:14:50.813 UTC",
        "Owner_last_access_date":"2022-09-23 19:36:58.46 UTC",
        "Owner_location":null,
        "Owner_reputation":121,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to add sagemaker createApp to user profile executionrole?",
        "Question_body":"<p>I created a aws sagemaker user profile using terraform. I tried to launch the sagemaker studio from the user profile but was confronted with this error: <code>SageMaker is unable to use your associated ExecutionRole [arn:aws:iam::xxxxxxxxxxxx:role\/sagemaker-workshop-data-ml] to create app. Verify that your associated ExecutionRole has permission for 'sagemaker:CreateApp'<\/code>. The role has sagemaker full access policy attached to it, but that policy doesn't have the createApp permission which is weird. Are there any policies I can attach to the role with the sagemaker createApp permission, or do I need to attach a policy to the role through terraform?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-06-07 16:15:48.583 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":199,
        "Owner_creation_date":"2022-05-25 20:48:45.307 UTC",
        "Owner_last_access_date":"2022-09-15 14:23:04.53 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How does im2rec works? I keep getting syntax error",
        "Question_body":"<p>I am trying to create lst files for aws image classification algorithm.\nMy main directory is train which has 20 sub-directories of 40 images each.\nI want to create a train_1st which contains all the converted lst files.\nBut I am getting issues with the below code. I'm new to this .. So please help me.. what do i do?<\/p>\n\n<p>I have tried changing the current working directory(cwd) as well. I tried setting cwd as train\/ and also actual directory home\/ec-2\/sagemaker. Nothing helped.<\/p>\n\n<pre><code>%%bash\n\nmkdir -p  train_lst\nfor i in  train\/*; do\n    c=`basename $i`\n    mkdir -p train_lst\/$c\n    for j in `ls $i\/*.jpg | shuf | head -n 60`; do\n        mv $j train_lst\/$c\/\n    done\ndone\n\npython im2rec.py --list --recursive train train_lst\/\n<\/code><\/pre>\n\n<pre><code>ls: cannot access train\/*\/*.jpg: No such file or directory\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-07-16 00:52:12.763 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|bash|amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":167,
        "Owner_creation_date":"2019-01-03 15:38:43.733 UTC",
        "Owner_last_access_date":"2021-01-08 11:44:28.74 UTC",
        "Owner_location":"Ireland",
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-07-16 02:18:49.127 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"venv on AWS sagemaker notebook not working",
        "Question_body":"<p>I just launched AWS Sagemaker studio and trying to create virtual environment to work with as below:<\/p>\n<pre><code>python3 -m venv venv\nsource venv\/bin\/activate\npip install -r requirements.txt \n<\/code><\/pre>\n<p>venv works for my python scripts on Sagemaker. I am able to import the packages in my script as &quot;import pandas as pd&quot;<\/p>\n<p>But When I tried opening a Jupyter notebook on Sagemaker and run <code>import pandas as pd<\/code> I am getting error as <code>ModuleNotFoundError: No module named 'pandas'<\/code><\/p>\n<p>How do I use venv in my notebook? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-19 01:17:16.283 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":364,
        "Owner_creation_date":"2019-08-15 06:38:39.327 UTC",
        "Owner_last_access_date":"2022-09-23 09:38:31.217 UTC",
        "Owner_location":null,
        "Owner_reputation":844,
        "Owner_up_votes":83,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Question_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-07-06 22:54:17.097 UTC",
        "Question_favorite_count":0.0,
        "Question_score":0,
        "Question_tags":"python|python-3.x|tensorflow|data-science|amazon-sagemaker",
        "Question_view_count":1530,
        "Owner_creation_date":"2019-08-08 23:43:57.78 UTC",
        "Owner_last_access_date":"2022-09-23 18:27:04.643 UTC",
        "Owner_location":null,
        "Owner_reputation":570,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-07 13:47:19.003 UTC",
        "Answer_last_edit_date":"2020-07-07 22:13:14.64 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to deploy AWS using CDK, sagemaker?",
        "Question_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-21 08:54:45.383 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|aws-cdk|amazon-sagemaker",
        "Question_view_count":100,
        "Owner_creation_date":"2020-11-02 10:25:40.477 UTC",
        "Owner_last_access_date":"2022-07-21 21:41:49.39 UTC",
        "Owner_location":null,
        "Owner_reputation":141,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-06-21 22:16:08.797 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Question_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-01-31 10:13:02.673 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|python-3.x|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":690,
        "Owner_creation_date":"2011-11-21 16:37:22.023 UTC",
        "Owner_last_access_date":"2022-05-11 09:53:59.693 UTC",
        "Owner_location":null,
        "Owner_reputation":1611,
        "Owner_up_votes":51,
        "Owner_down_votes":6,
        "Owner_views":189,
        "Answer_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-02-15 17:26:35.183 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"running r script in AWS",
        "Question_body":"<p>Looking at <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/r_examples\/r_in_sagemaker_processing\/r_in_sagemaker_processing.html\" rel=\"nofollow noreferrer\">this page<\/a> and this piece of code in particular:<\/p>\n<pre><code>import boto3\n\naccount_id = boto3.client(&quot;sts&quot;).get_caller_identity().get(&quot;Account&quot;)\nregion = boto3.session.Session().region_name\n\necr_repository = &quot;r-in-sagemaker-processing&quot;\ntag = &quot;:latest&quot;\n\nuri_suffix = &quot;amazonaws.com&quot;\nprocessing_repository_uri = &quot;{}.dkr.ecr.{}.{}\/{}&quot;.format(\n    account_id, region, uri_suffix, ecr_repository + tag\n)\n\n# Create ECR repository and push Docker image\n!docker build -t $ecr_repository docker\n!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n!aws ecr create-repository --repository-name $ecr_repository\n!docker tag {ecr_repository + tag} $processing_repository_uri\n!docker push $processing_repository_uri\n<\/code><\/pre>\n<p>This is not pure Python obviously? Are these AWS CLI commands? I have used docker previously but I find this example very confusing. Is anyone aware of an end-2-end example of simply running some R job in AWS using sage maker\/docker? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-19 13:10:43.3 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":105,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>This is Python code mixed with shell script magic calls (the <code>!commands<\/code>).<\/p>\n<p>Magic commands aren't unique to this platform, you can use them in <a href=\"https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html\" rel=\"nofollow noreferrer\">Jupyter<\/a>, but this particular code is meant to be run on their platform. In what seems like a fairly convoluted way of running R scripts as processing jobs.<\/p>\n<p>However, the only thing you really need to focus on is the R script, and the final two cell blocks. The instruction at the top (don't change this line) creates a file (preprocessing.R) which gets executed later, and then you can see the results.<\/p>\n<p>Just run all the code cells in that order, with your own custom R code in the first cell. Note the line <code>plot_key = &quot;census_plot.png&quot;<\/code> in the last cell. This refers to the image being created in the R code. As for other output types (eg text) you'll have to look up the necessary Python package (PIL is an image manipulation package) and adapt accordingly.<\/p>\n<p>Try this to get the CSV file that the R script is also generating (this code is not validated, so you might need to fix any problems that arise):<\/p>\n<pre><code>import csv\n\ncsv_key = &quot;plot_data.csv&quot;\ncsv_in_s3 = &quot;{}\/{}&quot;.format(preprocessed_csv_data, csv_key)\n!aws s3 cp {csv_in_s3} .\n\nfile = open(csv_key)\ndat = csv.reader(file)\n\ndisplay(dat)\n<\/code><\/pre>\n<p>So now you should have an idea of how two different output types the R script example generates are being handled, and from there you can try and adapt your own R code based on what it outputs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-19 14:19:40.88 UTC",
        "Answer_last_edit_date":"2021-12-19 14:41:30.28 UTC",
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker endpoint inference with third-party libraries",
        "Question_body":"<p>I'm trying to create an endpoint with custom libraries, particularly trying to install <code>fastai<\/code>.\nGoing over this <a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#setup-your-sagemaker-notebook-instance\" rel=\"nofollow noreferrer\">tutorial<\/a> which is apparently incomplete. Endpoint deployment was failing at health check because of the missing library.\n<br>This <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">doc<\/a> says I can pass a list of third-party libraries in <code>requirements.txt<\/code> file. The file should be in <code>code<\/code> folder inside the <code>model.tar.gz<\/code> archive. This actually lets me deploy the endpoint with no issues but still fails when running predictions: <code>Received server error (500) from model with message \"No module named 'fastai'\"<\/code>.\n<br>Using PyTorch 1.3.1 estimator.\n<br>Contents of the <code>requirements.txt<\/code> is just <code>fastai<\/code>.\n<br>Wondering what could have gone wrong.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-05 21:41:15.58 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"pytorch|endpoint|amazon-sagemaker|fast-ai",
        "Question_view_count":310,
        "Owner_creation_date":"2014-10-03 14:59:22.923 UTC",
        "Owner_last_access_date":"2022-09-23 18:31:23.337 UTC",
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to retrain a sagemaker model on new data, after a training job is created",
        "Question_body":"<p>I have a sagemaker model that is trained on a specific dataset, and training job is created. Now i have a new dataset that the model has to be trained on, how do I retrain the model on new data from the already existing model ? Can we have the model checkpoints saved ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-16 19:37:49.987 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker|pre-trained-model",
        "Question_view_count":660,
        "Owner_creation_date":"2017-07-19 21:52:51.967 UTC",
        "Owner_last_access_date":"2020-06-02 23:45:58.22 UTC",
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<blockquote>\n  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-03-17 16:31:44.473 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can gzip tar files be used for training data in Sagemaker?",
        "Question_body":"<p>I have 50TB of uncompressed data (images) that is in dozens of tar.gz files in S3. I'm training tensorflow models with a dozen of these tar.gz files at a time. I would like to use a Sagemaker training job to pull this data and unpack it before training. Is this possible? Do I have to change the way that the data is stored before running training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-28 16:54:46.873 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":433,
        "Owner_creation_date":"2016-10-20 18:11:51.723 UTC",
        "Owner_last_access_date":"2022-09-23 19:32:59.27 UTC",
        "Owner_location":"Canada",
        "Owner_reputation":2021,
        "Owner_up_votes":1876,
        "Owner_down_votes":4,
        "Owner_views":134,
        "Answer_body":"<p><strong>Short Answer<\/strong> : No<\/p>\n<p><strong>Long Answer<\/strong>:\nThe recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf<\/strong><\/p>\n<p>The benefits are<\/p>\n<ol>\n<li>reducing delay when the container is launched<\/li>\n<li>not needing to scale the instance storage to the size of the training data<\/li>\n<li>increasing throughput by moving most preprocessing before model training<\/li>\n<\/ol>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233<\/a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)<\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-28 18:22:51.047 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to pass a request to sagemaker using postman",
        "Question_body":"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-05 15:03:12.383 UTC",
        "Question_favorite_count":2.0,
        "Question_score":8,
        "Question_tags":"web-services|amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":6746,
        "Owner_creation_date":"2014-09-17 16:42:55.307 UTC",
        "Owner_last_access_date":"2022-09-22 07:49:15.917 UTC",
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. <\/p>\n\n<p>I am guessing, there could be two places where might be stuck. \nOne could be, sending an actual PostMan Request with all the headers and everything. \nNewer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=\"https:\/\/github.com\/postmanlabs\/postman-app-support\/issues\/1663\" rel=\"noreferrer\">issue-1663<\/a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. <\/p>\n\n<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. <\/p>\n\n<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows <\/p>\n\n<pre><code>import boto3\nruntime= boto3.client('runtime.sagemaker')\n\npayload = getImageData()\n\n\nresult  = runtime.invoke_endpoint(\n    EndpointName='my_endpoint_name',\n    Body=payload,\n    ContentType='image\/jpeg'\n)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-05 16:37:35.433 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":11.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to make inference to a keras model hosted on AWS SageMaker via AWS Lambda function?",
        "Question_body":"<p>I have a pre-trained <code>keras<\/code> model which I have hosted on <code>AWS<\/code> using <code>AWS SageMaker<\/code>. I've got an <code>endpoint<\/code> and I can make successful <code>predictions<\/code> using the <code>Amazon SageMaker Notebook instance<\/code>.<\/p>\n<p>What I do there is that I serve a <code>.PNG image<\/code> like the following and the model gives me correct prediction.<\/p>\n<pre><code>file= s3.Bucket(bucketname).download_file(filename_1, 'normal.png')\nfile_name_1='normal.png'\n\n\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\nendpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint\n\npredictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\ndata = np.array([resize(imread(file_name), (137, 310, 3))])\npredictor.predict(data)\n<\/code><\/pre>\n<p>Now I wanted to make predictions using a <code>mobile application<\/code>. For that I have to wrote a <code>Lambda function<\/code> in python and attached an <code>API gateway<\/code> to it. My <code>Lambda function<\/code> is the following.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nfrom scipy import signal\nfrom scipy.signal import butter, lfilter\nfrom scipy.io import wavfile\nimport scipy.signal as sps\nimport io\nfrom io import BytesIO\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom datetime import datetime\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\n\nENDPOINT_NAME = 'tensorflow-inference-0000-11-22-33-44-55-666'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    image = Image.open(io.BytesIO(decoded_file_name))\n\n    data = np.array([resize(imread(image), (137, 310, 3))])\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='text\/csv', Body=data)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>The third last line is giving me error <code>'PngImageFile' object has no attribute 'read'<\/code>.\nAny idea what I am missing here?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-05 09:38:35.81 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|aws-lambda|aws-api-gateway|amazon-sagemaker",
        "Question_view_count":114,
        "Owner_creation_date":"2018-06-10 11:15:50.99 UTC",
        "Owner_last_access_date":"2022-09-22 05:29:23.777 UTC",
        "Owner_location":"Pakistan",
        "Owner_reputation":439,
        "Owner_up_votes":80,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Answer_body":"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump<\/code> that list (of lists). Below is the code for reference.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nimport io\nfrom io import BytesIO\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# grab environment variable of Lambda Function\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])\n    \n    payload = json.dumps(data.tolist())\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n        \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-10 11:05:04.637 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2021-04-10 11:08:32.97 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon SageMaker could not get a response from the endpoint",
        "Question_body":"<p>I have built an anomaly detection model using AWS SageMaker inbuilt model: random cut forest.<\/p>\n<pre><code>    rcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    num_samples_per_tree=1000,\n    num_trees=100,\n    encrypt_inter_container_traffic=True,\n    enable_network_isolation=True,\n    enable_sagemaker_metrics=True)\n<\/code><\/pre>\n<p>and created the endpoint:-<\/p>\n<pre><code>    rcf_inference = rcf.deploy(\n              initial_instance_count=4, instance_type=&quot;ml.m5.xlarge&quot;,\n              endpoint_name='RCF-container2',\n              enable_network_isolation=True)\n<\/code><\/pre>\n<p>But when I tried to get the prediction using the endpoint I am running into the following error:-<\/p>\n<pre><code>    results = rcf_inference.predict(df.values)\n\n    ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Amazon SageMaker could not get a response from the RCF-container2 endpoint. This can occur when CPU or memory utilization is high. To check your utilization, see Amazon CloudWatch. To fix this problem, use an instance type with more CPU capacity or memory.&quot;\n<\/code><\/pre>\n<p>I have tried with larger cpu instance but still I am getting the same issue. I guess the issue is functional.<\/p>\n<p>Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-12 11:50:25.017 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_date":"2018-07-15 07:40:34.317 UTC",
        "Owner_last_access_date":"2022-06-20 05:07:35.513 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":205,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Chalice, can't get image from POST request",
        "Question_body":"<p>I'm trying to invoke my sagemaker model using aws chalice, a lambda function, and an API Gateaway.<\/p>\n\n<p>I'm attempting to send the image over <code>POST<\/code> request but I'm having problem receiving it on the lambda function.<\/p>\n\n<p>My code looks like:<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\n\napp = Chalice(app_name='foo')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'], content_types=['application\/json'])\ndef index():\n    body = ''\n\n    try:\n        body = app.current_request.json_body # &lt;- I suspect this is the problem\n        return {'response': body}\n    except Exception as e:\n        return  {'error':  str(e)}\n<\/code><\/pre>\n\n<p>It's just returning<\/p>\n\n<p><code>&lt;Response [200]&gt; {'error': 'BadRequestError: Error Parsing JSON'}<\/code><\/p>\n\n<p>As I mentioned before, my end goal is to receive my image and make a sagemaker request with it. But I just can't seem to read the image. <\/p>\n\n<p>My python test client looks like this:<\/p>\n\n<pre><code>import base64, requests, json\n\ndef test():\n\n    url = 'api_url_from_chalice'\n    body = ''\n\n    with open('b1.jpg', 'rb') as image:\n        f = image.read()\n        body = base64.b64encode(f)\n\n    payload = {'data': body}\n    headers = {'Content-Type': 'application\/json'}\n\n    r = requests.post(url, data=payload, headers=headers)\n    print(r)\n    r = r.json()\n    # r = r['response']\n\n    print(r)\n\ntest()\n<\/code><\/pre>\n\n<p>Please help me, I spent way to much time trying to figure this out<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-28 16:09:27.37 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|rest|amazon-sagemaker|chalice",
        "Question_view_count":1229,
        "Owner_creation_date":"2019-08-09 23:05:38.427 UTC",
        "Owner_last_access_date":"2022-05-24 07:32:00.683 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>So I was able to figure it out with the help of an aws engineer (i got lucky I suppose). I'm including the complete lambda function. Nothing changed on the client.<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\nimport sys\n\n\nfrom chalice import Chalice\nif sys.version_info[0] == 3:\n    # Python 3 imports.\n    from urllib.parse import urlparse, parse_qs\nelse:\n    # Python 2 imports.\n    from urlparse import urlparse, parse_qs\n\napp = Chalice(app_name='app_name')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'])\ndef index():\n    parsed = parse_qs(app.current_request.raw_body.decode())\n\n    body = parsed['data'][0]\n    print(type(body))\n\n    try:\n        body = base64.b64decode(body)\n        body = bytearray(body)\n    except e:\n        return {'error': str(e)}\n\n\n    endpoint = \"object-detection-endpoint_name\"\n    runtime = boto3.Session().client(service_name='sagemaker-runtime', region_name='us-east-2')\n\n    response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='image\/jpeg', Body=body)\n\n    print(response)\n    results = response['Body'].read().decode(\"utf-8\")\n    results = results['predictions']\n\n    results = json.loads(results)\n    results = results['predictions']\n\n    return {'result': results}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-02 23:26:08.03 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-03-28 16:22:52.92 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Running Tensorboard in Jupyter in Sagemaker: this site can't be reached",
        "Question_body":"<p>I'm trying to run tensorboard from my Jupyter notebook in Sagemaker. The below is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport datetime, os\n\n%load_ext tensorboard\nlogs_base_dir = &quot;.\/logs&quot;\nos.makedirs(logs_base_dir)\n\n!tensorboard --logdir=data\/ --host localhost --port=8080\n<\/code><\/pre>\n<p>The output I get looks fine:\n<code>TensorBoard 1.14.0 at http:\/\/localhost:8080\/ (Press CTRL+C to quit)<\/code>\nbut when I click on the link, I'm taken to a page with ERR_CONNECTION_REFUSED.<\/p>\n<p>Does anyone have suggestions about what to try next? Thanks so much!<\/p>\n<p>Tensorflow: 1.14\nPython: 2<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-06-17 18:06:30.457 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|localhost|jupyter|tensorboard|amazon-sagemaker",
        "Question_view_count":286,
        "Owner_creation_date":"2012-06-22 19:11:27 UTC",
        "Owner_last_access_date":"2022-02-01 02:25:09.997 UTC",
        "Owner_location":null,
        "Owner_reputation":1843,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Answer_body":"<p>Based on the comments it seems to me that you are trying to open the wrong URL. If I understand you question, you are not running in a local environment, so you can not open <code>localhost<\/code>. The right URL for <code>sagemaker<\/code> from the docs is <code>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-21 08:17:03.263 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2021-06-18 06:05:19.287 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Control AWS Sagemaker costs",
        "Question_body":"<p>I want to use GPU capacity for deep learning models. Sagemaker is great in its flexibility of starting on demand clusters for training. However, my department wants to have guarantees we won't overspend on the AWS budget. Is there a way to 'cap' the costs without resorting to using a dedicated machine? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-15 09:06:54.72 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":933,
        "Owner_creation_date":"2011-11-20 21:12:53.477 UTC",
        "Owner_last_access_date":"2022-05-25 11:12:08.817 UTC",
        "Owner_location":"Hilversum, Netherlands",
        "Owner_reputation":572,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS API Gateway returns TARGET_MODEL_HEADER_MISSING even though target model passed in header",
        "Question_body":"<p>I have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.\nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.<\/p>\n<p>My https request looks like this:<\/p>\n<pre><code>import requests\nheaders = {'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'}\nresponse = requests.request(&quot;POST&quot;\n, &quot;https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles&quot;\n, headers = headers\n, data = data\n)\n<\/code><\/pre>\n<p>According to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> and the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py\" rel=\"nofollow noreferrer\">source code<\/a>\nit seems like I am providing the header with the target model correctly. But this is obvously not the case.<\/p>\n<p>How am I supposed to provide the target model in the header with the https-request?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-28 08:52:58.577 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|api-gateway",
        "Question_view_count":123,
        "Owner_creation_date":"2017-02-06 20:25:25.5 UTC",
        "Owner_last_access_date":"2022-09-23 12:39:06.777 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemker debugger examples TypeError missing argument",
        "Question_body":"<p><strong>TypeError: __init__() missing 1 required positional argument: 'action_str'<\/strong><\/p>\n<p>The error pops up in the official example jupyter notebook (<strong>mnist_tensor_analysis.ipynb<\/strong>) of the amazon Sagemaker debugger. Occurs when calling the constructor of the Rule package from smdebug.rules.rule.\n<a href=\"https:\/\/i.stack.imgur.com\/O00Um.jpg\" rel=\"nofollow noreferrer\">screenshot of the error<\/a><\/p>\n<p>Are there any solutions to this problem? The error is visible in the official repo of the AWS Sagemaker debugger. Below is the link to the notebook.<\/p>\n<p>link : <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-debugger\/mnist_tensor_analysis\/mnist_tensor_analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-debugger\/mnist_tensor_analysis\/mnist_tensor_analysis.ipynb<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-17 07:13:10.16 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ec2|typeerror|amazon-sagemaker",
        "Question_view_count":147,
        "Owner_creation_date":"2022-02-17 06:50:55.17 UTC",
        "Owner_last_access_date":"2022-04-20 17:12:31.137 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"What is the best and correct way of hosting an endpoint running R code?",
        "Question_body":"<p>I think it must be a relatively common use case to load a model and invoke an endpoint to call R's <code>predict(object, newdata, ...)<\/code> function.  I wanted to do this with a custom AWS Sagemaker container, using <code>plumber<\/code> on the R side.  This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">example<\/a> gives all the details, I think, and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">this bit of documentation<\/a> also explains how the container should be built and react.\nI followed the steps of these documents, but I get<\/p>\n<blockquote>\n<p>The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.<\/p>\n<\/blockquote>\n<p>in the Sagemaker console after a couple of long minutes, and the endpoint creation fails.<\/p>\n<p>This is my container:<\/p>\n<pre><code># --- Dockerfile\nFROM rocker\/r-base\nRUN apt-get -y update &amp;&amp; apt-get install -y libsodium-dev libcurl4-openssl-dev\nRUN apt-get install -y  \\\n    ca-certificates\n\nRUN R -e &quot;install.packages(c('lme4', 'plumber'))&quot;\n\nADD .\/plumber.R \/\n\nENTRYPOINT [&quot;R&quot;, &quot;-e&quot;, &quot;plumber::pr_run(plumber::pr('plumber.R'), port=8080)&quot;, \\\n            &quot;--no-save&quot;]\n<\/code><\/pre>\n<pre><code># --- plumber.R\nlibrary(plumber)\nlibrary(lme4)\n\nprefix &lt;- '\/opt\/ml'\nprint(dir('\/opt\/ml', recursive = TRUE))\nmodel &lt;- readRDS(file.path(prefix, 'model', 'model.RDS'))\n\n#* @apiTitle Guess the likelihood of something\n\n#' Ping to show server is there\n#' @get \/ping\nfunction() {\n  print(paste('successfully pinged at', Sys.time()))\n  return('')}\n\n#' Parse input and return prediction from model\n#' @param req The http request sent\n#' @post \/invocations\nfunction(req) {\n  print(paste('invocation triggered at', Sys.time()))\n  conn &lt;- textConnection(gsub('\\\\\\\\n', '\\n', req$postBody))\n  data &lt;- read.csv(conn)\n  close(conn)\n  \n  print(data)\n  \n  predict(model, data,\n          allow.new.levels = TRUE,\n          type = 'response')\n}\n\n<\/code><\/pre>\n<p>And then the endpoint is created using this code:<\/p>\n<pre><code># run_on_sagemaker.py\n# [...]\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    ExecutionRoleArn=role,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': s3_model_location\n    }\n)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType': instance_type,\n        'InitialInstanceCount': 1,\n        'ModelName': model_name,\n        'VariantName': 'AllTraffic'}])\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response['EndpointConfigArn'])\n\nprint('Endpoint Response:')\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\nprint(create_endpoint_response['EndpointArn'])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp['EndpointStatus']\nprint(&quot;Status: &quot; + status)\n\ntry:\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\nfinally:\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(&quot;Arn: &quot; + resp['EndpointArn'])\n    print(&quot;Status: &quot; + status)\n    if status != 'InService':\n        raise Exception('Endpoint creation did not succeed')\n    print(create_model_response['ModelArn'])\n<\/code><\/pre>\n<p>Most code is actually copied from the abovementioned example, the most significant difference I note is that in my container the model is loaded right away while in the example it loads the model object every time an invocation is made (which must be slowing responses down, so i wonder, why?).<\/p>\n<p>The logs on Cloudwatch equal the output of the container when it's run locally and indicate no failure.  Locally I can query the container with\n<code>curl -d &quot;data\\nin\\ncsv\\nformat&quot; -i localhost:8080\/invocations<\/code> and it works fine and gives back a prediction for every row in the POST data.  Also, <code>curl localhost:8080\/ping<\/code> returns <code>[&quot;&quot;]<\/code>, as it should, I think.  And it shows no signs of being slow, the model object is a 4.4MiB in size (although this is to be extended greatly once this simple version runs).<\/p>\n<p>The error on the terminal is<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 57, in &lt;module&gt;\n    sm.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 53, in wait\n    Waiter.wait(self, **kwargs)\n  File &quot;[...]\/lib\/python3.8\/site-packages\/botocore\/waiter.py&quot;, line 320, in wait\n    raise WaiterError(\nbotocore.exceptions.WaiterError: Waiter EndpointInService failed: Waiter encountered a terminal failure state\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;run_on_sagemaker.py&quot;, line 64, in &lt;module&gt;\n    raise Exception('Endpoint creation did not succeed')\n\n<\/code><\/pre>\n<p>So, why is this failing on the Sagemaker console?  Is this a good way, are there better ways, and how can I do further diagnostics?  Generally, I also could not get the AWS example (see above) for your own R container running, so I wonder what the best way to run R predictions of a Sagemaker model is.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-04 15:00:42.99 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker|plumber",
        "Question_view_count":163,
        "Owner_creation_date":"2017-10-14 20:40:12.543 UTC",
        "Owner_last_access_date":"2022-08-31 14:40:35.367 UTC",
        "Owner_location":null,
        "Owner_reputation":121,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-02-04 15:08:26.6 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Running mlflow ui in AWS Sagemaker",
        "Question_body":"<p>I want to run mlflow UI in sagemaker but it simply does not work, When it outputs the http address going to it results in a &quot;this site cannot be reached&quot;<\/p>\n<p>Here is the code:<\/p>\n<pre><code>def mlflow_test(server_uri, experiment_name):\n    mlflow.set_tracking_uri(server_uri)\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run():\n        params = {\n            &quot;n-estimators&quot;: 100,\n            &quot;min-samples-leaf&quot;: 10,\n            &quot;features&quot;: 'feature_test'\n        }\n        mlflow.log_params(params)\n        mlflow.log_metric('foo', 5)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>running that code will return:<\/p>\n<pre><code>[2022-05-24 15:48:44 +0000] [27820] [INFO] Starting gunicorn 20.1.0\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Listening at: http:\/\/127.0.0.1:5000 (27820)\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Using worker: sync\n[2022-05-24 15:48:44 +0000] [27823] [INFO] Booting worker with pid: 27823\n<\/code><\/pre>\n<p>Going to the <a href=\"http:\/\/127.0.0.1:5000\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000<\/a> link won't work. Anyone know how to get mlflow ui running in sagemaker? There's not much info on this that's at an easy to understand level. I just want to log my metrics and params in sagemaker and view them using the mlflow ui<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-24 15:54:09.377 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":93,
        "Owner_creation_date":"2021-03-17 15:21:32.347 UTC",
        "Owner_last_access_date":"2022-07-14 10:53:41.117 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Auto-Scaling | assumed-role role does not have access",
        "Question_body":"<p>I have a sagemaker instance which I want to auto scale, currently it is working on 4 instances but I want to auto-scale it from 1 to 4, as per the load.<\/p>\n\n<p>This is the code I am using to auto-scale <\/p>\n\n<pre><code>resource_id = 'endpoint\/[end-point-name]\/variant\/config1'\nsc_client = boto3.client('application-autoscaling')\nrole = 'arn:aws:iam::[1234]:role\/service-role\/AmazonSageMaker-ExecutionRole-[1234]'\n\nresponse = sc_client.register_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n    MinCapacity=1,\n    MaxCapacity=4,\n    RoleARN= role,\n    SuspendedState={\n        'DynamicScalingInSuspended': True,\n        'DynamicScalingOutSuspended': True,\n        'ScheduledScalingSuspended': True\n    }\n)\n<\/code><\/pre>\n\n<p>I have given all the access (sagemaker and cloudwatch) on all resources to this role : AmazonSageMaker-ExecutionRole-[1234]<\/p>\n\n<p>Now I am getting this error whenever i ran this code <\/p>\n\n<pre><code>ClientError: An error occurred (AccessDeniedException) when calling the RegisterScalableTarget \noperation: User: arn:aws:sts::[1234]:assumed-role\/AmazonSageMaker-ExecutionRole-[1234]\/SageMaker \nis not authorized to perform: iam:PassRole on resource: arn:aws:iam::[1234]:role\/service-role\/AmazonSageMaker-ExecutionRole-[1234]\n<\/code><\/pre>\n\n<p>Now I am not sure how it is pickin 'assumed-role' instead of 'service-role' and how to fix the issue, I am using admin account which have all the access and the above 'service-role' also have all the access <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-09-18 07:31:53.883 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-sdk|amazon-sagemaker",
        "Question_view_count":414,
        "Owner_creation_date":"2017-07-30 08:26:08.107 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.87 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to load trained model in amazon sagemaker?",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-19 12:36:37.333 UTC",
        "Question_favorite_count":2.0,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":4776,
        "Owner_creation_date":"2011-07-16 13:02:36.88 UTC",
        "Owner_last_access_date":"2022-09-24 20:19:39.59 UTC",
        "Owner_location":"Slovenia",
        "Owner_reputation":14913,
        "Owner_up_votes":307,
        "Owner_down_votes":1,
        "Owner_views":1093,
        "Answer_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-07-19 12:49:45.81 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":8.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError",
        "Question_body":"<p>Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:<\/p>\n<p>The script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.<\/p>\n<pre><code>! python script.py --n-estimators 100 \\\n                   --max_depth 2 \\\n                   --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\ \n<\/code><\/pre>\n<p>Confusion Matrix:\n[[13  8]\n[ 1 17]]\nAccuracy:\n0.7692307692307693<\/p>\n<p>I used the Estimator from Sagemaker Python SDK<\/p>\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\nsklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n<\/code><\/pre>\n<p>I launched the training job as follows<\/p>\n<pre><code>sklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n<\/code><\/pre>\n<p>Here I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix<\/p>\n<pre><code>sklearn_estimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n<\/code><\/pre>\n<pre><code>\n2022-08-25 12:03:27 Starting - Starting the training job....\n2022-08-25 12:03:52 Starting - Preparing the instances for training............\n2022-08-25 12:04:55 Downloading - Downloading input data......\n2022-08-25 12:05:31 Training - Downloading the training image.........\n2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n2022-08-25 12:06:32 Uploading - Uploading generated training model.\n2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-37-628f942a78d3&gt; in &lt;module&gt;\n----&gt; 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-&gt; 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-&gt; 3228         self._check_job_status(job, desc, &quot;TrainingJobStatus&quot;)\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3392                 actual_status=status,\n   3393             )\n   3394 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\n    entrypoint()\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 39, in main\n    train(environment.Environment())\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py&quot;, line 100, in run\n    wait, capture_error\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 291, in run\n    cwd=environment.code_dir,\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage &quot;&quot;\nCommand &quot;\/miniconda3\/bin\/python script.py&quot;\n\nExecuteUserScriptErr\n<\/code><\/pre>\n<p>I am happy for some help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-25 12:23:07.797 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|random-forest|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_date":"2021-11-26 08:25:52.293 UTC",
        "Owner_last_access_date":"2022-09-22 11:50:10.577 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Question_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-16 15:58:06.157 UTC",
        "Question_favorite_count":0.0,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":1641,
        "Owner_creation_date":"2018-06-28 13:47:43.837 UTC",
        "Owner_last_access_date":"2018-07-31 15:13:18.053 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-16 22:05:53.543 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How can I specify certificate authority when creating a model in Sagemaker with boto3",
        "Question_body":"<p>We try to create a model in Sagemaker with boto3 based on an image we saved in a private artifactory. We found a way to setup the basic credentials acording to the doc <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/your-algorithms-containers-inference-private.html\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/your-algorithms-containers-inference-private.html<\/a> but I cant find a solution to pass the certificate authority.<\/p>\n<p>The code we use to create the model is<\/p>\n<pre><code>import boto3\n\nclient = boto3.client('sagemaker')\n\nresponse = client.create_model(\n    ModelName='model-name',\n    PrimaryContainer={\n        'Image': 'local-repo.my_artifactory_endpoint\/train:latest',\n        'ImageConfig': {\n            'RepositoryAccessMode': 'Vpc',\n            \n        },\n        'Environment': {\n            'SAGEMAKER_PROGRAM': 'train.py',\n            &quot;SAGEMAKER_SUBMIT_DIRECTORY&quot;: &quot;\/opt\/ml\/model\/code&quot;,\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;,\n            &quot;SAGEMAKER_REGION&quot;: &lt;my-region&gt;,\n            &quot;MMS_DEFAULT_RESPONSE_TIMEOUT&quot;: &quot;500&quot;\n        }\n    },\n    ExecutionRoleArn=&lt;my-role&gt;,\n    VpcConfig={\n        'SecurityGroupIds': [\n            ...\n        ],\n        'Subnets': [\n            ...\n        ]\n    },\n    EnableNetworkIsolation=False\n)\n<\/code><\/pre>\n<p>After, when we try to create an endpoint based on that model we endup with ther error:<\/p>\n<pre><code>Attempt to pull model image local-repo.my_artifactory_endpoint\/train:latest failed due to error constructing client to call registry: constructing authentication challenge manager: Get &quot;https:\/\/local-repo.my_artifactory_endpoint\/v2\/&quot;: x509: certificate signed by unknown authority.\n<\/code><\/pre>\n<p>We don't find a way to pass this certificate or to ignore ssl option.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2022-06-20 16:40:37.693 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|ssl-certificate|boto3|amazon-sagemaker",
        "Question_view_count":60,
        "Owner_creation_date":"2019-04-12 10:17:43.45 UTC",
        "Owner_last_access_date":"2022-07-12 07:59:12.82 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Customising model in AWS sagemaker",
        "Question_body":"<p>I have a python script which I wrote using tensorflow python 3.6 AWS sagemaker jupyter notebook inside AWS sagemaker instance. I have to use sagemaker debugger for my Deep Learning model. I can see many links suggesting that first dockerise the algorithm image and then use it over sagemaker. Can anyone please suggest that is there any available alternative such that Tensorflow-1 docker image is available and I can include some other packages via pip in this image and then run my model on sagemaker ? I am using keras 2.3.0 with tensorflow 1.15 .Please guide and share necessary references.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-22 07:16:02.16 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|docker|amazon-sagemaker|keras-2|tensorflow1.15",
        "Question_view_count":181,
        "Owner_creation_date":"2016-09-14 05:43:44.143 UTC",
        "Owner_last_access_date":"2022-08-16 12:07:05.64 UTC",
        "Owner_location":null,
        "Owner_reputation":348,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-10-22 08:49:45.23 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Question_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-25 22:22:31.157 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_date":"2019-06-15 13:48:18.25 UTC",
        "Owner_last_access_date":"2022-09-12 19:29:36.3 UTC",
        "Owner_location":"Provo, UT, USA",
        "Owner_reputation":528,
        "Owner_up_votes":351,
        "Owner_down_votes":61,
        "Owner_views":39,
        "Answer_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-29 10:29:18.097 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker custom model output path for tensorflow when creating from s3 artifacts",
        "Question_body":"<p>I'm running the following code to create an endpoint with a preexisting model:<\/p>\n<pre><code>from sagemaker.tensorflow import serving\nsagemaker_session = sagemaker.Session()\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',\n                                     entry_point=&quot;inference.py&quot;,\n                                     source_dir=&quot;inf_source_dir&quot;,\n                                     role=get_execution_role(),\n                                     framework_version='1.14',\n                                     sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<p>However this create a copy of the model into the default sagemaker bucket. How can I pass a custom path? I've tried model_dir, and output_path but neither are accepted as parameters<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-31 22:56:18.813 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":216,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point<\/code> and <code>source_dir<\/code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.<\/p>\n<p>You can change this behavior by setting the <code>default_bucket<\/code> in your <code>sagemaker_session<\/code> as follows:<\/p>\n<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)\n\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',         \n                    .\n                    .\n                    sagemaker_session=sagemaker_session)\n                    .\n                    )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-24 18:24:01.933 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker Batch Transform fails to access nginx",
        "Question_body":"<p>I had been trying to deploy a ML model on SageMaker using Docker.\nSo far I have been able to run the Training job, however the model inference using batch transform is having issue with nginx log:\n<a href=\"https:\/\/i.stack.imgur.com\/pSN91.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pSN91.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The nginx.conf file has the following:<\/p>\n<pre><code>worker_processes 1;\ndaemon off; # Prevent forking\n\n\npid \/tmp\/nginx.pid;\nerror_log \/var\/log\/nginx\/error.log;\n\nevents {\n  # defaults\n}\n\nhttp {\n  include \/etc\/nginx\/mime.types;\n  default_type application\/octet-stream;\n  access_log \/var\/log\/nginx\/access.log combined;\n  \n  upstream gunicorn {\n    server unix:\/tmp\/gunicorn.sock;\n  }\n\n  server {\n    listen 8080 deferred;\n    client_max_body_size 5m;\n\n    keepalive_timeout 5;\n    proxy_read_timeout 1200s;\n\n    location ~ ^\/(ping|invocations) {\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n      proxy_pass http:\/\/gunicorn;\n    }\n\n    location \/ {\n      return 404 &quot;{}&quot;;\n    }\n  }\n}\n<\/code><\/pre>\n<p>And the 'serve' file looks like this:<\/p>\n<pre><code>#!\/usr\/bin\/env python\n\n# This file implements the scoring service shell. You don't necessarily need to modify it for various\n# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until\n# gunicorn exits.\n#\n# The flask server is specified to be the app object in wsgi.py\n#\n# We set the following parameters:\n#\n# Parameter                Environment Variable              Default Value\n# ---------                --------------------              -------------\n# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\n# timeout                  MODEL_SERVER_TIMEOUT              60 seconds\n\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/program\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-k', 'sync',\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n# The main routine just invokes the start function.\n\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n<p>I have predictor.py that has flask handler as below:<\/p>\n<pre><code># This is the file that implements a flask server to do inferences. It's the file that you will modify to\n# implement the scoring for your own algorithm.\n\nfrom __future__ import print_function\n\nimport io\nimport json\nimport os\nimport pickle\nimport signal\nimport sys\nimport traceback\n\nimport flask\nimport pandas as pd\n\nprefix = &quot;\/opt\/ml\/&quot;\nmodel_path = os.path.join(prefix, &quot;model&quot;)\n\n# A singleton for holding the model. This simply loads the model and holds it.\n# It has a predict function that does a prediction based on the model and the input data.\n\n\nclass ScoringService(object):\n    model = None  # Where we keep the model when it's loaded\n\n    @classmethod\n    def get_model(cls):\n        &quot;&quot;&quot;Get the model object for this instance, loading it if it's not already loaded.&quot;&quot;&quot;\n        if cls.model == None:\n            with open(os.path.join(model_path, &quot;_logreg_cap.pkl&quot;), &quot;rb&quot;) as inp:\n                cls.model = pickle.load(inp)\n        return cls.model\n\n    @classmethod\n    def predict(cls, input):\n        &quot;&quot;&quot;For the input, do the predictions and return them.\n\n        Args:\n            input (a pandas dataframe): The data on which to do the predictions. There will be\n                one prediction per row in the dataframe&quot;&quot;&quot;\n        clf = cls.get_model()\n        return clf.predict(input)\n\n\n# The flask app for serving predictions\napp = flask.Flask(__name__)\n\n\n@app.route(&quot;\/ping&quot;, methods=[&quot;GET&quot;])\ndef ping():\n    &quot;&quot;&quot;Determine if the container is working and healthy. In this sample container, we declare\n    it healthy if we can load the model successfully.&quot;&quot;&quot;\n    health = ScoringService.get_model() is not None  # You can insert a health check here\n\n    status = 200 if health else 404\n    return flask.Response(response=&quot;\\n&quot;, status=status, mimetype=&quot;application\/json&quot;)\n\n\n@app.route(&quot;\/invocations&quot;, methods=[&quot;POST&quot;])\ndef transformation():\n    &quot;&quot;&quot;Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n    just means one prediction per line, since there's a single column.\n    &quot;&quot;&quot;\n    data = None\n\n    # Convert from CSV to pandas\n    if flask.request.content_type == &quot;text\/csv&quot;:\n        data = flask.request.data.decode(&quot;utf-8&quot;)\n        s = io.StringIO(data)\n        data = pd.read_csv(s, header=None)\n    else:\n        return flask.Response(\n            response=&quot;This predictor only supports CSV data&quot;, status=415, mimetype=&quot;text\/plain&quot;\n        )\n\n    print(&quot;Invoked with {} records&quot;.format(data.shape[0]))\n\n    # Do the prediction\n    predictions = ScoringService.predict(data)\n\n    # Convert from numpy back to CSV\n    out = io.StringIO()\n    pd.DataFrame({&quot;results&quot;: predictions}).to_csv(out, header=False, index=False)\n    result = out.getvalue()\n\n    return flask.Response(response=result, status=200, mimetype=&quot;text\/csv&quot;)\n\n\n<\/code><\/pre>\n<p>Any solution?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-14 22:56:07.067 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|amazon-sagemaker|nginx-config",
        "Question_view_count":72,
        "Owner_creation_date":"2017-07-27 22:34:49.14 UTC",
        "Owner_last_access_date":"2022-09-23 22:06:42.447 UTC",
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":119,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-07-15 18:23:52.093 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1 training-job\/object-detection-2019-12-10-07-43-06-930 with an explicit deny",
        "Question_body":"<p>I am using <strong>amazon sagemake<\/strong>r to perform object detection task and while running the exucution task i am facing this issue. Am i missing any policies in I am role?<\/p>\n\n<p>the whole error is <\/p>\n\n<p>SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:576481626755:training-job\/object-detection-2019-12-10-07-43-06-930 with an explicit deny<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-10 08:11:13.507 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":431,
        "Owner_creation_date":"2017-06-04 17:42:26.653 UTC",
        "Owner_last_access_date":"2020-10-28 15:43:23.88 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Copy a SageMaker Notebook to SageMaker Studio",
        "Question_body":"<p>My colleague made a notebook in SageMaker but I want to copy that notebook into SageMaker Studio so that future collaborations and changes are smoother.\nFrom Studio, I can't see anything that relates it to SageMaker classic.<\/p>\n<p>Any advice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-21 16:01:20.27 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":265,
        "Owner_creation_date":"2021-07-13 19:03:16.31 UTC",
        "Owner_last_access_date":"2022-09-23 15:03:18.587 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Question_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-07-27 12:01:33.46 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":474,
        "Owner_creation_date":"2014-06-14 14:25:34.69 UTC",
        "Owner_last_access_date":"2022-09-25 03:04:43.037 UTC",
        "Owner_location":"Porto, Portugal",
        "Owner_reputation":5998,
        "Owner_up_votes":2638,
        "Owner_down_votes":56,
        "Owner_views":426,
        "Answer_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-07-28 16:11:01.83 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-07-28 10:24:44.943 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to deploy image classifier with resnet50 model on AWS endpoint to predict without worker dying?",
        "Question_body":"<p>Created a imageclassifier model built on renet50 to identify dog breeds. I created it in sagemaker studio. Tuning and training are done, I deployed it, but when I try to predict on it, it fails. I believe this is related to the pid of the worker because its first warning I see.\nGetting following Cloudwatch log output says worker pid not available yet then soon after the worker dies.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>timestamp,message,logStreamName\n1648240674535,&quot;2022-03-25 20:37:54,107 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,&quot;2022-03-25 20:37:54,188 [INFO ] main org.pytorch.serve.ModelServer - &quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Torchserve version: 0.4.0,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,TS Home: \/opt\/conda\/lib\/python3.6\/site-packages,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Current directory: \/,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Temp directory: \/home\/model-server\/tmp,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Number of GPUs: 0,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Number of CPUs: 1,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Max heap size: 6838 M,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Python executable: \/opt\/conda\/bin\/python3.6,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Config file: \/etc\/sagemaker-ts.properties,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Inference address: http:\/\/0.0.0.0:8080,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Management address: http:\/\/0.0.0.0:8080,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Metrics address: http:\/\/127.0.0.1:8082,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Model Store: \/.sagemaker\/ts\/models,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Initial Models: model.mar,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Log dir: \/logs,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Metrics dir: \/logs,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Netty threads: 0,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Netty client threads: 0,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Default workers per model: 1,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Blacklist Regex: N\/A,AllTraffic\/i-055c5d00e53e84b93\n1648240674535,Maximum Response Size: 6553500,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Maximum Request Size: 6553500,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Prefer direct buffer: false,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Allowed Urls: [file:\/\/.*|http(s)?:\/\/.*],AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Custom python dependency for model allowed: false,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Metrics report format: prometheus,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Enable metrics API: true,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,Workflow Store: \/.sagemaker\/ts\/models,AllTraffic\/i-055c5d00e53e84b93\n1648240674536,&quot;2022-03-25 20:37:54,195 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675536,&quot;2022-03-25 20:37:54,217 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675536,&quot;2022-03-25 20:37:55,505 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675786,&quot;2022-03-25 20:37:55,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240675786,Model server started.,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,727 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,812 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,813 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:38.02598190307617|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,813 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:12.715518951416016|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,814 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:25.1|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,815 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29583.98046875|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,815 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1355.765625|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,816 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:5.7|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9000&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]48&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676036,&quot;2022-03-25 20:37:55,999 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9000&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,006 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9000.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG - Backend worker process died.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 182, in &lt;module&gt;&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -     worker.run_server()&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 154, in run_server&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -     self.handle_connection(cl_socket)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 116, in handle_connection&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 89, in load_model&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py&quot;&quot;, line 110, in load&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     initialize_fn(service.context)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/home\/model-server\/tmp\/models\/23b30361031647d08792d32672910688\/handler_service.py&quot;&quot;, line 51, in initialize&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -     super().initialize(context)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stderr&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stdout&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py&quot;&quot;, line 66, in initialize&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stdout&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676536,&quot;2022-03-25 20:37:56,114 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676536,&quot;2022-03-25 20:37:56,416 [INFO ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stderr&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240676536,&quot;2022-03-25 20:37:56,461 [INFO ] W-9000-model_1 ACCESS_LOG - \/169.254.178.2:39848 &quot;&quot;GET \/ping HTTP\/1.1&quot;&quot; 200 9&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:56,461 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,567 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9000&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]86&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9000&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,569 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9000.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG - Backend worker process died.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 182, in &lt;module&gt;&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     worker.run_server()&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 154, in run_server&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     self.handle_connection(cl_socket)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 116, in handle_connection&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stderr&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stdout&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;&quot;, line 89, in load_model&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stdout&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240678037,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240679288,&quot;2022-03-25 20:37:57,991 [INFO ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stderr&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240679288,&quot;2022-03-25 20:37:59,096 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9000&quot;,AllTraffic\/i-055c5d00e53e84b93\n1648240679288,&quot;2022-03-25 20:37:59,097 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]114&quot;,AllTraffic\/i-055c5d00e53e84b93\n<\/code><\/pre>\n<p>Model tuning and training came out alright so I'm not sure why it won't predict if that is fine. Someone mentioned to me that it might be due to entry point script, but I don't know what would cause it fail in predicting after deployed if it can predict fine during training.<\/p>\n<p>Entry point script:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport json\n\nimport copy\nimport argparse\nimport os\nimport logging\nimport sys\nfrom tqdm import tqdm\nfrom PIL import ImageFile\nimport smdebug.pytorch as smd\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nlogger=logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\ndef test(model, test_loader, criterion, hook):\n    model.eval()\n    running_loss=0\n    running_corrects=0\n    hook.set_mode(smd.modes.EVAL)\n    \n    \n    for inputs, labels in test_loader:\n        outputs=model(inputs)\n        loss=criterion(outputs, labels)\n        _, preds = torch.max(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n    ##total_loss = running_loss \/\/ len(test_loader)\n    ##total_acc = running_corrects.double() \/\/ len(test_loader)\n    \n    ##logger.info(f&quot;Testing Loss: {total_loss}&quot;)\n    ##logger.info(f&quot;Testing Accuracy: {total_acc}&quot;)\n    logger.info(&quot;New test acc&quot;)\n    logger.info(f'Test set: Accuracy: {running_corrects}\/{len(test_loader.dataset)} = {100*(running_corrects\/len(test_loader.dataset))}%)')\n\ndef train(model, train_loader, validation_loader, criterion, optimizer, hook):\n    epochs=50\n    best_loss=1e6\n    image_dataset={'train':train_loader, 'valid':validation_loader}\n    loss_counter=0\n    hook.set_mode(smd.modes.TRAIN)\n    \n    for epoch in range(epochs):\n        logger.info(f&quot;Epoch: {epoch}&quot;)\n        for phase in ['train', 'valid']:\n            if phase=='train':\n                model.train()\n                logger.info(&quot;Model Trained&quot;)\n            else:\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in image_dataset[phase]:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                if phase=='train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    logger.info(&quot;Model Optimized&quot;)\n\n                _, preds = torch.max(outputs, 1)\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/\/ len(image_dataset[phase])\n            epoch_acc = running_corrects \/\/ len(image_dataset[phase])\n            \n            \n            if phase=='valid':\n                logger.info(&quot;Model Validating&quot;)\n                if epoch_loss&lt;best_loss:\n                    best_loss=epoch_loss\n                else:\n                    loss_counter+=1\n\n            logger.info(loss_counter)\n            '''logger.info('{} loss: {:.4f}, acc: {:.4f}, best loss: {:.4f}'.format(phase,\n                                                                                 epoch_loss,\n                                                                                 epoch_acc,\n                                                                                 best_loss))'''\n            \n            if phase==&quot;train&quot;:\n                logger.info(&quot;New epoch acc for Train:&quot;)\n                logger.info(f&quot;Epoch {epoch}: Loss {loss_counter\/len(train_loader.dataset)}, Accuracy {100*(running_corrects\/len(train_loader.dataset))}%&quot;)\n            if phase==&quot;valid&quot;:\n                logger.info(&quot;New epoch acc for Valid:&quot;)\n                logger.info(f&quot;Epoch {epoch}: Loss {loss_counter\/len(train_loader.dataset)}, Accuracy {100*(running_corrects\/len(train_loader.dataset))}%&quot;)\n            \n        ##if loss_counter==1:\n        ##    break\n        ##if epoch==0:\n        ##    break\n    return model\n    \ndef net():\n    model = models.resnet50(pretrained=True)\n\n    for param in model.parameters():\n        param.requires_grad = False   \n\n    model.fc = nn.Sequential(\n                   nn.Linear(2048, 128),\n                   nn.ReLU(inplace=True),\n                   nn.Linear(128, 133))\n    return model\n\ndef create_data_loaders(data, batch_size):\n    train_data_path = os.path.join(data, 'train')\n    test_data_path = os.path.join(data, 'test')\n    validation_data_path=os.path.join(data, 'valid')\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        ])\n\n    train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=train_transform)\n    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n    test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=test_transform)\n    test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n    validation_data = torchvision.datasets.ImageFolder(root=validation_data_path, transform=test_transform)\n    validation_data_loader  = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True) \n    \n    return train_data_loader, test_data_loader, validation_data_loader\n\ndef main(args):\n    logger.info(f'Hyperparameters are LR: {args.lr}, Batch Size: {args.batch_size}')\n    logger.info(f'Data Paths: {args.data}')\n\n    \n    train_loader, test_loader, validation_loader=create_data_loaders(args.data, args.batch_size)\n    model=net()\n    \n    hook = smd.Hook.create_from_json_file()\n    hook.register_hook(model)\n    \n    criterion = nn.CrossEntropyLoss(ignore_index=133)\n    optimizer = optim.Adam(model.fc.parameters(), lr=args.lr)\n    \n    logger.info(&quot;Starting Model Training&quot;)\n    model=train(model, train_loader, validation_loader, criterion, optimizer, hook)\n    \n    logger.info(&quot;Testing Model&quot;)\n    test(model, test_loader, criterion, hook)\n    \n    logger.info(&quot;Saving Model&quot;)\n    torch.save(model.cpu().state_dict(), os.path.join(args.model_dir, &quot;model.pth&quot;))\n    \nif __name__=='__main__':\n    parser=argparse.ArgumentParser()\n    '''\n    TODO: Specify any training args that you might need\n    '''\n    parser.add_argument(\n        &quot;--batch-size&quot;,\n        type=int,\n        default=64,\n        metavar=&quot;N&quot;,\n        help=&quot;input batch size for training (default: 64)&quot;,\n    )\n    parser.add_argument(\n        &quot;--test-batch-size&quot;,\n        type=int,\n        default=1000,\n        metavar=&quot;N&quot;,\n        help=&quot;input batch size for testing (default: 1000)&quot;,\n    )\n    parser.add_argument(\n        &quot;--epochs&quot;,\n        type=int,\n        default=5,\n        metavar=&quot;N&quot;,\n        help=&quot;number of epochs to train (default: 10)&quot;,\n    )\n    parser.add_argument(\n        &quot;--lr&quot;, type=float, default=0.01, metavar=&quot;LR&quot;, help=&quot;learning rate (default: 0.01)&quot;\n    )\n    parser.add_argument(\n        &quot;--momentum&quot;, type=float, default=0.5, metavar=&quot;M&quot;, help=&quot;SGD momentum (default: 0.5)&quot;\n    )\n\n    # Container environment\n    parser.add_argument(&quot;--hosts&quot;, type=list, default=json.loads(os.environ[&quot;SM_HOSTS&quot;]))\n    parser.add_argument(&quot;--current-host&quot;, type=str, default=os.environ[&quot;SM_CURRENT_HOST&quot;])\n    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])\n    parser.add_argument(&quot;--data&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TRAINING&quot;])\n    parser.add_argument(&quot;--num-gpus&quot;, type=int, default=os.environ[&quot;SM_NUM_GPUS&quot;])\n    args=parser.parse_args()\n    \n    main(args)\n<\/code><\/pre>\n<p>To test the model on the endpoint I sent over an image using the following code:<\/p>\n<pre><code>from sagemaker.serializers import IdentitySerializer\nimport base64\n\npredictor.serializer = IdentitySerializer(&quot;image\/png&quot;)\nwith open(&quot;Akita_00282.jpg&quot;, &quot;rb&quot;) as f:\n    payload = f.read()\n\n    \nresponse = predictor.predict(payload)```\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-28 19:32:16.573 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|neural-network|pytorch|amazon-sagemaker|resnet",
        "Question_view_count":229,
        "Owner_creation_date":"2022-01-03 18:22:34.077 UTC",
        "Owner_last_access_date":"2022-09-23 19:30:02.803 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-05 06:38:38.163 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Semantic Segmentation: iou and pixel accuracy per class",
        "Question_body":"<p>I am doing some semantic segemnetation work on AWS sagemaker. My output is the mean iou and pixel accuracy of all my classes but i need more granularity so require the iou and pixel accuracy for each class. This is to see what classes need to be improved.<\/p>\n\n<p>I imagine its possible but how? <\/p>\n\n<p>I have had a look at these similar questions but no solution as yet.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54837529\/tensorflow-iou-per-class\">Tensorflow : IOU per class<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/44041096\/iou-for-semantic-segmentation-implementation-in-python-caffe-per-class\">IoU for semantic segmentation implementation in python\/caffe per class<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-18 10:40:02.057 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":393,
        "Owner_creation_date":"2019-05-22 09:45:28.153 UTC",
        "Owner_last_access_date":"2022-09-02 09:43:13.34 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Best Practices for Distributed Training with PyTorch custom containers (BYOC) in SageMaker",
        "Question_body":"<p>What are the best practices for distributed training with PyTorch custom containers (BYOC) in Amazon Sagemaker? I understand that PyTorch framework supports native distributed training or using the Horovod library for PyTorch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 03:24:29.01 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"deep-learning|pytorch|containers|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"What does 100% utilisation mean in SageMaker Studio?",
        "Question_body":"<p>(This is related to <a href=\"https:\/\/stackoverflow.com\/questions\/68569742\/sage-maker-studio-cpu-usage\">Sage Maker Studio CPU Usage<\/a> but focuses on interpreting meaning rather than modifying behaviour)<\/p>\n<p>SageMaker Studio shows Kernel and Instance usage for CPU and Memory:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" alt=\"Screenshot of Kernel and Instance usage\" \/><\/a><\/p>\n<p>The kernel is just the selected Jupyter kernel and so would appear as a single process on a local machine, while the instance is the EC2 instance that they're running on.<\/p>\n<p>The only documentation from Amazon appears to be in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-menu.html\" rel=\"nofollow noreferrer\">Use the SageMaker Studio Notebook Toolbar<\/a> which says that it &quot;Displays the CPU usage and memory usage. Double-click to toggle between the current kernel and the current instance&quot; (this is outdated and relates to the old position of the information).<\/p>\n<p>In the context of SageMaker Studio, does 100% CPU mean 100% of one CPU or 100% of all CPUs? (<code>top<\/code> shows multi-core as &gt;100% but consolidated measures like Windows Task Manager's default representation show all cores as 100%)<\/p>\n<p>And does 25% instance utilisation then mean that my instance is over-specced? (Intuitively, it should do because I'm not using 100% even when training a model, but I've tried smaller instances and still never maxes Instance CPU usage, only Kernel CPU usage)<\/p>\n<p>I've tried using <code>joblib<\/code> to make some parallel &quot;wheel spinning&quot; tasks to check usage, but that just resulted in Kernel being quiet and Instance having all of the usage!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2021-08-16 19:56:33.27 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":270,
        "Owner_creation_date":"2010-02-28 20:24:02.537 UTC",
        "Owner_last_access_date":"2022-09-22 20:01:39.047 UTC",
        "Owner_location":"United Kingdom",
        "Owner_reputation":869,
        "Owner_up_votes":89,
        "Owner_down_votes":5,
        "Owner_views":129,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How do you make one AWS sagemaker pipeline trigger another one?",
        "Question_body":"<p>I know you can trigger sagemaker pipelines with all kind of <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/04\/new-options-trigger-amazon-sagemaker-pipeline-executions\/\" rel=\"nofollow noreferrer\">events<\/a>. Can you also trigger sagemaker pipelines when another pipeline finishes it's execution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-15 10:01:44.637 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":602,
        "Owner_creation_date":"2017-01-19 15:07:44.573 UTC",
        "Owner_last_access_date":"2022-09-22 14:55:11.743 UTC",
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"The current AWS identity is not a role: arn:aws:iam::XXXXXXXXXXX:root, therefore it cannot be used as a SageMaker execution role",
        "Question_body":"<p>I am getting the error, <code>The current AWS identity is not a role<\/code> while I am training my models using sagemaker from my local pc, I tried setting up <code>aws configure<\/code> by following the steps on the AWS docs: <a href=\"https:\/\/docs.aws.amazon.com\/zh_tw\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/zh_tw\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/p>\n<p>I have inputted everything and the error still exists, any help?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nhvwd.png\" rel=\"nofollow noreferrer\">The error<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2020-11-18 08:36:17.64 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|python-2.7|amazon-sagemaker",
        "Question_view_count":490,
        "Owner_creation_date":"2020-11-18 08:27:50.2 UTC",
        "Owner_last_access_date":"2021-06-26 10:48:49.523 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-07-20 22:02:42.373 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|amazon-ecr",
        "Question_view_count":61,
        "Owner_creation_date":"2022-05-25 20:48:45.307 UTC",
        "Owner_last_access_date":"2022-09-15 14:23:04.53 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-22 16:50:49.573 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2022-07-20 22:08:12.733 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Defining Metrics on SageMaker to CloudWatch",
        "Question_body":"<p>From AWS Sagemaker Documentation, In order to track metrics in cloudwatch for custom ml algorithms (non-builtin), I read that I have to define my estimaotr as below.<\/p>\n<p>But I am not sure how to alter my training script so that the metric definitions declared inside my estimators can pick up these values.<\/p>\n<pre><code>estimator =\n                Estimator(image_name=ImageName,\n                role='SageMakerRole', \n                instance_count=1,\n                instance_type='ml.c4.xlarge',\n                k=10,\n                sagemaker_session=sagemaker_session,\n                metric_definitions=[\n                   {'Name': 'train:error', 'Regex': 'Train_error=(.*?);'},\n                   {'Name': 'validation:error', 'Regex': 'Valid_error=(.*?);'}\n                ]\n            )\n<\/code><\/pre>\n<p>In my training code, I have<\/p>\n<pre><code>    for epoch in range(1, args.epochs + 1):\n        total_loss = 0\n        model.train()\n        for step, batch in enumerate(train_loader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()\n\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs[0]\n\n            total_loss += loss.item()\n            loss.backward() # Computes the gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip for error prevention\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step() # Back Prop\nlogger.info(&quot;Average training loss: %f\\n&quot;, total_loss \/ len(train_loader))\n<\/code><\/pre>\n<p>Here, I want the train:error to pick up <code>total_loss \/ len(train_loader)<\/code> but I am not sure how to assign this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-21 13:43:45.787 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":1420,
        "Owner_creation_date":"2020-09-21 20:00:48.277 UTC",
        "Owner_last_access_date":"2022-09-16 06:59:29.497 UTC",
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can I check GPU availability prior to starting the training job?",
        "Question_body":"<p>I want to start a TrainingJob using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">AWS Sagemaker SDK<\/a>. As per documentation of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">Estimator Object<\/a> There is a function <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.Estimator.fit\" rel=\"nofollow noreferrer\">fit<\/a> which will start the training job.<\/p>\n<p>Each user has a limit of the number of GPUs that they can simultaneously run. If I write<\/p>\n<pre><code>estimator.fit()\n<\/code><\/pre>\n<p>It might return a <code>botocore<\/code> exception saying that &quot;Resource Limit is Exceeded&quot;.<\/p>\n<p>I was wondering if there's a way to figure out if the resource limit will be exceeded without actually running <code>estimator.fit()<\/code> and trying to catch the exception.<\/p>\n<p>Basically run the code in the following way:<\/p>\n<pre><code>if not sagemaker.resources_will_be_exceeded(job_description):\n    estimator.fit()\n\n<\/code><\/pre>\n<p>Looking at the documentation I could not find such a function, however maybe I am missing something.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-24 08:00:39.177 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":47,
        "Owner_creation_date":"2015-07-17 19:10:35.083 UTC",
        "Owner_last_access_date":"2022-08-14 18:21:57.947 UTC",
        "Owner_location":"Armenia",
        "Owner_reputation":461,
        "Owner_up_votes":35,
        "Owner_down_votes":6,
        "Owner_views":153,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Receiving parse error from SageMaker Multi Model Endpoint using TensorFlow",
        "Question_body":"<p>We are currently moving our models from single model endpoints to multi model endpoints within AWS SageMaker. After deploying the Multi Model Endpoint using prebuilt TensorFlow containers I receive the following error when calling the predict() method:<\/p>\n<p><code>{&quot;error&quot;: &quot;JSON Parse error: The document root must not be followed by other value at offset: 17&quot;}<\/code><\/p>\n<p>I invoke the endpoint like this:<\/p>\n<pre><code>data = np.random.rand(n_samples, n_features)\n\npredictor = Predictor(endpoint_name=endpoint_name)\nprediction = predictor.predict(data=serializer.serialize(data), target_model=model_name)\n<\/code><\/pre>\n<p>My function for processing the input is the following:<\/p>\n<pre><code>def _process_input(data, context):\n    data = data.read().decode('utf-8')\n    data = [float(x) for x in data.split(',')]\n    return json.dumps({'instances': [data]})\n<\/code><\/pre>\n<p>For the training I configured my container as follows:<\/p>\n<pre><code>tensorflow_container = TensorFlow(\n    entry_point=path_script,\n    framework_version='2.4',\n    py_version='py37',\n    instance_type='ml.m4.2xlarge',\n    instance_count=1,\n    role=EXECUTION_ROLE,\n    sagemaker_session=sagemaker_session,\n    hyperparameters=hyperparameters)\n\ntensorflow_container.fit()\n<\/code><\/pre>\n<p>For deploying the endpoint I first initializing a Model from a given Estimator and then a MultiDataModel:<\/p>\n<pre><code>model = estimator.create_model(\n            role=EXECUTION_ROLE, \n            image_uri=estimator.training_image_uri(),\n            entry_point=path_serving)\n\nmdm = MultiDataModel(\n        name=endpoint_name, \n        model_data_prefix=dir_model_data,\n        model=model, \n        sagemaker_session=sagemaker.Session())\n\nmdm.deploy(\n        initial_instance_count=1, \n        instance_type=instance_type,\n        endpoint_name=endpoint_name)\n\n\n<\/code><\/pre>\n<p>Afterwards the single models are added using:<\/p>\n<pre><code>mdm.add_model(\n    model_data_source=source_path,\n    model_data_path=model_name)\n<\/code><\/pre>\n<p>Thank you for any hints and help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-04 12:51:18.24 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":112,
        "Owner_creation_date":"2020-11-01 18:51:29.893 UTC",
        "Owner_last_access_date":"2022-03-26 07:56:45.483 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"trying to use monotonicty constraints in XBGoost",
        "Question_body":"<p>I am using Sagemaker's XGBoost as a built-in algorithm and code along those lines (assuming for simplicity that I have <strong>3<\/strong> independent variables\/feature\/predictors):<\/p>\n<pre><code>hyperparameters = {\n        'max_depth': '10',\n        'num_round': '100',\n        'objective': 'count:poisson',\n        'tree_method': 'exact', \n        'monotone_constraints': '(0,-1,0)'\n}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.5-1&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>Unfortunately, I get:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\Repos\\ds_cs_ipt2\\modeling\\fit_occupancy_model_in_sagemaker.py&quot;, line 67, in &lt;module&gt;\n    estimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n  File &quot;C:\\Python\\Python310\\lib\\site-packages\\sagemaker\\estimator.py&quot;, line 956, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;C:\\Python\\Python310\\lib\\site-packages\\sagemaker\\estimator.py&quot;, line 1957, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;C:\\Python\\Python310\\lib\\site-packages\\sagemaker\\session.py&quot;, line 3798, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;C:\\Python\\Python310\\lib\\site-packages\\sagemaker\\session.py&quot;, line 3336, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job sagemaker-xgboost-2022-05-25-12-32-37-624: Failed. Reason: AlgorithmError: framework error:\nTraceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 233, in train_job\n    feval=configured_feval, callbacks=callbacks, xgb_model=xgb_model, verbose_eval=False)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/xgboost\/training.py&quot;, line 196, in train\n    early_stopping_rounds=early_stopping_rounds)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/xgboost\/training.py&quot;, line 51, in _train_internal\n    bst = Booster(params, [dtrain] + [d[0] for d in evals])\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/xgboost\/core.py&quot;, line 1334, in __init__\n    params = self._configure_constraints(params)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/xgboost\/core.py&quot;, line 1400, in _configure_constraints\n    ] = self._transform_monotone_constrains(value)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/xgboost\/core.py&quot;, line 1361, in _transform_monotone_constrains\n    constrained_features = set(value.keys\n<\/code><\/pre>\n<p>which must be due to the XGBoost source code <a href=\"https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/python-package\/xgboost\/core.py\" rel=\"nofollow noreferrer\">here<\/a> and this bit:<\/p>\n<pre><code>def _transform_monotone_constrains(\n        self, value: Union[Dict[str, int], str, Tuple[int, ...]]\n    ) -&gt; Union[Tuple[int, ...], str]:\n        if isinstance(value, str):\n            return value\n        if isinstance(value, tuple):\n            return value\n\n        constrained_features = set(value.keys())\n        feature_names = self.feature_names or []\n        if not constrained_features.issubset(set(feature_names)):\n            raise ValueError(\n                &quot;Constrained features are not a subset of training data feature names&quot;\n            )\n\n        return tuple(value.get(name, 0) for name in feature_names)\n<\/code><\/pre>\n<p>Alas I do not fully understand what the issue may be. Here are some more details from cloud watch:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Enwp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Enwp.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 13:01:03.617 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":57,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Random cut forest anomaly detection on multi variant time series data",
        "Question_body":"<p>I have sensor data coming from equipment with times series along with many attributes,<\/p>\n\n<p>I have used RCF algorithm to detect anomalies.\nNow the challenge is,how to to convince the end user whether it is really anomaly or not.\nJust want to know which attribute is contributing to anomaly.<\/p>\n\n<p>Is there any best way to convince end user whether it is really anomaly or not.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-26 12:55:45.33 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|anomaly-detection",
        "Question_view_count":984,
        "Owner_creation_date":"2019-04-10 00:37:07.173 UTC",
        "Owner_last_access_date":"2021-02-05 10:45:12.313 UTC",
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":"<p>The simplest way to run the RCF model and to get the explanation for the anomaly is to use the version of RCF in Kinesis Analytics (KA). Here is a link to the documentation of how to run from the KA documentations: <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html<\/a><\/p>\n\n<p>Kinesis is taking care both for the training of the model, the inference after the initial training and for the attribution and explanation of the variables. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p25FR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p25FR.png\" alt=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/images\/anomaly_results.png\"><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-04-27 11:19:46.93 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2019-04-27 11:21:02.917 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Not able to deploy saved model into AWS Sagemaker",
        "Question_body":"<p>I am following this article <a href=\"https:\/\/machinelearningmastery.com\/how-to-perform-object-detection-with-yolov3-in-keras\/\" rel=\"nofollow noreferrer\">https:\/\/machinelearningmastery.com\/how-to-perform-object-detection-with-yolov3-in-keras\/<\/a> to deploy YOLOv3 inside the AWS Sagemaker. I have the model.weights having weights and model.json having model structure and also model.h5 having model structure + weights. When I convert these files to protobuf format so that I can tar them and deploy on Sagemaker, this error comes in. <\/p>\n\n<pre><code>UnexpectedStatusException: Error hosting endpoint sagemaker-tensorflow-2020-04-12-10-57-05- \n567: Failed. Reason:  The primary container for production variant AllTraffic did not pass the \nping health check. Please check CloudWatch logs for this endpoint..\n<\/code><\/pre>\n\n<p>Here is my code:<\/p>\n\n<hr>\n\n<pre><code>import tensorflow\ntensorflow.__version__\n\nOutput:\n'1.7.0'\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>import boto3, re\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nfrom tensorflow.keras.models import model_from_json\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>!ls keras_model\/\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>import tensorflow as tf\n\njson_file = open('\/home\/ec2-user\/SageMaker\/keras_model\/' + 'model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json, custom_objects={\"GlorotUniform\": tf.keras.initializers.glorot_uniform})\n\nloaded_model.load_weights('\/home\/ec2-user\/SageMaker\/keras_model\/model_weights.h5')\nprint(\"Loaded model from disk \")\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>from tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\n\n# this directory sturcture will be followed as below. Do not change it.\nmodel_version = '1' \nexport_dir = 'export\/Servo\/' + model_version\n\n#Build the protocol buffer savedmodel at export_dir\nbuild = builder.SavedModelBuilder(export_dir)\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>print(loaded_model.inputs)  \nprint([t for t in loaded_model.outputs])\n\nOutput:\n[&lt;tf.Tensor 'input_1:0' shape=(?, ?, ?, 3) dtype=float32&gt;]\n[&lt;tf.Tensor 'conv_81\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;, &lt;tf.Tensor 'conv_93\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;, &lt;tf.Tensor 'conv_105\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;]\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>tf.convert_to_tensor(loaded_model.output)\n\nOutput:\n&lt;tf.Tensor 'packed:0' shape=(3, ?, ?, ?, 255) dtype=float32&gt;\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>signature = predict_signature_def(inputs={\"input_image\": loaded_model.input}, \n                                  outputs={t.name: t for t in loaded_model.outputs})\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>from tensorflow.keras import backend as K\nwith K.get_session() as sess:\n    build.add_meta_graph_and_variables(sess=sess, tags=[tag_constants.SERVING], \n                                       signature_def_map={\"serving_default\":signature} )\n    build.save()\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>!ls export\/Servo\/1\/variables\/\n\nOutput:\nvariables.data-00000-of-00001  variables.index\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>import tarfile \nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>import sagemaker\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n<\/code><\/pre>\n\n<hr>\n\n<pre><code>!touch train.py\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data='s3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                 role = role,\n                                 entry_point= 'train.py')\n\n%%time\npredictor = sagemaker_model.deploy(initial_instance_count=1, \n                                   instance_type='ml.t2.large')\n<\/code><\/pre>\n\n<hr>\n\n<p>Error:<\/p>\n\n<pre><code>-----------------------------*\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;timed exec&gt; in &lt;module&gt;()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\n    478                 kms_key=kms_key,\n    479                 wait=wait,\n--&gt; 480                 data_capture_config_dict=data_capture_config_dict,\n    481             )\n    482 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\n   2849 \n   2850             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 2851         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   2852 \n   2853     def expand_role(self, role):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n   2381         )\n   2382         if wait:\n-&gt; 2383             self.wait_for_endpoint(endpoint_name)\n   2384         return endpoint_name\n   2385 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   2638                 ),\n   2639                 allowed_statuses=[\"InService\"],\n-&gt; 2640                 actual_status=status,\n   2641             )\n   2642         return desc\n\nUnexpectedStatusException: Error hosting endpoint sagemaker-tensorflow-2020-04-12-10-57-05-567: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..\n<\/code><\/pre>\n\n<hr>\n\n<p>I think that error is due to the difference between the Tensor shape of <code>loaded_model.inputs<\/code> and <code>loaded_model.outputs<\/code>. But I am still not sure about what do the 3 and 255 represent in the shape. Any help will be greatly appreciated. <\/p>\n\n<pre><code>print(loaded_model.inputs)\n[&lt;tf.Tensor 'input_1:0' shape=(?, ?, ?, 3) dtype=float32&gt;]\n\nprint([t for t in loaded_model.outputs])\n[&lt;tf.Tensor 'conv_81\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;, \n &lt;tf.Tensor 'conv_93\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;, \n &lt;tf.Tensor 'conv_105\/BiasAdd:0' shape=(?, ?, ?, 255) dtype=float32&gt;]\n<\/code><\/pre>\n\n<p>Cloudwatch log:<\/p>\n\n<pre><code>2020-04-12 11:01:33,439 INFO - root - running container entrypoint\n2020-04-12 11:01:33,440 INFO - root - starting serve task\n2020-04-12 11:01:33,440 INFO - container_support.serving - reading config\nDownloading s3:\/\/sagemaker-us-east-1-611475884433\/sagemaker-tensorflow-2020-04-12-10-57-05-375\/sourcedir.tar.gz to \/tmp\/script.tar.gz\n2020-04-12 11:01:33,828 INFO - container_support.serving - importing user module\n2020-04-12 11:01:33,828 INFO - container_support.serving - loading framework-specific dependencies\n2020-04-12 11:01:35,795 INFO - container_support.serving - starting nginx\n2020-04-12 11:01:35,797 INFO - container_support.serving - nginx config: \nworker_processes auto;\ndaemon off;\npid \/tmp\/nginx.pid;\nerror_log \/var\/log\/nginx\/error.log;\nworker_rlimit_nofile 4096;\nevents {\n  worker_connections 2048;\n}\nhttp {\n  include \/etc\/nginx\/mime.types;\n  default_type application\/octet-stream;\n  access_log \/var\/log\/nginx\/access.log combined;\n\n  upstream gunicorn {\n    server unix:\/tmp\/gunicorn.sock;\n  }\n\n  server {\n    listen 8080 deferred;\n    client_max_body_size 0;\n\n    keepalive_timeout 3;\n\n    location ~ ^\/(ping|invocations) {\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n      proxy_pass http:\/\/gunicorn;\n    }\n\n    location \/ {\n      return 404 \"{}\";\n    }\n  }\n}\n\n2020-04-12 11:01:35,815 INFO - container_support.serving - starting gunicorn\n2020-04-12 11:01:35,820 INFO - container_support.serving - inference server started. waiting on processes: set([24, 23])\n2020-04-12 11:01:35.904746: I tensorflow_serving\/model_servers\/server.cc:82] Building single TensorFlow model file config:  model_name: generic_model model_base_path: \/opt\/ml\/model\/export\/Servo\n2020-04-12 11:01:35.905995: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\n2020-04-12 11:01:35.906148: I tensorflow_serving\/model_servers\/server_core.cc:517]  (Re-)adding model: generic_model\n2020-04-12 11:01:35.907173: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: generic_model version: 1}\n2020-04-12 11:01:35.907349: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: generic_model version: 1}\n2020-04-12 11:01:35.907422: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: generic_model version: 1}\n2020-04-12 11:01:35.907578: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1\n2020-04-12 11:01:35.907687: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1\n2020-04-12 11:01:35.939232: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n2020-04-12 11:01:35.980215: I external\/org_tensorflow\/tensorflow\/core\/platform\/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[2020-04-12 11:01:36 +0000] [24] [INFO] Starting gunicorn 19.9.0\n2020-04-12 11:01:36.048327: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 140502 microseconds.\n2020-04-12 11:01:36.048617: E tensorflow_serving\/util\/retrier.cc:37] Loading servable: {name: generic_model version: 1} failed: Not found: Op type not registered 'FusedBatchNormV3' in binary running on model.aws.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2020-04-18 06:49:16.957 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":1410,
        "Owner_creation_date":"2018-02-21 04:45:56.827 UTC",
        "Owner_last_access_date":"2022-02-28 20:03:46.743 UTC",
        "Owner_location":null,
        "Owner_reputation":343,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-20 15:31:48.193 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Run SKLearnProcessor from AWS Lambda",
        "Question_body":"<p>I have below code written in AWS Sagemaker Jupyter Notebook. But I would like to <strong>run it from AWS Lambda (or AWS Step Functions)<\/strong> to enable automated execution.<\/p>\n<p>There is a <em>CreateProcessingJob<\/em> in Step Functions and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html?highlight=frameworkprocessor#sagemaker.processing.ProcessingJob\" rel=\"nofollow noreferrer\"><em>sagemaker.processing.ProcessingJob<\/em><\/a>  in <code>sagemaker<\/code> API as well as <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_processing_job\" rel=\"nofollow noreferrer\"><code>create_processing_job<\/code><\/a> in <code>boto3<\/code> - that's the closest I could find...<\/p>\n<p>Is it possible to achieve? What would be the point of creating all of these Sagemaker functionalities if they have to be executed manually from the notebook..?<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import FrameworkProcessor\nfrom sagemaker.processing import ProcessingOutput\n\nregion = boto3.session.Session().region_name    \nrole = get_execution_role()\n\nest_cls = sagemaker.sklearn.estimator.SKLearn\nframework_version_str=&quot;0.23-1&quot;\n\nscript_processor = FrameworkProcessor(\n    role=role,\n    instance_count=8,\n    instance_type=&quot;ml.r5.8xlarge&quot;,\n    volume_size_in_gb=120,\n    max_runtime_in_seconds=432000,\n    estimator_cls=est_cls,\n    framework_version=framework_version_str\n)\n\noutput_folder = 's3:\/\/bucket\/out'\n\nscript_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    source_dir = &quot;code&quot;,\n    outputs=[\n        ProcessingOutput(output_name='preprocessed_data', source=&quot;\/opt\/ml\/processing\/train&quot;),\n    ],\n    arguments=[&quot;--bucket&quot;, &quot;bucket&quot;, &quot;--subfolder&quot;, &quot;Training_data\/&quot;],\n)\n\nscript_processor_job_description = script_processor.jobs[-1].describe()\nprint(script_processor_job_description)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-06 16:19:46.583 UTC",
        "Question_favorite_count":2.0,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|aws-lambda|boto3|amazon-sagemaker",
        "Question_view_count":274,
        "Owner_creation_date":"2012-02-02 13:30:35.16 UTC",
        "Owner_last_access_date":"2022-09-23 16:00:40.663 UTC",
        "Owner_location":"Katowice, Poland",
        "Owner_reputation":10534,
        "Owner_up_votes":888,
        "Owner_down_votes":311,
        "Owner_views":984,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Deploying custom preprocessing and postprocessing scripts in SageMaker",
        "Question_body":"<p>I am trying to convert some python scripts into a callable endpoint in SageMaker. My preprocessing(feature engineering) and postprocessing scripts are written in python and have a few interdependent scripts and methods in them. The preprocessing steps are also not necessarily from SKLearn, they are customized functions and need to be called from the preprocessing endpoint every time on the raw data that will then be used for prediction using a model saved as a second endpoint. The third endpoint will be for the postprocessing steps and connecting these 3 endpoints we want to get our data from the raw format to the output format every time.<\/p>\n<p>We currently have normal python scripts that preprocesses the data using some highly customized functions( all features are ultimately derived features) and then performs some inference and then again using some highly customized postprocessing generates the final results. While the input is a CSV file, after each stage of preprocessing and post processing the dimensions of the data and also the format of the output(dataframe, list, list of lists) are likely to change.<\/p>\n<p>For reference, we are using, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/scikit_learn_estimator_example_with_batch_transform.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/scikit_learn_estimator_example_with_batch_transform.ipynb<\/a> and <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a>.<\/p>\n<p>Please let me know if there is any better reference that caters to our specific requirements.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-23 16:21:53.75 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":543,
        "Owner_creation_date":"2021-07-13 16:28:54.377 UTC",
        "Owner_last_access_date":"2022-03-21 06:24:34.5 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 12:56:47.89 UTC",
        "Question_favorite_count":2.0,
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_date":"2021-02-18 15:25:28.947 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:08.913 UTC",
        "Owner_location":null,
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-22 21:59:21.52 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-02-15 13:25:22.733 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-14 11:41:25.31 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_date":"2019-12-10 22:23:16.283 UTC",
        "Owner_last_access_date":"2022-08-11 09:37:11.827 UTC",
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-03-18 12:38:20.133 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2022-03-14 14:41:37.627 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Use SageMaker Lifecycle configuration to execute a jupyter notebook on start",
        "Question_body":"<p>I want to set up some automatic schedule for running my SageMaker Notebook.<br \/>\nCurrently I found link like this:<br \/>\n<a href=\"https:\/\/towardsdatascience.com\/automating-aws-sagemaker-notebooks-2dec62bc2c84\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/automating-aws-sagemaker-notebooks-2dec62bc2c84<\/a><\/p>\n<p>I followed the steps to set up the lamda, cloudwatch, and the Lifecycle configuration.<br \/>\nDuring different experiment, some times the on_start lifecycle configuration can execute the jupyter notebook (In the notebook i just install some package and load the package and save the loading status to S3 bucket). However, it failed due to it can't stop the notebook.<\/p>\n<p>Then I added permission to my IAM role for SageMaker autostop. Now the notebook instance can be turn on and turn off. But I don't see anything uploaded to my S3 any more. I am wondering if the on_start started the auto-stop too early before it finish the steps?<\/p>\n<p>Below is my script for the current lifecycle configuration<\/p>\n<pre><code>set -e\n\nENVIRONMENT=python3\nNOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/Test Notebook.ipynb&quot;\nAUTO_STOP_FILE=&quot;\/home\/ec2-user\/SageMaker\/auto-stop.py&quot;\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\n\nnohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &amp;\n\necho &quot;Finishing running the jupyter notebook&quot;\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n# PARAMETERS\nIDLE_TIME=60  # 1 minute\n\necho &quot;Fetching the autostop script&quot;\nwget -O autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/auto-stop-idle\/autostop.py\n\necho &quot;Starting the SageMaker autostop script in cron&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/1 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $DIR\/autostop.py --time ${IDLE_TIME} | tee -a \/home\/ec2-user\/SageMaker\/auto-stop-idle.log'&quot;) | crontab -\n<\/code><\/pre>\n<p>Note that, I do see the echo &quot;Finishing running the jupyter notebook&quot; from the cloudwatch log. But that's usually the first thing i saw from the log and it shows up immediately - faster than I expect how long it should take.<\/p>\n<p>Also, currently the notebook is only running some fake task. The real task may take more than an hour.<\/p>\n<p>Any suggestions help! Thank you for taking the time to read my questions.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-10-20 17:02:30.937 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|lifecycle|amazon-sagemaker",
        "Question_view_count":3101,
        "Owner_creation_date":"2020-09-22 03:10:31.77 UTC",
        "Owner_last_access_date":"2021-09-28 00:21:34.097 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"to source the data from Athena or Redshift to Sage maker or AWS Forecast instead of the flat file",
        "Question_body":"<p>I am trying to source the data from Athena or Redshift to Sage maker or AWS Forecast directly without using the flat data. In Sage maker I use Jupyter Notebook python code. Is there anyway to do so without even connecting to S3.  <\/p>\n\n<p>So far I have been using flat data which is not what I wanted.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-09 20:16:50.61 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-redshift|amazon-athena|amazon-sagemaker",
        "Question_view_count":2689,
        "Owner_creation_date":"2019-07-09 20:00:26.99 UTC",
        "Owner_last_access_date":"2019-09-04 18:58:21.97 UTC",
        "Owner_location":"Carlsbad, CA, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"TensorFlow Checkpoints to S3",
        "Question_body":"<p>I am executing a Python-Tensorflow script on Amazon Sagemaker.  I need to checkpoint my model to the S3 instance I am using, but I can't find out how to do this without using the Sagemake Tensorflow version.<\/p>\n\n<p>How does one checkpoint to an S3 instance without using the Sagemaker TF version?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-01-30 16:03:57.857 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|amazon-sagemaker",
        "Question_view_count":925,
        "Owner_creation_date":"2011-11-21 16:37:22.023 UTC",
        "Owner_last_access_date":"2022-05-11 09:53:59.693 UTC",
        "Owner_location":null,
        "Owner_reputation":1611,
        "Owner_up_votes":51,
        "Owner_down_votes":6,
        "Owner_views":189,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"MissingRequiredParameter: Missing required key 'FunctionName' in params",
        "Question_body":"<p>I'm working on a supervised machine learning problem, and I am setting up a custom labeling task to send out to Amazon Mechanical Turk for human annotation.<\/p>\n\n<p>I have uploaded the data to AWS S3 in the json-lines (<code>.jsonl<\/code>) format as follows, pursuant to the instructions as specified in the AWS documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html<\/a>: <\/p>\n\n<pre><code>{\"source\": \"value0\"}\n{\"source\": \"value1\"}    \n{\"source\": \"value2\"}\n...\n{\"source\": \"value2\"}\n<\/code><\/pre>\n\n<p>When I click on the default text classification template, I can see my data come through and everything appears to work.<\/p>\n\n<p>However, I am getting the following error when I attempt to use the custom annotation task template interface: <code>MissingRequiredParameter: Missing required key 'FunctionName' in params<\/code> <\/p>\n\n<p>The error resembles an AWS Lambda error, except the strange thing is that I am not using AWS Lambda. Suggestions for how to proceed? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-04 22:17:36.45 UTC",
        "Question_favorite_count":0.0,
        "Question_score":3,
        "Question_tags":"amazon-web-services|mechanicalturk|amazon-sagemaker",
        "Question_view_count":7776,
        "Owner_creation_date":"2012-05-07 22:27:38.227 UTC",
        "Owner_last_access_date":"2021-05-01 20:42:42.86 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":947,
        "Owner_up_votes":70,
        "Owner_down_votes":11,
        "Owner_views":61,
        "Answer_body":"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html<\/a><\/p>\n\n<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-03-11 22:11:47.057 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon SageMaker Downloading Files from S3",
        "Question_body":"<p>I'm storing midi files in an S3 bucket and am trying to download them into the SageMake jupyter notebook.  I am using this code<\/p>\n\n<pre><code>import os\nimport boto3  # Python library for Amazon API \nimport botocore\nfrom botocore.exceptions import ClientError\ndef download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<p>however I am getting An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n\n<p>Here are the permissions attached for the S3:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-21 15:11:27.46 UTC",
        "Question_favorite_count":2.0,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":8331,
        "Owner_creation_date":"2017-06-06 19:07:22.41 UTC",
        "Owner_last_access_date":"2022-09-23 19:51:52.233 UTC",
        "Owner_location":"Boca Raton, FL, United States",
        "Owner_reputation":305,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2018-07-05 13:22:58.847 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Value at 'body' failed to satisfy constraint: Member must not be null",
        "Question_body":"<p>I'm trying to run a prediction using a sagemaker endpoint. The input format is comma separated features and | separated observations.\nHowever when I try to iterate over the input data and invoke the end point on every iteration like this :<\/p>\n<pre><code>ENDPOINT_NAME = &quot;my_endpoint&quot;\nruntime= boto3.client('runtime.sagemaker')\nresults = []\nfor r in request_body.split('|'):\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                        ContentType='text\/csv',\n                                        Body=r)\nresult = json.loads(response['Body'].read().decode())\nresults.append(result)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>ValidationError: an error occurred (ValidationError) when calling the InvokeEndpoint operation: 1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/p>\n<\/blockquote>\n<p>As a sanity check I ran :<\/p>\n<pre><code>for r in request_body.split('|'):\n    print(r)\n<\/code><\/pre>\n<p>And I get the result I expect to get:<\/p>\n<pre><code>3.0,0.0,4795.0,0.0,1.0,24.0,30.0,25.0,3.0\n3.0,2.0,3818.0,0.0,3.0,10.0,22.0,11.0,11.0\n5.0,0.0,3565.0,0.0,1.0,79.0,89.0,80.0,-66.0\n5.0,-1.0,3227.7,0.0,0.0,16.0,17.0,17.0,1.0\n5.0,0.0,3375.0,0.0,2.0,21.0,45.0,22.0,6.0...etc\n<\/code><\/pre>\n<p>Which leads me to believe that the logic in extracting the separate observations is sound, but somehow when I execute the call I get this null value error.<\/p>\n<p>The idea is to get ordered predictions so that I can later map them to an id that is not part of the training features and hence not in the dataset.<\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-28 13:23:25.243 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1109,
        "Owner_creation_date":"2016-06-07 17:04:17.33 UTC",
        "Owner_last_access_date":"2022-09-22 11:38:41.687 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-31 10:16:36.667 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to schedule task in AWS Sagemaker to run training job",
        "Question_body":"<p>I have an optimization code running in sagemaker. I want to run the code in every 1 hr. How can I schedule the run in sagemaker?. I do not want to call model endpoint, but I want to run the whole code in every 1 hr.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-09 06:30:21.367 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1511,
        "Owner_creation_date":"2019-04-09 06:25:51.317 UTC",
        "Owner_last_access_date":"2021-11-02 05:06:34.58 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Problem connecting Livy to EMR via sagemaker",
        "Question_body":"<p>I have followed this tutorial: <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/<\/a> in order to be able to run pyspark code on EMR via apache-livy. I have only made some little change so that EMR configuration script run as a sagemaker lifecycle configuration script.<\/p>\n\n<p>When testing the connection with <code>curl &lt;EMR Master Private IP&gt;:8998\/sessions<\/code> the result seems completely fine: <code>{\"from\":0,\"total\":0,\"sessions\":[]}<\/code>. But, when I try to run an application the state go from <strong>starting<\/strong> directly to <strong>dead<\/strong> with the following message:<\/p>\n\n<p><code>{'id': 0, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'dead', 'kind': 'spark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['19\/02\/27 09:23:24 INFO Client: Requesting a new application from cluster with 2 NodeManagers', '19\/02\/27 09:23:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1024 MB per container)', '19\/02\/27 09:23:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead', '19\/02\/27 09:23:25 INFO Client: Setting up container launch context for our AM', '19\/02\/27 09:23:25INFO Client: Setting up the launch environment for our AM container', '19\/02\/27 09:23:25 INFO Client: Preparing resources for our AM container', '19\/02\/27 09:23:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.', '\\nYARN Diagnostics: ', 'java.lang.Exception: No YARN application is found with tag livy-session-0-v9wkutit in 120 seconds. Please check your cluster status, it is may be very busy.', 'org.apache.livy.utils.SparkYarnApp.org$apache$livy$utils$SparkYarnApp$$getAppIdFromTag(SparkYarnApp.scala:182) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:239) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:236) scala.Option.getOrElse(Option.scala:121) org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:236) org.apache.livy.Utils$$anon$1.run(Utils.scala:94)']}<\/code><\/p>\n\n<p>I have tried to investigate, but really have no clue of what's going on here, is there by any chance someone here which have an idea on how to debug that.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-27 10:56:32.44 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"amazon-emr|livy|amazon-sagemaker",
        "Question_view_count":904,
        "Owner_creation_date":"2014-08-21 08:30:20.407 UTC",
        "Owner_last_access_date":"2022-08-02 08:31:21.447 UTC",
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":36,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS lambda to run a sagemaker notebook but theres an error with imports via terminal",
        "Question_body":"<p>I have created a lambda function that runs a notebook on a sagemaker instance. The lambda accesses and runs the notebook but it fails to import packages. The lambda function runs the notebook via terminal so my issue lies there.\nAs you can see in the image below, datetime and urllib dont throw an error but beautiful soup does. Beautiful soup is installed via lifecylle configuration everytime the instance is started (also via lambda function).<\/p>\n<p>What am i doing wrong please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oK7dG.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oK7dG.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-01-13 17:29:56.043 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":213,
        "Owner_creation_date":"2019-05-22 09:45:28.153 UTC",
        "Owner_last_access_date":"2022-09-02 09:43:13.34 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Question_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 20:10:29.643 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-iot",
        "Question_view_count":48,
        "Owner_creation_date":"2020-03-16 17:16:50.35 UTC",
        "Owner_last_access_date":"2022-04-26 10:55:17.747 UTC",
        "Owner_location":null,
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-06 14:38:47.65 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Notebook instance error AttributeError: 'MaterializedLayer' object has no attribute 'pack_annotations'",
        "Question_body":"<p>I have a dask cluster active<\/p>\n<pre><code>from dask.distributed import Client, progress \n\nclient = Client()\nclient\n<\/code><\/pre>\n<p>When I try to encode my data I get the error:<\/p>\n<pre><code>AttributeError: 'MaterializedLayer' object has no attribute 'pack_annotations'\n<\/code><\/pre>\n<p>I encoded the data with the cluster closed and then tried to fit the model and I get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-06-15 16:03:48.523 UTC",
        "Question_favorite_count":0.0,
        "Question_score":1,
        "Question_tags":"dask|amazon-sagemaker|dask-distributed|dask-ml|distributed-training",
        "Question_view_count":63,
        "Owner_creation_date":"2020-04-17 01:31:21.62 UTC",
        "Owner_last_access_date":"2022-09-24 12:53:17.577 UTC",
        "Owner_location":"Caracas, Venezuela",
        "Owner_reputation":113,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Keras Custom Data Generator - Stuck on First Epoch, No Output?",
        "Question_body":"<p>I've been trying to get a multi-input data generator to work in Keras for a muti-input model. The inputs are in the form of an image and an associated number.<\/p>\n<p>I've tried two different custom data generators, but the simpler one merely uses ImageDataGenerator and flowfromdataframe with two outputs. Later on, I switch one of the outputs into an input and feed it into the model. The relevant code is as follows, where y_col is the output, number_col is the associated number and path_col is the path to the images:<\/p>\n<pre><code># data generator\ndf_gen = img_data_gen.flow_from_dataframe(\n        **all_args,\n        x_col=path_col,\n        y_col=[y_col, number_col],\n        shuffle=False,\n        class_mode='raw')\n\n  \n\n\n# sending data to model, wrapped in a larger function\nwhile True:\n    data_batch = next(df_gen)\n\n    #fake data, works in the model perfectly\n    number_labels = np.random.randint(1,219,len(data_batch[1]))\n  \n    outputdata, numberdata = data_batch[1].T\n    outputdata = np.asarray(outputdata).astype('float32')\n\n    #this code never works, the model freezes\n    numberdata = np.asarray(numberdata).astype(np.int32)\n  \n    yield [numberdata, data_batch[0]], outputdata\n\n#fitting the model\nhistory = model.fit(\n            train_generator,\n            steps_per_epoch=int(len(train_df) \/ batch_size),\n            validation_data=val_generator,\n            validation_steps=int(len(validation_df) \/ batch_size),\n            epochs=epochs,\n            callbacks=callbacks,\n            verbose = 1\n        )\n<\/code><\/pre>\n<p><strong>When I run this model, the output freezes at 'Epoch 1\/12'.<\/strong> I've checked that the data is in the right format, the right length, and matching properly to the other input.<\/p>\n<p>When I generate a random list of numbers, the model runs perfectly. I can also see that when fake data is generated, the number data is also getting generated.<\/p>\n<p>However, when I use the correct number data as an input into the model, the model <strong>freezes at the second 'next' call.<\/strong> I can also use a smaller sub-dataset with the same data structure and the model runs correctly. But when I use the entire dataset, the problem occurs again.<\/p>\n<p><strong>Do you know what could be causing this problem?<\/strong> I'm using AWS Sagemaker to run the model and can't seem to figure out where this problem is coming from. Thank you for your help!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-16 18:37:20.917 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":248,
        "Owner_creation_date":"2021-08-16 18:09:57.397 UTC",
        "Owner_last_access_date":"2022-05-23 17:12:16.697 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker training job stuck in progress state",
        "Question_body":"<p>I have created a training job yesterday, same as usual, just adding few more training data. I didn't have any problem with this in the last 2 years (the same exact procedure and code). This time after 14 hours more or less simply stalled.\nTraining job is still &quot;in processing&quot;, but cloudwatch is not logging anything since then. Right now 8 more hours passed and no new entry is in the logs, no errors no crash.\nCan someone explain this ? Unfortunately I don't have any AWS support plan.\nAs you can see from the picture below after 11am there is nothing..<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hswD7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hswD7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The training job is supposed to complete in the next couple of hours, but now I'm not sure if is actually running (in this case would be a cloudwatch problem) or not..<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>Suddenly the training job failed, without any further log. The reason is<\/p>\n<blockquote>\n<p>ClientError: Artifact upload failed:Error 7: The credentials received\nhave been expired<\/p>\n<\/blockquote>\n<p>But there is still nothing in the logs after 11am. Very weird.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-02-10 17:44:33.253 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":755,
        "Owner_creation_date":"2014-11-18 21:32:30.293 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437 UTC",
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-25 15:04:37.673 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-02-11 08:10:17.147 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker endpoint for pretrained model",
        "Question_body":"<p>I have a pre-trained model, now trying to create an endpoint using Sagemaker, my folder structure like this &quot;model.tar.gz&quot; looks like this:<\/p>\n<pre><code>model\n |- config.json\n |- pytorch_model.bin\n |- special_tokens_map.json\n |- spiece.model\n |- tokenizer_config.json\n |- training_args.bin\ncode\n |- inference.py\n | - requirements.txt\n<\/code><\/pre>\n<p>running following script to create endpoint:<\/p>\n<pre><code>pytorch_model = PyTorchModel(\n    model_data='s3:\/\/mck-dl-ai-studio\/answer_card\/answercard.tar.gz', \n    role=role, \n    entry_point='inference.py',\n    framework_version=&quot;1.3.1&quot;)\n\npredictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n<\/code><\/pre>\n<p>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;No module named 'transformers'&quot;. See <a href=\"https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/pytorch-inference-2020-07-20-16-45-51-564\" rel=\"nofollow noreferrer\">https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/pytorch-inference-2020-07-20-16-45-51-564<\/a> in account xxxxxx  for more information.<\/p>\n<p>what I am missing here tried adding source_dir and py_version but no success<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-20 17:13:46.68 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":377,
        "Owner_creation_date":"2012-03-09 06:32:20.743 UTC",
        "Owner_last_access_date":"2022-09-24 02:52:51.063 UTC",
        "Owner_location":"Gurgaon, India",
        "Owner_reputation":858,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":107,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker scikit_bring_your_own example",
        "Question_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 03:59:10.087 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"scikit-learn|amazon-sagemaker|svd",
        "Question_view_count":341,
        "Owner_creation_date":"2009-04-29 11:42:36.853 UTC",
        "Owner_last_access_date":"2022-09-18 14:23:33.13 UTC",
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Answer_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-17 15:03:32.89 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Is it possible to trigger Sagemaker notebook from AWS Lambda function?",
        "Question_body":"<p>I am trying to trigger Sagemaker notebook or Sagemaker Studio notebook from AWS Lambda when data is available in S3 bucket. I want to know if this is possible and if yes, how?<\/p>\n<p>All I want is once data is uploaded in S3, the lambda function should be able to spin up the Sagemaker notebook with a standard CPU cluster.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-08 01:35:17.017 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":621,
        "Owner_creation_date":"2020-06-29 19:21:19.09 UTC",
        "Owner_last_access_date":"2022-09-14 18:02:24.907 UTC",
        "Owner_location":null,
        "Owner_reputation":82,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Where to deploy machine learning model for API predictions?",
        "Question_body":"<p>I created a machine learning model with <a href=\"https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html\" rel=\"nofollow noreferrer\">Prophet<\/a>:<\/p>\n\n<p><a href=\"https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet\" rel=\"nofollow noreferrer\">https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet<\/a><\/p>\n\n<p>I have a web application running with Django. From that application, I want to be able to lookup predictions from the model I created. I assume the best way to do is to deploy my model on Google Cloud Platform or AWS (?) and access forecasts through API calls from my web application to one of these services.<\/p>\n\n<p>My question now: Is that way I described it the right way to do so? I still struggle to decide if either AWS or Google Cloud is the better solution for my case, especially with Prophet. I could only find examples with <code>scikit-learn<\/code>. Any of you who has experience with that and can point me in the right direction?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-13 09:28:47.43 UTC",
        "Question_favorite_count":0.0,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|google-cloud-platform|amazon-sagemaker|facebook-prophet",
        "Question_view_count":187,
        "Owner_creation_date":"2018-09-26 15:44:45.54 UTC",
        "Owner_last_access_date":"2021-10-12 09:14:54.903 UTC",
        "Owner_location":null,
        "Owner_reputation":2879,
        "Owner_up_votes":70,
        "Owner_down_votes":3,
        "Owner_views":5051,
        "Answer_body":"<p>It really depends on the type of model that you are using. In many cases, the model inference is getting a data point (similar to the data points you trained it with) and the model will generate a prediction to that requested data point. In such cases, you need to host the model somewhere in the cloud or on the edge. <\/p>\n\n<p>However, Prophet is often generating the predictions for the future as part of the training of the model. In this case, you only need to serve the predictions that were already calculated, and you can serve them as a CSV file from S3, or as lookup values from a DynamoDB or other lookup data stores. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-13 11:59:28.513 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker data preparation",
        "Question_body":"<p>I am trying to understand how to implement a machine learning algorithm, where the preprocessing and postprocessing is an heavy taskm inside AWS Sagemaker.\nThe main idea is to get data from S3, each time the data change in S3, Cloud watch triggers a lambda function to invoke a SageMaker endpoint. \nThe problem is that, once the algorithm is trained, before predicting the new data, i need to preprocess the data (custom NLP preprocessing).\nOnce the Algorithm have done the prediction, i need to take this prediction, do a post-process and then send the post-processed data to S3.\nThe idea i have in mind is to create a docker: <\/p>\n\n<pre><code>\u251c\u2500\u2500 text_classification\/                - ml scripts\n|   \u251c\u2500\u2500 app.py                            \n|   \u251c\u2500\u2500 config.py                         \n|   \u251c\u2500\u2500 data.py                           \n|   \u251c\u2500\u2500 models.py                         \n|   \u251c\u2500\u2500 predict.py                        - pre-processing data and post-processing data\n|   \u251c\u2500\u2500 train.py                          \n|   \u251c\u2500\u2500 utils.py                          \n<\/code><\/pre>\n\n<p>So i will do the pre-processing and the post-processing inside \"predict.py\". When i will invoke the endpoint for prediction, that script will run.\nIs this correct?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-05 12:37:59.797 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|machine-learning|aws-lambda|data-science|amazon-sagemaker",
        "Question_view_count":280,
        "Owner_creation_date":"2017-01-04 16:53:36.877 UTC",
        "Owner_last_access_date":"2021-03-25 06:26:26.257 UTC",
        "Owner_location":null,
        "Owner_reputation":294,
        "Owner_up_votes":27,
        "Owner_down_votes":4,
        "Owner_views":34,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Question_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-12-09 23:04:49.43 UTC",
        "Question_favorite_count":null,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1697,
        "Owner_creation_date":"2015-01-14 12:25:26.28 UTC",
        "Owner_last_access_date":"2022-09-24 21:33:27.523 UTC",
        "Owner_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Owner_reputation":1951,
        "Owner_up_votes":95,
        "Owner_down_votes":32,
        "Owner_views":217,
        "Answer_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-17 19:20:19.393 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2019-01-04 21:20:01.95 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-27 07:20:40.657 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_date":"2022-07-27 07:13:43.853 UTC",
        "Owner_last_access_date":"2022-09-23 08:24:17.75 UTC",
        "Owner_location":null,
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-27 23:32:23.5 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-08-11 12:28:40.907 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker suddenly unable to install python packages, missing python-dev",
        "Question_body":"<p>Not sure why, but sagemaker complaining about unable to install certain python packages. This is automatically deployed with cloudtemplate. Not sure how to go about installing python-dev on sagemaker.<\/p>\n<pre><code>    Running setup.py install for cymem: finished with status 'error'\n    ERROR: Command errored out with exit status 1:\n     command: \/usr\/bin\/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-he9ui3pu\/cymem\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-he9ui3pu\/cymem\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-spf4a86a\/install-record.txt --single-version-externally-managed --compile\n         cwd: \/tmp\/pip-install-he9ui3pu\/cymem\/\n    Complete output (27 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-3.6\n    creating build\/lib.linux-x86_64-3.6\/cymem\n    copying cymem\/__init__.py -&gt; build\/lib.linux-x86_64-3.6\/cymem\n    copying cymem\/about.py -&gt; build\/lib.linux-x86_64-3.6\/cymem\n    package init file 'cymem\/tests\/__init__.py' not found (or not a regular file)\n    creating build\/lib.linux-x86_64-3.6\/cymem\/tests\n    copying cymem\/tests\/test_import.py -&gt; build\/lib.linux-x86_64-3.6\/cymem\/tests\n    copying cymem\/cymem.pyx -&gt; build\/lib.linux-x86_64-3.6\/cymem\n    copying cymem\/__init__.pxd -&gt; build\/lib.linux-x86_64-3.6\/cymem\n    copying cymem\/cymem.pxd -&gt; build\/lib.linux-x86_64-3.6\/cymem\n    warning: build_py: byte-compiling is disabled, skipping.\n    \n    running build_ext\n    building 'cymem.cymem' extension\n    creating build\/temp.linux-x86_64-3.6\n    creating build\/temp.linux-x86_64-3.6\/cymem\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I\/usr\/include\/python3.6m -I\/usr\/include\/python3.6m -c cymem\/cymem.cpp -o build\/temp.linux-x86_64-3.6\/cymem\/cymem.o -O3 -Wno-strict-prototypes -Wno-unused-function\n    cc1plus: warning: command line option \u2018-Wno-strict-prototypes\u2019 is valid for C\/ObjC but not for C++\n    cymem\/cymem.cpp:4:10: fatal error: Python.h: No such file or directory\n     #include &quot;Python.h&quot;\n              ^~~~~~~~~~\n    compilation terminated.\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nRunning setup.py install for cymem: finished with status 'error' ERROR: Command errored out with exit status 1: command: \/usr\/bin\/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-he9ui3pu\/cymem\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-he9ui3pu\/cymem\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-spf4a86a\/install-record.txt --single-version-externally-managed --compile cwd: \/tmp\/pip-install-he9ui3pu\/cymem\/ Complete output (27 lines): running install running build running build_py creating build creating build\/lib.linux-x86_64-3.6 creating build\/lib.linux-x86_64-3.6\/cymem copying cymem\/__init__.py -&gt; build\/lib.linux-x86_64-3.6\/cymem copying cymem\/about.py -&gt; build\/lib.linux-x86_64-3.6\/cymem package init file 'cymem\/tests\/__init__.py' not found (or not a regular file) creating build\/lib.linux-x86_64-3.6\/cymem\/tests copying cymem\/tests\/test_import.py -&gt; build\/lib.linux-x86_64-3.6\/cymem\/tests copying cymem\/cymem.pyx -&gt; build\/lib.linux-x86_64-3.6\/cymem copying cymem\/__init__.pxd -&gt; build\/lib.linux-x86_64-3.6\/cymem copying cymem\/cymem.pxd -&gt; build\/lib.linux-x86_64-3.6\/cymem warning: build_py: byte-compiling is disabled, skipping. running build_ext building 'cymem.cymem' extension creating build\/temp.linux-x86_64-3.6 creating build\/temp.linux-x86_64-3.6\/cymem x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I\/usr\/include\/python3.6m -I\/usr\/include\/python3.6m -c cymem\/cymem.cpp -o build\/temp.linux-x86_64-3.6\/cymem\/cymem.o -O3 -Wno-strict-prototypes -Wno-unused-function cc1plus: warning: command line option \u2018-Wno-strict-prototypes\u2019 is valid for C\/ObjC but not for C++ cymem\/cymem.cpp:4:10: fatal error: Python.h: No such file or directory #include &quot;Python.h&quot; ^~~~~~~~~~ compilation terminated. error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 ----------------------------------------\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-09 18:32:36.723 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":196,
        "Owner_creation_date":"2016-11-30 21:11:08.197 UTC",
        "Owner_last_access_date":"2022-05-01 19:47:32.64 UTC",
        "Owner_location":"Manhattan, New York, NY, United States",
        "Owner_reputation":389,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS boto3 resource proxies with SageMaker services",
        "Question_body":"<p>I need to use a boto3 IAM connection with a proxy for SageMaker services run inside a VPC. Using the estimator class as just one example of a service I need to use (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html<\/a>) I am unsure exactly how to get this service to use a proxy for IAM when I call the fit() method. I am already passing in a configured SageMaker Session (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session<\/a>) and I believe it has something to do with this but I only see an option to pass in a boto3 session but I can't see any info on how to preconfigure an IAM resource to use a proxy.<\/p>\n<p>Can anyone help?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-28 11:15:33.8 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|boto3|amazon-iam|amazon-sagemaker",
        "Question_view_count":88,
        "Owner_creation_date":"2018-01-03 03:05:55.31 UTC",
        "Owner_last_access_date":"2021-10-28 11:06:49.813 UTC",
        "Owner_location":"Brisbane QLD, Australia",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to turn a matrix into a sparse matrix and protobuf it",
        "Question_body":"<p>I have a data set with 16 columns and 100,000 rows which I'm trying to prepare for a matrix-factorization training. I'm using the following code to split it and turn it into a sparse matrix.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\ny=data[[1]]\nX=lil_matrix(100000,15).astype('float32')\ny=np.array(y).astype('float32')\nX\n<\/code><\/pre>\n\n<p>But when I run it, I get this error:<\/p>\n\n<blockquote>\n  <p>&lt;1x1 sparse matrix of type ''  with 1 stored\n  elements in LInked List format> .<\/p>\n<\/blockquote>\n\n<p>When I try to plug it into a training\/testing split it gives me further errors:<\/p>\n\n<blockquote>\n  <p>Found input variables with inconsistent numbers of samples: [1,\n  100000]<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2019-07-09 16:44:49.4 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"numpy|scipy|sparse-matrix|amazon-sagemaker|matrix-factorization",
        "Question_view_count":498,
        "Owner_creation_date":"2018-12-23 00:19:51.677 UTC",
        "Owner_last_access_date":"2022-09-23 18:48:55.043 UTC",
        "Owner_location":"Kansas City, MO, USA",
        "Owner_reputation":91,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>Your linked <code>notebook<\/code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv<\/code>.<\/p>\n\n<p>A simple example of this:<\/p>\n\n<pre><code>In [565]: from scipy import sparse                                                                           \nIn [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         \nIn [567]: M                                                                                                  \nOut[567]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 0 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>Note that I use <code>(10,5)<\/code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs<\/code>.  In the link the relevant line is:<\/p>\n\n<pre><code>X = lil_matrix((lines, columns)).astype('float32')\n<\/code><\/pre>\n\n<p>Now I can set a couple elements, just as I would an dense array:<\/p>\n\n<pre><code>In [568]: M[1,2] = 12.3                                                                                      \nIn [569]: M[3,1] = 1.1                                                                                       \nIn [570]: M                                                                                                  \nOut[570]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>I can use <code>toarray<\/code> to display the matrix as a dense array (don't try this with large dimensions).<\/p>\n\n<pre><code>In [571]: M.toarray()                                                                                        \nOut[571]: \narray([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. , 12.3,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  1.1,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ]])\n<\/code><\/pre>\n\n<hr>\n\n<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.<\/p>\n\n<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            \nOut[572]: \n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1 stored elements in LInked List format&gt;\nIn [573]: _.A                                                                                                \nOut[573]: array([[10]], dtype=int64)\n<\/code><\/pre>\n\n<p>Look again at your code.  You set the <code>X<\/code> value twice, once it is a dataframe.  The second time is this bad <code>lil<\/code> initialization.  The second time does not make use of the first <code>X<\/code>.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\n...\nX=lil_matrix(100000,15).astype('float32')\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-07-10 23:27:53.077 UTC",
        "Answer_last_edit_date":"2019-07-10 23:34:16.23 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":"2019-07-09 20:49:03.183 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker: How to debug Model monitoring(data quality and model quality)?",
        "Question_body":"<p>I have created a Data Quality monitoring from Sagemaker Studio UI and also created using sagemaker SDK <a href=\"https:\/\/stackoverflow.com\/q\/69179914\/11844406\">code<\/a>, I referred to create model Data Quality monitoring job.<\/p>\n<p><strong>Errors:<\/strong><\/p>\n<ol>\n<li>when there is no captured data (this is expected)<\/li>\n<\/ol>\n<blockquote>\n<p>Monitoring job failure reason:<\/p>\n<p>Job inputs had no data<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li>From logs, I can see that it is using <code>Java<\/code> in background. Not sure how to debug?<\/li>\n<\/ol>\n<blockquote>\n<p>org.json4s.package$MappingException: Do not know how to convert\nJObject(List(0,JDouble(38.0))) into class java.lang.String.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3da3i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3da3i.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once we create the DataQuality monitoring job using Sagemaker Studio UI or Sagemkaer python sdk, it is taking a hour to start. I would like to know is there a way to debug monitoring job without waiting for a hour every time we get a error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-02 05:34:02.067 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":88,
        "Owner_creation_date":"2019-07-27 06:28:53.767 UTC",
        "Owner_last_access_date":"2022-09-23 13:42:33.487 UTC",
        "Owner_location":null,
        "Owner_reputation":491,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"cannot deploy YAMNet model to SageMaker",
        "Question_body":"<p>I followed <a href=\"https:\/\/www.tensorflow.org\/tutorials\/audio\/transfer_learning_audio\" rel=\"nofollow noreferrer\">this tutorial<\/a> and had the model fine-tuned.<\/p>\n<p>the model-saving part of serving model is like this:<\/p>\n<pre><code>saved_model_path = 'dogs_and_cats_yamnet\/yamnet-model\/00000001'\n\ninput_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\nembedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n                                            trainable=False, name='yamnet')\n_, embeddings_output, _ = embedding_extraction_layer(input_segment)\nserving_outputs = my_model(embeddings_output)\nserving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\nserving_model = tf.keras.Model(input_segment, serving_outputs)\nserving_model.save(saved_model_path, include_optimizer=False)\n<\/code><\/pre>\n<p>Then followed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">this page<\/a>, uploading the model to S3 and deploying the model.<\/p>\n<pre><code>!tar -C &quot;$PWD&quot; -czf dogs_and_cats_yamnet.tar.gz dogs_and_cats_yamnet\/\nmodel_data = Session().upload_data(path=&quot;dogs_and_cats_yamnet.tar.gz&quot;, key_prefix=&quot;model&quot;)\nmodel = TensorFlowModel(model_data=model_data, role=sagemaker_role, framework_version=&quot;2.3&quot;)\npredictor = model.deploy(initial_instance_count=1, instance_type=&quot;ml.c5.xlarge&quot;)\n<\/code><\/pre>\n<p>Deployment seems successful, but when I try to do inference,<\/p>\n<pre><code>waveform = np.zeros((3*48000), dtype=np.float32)\nresult = predictor.predict(waveform)\n<\/code><\/pre>\n<p>the following error occurs.<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{\n    &quot;error&quot;: &quot;The first dimension of paddings must be the rank of inputs[1,2] [1,144000]\\n\\t [[{{node yamnet_frames\/tf_op_layer_Pad\/Pad}}]]&quot;\n<\/code><\/pre>\n<p>I have no idea why this happens. I am struggling with it for hours and coming up with no clue.\nYAMNet works fine when I pulled the model from tf hub directly and take inference with it.\nThis is kind of a minor question I guess, but I would appreciate any helpful answers.\nThank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-18 10:10:14.527 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_date":"2016-09-29 00:32:31.95 UTC",
        "Owner_last_access_date":"2022-05-17 13:21:20.043 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Properly return a label in post-annotation lambda for AWS SageMaker Ground Truth custom labeling job",
        "Question_body":"<p>I'm working on a SageMaker labeling job with custom datatypes. For some reason though, I'm not getting the correct label in the AWS web console. It should have the selected label which is &quot;Native&quot;, but instead, I'm getting the <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py#L65\" rel=\"nofollow noreferrer\"><code>&lt;labelattributename&gt;<\/code><\/a> which is &quot;new-test-14&quot;.<\/p>\n<p>After Ground Truth runs the post-annotation lambda, it seems to modify the metadata before returning a data object. The data object it returns doesn't contain a class-name key inside the metadata attribute, even when I hard-code the lambda to return an object that contains it.<\/p>\n<p>My manifest file looks like this:<\/p>\n<pre><code>{&quot;source-ref&quot; : &quot;s3:\/\/&lt;file-name&gt;&quot;, &quot;text&quot; : &quot;Hello world&quot;}\n{&quot;source-ref&quot; : &quot;s3:\/\/&quot;&lt;file-name&gt;&quot;, &quot;text&quot; : &quot;Hello world&quot;}\n<\/code><\/pre>\n<p>And the worker response looks like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{&quot;answers&quot;:[{&quot;acceptanceTime&quot;:&quot;2021-05-18T16:08:29.473Z&quot;,&quot;answerContent&quot;:{&quot;new-test-14&quot;:{&quot;label&quot;:&quot;Native&quot;}},&quot;submissionTime&quot;:&quot;2021-05-18T16:09:15.960Z&quot;,&quot;timeSpentInSeconds&quot;:46.487,&quot;workerId&quot;:&quot;private.us-east-1.ea05a03fcd679cbb&quot;,&quot;workerMetadata&quot;:{&quot;identityData&quot;:{&quot;identityProviderType&quot;:&quot;Cognito&quot;,&quot;issuer&quot;:&quot;https:\/\/cognito-idp.us-east-1.amazonaws.com\/us-east-1_XPxQ9txEq&quot;,&quot;sub&quot;:&quot;edc59ce1-e09d-4551-9e0d-a240465ea14a&quot;}}}]}\n<\/code><\/pre>\n<p>That worker response gets processed by my post-annotation lambda which is modeled after <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">this aws sample ground truth recipe<\/a>. Here's my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport sys\nimport boto3\nfrom datetime import datetime\n\n\n\ndef lambda_handler(event, context):\n\n\n    # Event received\n    print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n\n    labeling_job_arn = event[&quot;labelingJobArn&quot;]\n    label_attribute_name = event[&quot;labelAttributeName&quot;]\n\n    label_categories = None\n    if &quot;label_categories&quot; in event:\n        label_categories = event[&quot;labelCategories&quot;]\n        print(&quot; Label Categories are : &quot; + label_categories)\n\n    payload = event[&quot;payload&quot;]\n    role_arn = event[&quot;roleArn&quot;]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if &quot;outputConfig&quot; in event:\n        output_config = event[&quot;outputConfig&quot;]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    # kms_key_id = None\n    # if &quot;kmsKeyId&quot; in event:\n    #     kms_key_id = event[&quot;kmsKeyId&quot;]\n\n    # # Create s3 client object\n    # s3_client = S3Client(role_arn, kms_key_id)\n    s3_client = boto3.client('s3')\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n\n\ndef do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client):\n    &quot;&quot;&quot;\n        Core Logic for consolidation\n\n    :param labeling_job_arn: labeling job ARN\n    :param payload:  payload data for consolidation\n    :param label_attribute_name: identifier for labels in output JSON\n    :param s3_client: S3 helper class\n    :return: output JSON string\n    &quot;&quot;&quot;\n\n    # Extract payload data\n    if &quot;s3Uri&quot; in payload:\n        s3_ref = payload[&quot;s3Uri&quot;]\n        payload_bucket, payload_key = s3_ref.split('\/',2)[-1].split('\/',1)\n        payload = json.loads(s3_client.get_object(Bucket=payload_bucket, Key=payload_key)['Body'].read())\n#         print(payload)\n\n    # Payload data contains a list of data objects.\n    # Iterate over it to consolidate annotations for individual data object.\n    consolidated_output = []\n    success_count = 0  # Number of data objects that were successfully consolidated\n    failure_count = 0  # Number of data objects that failed in consolidation\n\n    for p in range(len(payload)):\n        response = None\n\n        dataset_object_id = payload[p]['datasetObjectId']\n        log_prefix = &quot;[{}] data object id [{}] :&quot;.format(labeling_job_arn, dataset_object_id)\n        print(&quot;{} Consolidating annotations BEGIN &quot;.format(log_prefix))\n\n        annotations = payload[p]['annotations']\n#             print(&quot;{} Received Annotations from all workers {}&quot;.format(log_prefix, annotations))\n\n        # Iterate over annotations. Log all annotation to your CloudWatch logs\n        annotationsFromAllWorkers = []\n        for i in range(len(annotations)):\n            worker_id = annotations[i][&quot;workerId&quot;]\n            anotation_data = annotations[i][&quot;annotationData&quot;]\n            annotation_content = anotation_data[&quot;content&quot;]\n            annotation_content_json = json.loads(annotation_content)\n            annotation_job = annotation_content_json[&quot;new_test&quot;]\n            annotation_label = annotation_job[&quot;label&quot;]\n            consolidated_annotation= {\n                &quot;workerId&quot;: worker_id,\n                &quot;annotationData&quot;: {\n                    &quot;content&quot;: {\n                        &quot;annotatedResult&quot;: {\n                            &quot;instances&quot;: [{&quot;label&quot;:annotation_label }]    \n                        }\n                    }\n                }\n            }\n            annotationsFromAllWorkers.append(consolidated_annotation)\n\n        consolidated_annotation = {&quot;annotationsFromAllWorkers&quot;: annotationsFromAllWorkers} # TODO : Add your consolidation logic\n\n        # Build consolidation response object for an individual data object\n        response = {\n            &quot;datasetObjectId&quot;: dataset_object_id,\n            &quot;consolidatedAnnotation&quot;: {\n                &quot;content&quot;: {\n                    label_attribute_name: consolidated_annotation,\n                    label_attribute_name+ &quot;-metadata&quot;: {\n                        &quot;class-name&quot;: &quot;Native&quot;,\n                        &quot;confidence&quot;: 0.00,\n                        &quot;human-annotated&quot;: &quot;yes&quot;,\n                        &quot;creation-date&quot;: datetime.strftime(datetime.now(), &quot;%Y-%m-%dT%H:%M:%S&quot;),\n                        &quot;type&quot;: &quot;groundtruth\/custom&quot;\n                    }\n \n                }\n            }\n        }\n\n        success_count += 1\n#             print(&quot;{} Consolidating annotations END &quot;.format(log_prefix))\n\n        # Append individual data object response to the list of responses.\n        if response is not None:\n            consolidated_output.append(response)\n\n\n        failure_count += 1\n        print(&quot; Consolidation failed for dataobject {}&quot;.format(p))\n        print(&quot; Unexpected error: Consolidation failed.&quot; + str(sys.exc_info()[0]))\n\n    print(&quot;Consolidation Complete. Success Count {}  Failure Count {}&quot;.format(success_count, failure_count))\n\n    print(&quot; -- Consolidated Output -- &quot;)\n    print(consolidated_output)\n    print(&quot; ------------------------- &quot;)\n    return consolidated_output\n<\/code><\/pre>\n<p>As you can see above, the <code>do_consolidation<\/code> method returns an object hard-coded to include a class-name of &quot;Native&quot;, and the <code>lambda_handler<\/code> method returns that same object. Here's the post-annotation function response:<\/p>\n<pre><code>[{\n    &quot;datasetObjectId&quot;: &quot;4&quot;,\n    &quot;consolidatedAnnotation&quot;: {\n        &quot;content&quot;: {\n            &quot;new-test-14&quot;: {\n                &quot;annotationsFromAllWorkers&quot;: [{\n                    &quot;workerId&quot;: &quot;private.us-east-1.ea05a03fcd679cbb&quot;,\n                    &quot;annotationData&quot;: {\n                        &quot;content&quot;: {\n                            &quot;annotatedResult&quot;: {\n                                &quot;instances&quot;: [{\n                                    &quot;label&quot;: &quot;Native&quot;\n                                }]\n                            }\n                        }\n                    }\n                }]\n            },\n            &quot;new-test-14-metadata&quot;: {\n                &quot;class-name&quot;: &quot;Native&quot;,\n                &quot;confidence&quot;: 0,\n                &quot;human-annotated&quot;: &quot;yes&quot;,\n                &quot;creation-date&quot;: &quot;2021-05-19T07:06:06&quot;,\n                &quot;type&quot;: &quot;groundtruth\/custom&quot;\n            }\n        }\n    }\n}]\n<\/code><\/pre>\n<p>As you can see, the post-annotation function return value has the class-name of &quot;Native&quot; in the metadata so I would expect the class-name to be present in the data object metadata, but it's not. And here's a screenshot of the data object summary:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wHpEl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wHpEl.png\" alt=\"Labeled object summary\" \/><\/a><\/p>\n<p>It seems like Ground Truth overwrote the metadata, and now the object doesn't contain the correct label. I think perhaps that's why my label is coming through as the label attribute name &quot;new-test-14&quot; instead of as the correct label &quot;Native&quot;. Here's a screenshot of the labeling job in the AWS web console:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/yOdfP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yOdfP.png\" alt=\"labeling job\" \/><\/a><\/p>\n<p>The web console is supposed to show the label <strong>&quot;Native&quot;<\/strong> inside the &quot;Label&quot; column but instead I'm getting the <a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py#L65\" rel=\"nofollow noreferrer\"><code>&lt;labelattributename&gt;<\/code><\/a> <strong>&quot;new-test-14&quot;<\/strong> in the label column.<\/p>\n<p>Here is the output.manifest file generated by Ground Truth at the end:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;source-ref&quot;: &quot;s3:\/\/&lt;file-name&gt;&quot;,\n    &quot;text&quot;: &quot;Hello world&quot;,\n    &quot;new-test-14&quot;: {\n        &quot;annotationsFromAllWorkers&quot;: [{\n            &quot;workerId&quot;: &quot;private.us-east-1.ea05a03fcd679ert&quot;,\n            &quot;annotationData&quot;: {\n                &quot;content&quot;: {\n                    &quot;annotatedResult&quot;: {\n                        &quot;label&quot;: &quot;Native&quot;\n                    }\n                }\n            }\n        }]\n    },\n    &quot;new-test-14-metadata&quot;: {\n        &quot;type&quot;: &quot;groundtruth\/custom&quot;,\n        &quot;job-name&quot;: &quot;new-test-14&quot;,\n        &quot;human-annotated&quot;: &quot;yes&quot;,\n        &quot;creation-date&quot;: &quot;2021-05-18T12:34:17.400000&quot;\n    }\n}\n<\/code><\/pre>\n<p>What should I return from the Post-Annotation function? Am I missing something in my response? How do I get the proper label to appear in the AWS web console?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-05-19 07:50:50.207 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":430,
        "Owner_creation_date":"2021-05-19 06:25:31.013 UTC",
        "Owner_last_access_date":"2021-09-27 05:23:53.383 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-05-28 08:14:08.607 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Random Cut Forest Training with Validation",
        "Question_body":"<p>troubling for some days with the sagemaker built-in rcf algorithm.<\/p>\n<p>I would like to validate the model during training, but there might be things I didn't understand correctly.<\/p>\n<p>First fitting only with training channel works fine:<\/p>\n<pre><code>container=sagemaker.image_uris.retrieve(&quot;randomcutforest&quot;, region, &quot;us-east-1&quot;)\nprint(container)\n\nrcf = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=role,\n    instance_count=1,\n    sagemaker_session=sagemaker.Session(),\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    data_location=f&quot;s3:\/\/{bucket}\/{prefix}\/&quot;,\n    output_path=f&quot;s3:\/\/{bucket}\/{prefix}\/output&quot;\n)\n\nrcf.set_hyperparameters(\n    feature_dim = 116,\n    eval_metrics = 'precision_recall_fscore',\n    num_samples_per_tree=256,\n    num_trees=100,\n    \n)\n\ntrain_data = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='text\/csv;label_size=0', distribution='ShardedByS3Key')\n\nrcf.fit({'train': train_data})\n<\/code><\/pre>\n<pre><code>[06\/28\/2021 09:45:24 INFO 140226936620864] Test data is not provided.\n#metrics {&quot;StartTime&quot;: 1624873524.6154933, &quot;EndTime&quot;: 1624873524.6156445, &quot;Dimensions&quot;: {&quot;Algorithm&quot;: &quot;RandomCutForest&quot;, &quot;Host&quot;: &quot;algo-1&quot;, &quot;Operation&quot;: &quot;training&quot;}, &quot;Metrics&quot;: {&quot;setuptime&quot;: {&quot;sum&quot;: 40.169477462768555, &quot;count&quot;: 1, &quot;min&quot;: 40.169477462768555, &quot;max&quot;: 40.169477462768555}, &quot;totaltime&quot;: {&quot;sum&quot;: 13035.491704940796, &quot;count&quot;: 1, &quot;min&quot;: 13035.491704940796, &quot;max&quot;: 13035.491704940796}}}\n\n\n2021-06-28 09:45:50 Completed - Training job completed\nProfilerReport-1624873226: NoIssuesFound\nTraining seconds: 78\nBillable seconds: 78\n<\/code><\/pre>\n<p>But when I want to validate my model during training:<\/p>\n<pre><code>train_data = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='text\/csv;label_size=0', distribution='ShardedByS3Key')\nval_data = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='text\/csv;label_size=1', distribution='FullyReplicated')\n\n\nrcf.fit({'train': train_data, 'validation': val_data}, wait=True)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>AWS Region: us-east-1\nRoleArn: arn:aws:iam::517714493426:role\/service-role\/AmazonSageMaker-ExecutionRole-20210409T152960\n382416733822.dkr.ecr.us-east-1.amazonaws.com\/randomcutforest:1\n2021-06-28 10:14:12 Starting - Starting the training job...\n2021-06-28 10:14:14 Starting - Launching requested ML instancesProfilerReport-1624875252: InProgress\n......\n2021-06-28 10:15:27 Starting - Preparing the instances for training.........\n2021-06-28 10:17:07 Downloading - Downloading input data...\n2021-06-28 10:17:27 Training - Downloading the training image..Docker entrypoint called with argument(s): train\nRunning default environment configuration script\n[06\/28\/2021 10:17:53 INFO 140648505521984] Reading default configuration from \/opt\/amazon\/lib\/python3.7\/site-packages\/algorithm\/resources\/default-conf.json: {'num_samples_per_tree': 256, 'num_trees': 100, 'force_dense': 'true', 'eval_metrics': ['accuracy', 'precision_recall_fscore'], 'epochs': 1, 'mini_batch_size': 1000, '_log_level': 'info', '_kvstore': 'dist_async', '_num_kv_servers': 'auto', '_num_gpus': 'auto', '_tuning_objective_metric': '', '_ftp_port': 8999}\n[06\/28\/2021 10:17:53 INFO 140648505521984] Merging with provided configuration from \/opt\/ml\/input\/config\/hyperparameters.json: {'num_trees': '100', 'num_samples_per_tree': '256', 'feature_dim': '116', 'eval_metrics': 'precision_recall_fscore'}\n[06\/28\/2021 10:17:53 INFO 140648505521984] Final configuration: {'num_samples_per_tree': '256', 'num_trees': '100', 'force_dense': 'true', 'eval_metrics': 'precision_recall_fscore', 'epochs': 1, 'mini_batch_size': 1000, '_log_level': 'info', '_kvstore': 'dist_async', '_num_kv_servers': 'auto', '_num_gpus': 'auto', '_tuning_objective_metric': '', '_ftp_port': 8999, 'feature_dim': '116'}\n[06\/28\/2021 10:17:53 ERROR 140648505521984] Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\n\nCaused by: Additional properties are not allowed ('validation' was unexpected)\n\nFailed validating 'additionalProperties' in schema:\n    {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#',\n     'additionalProperties': False,\n     'definitions': {'data_channel_replicated': {'properties': {'ContentType': {'type': 'string'},\n                                                                'RecordWrapperType': {'$ref': '#\/definitions\/record_wrapper_type'},\n                                                                'S3DistributionType': {'$ref': '#\/definitions\/s3_replicated_type'},\n                                                                'TrainingInputMode': {'$ref': '#\/definitions\/training_input_mode'}},\n                                                 'type': 'object'},\n                     'data_channel_sharded': {'properties': {'ContentType': {'type': 'string'},\n                                                             'RecordWrapperType': {'$ref': '#\/definitions\/record_wrapper_type'},\n                                                             'S3DistributionType': {'$ref': '#\/definitions\/s3_sharded_type'},\n                                                             'TrainingInputMode': {'$ref': '#\/definitions\/training_input_mode'}},\n                                              'type': 'object'},\n                     'record_wrapper_type': {'enum': ['None', 'Recordio'],\n                                             'type': 'string'},\n                     's3_replicated_type': {'enum': ['FullyReplicated'],\n                                            'type': 'string'},\n                     's3_sharded_type': {'enum': ['ShardedByS3Key'],\n                                         'type': 'string'},\n                     'training_input_mode': {'enum': ['File', 'Pipe'],\n                                             'type': 'string'}},\n     'properties': {'state': {'$ref': '#\/definitions\/data_channel'},\n                    'test': {'$ref': '#\/definitions\/data_channel_replicated'},\n                    'train': {'$ref': '#\/definitions\/data_channel_sharded'}},\n     'required': ['train'],\n     'type': 'object'}\n\nOn instance:\n    {'train': {'ContentType': 'text\/csv;label_size=0',\n               'RecordWrapperType': 'None',\n               'S3DistributionType': 'ShardedByS3Key',\n               'TrainingInputMode': 'File'},\n     'validation': {'ContentType': 'text\/csv;label_size=1',\n                    'RecordWrapperType': 'None',\n                    'S3DistributionType': 'FullyReplicated',\n                    'TrainingInputMode': 'File'}}\n\n2021-06-28 10:18:10 Uploading - Uploading generated training model\n2021-06-28 10:18:10 Failed - Training job failed\nProfilerReport-1624875252: Stopping\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-34-c624ace00c69&gt; in &lt;module&gt;\n     33 \n     34 \n---&gt; 35 rcf.fit({'train': train_data, 'validation': val_data}, wait=True)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    680         self.jobs.append(self.latest_training_job)\n    681         if wait:\n--&gt; 682             self.latest_training_job.wait(logs=logs)\n    683 \n    684     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1623         # If logs are requested, call logs_for_jobs.\n   1624         if logs != &quot;None&quot;:\n-&gt; 1625             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1626         else:\n   1627             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3679 \n   3680         if wait:\n-&gt; 3681             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3682             if dot:\n   3683                 print()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3243                 ),\n   3244                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3245                 actual_status=status,\n   3246             )\n   3247 \n\nUnexpectedStatusException: Error for Training job randomcutforest-2021-06-28-10-14-12-783: Failed. Reason: ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\n\nCaused by: Additional properties are not allowed ('validation' was unexpected)\n\nFailed validating 'additionalProperties' in schema:\n    {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#',\n     'additionalProperties': False,\n     'definitions': {'data_channel_replicated': {'properties': {'ContentType': {'type': 'string'},\n                                                                'RecordWrapperType': {'$ref': '#\/definitions\/record_wrapper_type'},\n                                                                'S3DistributionType': {'$ref': '#\/definitions\/s3_replicated_type'},\n                                                                'TrainingInputMode': {'$ref': '#\/definitions\/training_input_mode'}},\n                                                 'type': 'object'},\n                     'data_channel_sharded': {'properties': {'ContentType': {'type': 'string'},\n<\/code><\/pre>\n<p>Could someone help me, how I correctly implement this validation during training?\nThis would the best what could actually happen to me. :-D<\/p>\n<p>Kind greetings,\nChristina<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-28 10:25:18.403 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":344,
        "Owner_creation_date":"2021-06-28 09:26:56.993 UTC",
        "Owner_last_access_date":"2022-04-19 14:56:40.593 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>I found the error: instead of 'validation' you need to name the channel 'test', then it works:\nrcf.fit({'train': train_data, 'test': test_data}, wait=True)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-30 13:42:45.42 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Unable to Parse Augmented Manifest File",
        "Question_body":"<p>I've read all of the existing documentation on Augmented Manifest files. I see no difference from my file, but I keep experiencing this error when training:<\/p>\n<pre><code>ClientError: Data download failed:Failed to download data. Unable to parse augmented manifest, error in line: 1\n<\/code><\/pre>\n<p>My example first line:<\/p>\n<pre><code>{&quot;source-ref&quot;: &quot;s3:\/\/test-bucket\/test-data\/test\/bucket\/10done.png&quot;, &quot;video-frame-object-detection&quot;: {&quot;annotations&quot;: [{&quot;class_id&quot;: 1, &quot;top&quot;: 880, &quot;left&quot;: 43, &quot;width&quot;: 2499, &quot;height&quot;: 324}], &quot;image_size&quot;: [{&quot;width&quot;: 2543, &quot;height&quot;: 2543, &quot;depth&quot;: 3}]}, &quot;video-frame-object-detection-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;Good&quot;, &quot;1&quot;: &quot;Bad&quot;}, &quot;human-annotated&quot;: &quot;no&quot;, &quot;creation-date&quot;: &quot;2022-06-09T11:01:27.440682&quot;, &quot;type&quot;: &quot;programmatically-created-labels&quot;}}\n<\/code><\/pre>\n<p>These are how my breaks look (end of file):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PkRW3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PkRW3.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>How I manually created the manifest file:\n<a href=\"https:\/\/i.stack.imgur.com\/uAQmR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAQmR.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Sagemaker is recognizing my attributes as well.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-09 17:06:09.303 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|json|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":129,
        "Owner_creation_date":"2020-08-06 11:57:10.037 UTC",
        "Owner_last_access_date":"2022-06-27 20:22:44.427 UTC",
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-06-09 18:32:27.79 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker lifecycle config: could not find conda environment conda_python3",
        "Question_body":"<p>the below script should run a notebook called prepTimePreProcessing whenever a AWS notebook instance starts runing.\nhowever I am getting &quot;could not find conda environment conda_python3&quot; error from the lifecycle config file.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>set -e\nENVIRONMENT=python3\nNOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/prepTimePreProcessing.ipynb&quot;\necho &quot;Activating conda env&quot;\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\necho &quot;Starting notebook&quot;\nnohup jupyter nbconvert  --to notebook --inplace --ExecutePreprocessor.timeout=600 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &amp;\n<\/code><\/pre>\n<p>Any help whould be appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 08:40:25.847 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":9,
        "Owner_creation_date":"2022-09-23 07:15:37.043 UTC",
        "Owner_last_access_date":"2022-09-23 13:50:30.643 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-23 08:41:16.307 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to save Tensorflow model in S3 (as \/output\/model.tar.gz) when using Tensorflow Estimator in AWS Sagemaker",
        "Question_body":"<p>I have a Keras model getting trained using an entry_point script and I am using the following pieces of code to store the model artifacts (in the entry_point script).<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\nargs, _ = parser.parse_known_args()\nmodel_dir  = args.model_dir\n---\n\ntf.keras.models.save_model(\n      model,\n      os.path.join(model_dir, 'model\/1'),\n      overwrite=True,\n      include_optimizer=True\n     )\n<\/code><\/pre>\n<p>Ideally, the model_dir should be <code>opt\/ml\/model<\/code> and Sagemaker should automatically move the contents of this folder to S3 as <code>s3:\/\/&lt;default_bucket&gt;\/&lt;training_name&gt;\/output\/model.tar.gz<\/code><\/p>\n<p>When I run the <code>estimator.fit({'training': training_input_path})<\/code>, the training is successful, but the Cloudwatch logs show the following:<\/p>\n<pre><code>2020-09-16 02:49:12,458 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under the path \/opt\/ml\/model. Your training job will not save any model files to S3.\n<\/code><\/pre>\n<p>Even then, <strong>Sagemaker does store<\/strong> my model artifacts, with the only difference being that instead of storing them in <code>s3:\/\/&lt;default_bucket&gt;\/&lt;training_name&gt;\/output\/model.tar.gz<\/code>, they are now stored unzipped as <code>s3:\/\/&lt;default_bucket&gt;\/&lt;training_name&gt;\/model\/model\/1\/saved_model.pb<\/code> along with the <em>variables and assets folder<\/em>. Because of this, <code>estimator.deploy()<\/code> call fails as it is unable to find the artifacts in the <em>output\/<\/em> directory.<\/p>\n<pre><code>Sagemaker Python SDK - 2.6.0\n<\/code><\/pre>\n<p>Estimator code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='autoencoder-model.py',\n                       role=role,\n                       instance_count=1,\n                       instance_type='ml.m5.large',\n                       framework_version=&quot;2.3.0&quot;,\n                       py_version=&quot;py37&quot;,\n                       debugger_hook_config=False,\n                       hyperparameters={'epochs': 20},\n                       source_dir='\/home\/ec2-user\/SageMaker\/model',\n                       subnets=['subnet-1', 'subnet-2'],\n                       security_group_ids=['sg-1', 'sg-1'])\n<\/code><\/pre>\n<p>What could I be doing wrong here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-09-16 03:38:39.303 UTC",
        "Question_favorite_count":2.0,
        "Question_score":4,
        "Question_tags":"python|tensorflow|amazon-s3|keras|amazon-sagemaker",
        "Question_view_count":2670,
        "Owner_creation_date":"2015-06-14 15:03:55.573 UTC",
        "Owner_last_access_date":"2022-05-16 16:38:22.99 UTC",
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":453,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":90,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"\"Your invocation timed out while waiting for a response from container primary\". What does this error mean?",
        "Question_body":"<p>I have a semantic segmentation model which I deployed on ml.m4.xlarge I am using invoke_endpoint from inside an AWS Lambda function using the following bit of code.<\/p>\n<pre><code>with open('\\tmp\\image.jpg', 'rb') as imfile:\n    imbytes = imfile.read()\n\nresponse = runtime.invoke_endpoint(EndpointName = 'xyx', ContentType = 'image\/jpeg',\n                                   Body = imbytes)\n<\/code><\/pre>\n<p>This is when I get the error as mentioned above<\/p>\n<pre><code>Your invocation timed out while waiting for a response from container primary\n<\/code><\/pre>\n<p>Does it mean my datapoint is reaching the model endpoint but it's taking too long to do the inference or is my data not even transferring over to the endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-18 19:46:43.973 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|deployment|amazon-sagemaker",
        "Question_view_count":923,
        "Owner_creation_date":"2017-07-02 18:59:25.057 UTC",
        "Owner_last_access_date":"2022-09-22 19:21:39.343 UTC",
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-18 21:02:20.063 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"unicode error during model inference in Sagemaker notebook",
        "Question_body":"<p>I am doing inference on a model trained in the sagemaker notebook. I am getting Unicode error while passing the input.<\/p>\n<p>Before deploying, I tried the following and it worked - process the text with input_fn and then pass its output to predict_fn for prediction. But I am facing issue when I use the deploy fn of the sagemaker endpoint.  How can I resolve this.<\/p>\n<pre><code>input_text = &quot;BACKGROUND: COVID-19 is associated with pulmonary embolism (PE) in adults.&quot;\ndeployment.predict(json.dumps({&quot;data&quot;:input_text}))\n<\/code><\/pre>\n<p>Error\n<code>Traceback (most recent call last):   File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_functions.py&quot;, line 93, in wrapper     return fn(*args, **kwargs)   File &quot;\/opt\/ml\/code\/train_nmf.py&quot;, line 311, in input_fn     input_data = json.loads(serialized_input_data)   File &quot;\/miniconda3\/lib\/python3.7\/json\/__init__.py&quot;, line 343, in loads     s = s.decode(detect_encoding(s), 'surrogatepass')<\/code><\/p>\n<p>Training in Sagemaker Notebook<\/p>\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'train_nmf.py'\n\nsklearn = SKLearn(\n    entry_point=script_path,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    framework_version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    role=role,\n    sagemaker_session=sagemaker_session,\n    output_path=output_data_uri,\n    code_location=training_desc_uri,\n    source_dir='\/home\/ec2-user\/SageMaker\/src')\n<\/code><\/pre>\n<p>Train NMF Code<\/p>\n<pre><code>import os\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport json\nCONTENT_TYPE_JSON = &quot;application\/json&quot; \n\ndef process_text(text):\n    text = [each.lower() for each in text]\n    return text\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n    model = joblib.load(os.path.join(model_dir, &quot;nmf_model.pkl&quot;))\n    return model\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    predicted_topics = model.transform(input_data)\n    return predicted_topics\n\ndef input_fn(serialized_input_data, model_dir, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    input_text_processed = pd.Series(input_data).apply(process_text)\n    tf_idf_model = joblib.load(os.path.join(model_dir, &quot;tf_idf.pkl&quot;))\n    processed_sample_text = tf_idf_model.transform(input_text_processed)\n    return processed_sample_text\n\ndef output_fn(prediction_output, model_dir, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        topic_keywords = joblib.load(\n            os.path.join(model_dir, &quot;topic_keywords.pkl&quot;)\n        )\n        pred_dominant_topic = np.argmax(prediction_output, axis=1)\n        pred_df = pd.DataFrame(prediction_output, columns=topic_keywords)\n        pred_df[&quot;dominant_topic&quot;] = pred_dominant_topic\n        return json.dumps(pred_df.to_dict(&quot;records&quot;)), accept\n    raise Exception('Unsupported content type')\n    \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-30 12:17:25.433 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|scikit-learn|amazon-sagemaker|topic-modeling",
        "Question_view_count":82,
        "Owner_creation_date":"2020-01-11 08:29:02.73 UTC",
        "Owner_last_access_date":"2022-09-23 19:27:27.437 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"data format to predict with model fitted via Sagemaker's XGBoost built-in algorithm and training container",
        "Question_body":"<p>Looking at the following code, taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a>, I wonder what format dtest is (sorry I could not gleen this from the post):<\/p>\n<pre><code>import pickle as pkl \nimport tarfile\n\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel = pkl.load(open(model_file_path, 'rb'))\n\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>\n<p>In my case the training and validation data are in csv format coming from a S3 bucket:<\/p>\n<pre><code>content_type = &quot;csv&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\n<\/code><\/pre>\n<p>So ideally, I would also like to use the same format for scoring\/prediction\/inference.<\/p>\n<p>PS:<\/p>\n<p>This little function appears to work fine:<\/p>\n<pre><code>def write_prediction_data(data_file_name, target_name, model_file_name, output_file_name):\n\n    model = pkl.load(open(model_file_name, 'rb'))\n    data = pd.read_csv(data_file_name) \n    target = data[target_name]\n    data = data.drop([target_name], axis=1)\n    xgb_data = xgb.DMatrix(data.values, target.values)\n\n    data = pd.read_csv(data_file_name)\n    data['Prediction'] = model.predict(xgb_data)\n\n    data.to_csv(output_file_name, index=False)\n<\/code><\/pre>\n<p>Improvement suggestions always welcome (-:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-29 07:25:35.783 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-30 10:05:38.547 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can not pull public ECR image using AWS SageMaker Estimator",
        "Question_body":"<p>I would like to pass a public Docker Image on ECR through the image_uri parameter to the Estimator. However, the training job fails to find this image.\nI'm running my code using AWS SageMaker Notebook instance.<\/p>\n<pre><code>from sagemaker.estimator import Estimator\n\nbyoc_image_uri = 'public.ecr.aws\/v8x6t9d9\/train-image-classifier-firefly'\n\nestimator = Estimator(image_uri=byoc_image_uri,\n                      role=role,\n                      base_job_name='tf-custom-container-test-job',\n                      instance_count=1,\n                      hyperparameters=hyperparameters,\n                      output_path=output_path,\n                      instance_type='ml.p2.xlarge')\n<\/code><\/pre>\n<p>What I have tried:<\/p>\n<ul>\n<li>I first tried to use the same image on an ECR privet repo. This works<\/li>\n<li>Then pushed the same image to a public ECR repo that I had created. The image was successfully pushed and I could pull this image successfully.<\/li>\n<li>However, when I tried to use the same image url through Estimator image_uri parameter. I get the following error message<\/li>\n<\/ul>\n<pre><code>    ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: TrainingImageConfig with TrainingRepositoryAccessMode set to VPC must be provided when using a training image from a private Docker registry. Please provideTrainingImageConfig and TrainingRepositoryAccessMode set to VPC when using a training image from a private Docker registry.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-16 01:39:49.253 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr|amazon-ecr-public",
        "Question_view_count":396,
        "Owner_creation_date":"2021-09-16 01:22:12.637 UTC",
        "Owner_last_access_date":"2021-10-01 23:54:42.837 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-09-24 22:08:37.307 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Question_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-03 13:01:49.37 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":72,
        "Owner_creation_date":"2020-11-29 09:28:19.553 UTC",
        "Owner_last_access_date":"2022-06-03 12:16:49.767 UTC",
        "Owner_location":null,
        "Owner_reputation":371,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Answer_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-04 01:59:22.457 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-05-03 13:13:58.13 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"aws sagemaker giving The model data archive is too large. Please reduce the size of the model data archive",
        "Question_body":"<p>I am using aws sagemaker to deploy a model whose generated artifacts are huge. The compressed size is about 80GB. Deploying on sage maker on a ml.m5.12xlarge instance is throwing this error while deploying to the endpoint\n <code>The model data archive is too large. Please reduce the size of the model data archive or move to an instance type with more memory.<\/code><\/p>\n\n<p>I found that aws attaches EBS volume based on instance size(<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html<\/a>)  and i couldnot find anything more that 30Gb here. Should i go with a multi model endpoint here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-05 12:46:49.263 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"training-data|amazon-sagemaker",
        "Question_view_count":353,
        "Owner_creation_date":"2013-10-10 06:40:54.25 UTC",
        "Owner_last_access_date":"2022-02-23 12:44:39.17 UTC",
        "Owner_location":null,
        "Owner_reputation":582,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Deepracer log_analysis tool - sagemaker role errors",
        "Question_body":"<p>I'm trying to run the Deepracer log analysis tool from <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a> on my local laptop. However I get below error while trying to run step [5] \"Create an IAM role\". <\/p>\n\n<pre><code>try:\n    sagemaker_role = sagemaker.get_execution_role()\nexcept:\n    sagemaker_role = get_execution_role('sagemaker')\n\nprint(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nCouldn't call 'get_role' to get Role ARN from role name arn:aws:iam::26********:root to get Role path.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      1 try:\n----&gt; 2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in get_execution_role(sagemaker_session)\n   3302     )\n-&gt; 3303     raise ValueError(message.format(arn))\n   3304 \n\nValueError: The current AWS identity is not a role: arn:aws:iam::26********:root, therefore it cannot be used as a SageMaker execution role\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n----&gt; 4     sagemaker_role = get_execution_role('sagemaker')\n      5 \n      6 print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nNameError: name 'get_execution_role' is not defined\n<\/code><\/pre>\n\n<p>Does anybody know what needs to be done to execute above code without errors? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-27 19:23:39.477 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|log-analysis",
        "Question_view_count":491,
        "Owner_creation_date":"2014-07-01 15:16:03.68 UTC",
        "Owner_last_access_date":"2022-08-03 13:15:26.86 UTC",
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":1807,
        "Owner_up_votes":303,
        "Owner_down_votes":2,
        "Owner_views":207,
        "Answer_body":"<p>AWS support recommended below solution:<\/p>\n\n<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws\/sagemaker-python-sdk#300 (comment)<\/p>\n\n<p>The steps in the work-around given in the above link are:<\/p>\n\n<ol>\n<li><p>Login to the AWS console -> IAM -> Roles -> Create Role<\/p><\/li>\n<li><p>Create an IAM role and select the \"SageMaker\" service<\/p><\/li>\n<li><p>Give the role \"AmazonSageMakerFullAccess\" permission<\/p><\/li>\n<li><p>Review and create the role<\/p><\/li>\n<li><p>Next, also attach the \"AWSRoboMakerFullAccess\" permission policy to the above created role (as required in the Github notebook [1]).<\/p><\/li>\n<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:<\/p><\/li>\n<\/ol>\n\n<pre><code>try:\n   sagemaker_role = sagemaker.get_execution_role()\n except ValueError:\n   iam = boto3.client('iam')\n   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']\n<\/code><\/pre>\n\n<p>In the above snippet, replace the \"\" text with the IAM role name created in Step 4.<\/p>\n\n<p>References:<\/p>\n\n<p>[1] <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a><\/p>\n\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html<\/a><\/p>\n\n<p>[3] aws\/sagemaker-python-sdk#300<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-01 17:00:37.23 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Is there a way to export or view a classifier created in sagemaker so that we can see what weights\/constants are used in model evaluation",
        "Question_body":"<p>I created a simple linear learner model using sagemaker, and although I can deploy it on a test data set, I would like to be able to get the actual equation that the model uses to classify values (ie for linear regression the equation of the line).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-03 19:43:35.333 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2015-10-31 23:05:25.74 UTC",
        "Owner_last_access_date":"2020-12-27 15:50:38.097 UTC",
        "Owner_location":"Pristina",
        "Owner_reputation":129,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Cannot upload data from pandas data-frame to AWS athena table due to 'import package error'",
        "Question_body":"<p>When I am trying to import the awswrangler package in sagemaker I am getting the below error<\/p>\n\n<pre><code>import awswrangler as aws\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<pre><code>ImportError                               Traceback (most recent call last)\n&lt;ipython-input-9-cc67bb4c1dd7&gt; in &lt;module&gt;\n----&gt; 1 import awswrangler as aws\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/__init__.py in &lt;module&gt;\n     15 from awswrangler.emr import EMR  # noqa\n     16 from awswrangler.glue import Glue  # noqa\n---&gt; 17 from awswrangler.pandas import Pandas  # noqa\n     18 from awswrangler.redshift import Redshift  # noqa\n     19 from awswrangler.s3 import S3  # noqa\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/pandas.py in &lt;module&gt;\n     17 from boto3 import client  # type: ignore\n     18 from botocore.exceptions import ClientError, HTTPClientError  # type: ignore\n---&gt; 19 from pandas.io.common import infer_compression  # type: ignore\n     20 from pyarrow import parquet as pq  # type: ignore\n     21 from s3fs import S3FileSystem  # type: ignore\n\nImportError: cannot import name 'infer_compression'\n<\/code><\/pre>\n\n<p>Hoe to fix this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2020-03-14 15:13:04.503 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|amazon-athena|amazon-sagemaker",
        "Question_view_count":1470,
        "Owner_creation_date":"2017-11-05 08:39:13.69 UTC",
        "Owner_last_access_date":"2022-09-22 17:02:39.26 UTC",
        "Owner_location":null,
        "Owner_reputation":731,
        "Owner_up_votes":37,
        "Owner_down_votes":3,
        "Owner_views":185,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-03-15 01:42:36.403 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Tensorflow Serving Error: Tensor name: <> has no shape information",
        "Question_body":"<p>When I am preparing a tensorflow model on my system and loading the saved model for prediction, I am getting prediction output<\/p>\n<pre><code>from tensorflow.contrib import predictor\nloaded = predictor.from_saved_model(export_path + &quot;\/1641219436&quot;)\npayload = b&quot;\\n\\xbd\\x05\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\xde\\x01\\n\\x0bsegment_ids\\x12\\xce\\x01\\x1a\\xcb\\x01\\n\\xc8\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\xdd\\x01\\n\\ninput_mask\\x12\\xce\\x01\\x1a\\xcb\\x01\\n\\xc8\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\xe5\\x01\\n\\tinput_ids\\x12\\xd7\\x01\\x1a\\xd4\\x01\\n\\xd1\\x01e\\x89\\x10\\xe8\\x0f\\xa2'\\x93\\x10\\xc3\\x10\\x85E\\xde\\xdb\\x01\\xdf\\x0ff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00&quot;\npredictions = loaded({&quot;example&quot;:[payload]})\npredictions\n\nINFO:tensorflow:Restoring parameters from \/content\/bert-multiclass-final\/\/1641219436\/variables\/variables\nINFO:tensorflow:Restoring parameters from \/content\/bert-multiclass-final\/\/1641219436\/variables\/variables\n{'labels': 0,\n 'probabilities': array([[-0.00980266, -5.696986  , -5.606253  , -5.9056935 ]],\n       dtype=float32)}\n<\/code><\/pre>\n<p>However, when I am building the exact same model on Sagemaker Tensorflow and deploying it I am getting the following error when I am trying to make a prediction<\/p>\n<pre><code>estimator = TensorFlow(\n    source_dir= &quot;code&quot;,\n    entry_point=&quot;tf-train.py&quot;,\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.p3.2xlarge&quot;,\n    framework_version=&quot;1.15.2&quot;,\n    py_version=&quot;py37&quot;,\n)\n\nexample = b&quot;\\n\\xbd\\x05\\n\\xe5\\x01\\n\\tinput_ids\\x12\\xd7\\x01\\x1a\\xd4\\x01\\n\\xd1\\x01e\\x89\\x10\\xe8\\x0f\\xa2'\\x93\\x10\\xc3\\x10\\x85E\\xde\\xdb\\x01\\xdf\\x0ff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\xdd\\x01\\n\\ninput_mask\\x12\\xce\\x01\\x1a\\xcb\\x01\\n\\xc8\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\xde\\x01\\n\\x0bsegment_ids\\x12\\xce\\x01\\x1a\\xcb\\x01\\n\\xc8\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x00&quot;\ninput = {\n     &quot;signature_name&quot;: &quot;serving_default&quot;,\n     &quot;instances&quot;: [{'example':{'b64': base64.b64encode(example).decode('utf-8')}}]\n}\ncheck = predictor.predict(input)\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{ &quot;error&quot;: &quot;Tensor name: labels has no shape information &quot; }&quot;.\n<\/code><\/pre>\n<p>Not sure if this is a tensorflow or sagemaker specific error :(<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-01-03 14:33:07.623 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-sagemaker|tensorflow-serving",
        "Question_view_count":64,
        "Owner_creation_date":"2014-03-21 18:42:50.983 UTC",
        "Owner_last_access_date":"2022-05-10 20:13:57.157 UTC",
        "Owner_location":"India",
        "Owner_reputation":1746,
        "Owner_up_votes":85,
        "Owner_down_votes":1,
        "Owner_views":275,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to get the Endpoint for a model built in my machine",
        "Question_body":"<p>I have built a model in my machine and want to deploy the model (catboost classifier) in the amazon sagemaker. Not able to get the endpoint of the model. Looking for a code\/ process to get the end point. I tried with deploy function, but it is not giving the endpoint, the message is deploy function don't exist for catboostclassifier()<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 02:04:57.11 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|deployment|amazon-sagemaker|endpoint|catboost",
        "Question_view_count":34,
        "Owner_creation_date":"2022-09-15 01:56:16.433 UTC",
        "Owner_last_access_date":"2022-09-20 19:15:06.92 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to delete worker task templates in AWS Augmented AI?",
        "Question_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-15 09:09:10.363 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":38,
        "Owner_creation_date":"2018-07-24 08:52:28.877 UTC",
        "Owner_last_access_date":"2020-11-27 01:51:12.017 UTC",
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-22 23:37:30.403 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Python Notebook Invoke Endpoint Sagemaker from Local",
        "Question_body":"<p>I am trying to invoke an Amazon Sagemaker Endpoint from a local python notebook. This is the code I am using.  <\/p>\n\n<pre><code>import boto3\n\naws_access_key_id = '...............'\naws_secret_access_key = '................'\ntkn = '..........'\nregion_name = '............'\n\namz = boto3.client('sagemaker-runtime',\n                   aws_access_key_id=aws_access_key_id,\n                   aws_secret_access_key=aws_secret_access_key,\n                   aws_session_token=tkn,\n                   region_name=region_name)\n\n\nresponse = amz.invoke_endpoint(\n    EndpointName='mymodel',\n    Body=b'bytes'\n)               \n<\/code><\/pre>\n\n<p>However, this doesn't work. Do I have to specify something else in <em>Body<\/em> ?   <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-15 08:19:27.617 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":3972,
        "Owner_creation_date":"2015-05-26 22:53:10.12 UTC",
        "Owner_last_access_date":"2019-06-03 07:51:39.51 UTC",
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"ImportError libopenblasp-r0 cannot open shared object file No such file or directory -SageMaker",
        "Question_body":"<p>I am trying train my model code using Docker Container - AWS SageMaker using following code.<\/p>\n\n<pre>\n    'https:\/\/github.com\/awslabs\/amazon-sagemaker- \nexamples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb'\n<\/pre>\n\n<p>But I get below error when I Try to train my model using <\/p>\n\n<pre><code> tree.fit(data_location)\n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n    File \"\/opt\/program\/train\", line 17, in &lt;module&gt;\n     from sklearn import tree\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/__init__.py\", line \n      64, in &lt;module&gt;\n     from .base import clone\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/base.py\", line \n       13, in &lt;module&gt;\n      from .utils.fixes import signature\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/sklearn\/utils\/__init__.py\", line 16, in &lt;module&gt;\n      from .fixes import _Sequence as Sequence\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/utils\/fixes.py\", \n       line 85, in &lt;module&gt;\n      from scipy.special import boxcox  # noqa\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/scipy\/special\/__init__.py\", line 641, in &lt;module&gt;\n       from ._ufuncs import *\n    ImportError: libopenblasp-r0-8dca6697.3.0.dev.so: cannot open shared \n       object file: No such file or directory\n<\/code><\/pre>\n\n<p>error message 2<\/p>\n\n<pre><code>   Error for Training job decision-trees-sample-2019-01-18-07-44-37-282: Failed Reason: AlgorithmError: Exit Code: 1\n<\/code><\/pre>\n\n<p>I wend to the directory and did not find 'sklearn' directory.<\/p>\n\n<pre><code>  sh-4.2$ pwd\n    \/usr\/local\/lib\/python2.7\/dist-packages\n  sh-4.2$ ls -l\n    total 3244\n  -rwxr-xr-x 1 root root 3318568 Sep 18 03:23 cv2.so\n<\/code><\/pre>\n\n<p>My current jupyter notebook points to root environment and it has sklearn package available , not sure how make it available in above location where I see error, not sure if this is what will resolve the issue  or something else needs to be done.<\/p>\n\n<p>I am new to Amazon SageMaker.<\/p>\n\n<p>Expected result: I am expecting the training job to complete without error<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-18 07:19:02.457 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|python-2.7|docker|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1661,
        "Owner_creation_date":"2014-12-31 03:46:45.11 UTC",
        "Owner_last_access_date":"2022-09-13 06:55:27.303 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1050,
        "Owner_up_votes":749,
        "Owner_down_votes":10,
        "Owner_views":168,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-01-18 07:49:29.223 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker delete Models and Endpoint configurations with python API",
        "Question_body":"<p>I've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before I realized that changes do not get applied unless you also delete the corresponding Model and Endpoint configuration so that new ones can be created with that name. <\/p>\n\n<p>Is there a way with the sagemaker python api to delete all three instead of just the endpoint?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-21 01:07:49.467 UTC",
        "Question_favorite_count":1.0,
        "Question_score":4,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":4760,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/647\" rel=\"nofollow noreferrer\" title=\"sagemaker-python-sdk\/pull\/647\">this<\/a> pull request. <\/p>\n\n<p>For the time being Amazon's only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\" rel=\"nofollow noreferrer\" title=\"docs.aws.amazon.com\/sagemaker\">recommendation<\/a> is to delete everything via the console. <\/p>\n\n<p>If this is critical to your system you can probably manage everything via Cloud Formation and create\/delete services containing your Sagemaker models and endpoints.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-02-21 01:54:42.127 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"debug and deploy featurizer (data processor for imodel inference) of sagemaker endpoint",
        "Question_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/sklearn_abalone_featurizer.py\" rel=\"nofollow noreferrer\">this example<\/a> to implement the data processing of incoming raw data for a sagemaker endpoint prior to model inference\/scoring. This is all great but I have 2 questions:<\/p>\n<ul>\n<li>How can one debug this (e.g can I invoke endpoint without it being exposed as restful API and then use Sagemaker debugger)<\/li>\n<li>Sagemaker can be used &quot;remotely&quot; - e.g. via VSC. Can such a script be uploaded programatically?<\/li>\n<\/ul>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 07:37:20.2 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker|scikit-learn-pipeline",
        "Question_view_count":33,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>Sagemaker Debugger is only to monitor the training jobs.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html<\/a><\/p>\n<p>I dont think you can use it on Endpoints.<\/p>\n<p>The script that you have provided is used both for training and inference. The container used by the estimator will take care of what functions to run. So it is not possible to debug the script directly. But what are you debugging in the code ? Training part or the inference part ?<\/p>\n<p>While creating the estimator we need to give either the entry_point or the source directory. If you are using the &quot;entry_point&quot; then the value should be relative path to the file, if you are using &quot;source_dir&quot; then you should be able to give an S3 path. So before running the estimator, you can programmatically tar the files and upload it to S3 and then use the S3 path in the estimator.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2022-04-29 02:39:16.773 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Propagate the error in sagemaker from ProcessingStep to Worflow functions",
        "Question_body":"<p>So I have a sagemaker worflow composed of multiple processing steps, training steps ...<\/p>\n<p>I'm trying to change the <code>FailureReason<\/code> of the <code>describe_pipeline_execution()<\/code> to return useful information concerning the failure of my Sagemaker pipeline.<\/p>\n<p>Indeed in my processing step and in my training step, having their code inside a docker container,  I want the pipeline to fail for various reasons. In the documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html\" rel=\"nofollow noreferrer\">here<\/a> for processing step and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">here<\/a> for training step, it is possible to return something by writing it in a file during the step through the functions <code>DescribeProcessingJob<\/code> or <code>DescribeTrainingJob<\/code>.<\/p>\n<p>I was wondering if there is a path like <code>\/opt\/ml\/failure<\/code> to put the output of my process so the function <code>describe_pipeline_execution()<\/code> can return what I want. Can't find it in the documentation!<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-02 13:03:03.17 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":152,
        "Owner_creation_date":"2022-02-02 12:51:02.047 UTC",
        "Owner_last_access_date":"2022-09-23 15:29:50.04 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Setup env variable in aws SageMaker container (bring your own container)",
        "Question_body":"<p>We are using aws sagemaker that is using ecs container, Is there a way, we can setup environment variable (e.g. stage or prod) in container when calling sagemaker api using low level python sdk <\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-06 17:16:18.177 UTC",
        "Question_favorite_count":1.0,
        "Question_score":8,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":8657,
        "Owner_creation_date":"2013-01-08 21:50:01.92 UTC",
        "Owner_last_access_date":"2022-01-06 19:13:19.32 UTC",
        "Owner_location":"San Francisco",
        "Owner_reputation":750,
        "Owner_up_votes":222,
        "Owner_down_votes":2,
        "Owner_views":64,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"ValueError: Cannot Convert String to Float With Pandas and Amazon Sagemaker",
        "Question_body":"<p>I'm trying to deploy a simple ML model on SageMaker to get the hang of it, and I am not having any luck because I get the following error:  <\/p>\n\n<pre><code>ValueError: could not convert string to float: '6.320000000000000097e-03 1.800000000000000000e+01 2.310000000000000053e+00 0.000000000000000000e+00 5.380000000000000338e-01 6.575000000000000178e+00 6.520000000000000284e+01 4.089999999999999858e+00 1.000000000000000000e+00 2.960000000000000000e+02 1.530000000000000071e+01 3.968999999999999773e+02 4.980000000000000426e+00 2.400000000000000000e+01'\n<\/code><\/pre>\n\n<p>This is the first row of my dataframe.  <\/p>\n\n<p>This is the code in my notebook that I'm using right now:<\/p>\n\n<pre><code>from sagemaker import get_execution_role, Session\nfrom sagemaker.sklearn.estimator import SKLearn\nwork_dir = 'data'\nsession  = Session()\nrole     = get_execution_role()\ntrain_input = session.upload_data('data')\nscript      = 'boston_housing_prep.py'\n\nmodel = SKLearn(\nentry_point         = script,\ntrain_instance_type = 'ml.c4.xlarge',\nrole                = role,\nsagemaker_session   = session,\nhyperparameters     = {'alpha': 10}\n)\n\nmodel.fit({'train': train_input})\n<\/code><\/pre>\n\n<p>My script for boston_housing_prep.py looks like this:<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--alpha', type=int, default=1)\n\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n    if len(input_files) == 0:\n        raise ValueError(('There are no files in {}.\\n' +\n                      'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                      'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                      'does not have permission to access the data.').format(args.train, \"train\"))\n    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n    df       = pd.concat(raw_data)\n\n    y_train = df.iloc[:, -1]\n    X_train = df.iloc[:, :5]\n\n    scaler  = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    alpha = args.alpha\n\n    clf = Ridge(alpha=alpha)\n    clf = clf.fit(X_train, y_train)\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<p>The line that's giving the problem is this one: <\/p>\n\n<pre><code>X_train = scaler.fit_transform(X_train)\n<\/code><\/pre>\n\n<p>I tried <code>df = df.astype(np.float) <\/code> after I loaded in the df, but that didn't work either.<\/p>\n\n<p>This file loads in without a problem when I'm not in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2019-06-10 08:05:10.92 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"pandas|numpy|scikit-learn|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_date":"2014-09-15 23:32:32.337 UTC",
        "Owner_last_access_date":"2022-09-23 22:20:44.79 UTC",
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":3257,
        "Owner_up_votes":451,
        "Owner_down_votes":0,
        "Owner_views":319,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-06-10 10:13:03.33 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker Pipelines not being triggered by EventBridge",
        "Question_body":"<p>I've created a new SageMaker pipeline using AWS Python SDK, and everything is working fine, I can trigger my pipeline and it works perfectly using the SDK with these simples commands:<\/p>\n<pre><code>pipeline.upsert(role_arn=get_execution_role())\nexecution = pipeline.start()\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I would like to schedule the pipeline execution to run every day during the morning (let's say 8 a.m for example). And here's my problem. I configured the EventBridge as shown in this tutorial: <a href=\"https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines<\/a>, but instead of creating a new role, I used an existing one (the same returned from the command get_execution_role() above). My event is triggered in the correct hour (every day at 8 am), but the pipeline doesn't execute. When checking the logs on Cloud Watch, It shows that I got a FailedInvocations for the event, but I don't know how to get the logs from this failed execution. I tried to search on cloud trail but don't found nothing.<\/p>\n<p>Anyone could help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-11 15:27:24.973 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":470,
        "Owner_creation_date":"2017-08-02 12:56:04.607 UTC",
        "Owner_last_access_date":"2022-09-25 04:34:07.277 UTC",
        "Owner_location":"Tup\u00e3, SP, Brasil",
        "Owner_reputation":121,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Passing an image through a Lambda function to invoke a sagemaker endpoint",
        "Question_body":"<p>im trying to invoke a sagemaker endpoint passing in the image from a POST request using this lambda function but whenever i make the request using postman i get this error<\/p>\n<pre><code>{\n&quot;message&quot;: &quot;Could not parse request body into json: Could not parse payload into json: Unexpected character (\\'\/\\' (code 47)): maybe a (non-standard) comment? (not recognized as one since Feature \\'ALLOW_COMMENTS\\' not enabled for parser)\\n at [Source: (byte[])\\&quot;\/9j\/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL\/wAAUCAEAAQAEABEAAREAAhEAAxEA\/8QAGwAAAwEBAAMAAAAAAAAAAAAAAAECAwQFBgf\/xAAwEAACAgEEAgEEAQQDAAMBAQAAAQIRIQMSMUEiUWETMnGBkQQjQqGxwdEz8PFS4f\/aAA4EAAABAAIAAwAAPwD5\/Bf3MnpKj\/cyehKPlZ9\/NJxSzRcoqMW0htJAT9Zvxr\/ZL1W4pdsFK8ATmLTwKKyn0L9IC4yc3Zae52WgNn9uTV5gU3aA55OMZXRzzpO3VGUqTQF\/VlJY4K+o2sFN4Azbblkh23chctNgbx4wbrhMvL\/AFRbcPwUncR\/40gJhPdK\/REZbpEp5AU3JWmKTkmD3LkDXQ1Nqt8FaU68ioTrLAnUlukmhaktzQpPdICO38Ci8tPoXDAmdxVoJ\\&quot;[truncated 15352 bytes]; line: 1, column: 2]&quot;\n<\/code><\/pre>\n<p>}<\/p>\n<p>here's my lambda function<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    \n    data = json.loads(json.dumps(event))\n    payload = base64.b64encode(data['data-binary'])\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read())\n    \n    return result\n<\/code><\/pre>\n<p>can you show me how to properly implement this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-10 05:28:51.317 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"image|api|aws-lambda|aws-api-gateway|amazon-sagemaker",
        "Question_view_count":162,
        "Owner_creation_date":"2021-05-14 08:00:38.513 UTC",
        "Owner_last_access_date":"2022-03-05 23:59:44.123 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sending CSV records to Amazon Sagemaker through lambda function",
        "Question_body":"<p>I am looking to send CSV records coming from kinesis analytics to sagemaker endpoint and getting an inference through a lambda function and then passing it on to a firehose API to dump it into S3. But the data is not getting into sagemaker for some reason.<\/p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>'use strict';\nconsole.log('Loading function');\nvar AWS = require('aws-sdk');\nvar sagemakerruntime = new AWS.SageMakerRuntime({apiVersion: '2017-05-13'});\nvar firehose = new AWS.Firehose({apiVersion: '2015-08-04'});\nexports.handler = (event, context, callback) =&gt; {\n    let success = 0;\n    let failure = 0;\n    const output = event.records.map((record) =&gt; {\n        \/* Data is base64 encoded, so decode here *\/\n        const recordData = Buffer.from(record.data, 'base64');\n        try {\n            var params = {\n                Body: new Buffer('...') || recordData \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n                EndpointName: 'String', \/* required *\/\n                Accept: 'text\/csv',\n                ContentType: 'text\/csv'\n            };\n            sagemakerruntime.invokeEndpoint(params, function(err, data) {\n                var result1;\n                if (err) console.log(err, err.stack); \/\/ an error occurred\n                else     console.log(data);           \/\/ successful response\n                result1=data;\n                var params = {\n                    DeliveryStreamName: 'String', \/* required *\/\n                    Record: { \/* required *\/\n                        Data: new Buffer('...') || result1 \/* Strings will be Base-64 encoded on your behalf *\/ \/* required *\/\n                    }\n                };\n                firehose.putRecord(params, function(err, data) {\n                    if (err) console.log(err, err.stack); \/\/ an error occurred\n                    else     console.log(data);           \/\/ successful response\n                });\n            });\n            success++;\n            return {\n                recordId: record.recordId,\n                result: 'Ok',\n            };\n        } catch (err) {\n            failure++;\n            return {\n                recordId: record.recordId,\n                result: 'DeliveryFailed',\n            };\n        }\n    });\n    console.log(`Successful delivered records ${success}, Failed delivered records ${failure}.`);\n    callback(null, {\n        records: output,\n    });\n};<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2018-06-20 11:47:36.51 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"javascript|aws-lambda|amazon-sagemaker",
        "Question_view_count":351,
        "Owner_creation_date":"2018-06-20 11:39:09.78 UTC",
        "Owner_last_access_date":"2018-08-09 09:03:43.103 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2018-06-20 11:49:09.167 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"No space left on device: Amazon Sagemaker can't save more files in directory even when HDD is not full",
        "Question_body":"<p>I'm getting the OS error: No space left on device, when saving somewhere north of 17 million small files into a single directory on Amazon Sagemaker local storage. I'm using the <code>numpy.save<\/code> function, python 3.8.12. <code>df -h<\/code> shows that the drive is only about 80% full. <code>cat \/proc\/sys\/fs\/file-max<\/code> returns 6,269,329, which is a lot less than the number of saved files, <code>df -i<\/code> returns 28% IUse% for the parent folder. What could be the problem here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2022-01-28 20:46:25.627 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":181,
        "Owner_creation_date":"2016-12-04 15:24:36.473 UTC",
        "Owner_last_access_date":"2022-05-31 13:32:14.053 UTC",
        "Owner_location":"Europe",
        "Owner_reputation":333,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMager Studio Notebook Kernel keeps starting",
        "Question_body":"<p>Trying to execute cells in an Amazon SageMager Studio Notebook I continuously receive the message &quot;Note: The kernel is still starting. Please execute this cell again after the kernel is started.&quot; The bottom status bar claims &quot;Kernel: Starting...&quot; The &quot;Running Terminals and Kernels&quot; overview shows a running instance ml.t3.medium with running app datascience-1.0 and kernel session corresponding to the notebook title. I tried restarting SageMaker Studio and opened it in another region but neither helped.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-11 15:57:01.243 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":661,
        "Owner_creation_date":"2012-03-17 14:47:44.973 UTC",
        "Owner_last_access_date":"2022-08-24 18:17:30.443 UTC",
        "Owner_location":null,
        "Owner_reputation":377,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Pytorch lightning progress bar not working in AWS Sagemaker Jupyter Lab",
        "Question_body":"<p>We started using Sagemaker Jupyter Lab to run a few Depp Learning experiments we previously ran on GoogleColabPro+. The training starts fine and everything seems to work, however, the progress bar appears as follows:<\/p>\n<p><strong>Validation sanity check: 0it [00:00, ?it\/s]\nTraining: 0it [00:00, ?it\/s]<\/strong><\/p>\n<p>The progress bar was working fine on GoogleColab. I tried uninstalling ipywidgets as <a href=\"https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/11208\" rel=\"nofollow noreferrer\">suggested here<\/a>, but still no luck. Anyone has an idea of how to fix the problem?<\/p>\n<p>Below you will find a copy of the TrainerFunction I am using.<\/p>\n<pre><code>class T5FineTuner(pl.LightningModule):\n    def __init__(self, hparams):\n        super(T5FineTuner, self).__init__()\n        self.hparams = hparams\n\n        self.model = T5ForConditionalGeneration.from_pretrained(hparams['model_name_or_path'])\n        self.tokenizer = T5Tokenizer.from_pretrained(hparams['tokenizer_name_or_path'])\n\n    def hparams(self):\n        return self.hparams\n\n    def is_logger(self):\n        return True #self.trainer.proc_rank &lt;= 0\n\n    def forward(\n            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n    ):\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n        )\n\n    def _step(self, batch):\n        labels = batch[&quot;target_ids&quot;]\n        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(\n            input_ids=batch[&quot;source_ids&quot;],\n            attention_mask=batch[&quot;source_mask&quot;],\n            labels=labels,\n            decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n\n        tensorboard_logs = {&quot;train_loss&quot;: loss}\n        return {&quot;loss&quot;: loss, &quot;log&quot;: tensorboard_logs}\n\n    def training_epoch_end(self, outputs):\n        avg_train_loss = torch.stack([x[&quot;loss&quot;] for x in outputs]).mean()\n        tensorboard_logs = {&quot;avg_train_loss&quot;: avg_train_loss}\n        # return {&quot;avg_train_loss&quot;: avg_train_loss, &quot;log&quot;: tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        return {&quot;val_loss&quot;: loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[&quot;val_loss&quot;] for x in outputs]).mean()\n        tensorboard_logs = {&quot;val_loss&quot;: avg_loss}\n        return {&quot;avg_val_loss&quot;: avg_loss, &quot;log&quot;: tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def configure_optimizers(self):\n        &quot;Prepare optimizer and schedule (linear warmup and decay)&quot;\n\n        model = self.model\n        no_decay = [&quot;bias&quot;, &quot;LayerNorm.weight&quot;]\n        optimizer_grouped_parameters = [\n            {\n                &quot;params&quot;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                &quot;weight_decay&quot;: self.hparams['weight_decay'],\n            },\n            {\n                &quot;params&quot;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                &quot;weight_decay&quot;: 0.0,\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams['learning_rate'], eps=self.hparams['adam_epsilon'])\n        self.opt = optimizer\n        return [optimizer]\n\n    def optimizer_step(self, epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None, optimizer_closure=None,  second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n        # if self.trainer.use_tpu:\n        #     xm.optimizer_step(optimizer)\n        # else:\n        optimizer.step(closure=optimizer_closure)\n        optimizer.zero_grad()\n        self.lr_scheduler.step()\n\n    def get_tqdm_dict(self):\n        tqdm_dict = {&quot;loss&quot;: &quot;{:.3f}&quot;.format(self.trainer.avg_loss), &quot;lr&quot;: self.lr_scheduler.get_last_lr()[-1]}\n\n        return tqdm_dict\n\n    def train_dataloader(self):\n        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=&quot;translated_train&quot;, args=self.hparams)\n        dataloader = DataLoader(train_dataset, batch_size=self.hparams['train_batch_size'], drop_last=True, shuffle=True,\n                                num_workers=4)\n        t_total = (\n                (len(dataloader.dataset) \/\/ (self.hparams['train_batch_size'] * max(1, self.hparams['n_gpu'])))\n                \/\/ self.hparams['gradient_accumulation_steps']\n                * float(self.hparams['num_train_epochs'])\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            self.opt, num_warmup_steps=self.hparams['warmup_steps'], num_training_steps=t_total\n        )\n        self.lr_scheduler = scheduler\n        return dataloader\n\n    def val_dataloader(self):\n        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=&quot;test_2k&quot;, args=self.hparams)\n        return DataLoader(val_dataset, batch_size=self.hparams['eval_batch_size'], num_workers=4)\n\nlogger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n    def on_validation_end(self, trainer, pl_module):\n        logger.info(&quot;***** Validation results *****&quot;)\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n            # Log results\n            for key in sorted(metrics):\n                if key not in [&quot;log&quot;, &quot;progress_bar&quot;]:\n                    logger.info(&quot;{} = {}\\n&quot;.format(key, str(metrics[key])))\n\n    def on_test_end(self, trainer, pl_module):\n        logger.info(&quot;***** Test results *****&quot;)\n\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n\n            # Log and save results to file\n            output_test_results_file = os.path.join(pl_module.hparams[&quot;output_dir&quot;], &quot;test_results.txt&quot;)\n            with open(output_test_results_file, &quot;w&quot;) as writer:\n                for key in sorted(metrics):\n                    if key not in [&quot;log&quot;, &quot;progress_bar&quot;]:\n                        logger.info(&quot;{} = {}\\n&quot;.format(key, str(metrics[key])))\n                        writer.write(&quot;{} = {}\\n&quot;.format(key, str(metrics[key])))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-16 11:23:44.6 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"progress-bar|jupyter-lab|amazon-sagemaker|pytorch-lightning",
        "Question_view_count":354,
        "Owner_creation_date":"2021-11-28 17:43:31.363 UTC",
        "Owner_last_access_date":"2022-09-21 16:44:23.063 UTC",
        "Owner_location":"Paris",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Updating lines in Excel when changes to S3 occur",
        "Question_body":"<p>In a project I am running an AWS Sagemaker Jupyter notebook instance that heavily interacts with files (gathering, converting, computing, interacting) and after every step the files are moved from folder to folder to prepare for the next interaction. I was wondering if there was any way to set some form of chart (like excel) that creates\/updates a row when a file enters a folder. The charts end goal is to be used as some form of tracker, to see what stage all the different files are in.<\/p>\n\n<p>Examples of how the desired chart should look like below<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/yOnSH.png\" rel=\"nofollow noreferrer\">Chart Style 1<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uoHiB.png\" rel=\"nofollow noreferrer\">Chart Style 2<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-07-03 19:18:53.283 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"excel|python-3.x|amazon-s3|amazon-sagemaker",
        "Question_view_count":59,
        "Owner_creation_date":"2019-06-26 19:31:33.757 UTC",
        "Owner_last_access_date":"2022-08-26 13:57:10.1 UTC",
        "Owner_location":"New Jersey, USA",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-07-08 18:46:27.5 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Connect locally to Jupyter in Sagemaker",
        "Question_body":"<p>When I run <code>%connect_info<\/code> in Jupyterlab on Sagemaker I get session info. and<\/p>\n<pre><code>{&quot;shell_port&quot;: ,\n  &quot;iopub_port&quot;: ,\n  &quot;stdin_port&quot;: ,\n  &quot;control_port&quot;: ,\n  &quot;hb_port&quot;: ,\n  &quot;ip&quot;: &quot;&quot;,\n  &quot;key&quot;: &quot;&quot;,\n  &quot;transport&quot;: &quot;&quot;,\n  &quot;signature_scheme&quot;: &quot;&quot;,\n  &quot;kernel_name&quot;: &quot;&quot;}\n\nPaste the above JSON into a file, and connect with:\n    $&gt; jupyter &lt;app&gt; --existing &lt;file&gt;\nor, if you are local, you can connect with just:\n    $&gt; jupyter &lt;app&gt; --existing kernel-052ed888-e682-4786-aa4c-cdb19c6145bf.json\nor even just:\n    $&gt; jupyter &lt;app&gt; --existing\nif this is the most recent Jupyter kernel you have started.\n<\/code><\/pre>\n<p>But when I run the Jupiter statement against the saved text file, it doesn't connect.<\/p>\n<p>Sometimes, it would be convenient to connect my local VS code app to my notebooks instead of developing in the browser.<\/p>\n<p>Any suggestions are appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-11 22:08:37.783 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"jupyter|jupyter-lab|amazon-sagemaker",
        "Question_view_count":385,
        "Owner_creation_date":"2012-05-23 18:47:11.903 UTC",
        "Owner_last_access_date":"2022-09-15 21:30:03.487 UTC",
        "Owner_location":null,
        "Owner_reputation":971,
        "Owner_up_votes":132,
        "Owner_down_votes":8,
        "Owner_views":104,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Best way to run 1000s of training jobs on sagemaker",
        "Question_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-21 02:03:31.153 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":269,
        "Owner_creation_date":"2021-05-30 15:23:01.327 UTC",
        "Owner_last_access_date":"2022-06-06 14:05:53.057 UTC",
        "Owner_location":null,
        "Owner_reputation":103,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-04-22 08:01:42.72 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-04-21 02:41:38.3 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Question_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-04-25 17:03:45.007 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|django|machine-learning|amazon-sagemaker|amazon-comprehend",
        "Question_view_count":232,
        "Owner_creation_date":"2010-06-11 22:17:02.427 UTC",
        "Owner_last_access_date":"2022-09-24 23:17:42.98 UTC",
        "Owner_location":null,
        "Owner_reputation":4334,
        "Owner_up_votes":409,
        "Owner_down_votes":1,
        "Owner_views":496,
        "Answer_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-01 08:07:25.34 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-04-07 05:03:44.61 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Inconsistent \"'float' object is not iterable\" error in sklearn",
        "Question_body":"<p>I know that there are a lot of questions with the same problem but none of them solves my problem. I am using a Jupyter Notebook in Amazon Sagemaker and I want to use the hashing trick for some features. I have not been able to make a reproducible example with simple data, but here is a screen of the data I have:\n<a href=\"https:\/\/i.stack.imgur.com\/el7x2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/el7x2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So I have used:<\/p>\n\n<pre><code>from sklearn.feature_extraction import FeatureHasher\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['country_iso_code'] = h.transform(df['country_iso_code'])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['origen_tarjeta_country_iso'] = h.transform(df['origen_tarjeta_country_iso'])\n<\/code><\/pre>\n\n<p>The first transformation works, but the second one does not and I get the 'float' object is not iterable error. I have checked the types of both columns and they are objects and also I have checked that there are only strings in both columns. I have tried to reproduce the code in Spyder with a very little sample and it works:<\/p>\n\n<pre><code>import pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\n\ndf = pd.DataFrame({'ES':'ES','UK':'UK'},index=[0,1])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['UK'] = h.transform(df['UK'])\nh = FeatureHasher(n_features=10,input_type=\"string\")\ndf['ES'] = h.transform(df['ES'])\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2019-03-12 10:34:57.78 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|pandas|scikit-learn|amazon-sagemaker",
        "Question_view_count":299,
        "Owner_creation_date":"2018-04-09 18:36:08.403 UTC",
        "Owner_last_access_date":"2022-09-23 12:00:52.963 UTC",
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to set SAGEMAKER_SUBMIT_DIRECTORY environment variable in sagemaker",
        "Question_body":"<p>I am trying to deploy a model that i have registered. I registered the model using the following code:<\/p>\n<pre><code>step_register = RegisterModel(\n    name=&quot;RegisterCustomModel&quot;,\n    estimator=estimator,\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n    transform_instances=[&quot;ml.m5.large&quot;],\n    model_package_group_name=model_package_group_name,\n    approval_status=model_approval_status,\n    model_metrics=model_metrics,\n)\n<\/code><\/pre>\n<p>However, I am getting an error when i deploy this model which I believe is because the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY<\/code> is not set.<\/p>\n<p>My question is, can I set the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY<\/code> in the <code>RegisterModel<\/code> function and if I can, how do I do that?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-11-08 14:09:01.73 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":444,
        "Owner_creation_date":"2018-03-22 16:45:24.657 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747 UTC",
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to restrict model predicted value within range?",
        "Question_body":"<p>I want to do linear regression with aws sagemaker. Where i have trained my model with some values and it's predicting values as per inputs. but sometimes it predicts value out of range as in i am predicting percentage which can't go less than 0 and more than 100. how can i restrict it here:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss')\n\nlinear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>how can i make my model not to predict values out of range : [0,100].<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2018-04-05 12:59:38.11 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|linear-regression|amazon-sagemaker",
        "Question_view_count":817,
        "Owner_creation_date":"2016-05-11 07:19:18.703 UTC",
        "Owner_last_access_date":"2022-09-24 11:12:31.88 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":4928,
        "Owner_up_votes":176,
        "Owner_down_votes":7,
        "Owner_views":304,
        "Answer_body":"<p>Yes you can. You can implement the output_fn to \"brick wall\" your output. SageMaker would call the output_fn after the model returns the value to do any post-processing of the result. \nThis can be done by creating a separate python file, specify the output_fn method there. \nProvide this python file when instantiating your Estimator. \nsomething like <\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss', \n<\/code><\/pre>\n\n<blockquote>\n  <p>entry_point = 'entry.py'<\/p>\n<\/blockquote>\n\n<p>)<\/p>\n\n<pre><code>linear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>Your entry.py could look something like <\/p>\n\n<pre><code>def output_fn(data, accepts):\n    \"\"\"\n    Args:\n        data: A result from TensorFlow Serving\n        accepts: The Amazon SageMaker InvokeEndpoint Accept value. The content type the response object should be\n            serialized to.\n    Returns:\n        object: The serialized object that will be send to back to the client.\n\n    \"\"\"    \n<\/code><\/pre>\n\n<blockquote>\n  <p>Implement the logic to \"brick wall\" here.<\/p>\n<\/blockquote>\n\n<pre><code>    return data.outputs['outputs'].string_val\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-05 16:47:04.053 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Model Management in SageMaker : Use of Model Package Group",
        "Question_body":"<p>We have a huge number of models which we plan to train\/test\/deploy\/use. We have several levels of products - starting from Country -&gt; Region -&gt; Warehouse -&gt; Department -&gt; Class A -&gt; Class B -&gt; Product ID etc., Some of the models will be for overall country level, some Country+Region level, and of course, similarly it will go down to the Class B and Product level. Which means if there are total 10,000 products - we will have 10,000 models in the product level.\nIn this kind of scenario, where we can have 10s of thousands of models, we were wondering how to manage these models. Naming convention as well as grouping them together for easy look up etc. are part of the model management also.\nOur initial thought was to use Sagemaker Model Package Group to group these models - so we can easily organize and find them as needed. However, I just learned that one SageMaker Model Package Group is a flat structure - which means one Model Package Group cannot contain another Model Package Group - so this hierarchical organization of models could not be replicated using Model Package Group.<\/p>\n<p>Question is: How do we achieve this kind model organization in production? This is not an unusual situation or anything, I believe there are many retailers who are dealing with model management scenarios like this. How do they do it? If model package is not used, what else can be used to organize them and how?<\/p>\n<p>Any suggestions\/idea will be highly appreciated.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-28 05:58:08.727 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":63,
        "Owner_creation_date":"2019-05-08 16:39:57.23 UTC",
        "Owner_last_access_date":"2022-09-23 17:21:26.617 UTC",
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-28 23:04:21.227 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Question_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 13:30:58.327 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"github|jupyter-notebook|ssh-keys|amazon-sagemaker",
        "Question_view_count":1856,
        "Owner_creation_date":"2018-11-06 06:13:32.983 UTC",
        "Owner_last_access_date":"2021-12-08 23:53:21.34 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-02-02 22:32:47.163 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":"2019-01-31 13:38:52.587 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Cannot find folder that Python os module shows exists",
        "Question_body":"<p>This is definitely a first for me. Using the <code>os.listdir()<\/code> method, I'm able to view files \/ folders from a directory that doesn't seem to exist. Below is a lightly redacted snippet from the console showing the effect:<\/p>\n<pre><code>sh-4.2$ python\nPython 3.6.11 | packaged by conda-forge | (default, Aug  5 2020, 20:09:42)\n[GCC 7.5.0] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.listdir(&lt;full_file_path&gt;)\n['file01', 'file02', 'file03', 'file04', 'file05']\n&gt;&gt;&gt; exit()\nsh-4.2$ ls &lt;full_file_path&gt;\nsh-4.2$ ls -a &lt;full_file_path&gt;\n.  ..\nsh-4.2$\n<\/code><\/pre>\n<p>From the graphical file explorer, I am unable to see anything in the parent folder for the files I'm searching for. Python insists that the files are real and exist, but they cannot be accessed without using python to do so. They should not be hidden, or having special permissions to be able to view them. Any help is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-21 17:31:48.19 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-sagemaker|python-os",
        "Question_view_count":88,
        "Owner_creation_date":"2019-09-10 12:23:41.763 UTC",
        "Owner_last_access_date":"2022-08-08 16:55:01.517 UTC",
        "Owner_location":"Seaside, CA, USA",
        "Owner_reputation":125,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>This issue has been solved.<\/p>\n<p>The directory that I'm looking for was created with <code>os.makedirs()<\/code>. On closer inspection, I can see that it created the filepath from <code>os.getcwd()<\/code> exactly as it was entered.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.getcwd()\n'\/ec2-user\/SageMaker\/path\/to\/current\/'\n\n&gt;&gt;&gt; os.listdir('.\/results') # Should show me 5 different folders\n[]\n\n&gt;&gt;&gt; os.listdir('.\/~') # Uh oh\n['SageMaker']\n<\/code><\/pre>\n<p>So what happened was that the full file path was created from the original working directory, contrary to what was expected.<\/p>\n<pre><code>sh-4.2 $ ls ~\/SageMaker\/path\/to\/current\/~\/SageMaker\/path\/to\/current\/results\nfolder01 folder02 folder03 folder04 folder05\n<\/code><\/pre>\n<p><strong>TL;DR<\/strong>\nI did not confirm the location of the directory was being created from root as expected, and it was created in the wrong location. <code>os.listdir()<\/code> still showed the files in the &quot;correct location&quot; because it wasn't starting in root, but in the current working directory.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-21 18:19:48.847 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Question_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-05 16:23:42.437 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":3875,
        "Owner_creation_date":"2011-05-18 08:45:50.447 UTC",
        "Owner_last_access_date":"2022-09-21 23:58:14.443 UTC",
        "Owner_location":"Bologna, Italy",
        "Owner_reputation":14823,
        "Owner_up_votes":4638,
        "Owner_down_votes":85,
        "Owner_views":1847,
        "Answer_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-11 23:11:17.893 UTC",
        "Answer_last_edit_date":"2018-12-11 23:36:25.673 UTC",
        "Answer_score":5.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Run jupyter notebook through command line and log outputs to a file",
        "Question_body":"<p>I am running a notebook on command line in AWS SageMaker but I am not able to log outputs to a file. I added <code>print<\/code>, <code>logging.info<\/code> and <code>sys.stdout.write<\/code> statements in hope of capturing something but to no avail.<\/p>\n<p>Below is the code I am using<\/p>\n<pre><code>NOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/mynb.ipynb&quot;\nLOG_FILE=&quot;\/home\/ec2-user\/SageMaker\/logs.txt&quot;\n\nnohup jupyter nbconvert  --to notebook --inplace --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &gt; &quot;$LOG_FILE&quot; &amp;\n<\/code><\/pre>\n<p>On doing <code>cat &quot;$LOG_FILE&quot;<\/code> I get just a couple lines of logs and nothing after that. Output is below -<\/p>\n<pre><code>[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n[NbConvertApp] Converting notebook \/home\/ec2-user\/SageMaker\/tata1mg.ipynb to notebook\n[NbConvertApp] Executing notebook with kernel: python3\n<\/code><\/pre>\n<p>PS:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-01 07:32:22.19 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|python-3.x|jupyter|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_date":"2013-04-26 14:40:49.713 UTC",
        "Owner_last_access_date":"2022-09-24 12:34:18.883 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":7127,
        "Owner_up_votes":2089,
        "Owner_down_votes":92,
        "Owner_views":987,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Installing modules inside python .py file",
        "Question_body":"<p>I am deploying a custom pytorch model on AWS sagemaker, Following <a href=\"https:\/\/github.com\/abiodunjames\/MachineLearning\/blob\/master\/DeployYourModelToSageMaker\/inference.py\" rel=\"nofollow noreferrer\">this<\/a> tutorial.\nIn my case I have few dependencies to install some modules.<\/p>\n<p>I need pycocotools in my inference.py script. I can easily install pycocotool inside a separate notebook using this bash command,<\/p>\n<p><code>%%bash<\/code><\/p>\n<p><code>pip -g install pycocotools<\/code><\/p>\n<p>But when I create my endpoint for deployment, I get this error that pycocotools in not defined.\nI need pycocotools inside my inference.py script. How I can install this inside a .py file<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2021-04-20 21:24:41.933 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":167,
        "Owner_creation_date":"2019-09-07 18:22:12.003 UTC",
        "Owner_last_access_date":"2022-08-28 17:07:56.487 UTC",
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Answer_body":"<p>At the beginning of inference.py add these lines:<\/p>\n<pre><code>from subprocess import check_call, run, CalledProcessError\nimport sys\nimport os\n\n# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:\nif not os.environ.get(&quot;INSTALL_SUCCESS&quot;):\n    \n    try:\n        check_call(\n        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    except CalledProcessError:\n        run(\n        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-04-21 00:31:28.403 UTC",
        "Answer_last_edit_date":"2021-05-01 15:29:31.87 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-04-20 23:30:52.447 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Use ffprobe on Sagemaker Jupyter Notebook: \/bin\/sh: 1: ffprobe: not found",
        "Question_body":"<p>I need to use <code>FFprobe<\/code> to get the metadata(the orientation of the video) of videos on SageMaker Studio. Even I installed <code>FFmpeg<\/code> and <code>FFprobe<\/code> via <code>pip install<\/code>, the notebook doesn't recognize the packages.<\/p>\n<p><code>\/bin\/sh: 1: ffprobe: not found<\/code><\/p>\n<p>It is tricky since I've already tried to install them on System terminal and it worked, but it still doesn't work on the Image terminal or Jupyter Notebook.<\/p>\n<p>Is that because of the dependencies? I've checked other ways to get the metadata but couldn't find one working. I could get simple metadata via <code>hachoir-metadata<\/code> but it didn't give the orientation. Any advice is welcomed. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-02 21:15:16.073 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|ffmpeg|jupyter-notebook|amazon-sagemaker|ffprobe",
        "Question_view_count":190,
        "Owner_creation_date":"2014-10-15 11:59:57.41 UTC",
        "Owner_last_access_date":"2022-09-22 13:03:11.2 UTC",
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":39,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-07-02 21:23:47.37 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker fatal: could not read Username for 'https:\/\/gitlab.com\/my\/repo.git': terminal prompts disabled",
        "Question_body":"<p>I want to integrate my private gitlab repository into AWS Sagemaker.<\/p>\n<p>I added git repository on Sagemaker using https protocol (it allows only this protocol) and saved secrets(username and password of my gitlab account) for git repo.<\/p>\n<p>When I run notebook instance by linking git repo, it failed with following message.<\/p>\n<p><code>fatal: could not read Username for 'https:\/\/gitlab.com\/my\/repo.git': terminal prompts disabled<\/code><\/p>\n<p>Is there any step I am missing?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-08-20 07:24:08.45 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"git|amazon-web-services|jupyter-notebook|gitlab|amazon-sagemaker",
        "Question_view_count":197,
        "Owner_creation_date":"2016-03-16 06:06:26.687 UTC",
        "Owner_last_access_date":"2022-09-24 21:55:35.777 UTC",
        "Owner_location":"Europe",
        "Owner_reputation":349,
        "Owner_up_votes":45,
        "Owner_down_votes":2,
        "Owner_views":50,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-20 22:35:26.86 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to access python variables in Sagemaker Jupyter Notebook shell command",
        "Question_body":"<p>In one of the cells of Sagemaker notebook, I've set a variable<\/p>\n<pre><code>region=&quot;us-west-2&quot;\n<\/code><\/pre>\n<p>In subsequent cell, I run following 2 shell commands<\/p>\n<pre><code>!echo $region\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>us-west-2\n<\/code><\/pre>\n<p>However, unable to run aws shell command using this variable<\/p>\n<pre><code>!aws ecr get-login-password --region $region\n<\/code><\/pre>\n<p><code>$ variable-name<\/code> doesn't help inside jupyter cell <code>! shell command<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-23 18:35:08.72 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|bash|shell|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1285,
        "Owner_creation_date":"2015-07-26 12:56:05.627 UTC",
        "Owner_last_access_date":"2022-09-23 11:58:05.58 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":2665,
        "Owner_up_votes":1461,
        "Owner_down_votes":14,
        "Owner_views":722,
        "Answer_body":"<p>As answered here: <a href=\"https:\/\/stackoverflow.com\/a\/19674648\/5157515\">https:\/\/stackoverflow.com\/a\/19674648\/5157515<\/a><\/p>\n<p>There's no direct way to access python variables with <code>!<\/code> command.<\/p>\n<p>But with magic command <code>%%bash<\/code> it is possible<\/p>\n<pre><code>%%bash  -s &quot;$region&quot;\naws ecr get-login-password --region $1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-26 05:52:54.773 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Change datacapture encoding data to csv",
        "Question_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-04 05:55:14.543 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":620,
        "Owner_creation_date":"2018-07-24 08:52:28.877 UTC",
        "Owner_last_access_date":"2020-11-27 01:51:12.017 UTC",
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-25 13:26:00.01 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Model Deployment Error, ClientError: An error occurred (ValidationException) when calling the CreateModel operation",
        "Question_body":"<p>I am trying to deploy a model with AWS Sagemaker using SKlearn, and getting this error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-145-29a1d3175b01&gt; in &lt;module&gt;\n----&gt; 1 deployment = model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.xlarge&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, **kwargs)\n   1254             kms_key=kms_key,\n   1255             data_capture_config=data_capture_config,\n-&gt; 1256             serverless_inference_config=serverless_inference_config,\n   1257             async_inference_config=async_inference_config,\n   1258         )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, **kwargs)\n   1001                 self._base_name = &quot;-&quot;.join((self._base_name, compiled_model_suffix))\n   1002 \n-&gt; 1003         self._create_sagemaker_model(\n   1004             instance_type, accelerator_type, tags, serverless_inference_config\n   1005         )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config)\n    548             container_def,\n    549             vpc_config=self.vpc_config,\n--&gt; 550             enable_network_isolation=enable_network_isolation,\n    551             tags=tags,\n    552         )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2670 \n   2671         try:\n-&gt; 2672             self.sagemaker_client.create_model(**create_model_request)\n   2673         except ClientError as e:\n   2674             error_code = e.response[&quot;Error&quot;][&quot;Code&quot;]\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    414             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 415             return self._make_api_call(operation_name, kwargs)\n    416 \n    417         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    744             error_class = self.exceptions.from_code(error_code)\n--&gt; 745             raise error_class(parsed_response, operation_name)\n    746         else:\n    747             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-east-2-978433479050\/sagemaker-scikit-learn-2022-04-28-22-33-14-817\/output\/model.tar.gz.\n<\/code><\/pre>\n<hr \/>\n<p>The code I am running is:<\/p>\n<pre><code>from sagemaker import Session, get_execution_role\nfrom sagemaker.sklearn.estimator import SKLearn\n\nsagemaker_session = Session()\nrole = get_execution_role()\n\ntrain_input = sagemaker_session.upload_data(&quot;TSLA.csv&quot;)\n\nmodel = SKLearn(entry_point='lr.py',\n                      train_instance_type='ml.m4.xlarge',\n                      role=role, framework_version='0.231',\n                      sagemaker_session=sagemaker_session)\n\nmodel.fit({'train': train_input})\n\ndeployment = model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.xlarge&quot;)\n<\/code><\/pre>\n<p>And train_input is: s3:\/\/sagemaker-us-east-2-978433479050\/data\/TSLA.csv<\/p>\n<p>The training job is completed, but for some reason the model is not deploying.<\/p>\n<p>Please advise,\nthank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 22:54:41.083 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":241,
        "Owner_creation_date":"2021-08-23 21:06:51.59 UTC",
        "Owner_last_access_date":"2022-09-21 23:38:35.943 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-04-29 03:13:11.64 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to ensure software package version consistency in AWS SageMaker serverless compute?",
        "Question_body":"<p>I am learning AWS SageMaker which is supposed to be a serverless compute environment for Machine Learning. In this type of serverless compute environment, who is supposed to ensure the software package consistency and update the versions?<\/p>\n\n<p>For example, I ran the demo program that came with SageMaker, deepar_synthetic. In this second cell, it executes the following: !conda install -y s3fs<\/p>\n\n<p>However, I got the following warning message:<\/p>\n\n<p>Solving environment: done\n==> WARNING: A newer version of conda exists. &lt;==\n  current version: 4.4.10\n  latest version: 4.5.4\nPlease update conda by running\n    $ conda update -n base conda<\/p>\n\n<p>Since it is serverless compute, am I still supposed to update the software packages myself?<\/p>\n\n<p>Another example is as follows. I wrote a few simple lines to find out the package versions in Jupyter notebook:<\/p>\n\n<p>import platform<\/p>\n\n<p>import tensorflow as tf<\/p>\n\n<p>print(platform.python_version())<\/p>\n\n<p>print (tf.<strong>version<\/strong>)<\/p>\n\n<p>However, I got the following warning messages:<\/p>\n\n<p>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/importlib\/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\nreturn f(*args, **kwds)\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/h5py\/<strong>init<\/strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float<\/code> to <code>np.floating<\/code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type<\/code>.\nfrom ._conv import register_converters as _register_converters<\/p>\n\n<p>The prints still worked and I got the results shown beolow:<\/p>\n\n<p>3.6.4\n1.4.0<\/p>\n\n<p>I am wondering what I have to do to get the package consistent so that I don't get the warning messages. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-21 02:20:28.233 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|serverless|amazon-sagemaker",
        "Question_view_count":926,
        "Owner_creation_date":"2016-04-20 00:33:54.223 UTC",
        "Owner_last_access_date":"2022-04-02 22:37:53.663 UTC",
        "Owner_location":"San Jose, CA, United States",
        "Owner_reputation":1075,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":181,
        "Answer_body":"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. <\/p>\n\n<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: \u201cNew\u201d -> \u201cTerminal\u201d. \nNote: By default, conda installs to the root environment. <\/p>\n\n<p>The following are instructions you can follow <a href=\"https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html<\/a> on how to install libraries in the particular conda environment. <\/p>\n\n<p>In general you will need following commands, <\/p>\n\n<pre><code>conda env list \n<\/code><\/pre>\n\n<p>which list all of your conda environments <\/p>\n\n<pre><code>source activate &lt;conda environment name&gt; \n<\/code><\/pre>\n\n<p>e.g. source activate python3 <\/p>\n\n<pre><code>conda list | grep &lt;package&gt; \n<\/code><\/pre>\n\n<p>e.g. conda list | grep numpy \nlist what are the current package versions <\/p>\n\n<pre><code>pip install numpy \n<\/code><\/pre>\n\n<p>Or <\/p>\n\n<pre><code>conda install numpy \n<\/code><\/pre>\n\n<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. <\/p>\n\n<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-20 19:32:06.593 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":"2018-07-10 08:37:35.607 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Custom package installation from S3 in sagemaker",
        "Question_body":"<p>I have a whl file for a custom package (not published open source) in a S3 bucket.<\/p>\n<p>I now want to import\/install it in my sagemaker instance.\n<a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655<\/a> This link is what I tried to follow, but it did not work for me.<\/p>\n<p>Has anyone tried this before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-30 17:58:16.73 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-s3|amazon-sagemaker",
        "Question_view_count":297,
        "Owner_creation_date":"2016-02-04 09:23:44.63 UTC",
        "Owner_last_access_date":"2022-08-12 17:47:04.7 UTC",
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":99,
        "Owner_up_votes":76,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-11 13:31:06.647 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_date":"2019-09-12 20:07:41.627 UTC",
        "Owner_last_access_date":"2022-09-21 14:35:39.23 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-13 14:00:26.7 UTC",
        "Answer_last_edit_date":"2019-11-13 14:11:08.9 UTC",
        "Answer_score":6.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Load Python Pickle File from S3 Bucket to Sagemaker Notebook",
        "Question_body":"<p>I have attempted the code on the many posts on how to load a pickle file (1.9GB) from an S3 bucket, but none seem to work for our notebook instance on AWS Sagemaker.  Notebook size is 50GB.<\/p>\n<p>Some of the methods attempted:<\/p>\n<p>Method 1<\/p>\n<pre><code>import io\nimport boto3\n\nclient = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\n\nbytes_io.seek(0) \nbyte_value = pickle.load(bytes_io)\n<\/code><\/pre>\n<p>This gives:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Method 2: This actually gets me something back with no error:<\/p>\n<pre><code>client = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\nbyte_value = bytes_buffer.getvalue()\nimport sys\nsys.getsizeof(byte_value)\/(1024**3)\n<\/code><\/pre>\n<p>this returns: 1.93<\/p>\n<p>but how do I convert the byte_value into the pickled object?\nI tried this:<\/p>\n<pre><code>pickled_data = pickle.loads(byte_value)\n<\/code><\/pre>\n<p>But the kernel &quot;crashed&quot; - went idle and I lost all variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-01 15:50:32 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":2485,
        "Owner_creation_date":"2012-05-27 16:46:50 UTC",
        "Owner_last_access_date":"2022-08-30 09:49:13.917 UTC",
        "Owner_location":null,
        "Owner_reputation":1937,
        "Owner_up_votes":249,
        "Owner_down_votes":0,
        "Owner_views":221,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Save Multiple Dataframes on excel workbook then Upload to AWS S3 Bucket",
        "Question_body":"<p>Good afternoon everyone, <\/p>\n\n<p>I am trying to save multiple dataframes to an excel workbook on different sheets. Then upload that workbook to an Amazon S3 bucket. the code below works 99% of the way but the writer.save() cannot find my excel file on my S3 Bucket. Please assist if you know a way around this. thanks. <\/p>\n\n<pre><code>#Exports the data back to Excel - PLEASE READ LINE BELOW THIS CODE\nbucket='sagemaker-bucket-xxxx\/xxxx\/xxxxx'\ndata_key = 'Provider Data.xlsx'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nwriter = pd.ExcelWriter(data_location) #Targets the file where data is to be sent to\nComparison.to_excel(writer,'DATA') #Targets the worksheet data is to be sent too\ndf_current.to_excel(writer,'New Records') #Targets the worksheet data is to be sent too\ndf_prev.to_excel(writer,'Old Records') #Targets the worksheet data is to be sent too\ndf_same.to_excel(writer,'Same Records') #Targets the worksheet data is to be sent too\nALLCOUNT.to_excel(writer,'RPN Roll Up Count') #Targets the worksheet data is to be sent too\nwriter.save() #Saves files\n<\/code><\/pre>\n\n<p>error message is listed below. <\/p>\n\n<p>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-bucket-xxxx\/xxxx\/xxxx\/Provider Data.xlsx'<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-05-08 00:09:53.143 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|excel|pandas|amazon-s3|amazon-sagemaker",
        "Question_view_count":1049,
        "Owner_creation_date":"2018-09-20 14:35:40.907 UTC",
        "Owner_last_access_date":"2022-09-05 21:31:03.393 UTC",
        "Owner_location":null,
        "Owner_reputation":181,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":63,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Unable to deploy locally trained Logistic Regression model on AWS Sagemaker",
        "Question_body":"<p>I have trained a Logistic Regression model on my local machine. Saved the model using Joblib and tried deploying it on Aws Sagemaker using &quot;Linear-Learner&quot; image.<\/p>\n<p>Facing issues while deployment as the deployment process keeps continuing and the Status is always as &quot;Creating&quot; and does not turn to &quot;InService&quot;.<\/p>\n<pre><code>endpoint_name = &quot;DEMO-LogisticEndpoint&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(endpoint_name)\ncreate_endpoint_response = sm_client.create_endpoint(\n    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n)\nprint(create_endpoint_response[&quot;EndpointArn&quot;])\n\nresp = sm_client.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp[&quot;EndpointStatus&quot;]\nprint(&quot;Status: &quot; + status)\n\nwhile status == &quot;Creating&quot;:\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[&quot;EndpointStatus&quot;]\n    print(&quot;Status: &quot; + status)\n<\/code><\/pre>\n<p>The while loop keeps executing and the status never change.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-22 17:35:59.34 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|logistic-regression|amazon-sagemaker",
        "Question_view_count":108,
        "Owner_creation_date":"2022-03-22 11:09:20.697 UTC",
        "Owner_last_access_date":"2022-04-13 14:43:22.597 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Cloudpickle.dump(pyspark_Alsmodel_object),getting error py4j.Py4JException: Method __getnewargs__([]) does not exist?",
        "Question_body":"<p>After creating ALS model object,using pyspark. <\/p>\n\n<p>Sample code example:<\/p>\n\n<pre><code>from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\n\nlines = spark.read.text(\"data\/mllib\/als\/sample_movielens_ratings.txt\").rdd\nparts = lines.map(lambda row: row.value.split(\"::\"))\nratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n                                     rating=float(p[2]), timestamp=long(p[3])))\nratings = spark.createDataFrame(ratingsRDD)\n(rating_data, test) = ratings.randomSplit([0.8, 0.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\n\n    als_model = als_spec.fit(rating_data)\n<\/code><\/pre>\n\n<p>here i am just creating ALS model and making cloudepickel.\nif we are using fit then also need to do transform?<\/p>\n\n<p>I am trying pickel the my als_model object using the below code :<\/p>\n\n<pre><code>with open(os.path.join(model_path, 'als-als-model.pkl'), 'w') as out:\n                cloudpickle.dump(als_model, out)\n<\/code><\/pre>\n\n<p>I am getting error like below:<\/p>\n\n<pre><code>  File \"\/usr\/local\/spark\/python\/lib\/py4j-0.10.6-src.zip\/py4j\/protocol.py\", line 324, in get_return_value\n    format(target_id, \".\", name, value))\nPy4JError: An error occurred while calling o224.__getnewargs__. Trace:\npy4j.Py4JException: Method __getnewargs__([]) does not exist\n#011at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n#011at \n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-124-8c94f4ee0de9&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 tree.fit(data_location)\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name)\n    152         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    153         if wait:\n--&gt; 154             self.latest_training_job.wait(logs=logs)\n    155         else:\n    156             raise NotImplemented('Asynchronous fit not available')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2018-03-15 12:06:49.45 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|apache-spark|pyspark|pyspark-sql|amazon-sagemaker",
        "Question_view_count":255,
        "Owner_creation_date":"2015-04-03 14:45:43.217 UTC",
        "Owner_last_access_date":"2022-01-14 00:51:53.803 UTC",
        "Owner_location":null,
        "Owner_reputation":1015,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":304,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2018-03-16 11:08:40.33 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Install Voila on SageMaker based Jupyter Notebook",
        "Question_body":"<p>I'm trying to run the Voila! server on a SageMaker Notebook.<\/p>\n<p>Commands to install and enable voila:<\/p>\n<pre><code>!pip install voila\n!jupyter serverextension enable --sys-prefix voila\n<\/code><\/pre>\n<p>The pip command appears to work OK.<\/p>\n<p>Terminal output from the jupyter command:<\/p>\n<blockquote>\n<p>Config option <code>kernel_spec_manager_class<\/code> not recognized by <code>EnableServerExtensionApp<\/code>.\nEnabling: voila<\/p>\n<ul>\n<li>Writing config: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/etc\/jupyter\n<ul>\n<li>Validating...\nvoila 0.2.10 OK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/blockquote>\n<p>Navigating to a 'voila url' such as:<\/p>\n<pre><code>https:\/\/{instance}.notebook.{region}.sagemaker.aws\/voila\/sandbox\/test-notebook.ipynb\n<\/code><\/pre>\n<p>Returns a 404.<\/p>\n<p>The <em><strong>'nbextensions'<\/strong><\/em> tab shows the voila extension as 'possibly incompatible'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I can't find anything in the Jupyter logs that looks relevant.<\/p>\n<p>Restarting Jupyter didn't help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-05 02:57:51.717 UTC",
        "Question_favorite_count":2.0,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|voila",
        "Question_view_count":247,
        "Owner_creation_date":"2014-11-10 01:21:05.687 UTC",
        "Owner_last_access_date":"2022-09-12 03:45:00.267 UTC",
        "Owner_location":"Australia",
        "Owner_reputation":2899,
        "Owner_up_votes":63,
        "Owner_down_votes":3,
        "Owner_views":247,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"OCR in Sage Maker",
        "Question_body":"<p>Guys i am trying to build infrastructure on aws for getting help from others on annotation. currently we uses label-studio for text annotation. as might know you can label text by selecting through polygon and than writing what does selected area mean. ex: if polygon is made around english word than what writing out label  of it to annotate that given english word. for more see image below.<a href=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How can i do this in <strong>SageMaker Ground Truth<\/strong>. as far as i have gone i think it can just label pre defined words. you cant create custom label in it by selecting any given area using polygon in image am i right ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-09 17:47:14.663 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|ocr|amazon-sagemaker|text-classification",
        "Question_view_count":67,
        "Owner_creation_date":"2016-12-18 07:09:03.533 UTC",
        "Owner_last_access_date":"2022-09-24 16:28:21.133 UTC",
        "Owner_location":"Gurgaon, Haryana, India",
        "Owner_reputation":549,
        "Owner_up_votes":19,
        "Owner_down_votes":14,
        "Owner_views":50,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Training & Deploying SageMaker ML Models using AWS Lambda (NodeJS)",
        "Question_body":"<p>I am using AWS Lambda (NodeJS) for creating a sagemaker training job and deploy it using the Sagemaker Javascript SDK.<\/p>\n\n<p>I am following the below AWS JavaScript SDK docs<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html<\/a><\/p>\n\n<p>I am using the below script for creating the Training job.<\/p>\n\n<pre><code>Create Training Job:\n=====================\n\n    let TrainingJobName = 'Training-' + curr_date_time\n    let TrainingImage   = 'XXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xxxx:latest'\n    let S3Uri           = 's3:\/\/xxx.xxxx.sagemaker\/csv'\n\n    console.log(`TrainingJobName: ${TrainingJobName}`);\n\n    let params = {\n        AlgorithmSpecification: { \/* required *\/\n            TrainingInputMode: 'File', \/* required *\/\n            TrainingImage: TrainingImage\n        },\n        OutputDataConfig: { \/* required *\/\n            S3OutputPath: 's3:\/\/xxx.xxxx.sagemaker\/xxxx\/output', \/* required *\/\n        },\n        ResourceConfig: { \/* required *\/\n            InstanceCount: 1, \/* required *\/\n            InstanceType: 'ml.m4.xlarge', \/* required *\/\n            VolumeSizeInGB: 1, \/* required *\/\n        },\n        RoleArn: 'arn:aws:iam::xxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxx', \/* required *\/\n        StoppingCondition: { \/* required *\/\n            MaxRuntimeInSeconds: 86400\n        },\n        TrainingJobName: TrainingJobName, \/* required *\/\n        InputDataConfig: [\n            {\n                ChannelName: 'training', \/* required *\/\n                DataSource: { \/* required *\/\n                    S3DataSource: {\n                        S3DataType: 'S3Prefix', \/* required *\/\n                        S3Uri: S3Uri, \/* required *\/\n                        S3DataDistributionType: 'FullyReplicated'\n                    }\n                },\n                CompressionType: null,\n                ContentType: '',\n                RecordWrapperType: null,\n            }\n        ]\n    };\n\n    return await sagemaker.createTrainingJob(params).promise();\n<\/code><\/pre>\n\n<p>After the training job is created, i query the job status using the sagemaker describeTrainingJob function.\nI get the status as \"InProgress\"<\/p>\n\n<p>After that I call the sagemaker waitFor function to wait for the completion of the training job using the below method:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter<\/a><\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n<\/code><\/pre>\n\n<p>I find the sagemaker waitFor creates the second training job before the first training job is completed, and it goes on creating subsequent training jobs with the same job name.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think this is due to the StoppingCondition parameter (MaxRuntimeInSeconds:86400) in the createTrainingJob function.<\/p>\n\n<p>I want to know if there is any solution which creates a single training job and return the results after the trainining job is completed ?<\/p>\n\n<p>==========================================================\nUpdate:<\/p>\n\n<p>I am following the \"Scheduling the training of a SageMaker model with a Lambda function\" <a href=\"https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM<\/a>.<\/p>\n\n<p>I am able to create a training job if i am using the below code in my lambda function.<\/p>\n\n<pre><code>let training_job_result = await start_model_training();\nconsole.log(`Sagemaker training result : ${JSON.stringify(training_job_result)}`);\n\nlet training_job_arn = training_job_result[\"TrainingJobArn\"];\nlet training_job_name = training_job_arn.split(\"\/\")[1];\n\n\nlet desc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\nlet desc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 1 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>But I need to wait till the training job is completed and invoke the sagemaker deploy method for creating\/updating the endpoint.<\/p>\n\n<p>If I use the below code then it keeps on creating multiple training jobs and the lambda function never terminates.<\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n\n\ndesc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\ndesc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 2 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>I want to deploy\/update the endpoint once the training is completed.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-11 02:38:14.627 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|aws-sdk-js",
        "Question_view_count":1288,
        "Owner_creation_date":"2009-04-29 11:42:36.853 UTC",
        "Owner_last_access_date":"2022-09-18 14:23:33.13 UTC",
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-07-12 05:36:33.607 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Question_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 14:28:58.403 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|inference|torchserve|tritonserver",
        "Question_view_count":12,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-23 22:14:00.67 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to remotely launch a Juypter Notebook within AWS Sagemaker using AWS Lambda",
        "Question_body":"<p>I have a Juypter Notebook set up within AWS Sagemaker. I wanted to find a way to launch this notebook on an autonomous trigger when a new file is uploaded to a certain folder (hence AWS Lambda). I was looking for if there was a streamlined way to trigger a Juypter Notebook with an AWS Lambda trigger.  <\/p>\n\n<p>I have looked into using API and turning Sagemaker into and endpoint, but it didnt work.<\/p>\n\n<p>*edit Sorry if the question was a little vague. I have allot of code written in this notebook on in Juypter. What i was ideally looking for was, when a file is uploaded to \"RandomFile\" then the code within the notebook will run. I was looking to do this with AWS Lambda by setting up a S3 based trigger. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-08-26 18:45:01.027 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":1612,
        "Owner_creation_date":"2019-06-26 19:31:33.757 UTC",
        "Owner_last_access_date":"2022-08-26 13:57:10.1 UTC",
        "Owner_location":"New Jersey, USA",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-08-26 18:56:21.093 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Glue Sagemaker Notebook \"No module named awsglue.transforms\"",
        "Question_body":"<p>I've created a Sagemaker notebook to dev AWS Glue jobs, but when running through the provided example (\"Joining, Filtering, and Loading Relational Data with AWS Glue\") I get the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know what I've setup wrong\/haven't setup to cause the import to not work?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-04 13:50:38.96 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":3551,
        "Owner_creation_date":"2013-11-18 17:24:50.587 UTC",
        "Owner_last_access_date":"2022-09-23 11:35:05.217 UTC",
        "Owner_location":null,
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Answer_body":"<p>You'll need to download the library files from <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 0.9 or <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl-1.0\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 1.0 (Check your Glue jobs for the version). <\/p>\n\n<p>Put the zip in S3 and reference it in the \"Python library path\" on your Dev Endpoint.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-10-04 14:56:53.273 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Question_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-08-31 08:04:16.29 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"tensorflow|amazon-s3|amazon-sagemaker",
        "Question_view_count":174,
        "Owner_creation_date":"2020-08-28 09:27:06.113 UTC",
        "Owner_last_access_date":"2021-04-05 17:24:11.577 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-09 14:42:45.023 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker model deployment failing due to custom endpoint name",
        "Question_body":"<p>AWS Sagemaker model deployment is failing when endpoint_name argument is specified. Any thoughts?<\/p>\n\n<p>Without endpoint_name argument in deploy, model deployment works successfully.\nModel training and saving into S3 location is successful either way.<\/p>\n\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n\nbucket = 'Y'\nprefix = 'Z'\n\nrole = get_execution_role()\n\n    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=100), [int(0.5 * len(df)), int(0.8 * len(df))])\n\n    train_data.to_csv('train.csv', index=False, header=False)\n    validation_data.to_csv('validation.csv', index=False, header=False)\n    test_data.to_csv('test.csv', index=False)\n    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train\/X\/train.csv')).upload_file('train.csv')\n    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation\/X\/validation.csv')).upload_file('validation.csv')\n\n    container = get_image_uri(boto3.Session().region_name, 'xgboost')\n    #print(container)\n\n    s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/{}'.format(bucket, prefix, suffix), content_type='csv')\n    s3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/{}\/'.format(bucket, prefix, suffix), content_type='csv')\n\n    sess = sagemaker.Session()\n\n    output_loc = 's3:\/\/{}\/{}\/output'.format(bucket, prefix)\n    xgb = sagemaker.estimator.Estimator(container,\n                                        role, \n                                        train_instance_count=1, \n                                        train_instance_type='ml.m4.xlarge',\n                                        output_path=output_loc,\n                                        sagemaker_session=sess,\n                                        base_job_name='X')\n    #print('Model output to: {}'.format(output_location))\n\n    xgb.set_hyperparameters(eta=0.5,\n                            objective='reg:linear',\n                            eval_metric='rmse',\n                            max_depth=3,\n                            min_child_weight=1,\n                            gamma=0,\n                            early_stopping_rounds=10,\n                            subsample=0.8,\n                            colsample_bytree=0.8,\n                            num_round=1000)\n\n    #Model fitting\n    xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n    #Deploy model with automatic endpoint created\n    xgb_predictor_X = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name='X')\n\n    xgb_predictor_X.content_type = 'text\/csv'\n    xgb_predictor_X.serializer = csv_serializer\n    xgb_predictor_X.deserializer = None\n<\/code><\/pre>\n\n<p>INFO:sagemaker:Creating endpoint with name delaymins\nClientError: An error occurred (ValidationException) when calling the CreateEndpoint operation: Could not find model \"arn:aws:sagemaker:us-west-2::model\/X-2019-01-08-18-17-42-158\".<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-09 00:07:33.447 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1333,
        "Owner_creation_date":"2012-06-04 21:07:02.563 UTC",
        "Owner_last_access_date":"2021-01-22 16:23:41.947 UTC",
        "Owner_location":null,
        "Owner_reputation":355,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-01-09 00:47:10.41 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to assign users in SageMaker Studio?",
        "Question_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-17 14:01:36.847 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1029,
        "Owner_creation_date":"2015-12-26 21:09:38.85 UTC",
        "Owner_last_access_date":"2022-01-19 17:52:30.627 UTC",
        "Owner_location":"Bulgaria",
        "Owner_reputation":61,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-27 00:21:55.467 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2019-12-18 03:52:59.673 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden when trying cross account copy",
        "Question_body":"<p>I am doing a cross account copy of s3 objects. When I am trying to copy files from source bucket to destination bucket I am getting the error <strong>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden<\/strong>\nI am getting error at line <strong>s3_object.download_fileobj(buffer)<\/strong><\/p>\n<pre><code>    final_df=pd.DataFrame() \nfor file in files1: \n# file=file.split('\/')[-1]\n    bucket = 'source bucket'\n    buffer = io.BytesIO()\n    s3 = boto3.resource('s3')\n    s3_object = s3.Object(bucket,file)\n    s3_object.download_fileobj(buffer)\n    df = pd.read_parquet(buffer)\n    print(file)\n    s3 = boto3.client('s3')\n    file=file.split('\/')[-1]\n    print(file)\n    final_df=pd.concat([final_df,df],sort=False)\n<\/code><\/pre>\n<p>Files1 is the list of all parquet files in the bucket<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-31 03:34:07.703 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":702,
        "Owner_creation_date":"2016-07-15 16:19:18.957 UTC",
        "Owner_last_access_date":"2022-07-22 22:52:23.46 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":107,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-11-03 01:29:17.827 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-27 19:14:45.467 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":1337,
        "Owner_creation_date":"2015-04-07 18:40:18.42 UTC",
        "Owner_last_access_date":"2019-03-12 12:37:26.967 UTC",
        "Owner_location":"New York, United States",
        "Owner_reputation":301,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-29 16:33:54.973 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS sagemaker invokeEndpoint model internal error",
        "Question_body":"<p>I am trying to send a request on a model on sagemaker using .NET. The code I am using is: <\/p>\n\n<pre><code>var data = File.ReadAllBytes(@\"C:\\path\\file.csv\");\nvar credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\nvar awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\nvar request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n{\n    EndpointName = \"EndpointName\",\n    ContentType = \"text\/csv\",\n    Body = new MemoryStream(data),\n};\n\nvar response = awsClient.InvokeEndpoint(request);\nvar predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>\n\n<p>the error that I am getting on <code>awsClient.InvokeEndpoint(request)<\/code><\/p>\n\n<p>is:<\/p>\n\n<blockquote>\n  <p>Amazon.SageMakerRuntime.Model.ModelErrorException: 'The service\n  returned an error with Error Code ModelError and HTTP Body:\n  {\"ErrorCode\":\"INTERNAL_FAILURE_FROM_MODEL\",\"LogStreamArn\":\"arn:aws:logs:eu-central-1:xxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/myEndpoint\",\"Message\":\"Received\n  server error (500) from model with message \\\"\\\". See\n  \"https:\/\/ url_to_logs_on_amazon\"\n  in account xxxxxxxxxxx for more\n  information.\",\"OriginalMessage\":\"\",\"OriginalStatusCode\":500}'<\/p>\n<\/blockquote>\n\n<p>the url that the error message suggests for more information does not help at all.<\/p>\n\n<p>I believe that it is a data format issue but I was not able to find a solution.<\/p>\n\n<p>Does anyone has encountered this behavior before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-22 14:08:26.973 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"c#|.net|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4385,
        "Owner_creation_date":"2017-05-25 12:29:46.88 UTC",
        "Owner_last_access_date":"2020-06-17 08:32:20.743 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application\/json<\/code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. <\/p>\n\n<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.<\/p>\n\n<p>example of working code for my case:<\/p>\n\n<pre><code>        var data = new string[] { \"this movie was extremely good .\", \"the plot was very boring .\" };\n        var serializedData = JsonConvert.SerializeObject(data);\n\n        var credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\n        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\n        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n        {\n            EndpointName = \"endpoint\",\n            ContentType = \"application\/json\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),\n        };\n\n        var response = awsClient.InvokeEndpoint(request);\n        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-24 10:02:05.6 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Question_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2018-06-03 19:06:16.997 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|scikit-learn|dockerfile|amazon-sagemaker",
        "Question_view_count":4509,
        "Owner_creation_date":"2016-09-29 20:35:09.097 UTC",
        "Owner_last_access_date":"2022-09-25 02:50:38.173 UTC",
        "Owner_location":"Brazil",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Answer_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-06-04 15:31:01.857 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_last_edit_date":"2018-06-03 19:46:45.113 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How do I get tensorboard to work with sagemaker studio?",
        "Question_body":"<p>Currently playing around with tensorboard on Sagemaker studio. According to aws, it is supposedly possible. However, I keep encountering error code 500 after launching tensorboard and changing the file path to .....\/proxy\/{port number}<\/p>\n<p>Great if somebody can assists on this topic :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 13:49:48.783 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":22,
        "Owner_creation_date":"2022-02-16 03:15:56.94 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.55 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=\"https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample\" rel=\"nofollow noreferrer\">https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample<\/a><\/p>\n<p>Did you follow those instructions?<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-07-08 16:02:18.023 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Question_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-31 21:22:43.253 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"linux|amazon-web-services|amazon-ec2|conda|amazon-sagemaker",
        "Question_view_count":278,
        "Owner_creation_date":"2018-12-21 02:51:36.8 UTC",
        "Owner_last_access_date":"2022-09-25 01:54:35.743 UTC",
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Answer_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-31 22:01:21.237 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Question_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-22 10:44:12.607 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-cli|amazon-vpc|amazon-sagemaker",
        "Question_view_count":510,
        "Owner_creation_date":"2020-08-19 16:37:56.91 UTC",
        "Owner_last_access_date":"2021-06-04 19:05:16.51 UTC",
        "Owner_location":"Delft, Netherlands",
        "Owner_reputation":60,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-22 11:02:18.237 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to use AWS SageMaker to do XGBoost hyperparameter tuning externally?",
        "Question_body":"<p>No bias here, but I find it hard to find anything in AWS documentation. Microsoft Azure is much easier for me.<\/p>\n\n<p>Here is what I have now: <\/p>\n\n<ul>\n<li>A binary classification app fully built with Python, with xgboost being the ML model. Here xgboost has a set of optimized hyperparameters obtained from SageMaker.<\/li>\n<li>A SageMaker notebook to launch hyperparameter tuning jobs for xgboost. Then I manually copy and paste and hyperparameters into xgboost model in the Python app to do prediction.<\/li>\n<\/ul>\n\n<p>As you can see, the way I do it is far away from ideal. What I want to do now is adding a piece of code in the Python app to initiate the hyperparameters job in SageMaker automatically and return the best model as well. That way, the hyperparameter job is automated and I don't need to do the copy and paste again.<\/p>\n\n<p>However, I haven't been able to do that yet. I followed this <a href=\"https:\/\/aws.amazon.com\/blogs\/apn\/integrating-with-amazon-sagemaker-using-built-in-algorithms-from-external-applications\/\" rel=\"nofollow noreferrer\">documentation<\/a> to install Python SageMaker API. I also have the following code that do XGBoost hyperparameter tuning in SageMaker notebook:<\/p>\n\n<pre><code> def train_xgb_sagemaker(df_train, df_test):\n    pd.concat([df_train['show_status'], df_train.drop(['show_status'], axis=1)], axis=1).to_csv('train.csv',\n                                                                                                index=False,\n                                                                                                header=False)\n    pd.concat([df_test['show_status'], df_test.drop(['show_status'], axis=1)], axis=1).to_csv('validation.csv',\n                                                                                              index=False, header=False)\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'train.csv')\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'validation.csv')\n\n    s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\n    s3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\n    print('train_path: ', s3_input_train)\n    print('validation_path: ', s3_input_validation)\n\n    # hyperparameter tuning of XGBoost - SageMaker\n    sess = sagemaker.Session()\n\n    container = get_image_uri(region, 'xgboost', 0.90 - 1)\n    xgb = sagemaker.estimator.Estimator(container,\n                                        role,\n                                        train_instance_count=1,\n                                        train_instance_type='ml.m4.xlarge',\n                                        output_path='s3:\/\/{}\/{}\/output'.format(params['BUCKET'], prefix),\n                                        sagemaker_session=sess)\n\n    xgb.set_hyperparameters(eval_metric='auc',\n                            objective='binary:logistic',\n                            num_round=100,\n                            rate_drop=0.3,\n                            tweedie_variance_power=1.4)\n\n    hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                             'min_child_weight': ContinuousParameter(1, 10),\n                             'alpha': ContinuousParameter(0, 2),\n                             'max_depth': IntegerParameter(1, 10),\n                             'num_round': IntegerParameter(1, 300)}\n\n    objective_metric_name = 'validation:auc'\n\n    tuner = HyperparameterTuner(xgb,\n                                objective_metric_name,\n                                hyperparameter_ranges,\n                                max_jobs=20,\n                                max_parallel_jobs=3)\n\n    tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n\n    smclient.describe_hyper_parameter_tuning_job(\n        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']\n\n    print('Please check hyperparameter tuning for best models!')\n    time.sleep(4000)\n    # best_model_path = 's3:\/\/{}\/{}\/output\/{}\/output\/model.tar.gz'.format(bucket, prefix, tuner.best_training_job())\n    return tuner.best_training_job()\n<\/code><\/pre>\n\n<p>So the question is how to embed this piece of code into my Python app so that I can do everything in one place? Thanks very much for any hints as I've been hanging on this problem for days!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-19 18:10:09.52 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":1366,
        "Owner_creation_date":"2014-05-25 15:30:58.507 UTC",
        "Owner_last_access_date":"2022-07-24 23:13:53.413 UTC",
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":1021,
        "Owner_up_votes":92,
        "Owner_down_votes":0,
        "Owner_views":120,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-09-19 21:21:00.697 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to solve programming error when storing pandas data frame to snowflake",
        "Question_body":"<p>I'm trying to use SQLAlchemy to store a data frame I created in sagemaker to snowflake. The code only works with certain columns. When I add other columns it gives me an error even though they have the same data type. In the following example, if I only upload TA_ID it works, yet if I upload Cluster_ID, the code throws me an error. <\/p>\n\n<p>I checked SQLAlchemy website but didn't find much information on programming error. <\/p>\n\n<h2>SQL codes used to create table<\/h2>\n\n<pre><code>CREATE OR REPLACE TABLE test.m (\n    TA_ID string,\n     Cluster_ID string\n)\n<\/code><\/pre>\n\n<h2>Python code<\/h2>\n\n<pre><code>master2.to_sql(name='m', con=engine2, if_exists='append',  schema='test',index=False, index_label=None, chunksize=2000 )\n<\/code><\/pre>\n\n<p>ProgrammingError: <\/p>\n\n<pre><code>(snowflake.connector.errors.ProgrammingError) 000904 (42000): SQL compilation error: error line 1 at position 29\ninvalid identifier '\"Cluster_ID\"' [SQL: 'INSERT INTO test.m (\"TA_ID\", \"Cluster_ID\") VALUES (%(TA_ID)s, %(Cluster_ID)s)'] [parameters: ({'TA_ID': 'TA007', 'Cluster_ID': '0'}, {'TA_ID': 'TA007', 'Cluster_ID': '16'}, {'TA_ID': 'TA007', 'Cluster_ID': '40'}, {'TA_ID': 'TA007', 'Cluster_ID': '15'}, {'TA_ID': 'TA007', 'Cluster_ID': '29'}, {'TA_ID': 'TA007', 'Cluster_ID': '23'}, {'TA_ID': 'TA007', 'Cluster_ID': '9'}, {'TA_ID': 'TA007', 'Cluster_ID': '25'}, {'TA_ID': 'TA007', 'Cluster_ID': '42'}, {'TA_ID': 'TA007', 'Cluster_ID': '28'})] (Background on this error at: http:\/\/sqlalche.me\/e\/f405)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-23 01:30:22.99 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"sql|pandas|sqlalchemy|amazon-sagemaker|snowflake-cloud-data-platform",
        "Question_view_count":934,
        "Owner_creation_date":"2015-01-23 20:35:22.283 UTC",
        "Owner_last_access_date":"2019-10-22 15:28:10.93 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-05-23 04:54:03.9 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Question_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 02:50:03.78 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker|distributed-training|horovod",
        "Question_view_count":17,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-14 23:27:55.693 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker From RecordIO To Sparse Matrix",
        "Question_body":"<p>While preparing my data for Sagemaker's Factorization Machine implementation for training, I am successfully using the function <code>write_spmatrix_to_sparse_tensor<\/code> (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py\" rel=\"nofollow noreferrer\">source code<\/a>) to transform my data from a sparse matrix to the recordio format expected by Sagemaker's factorization machine implementation.<\/p>\n\n<p>Example where I have limited import statements to the provided function:<\/p>\n\n<pre><code>import os\nimport io\nimport boto3\nimport sagemaker.amazon.common as smac\n\ndef write_recordio(array, y, prefix, f):\n    # Convert to record protobuf\n    buf = io.BytesIO()\n    smac.write_spmatrix_to_sparse_tensor(array=array, file=buf, labels=y)\n    buf.seek(0)\n\n    fname = os.path.join(prefix, f)\n    boto3.Session().resource('s3').Bucket('bucket_name').Object(fname).upload_fileobj(buf)\n<\/code><\/pre>\n\n<p>An example snippet of the argument <code>array<\/code> which are features:<\/p>\n\n<pre><code>   (0, 990290)  1.0\n   (0, 1266265) 1.0\n   (1, 560338)  1.0\n   (1, 1266181) 1.0\n   (2, 182872)  1.0\n   (2, 1266205) 1.0\n   ...\n<\/code><\/pre>\n\n<p>An example format of y which is my target:<\/p>\n\n<p><code>[1. 1. 1. ... 3. 1. 5.]<\/code><\/p>\n\n<p><code>write_spmatrix_to_sparse_tensor<\/code> works as intended with the above function and input. After training my model, I then use Sagemaker's <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html\" rel=\"nofollow noreferrer\">Batch Transform<\/a> to receive a <code>.out<\/code> file with many outputs of type <code>&lt;class 'record_pb2.Record'&gt;<\/code><\/p>\n\n<p>Examples:<\/p>\n\n<p>One record from <code>write_spmatrix_to_sparse_tensor<\/code> output:<\/p>\n\n<pre><code>features {\n  key: \"values\"\n  value {\n    float32_tensor {\n      values: 1.0\n      values: 1.0\n      keys: 990290\n      keys: 1266265\n      shape: 1266394\n    }\n  }\n}\nlabel {\n  key: \"values\"\n  value {\n    float32_tensor {\n      values: 1.0\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>One record from batch transform output (<code>.out<\/code>) file where many of these records exist):<\/p>\n\n<pre><code>label {\n  key: \"score\"\n  value {\n    float32_tensor {\n      values: 1.5246734619140625\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>So now I have a file originally written using <code>write_spmatrix_to_sparse_tensor<\/code> and an output from <code>transformer.transform<\/code> and I would like to get back to my original sparse matrix format from these files. Essentially, if the function <code>write_sparse_tensor_to_spmatrix<\/code> existed, what would it look like? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-05 21:23:59.163 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|numpy|amazon-sagemaker",
        "Question_view_count":563,
        "Owner_creation_date":"2016-08-11 18:26:59.39 UTC",
        "Owner_last_access_date":"2022-09-23 20:48:59.05 UTC",
        "Owner_location":null,
        "Owner_reputation":1159,
        "Owner_up_votes":40,
        "Owner_down_votes":5,
        "Owner_views":53,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-09-05 21:58:57.97 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Set Spark version for Sagemaker on Glue Dev Endpoint",
        "Question_body":"<p>To create my Glue scripts, I use development endpoints with Sagemaker notebooks that run the Pyspark (Sparkmagic) kernel.\nThe latest version of Glue (version 1.0) supports Spark 2.4. However, my Sagemaker notebook uses Spark version 2.2.1. \nThe function I want to test only exists as of Spark 2.3. \nIs there a way to solve this mismatch between the dev endpoint and the Glue job? Can I somehow set the Spark version of the notebook?<br>\nI couldn't find anything in the documentation.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-26 09:49:16.087 UTC",
        "Question_favorite_count":2.0,
        "Question_score":2,
        "Question_tags":"amazon-web-services|apache-spark|aws-glue|amazon-sagemaker",
        "Question_view_count":642,
        "Owner_creation_date":"2017-09-14 09:29:36.317 UTC",
        "Owner_last_access_date":"2021-12-14 10:33:53.857 UTC",
        "Owner_location":null,
        "Owner_reputation":68,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. <\/p>\n\n<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.<\/p>\n\n<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-28 05:05:57.933 UTC",
        "Answer_last_edit_date":"2019-08-28 14:57:56.93 UTC",
        "Answer_score":5.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How should I version control sagemaker model?",
        "Question_body":"<p>So I have a pre-trained ML model stored in an S3 bucket (name<code>example.tar.gz<\/code>), I have a pipeline, and it will run terraform to create a model by using this file on SageMaker and get prediction, my question is what's the best practice of version control the created-model on SageMaker?<\/p>\n<p>My thought is I manually set this model artifact a name like <code>example-v1.tar.gz<\/code>, meaning this is the first artifact we will deploy, then because my pipeline can version control the terraform code (build a <code>terraform-v1.0.0.zip<\/code> file) then execute this file to create the model on SageMaker, my question is basically should I just use <code>model-v1<\/code> as model name on SageMaker or should I update this model name frequently, like everytime there's a new-versioned terraform zip file, we use the same version number for the model name on SageMaker (<code>model-v1.0.0<\/code>)?<\/p>\n<p>I'm new to ML model deployment, a bit confused about this, hope this makes sense, can someone help me?Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2020-12-10 11:02:02.173 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|deployment|amazon-sagemaker|concourse",
        "Question_view_count":747,
        "Owner_creation_date":"2018-10-30 17:35:56.27 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883 UTC",
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to call sagemaker endpoint from another server",
        "Question_body":"<p>I try to call aws sagemaker endpoint. but I don't use lambda function. only, I want to acess endpoint ARN, URL.<\/p>\n\n<p>if impossible method, I want to know lambda function<\/p>\n\n<p>my endpoint based keras model. I don't know json.dumps <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json \nimport boto3 \nclient = boto3.client('runtime.sagemaker')\n\nimport numpy as np\ntest = np.zeros((1, 1, 4325))\ntest[0][0][1] = 1\n\ndata = {\"instances\": test.tolist()} \nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  Body=json.dumps(data))\nresponse_body = response['Body'] \nprint(response_body.read())\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2019-07-24 01:48:17.447 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":597,
        "Owner_creation_date":"2019-07-24 01:24:52.393 UTC",
        "Owner_last_access_date":"2020-11-24 04:06:17.753 UTC",
        "Owner_location":"Korea",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2019-07-24 05:17:08.77 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"installing jupyterlab extensions on notebook startup",
        "Question_body":"<p>Every time my notebook shuts down and restarts, I lose the plugins and have to reinstall them from the terminal \nIs there a way to set jupyterlab extensions to be installed automatically on starting my sagemaker notebook?\nThe plugin I'm trying to install is:<\/p>\n\n<pre><code>jupyter nbextension enable --py widgetsnbextension\njupyter labextension install @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n\n<p>Any insight would be appreciated<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-04 16:50:31.67 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker|jupyter-lab",
        "Question_view_count":927,
        "Owner_creation_date":"2019-06-08 14:45:40.06 UTC",
        "Owner_last_access_date":"2022-09-24 20:45:13.11 UTC",
        "Owner_location":null,
        "Owner_reputation":947,
        "Owner_up_votes":267,
        "Owner_down_votes":6,
        "Owner_views":241,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-26 13:23:12.75 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pyspark|aws-glue|amazon-sagemaker|aws-step-functions",
        "Question_view_count":242,
        "Owner_creation_date":"2013-08-09 12:45:12.53 UTC",
        "Owner_last_access_date":"2021-04-28 18:42:48.283 UTC",
        "Owner_location":"Australia",
        "Owner_reputation":135,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-30 15:54:38.127 UTC",
        "Answer_last_edit_date":"2021-10-14 17:03:02.413 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2021-04-14 11:03:24.67 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to read large text file stored in S3 from sagemaker jupyter notebook?",
        "Question_body":"<p>I have a large (25 MB approx.) CSV file stored in S3. It contains two columns. Each cell of the first column contains the file references and each cell of the second column contains a large(500 to 1000 words) body of the text. There are several thousand rows in this CSV.<\/p>\n<p>I want to read it from <code>sagemaker jupyter notebook<\/code> and save it as a list of strings in memory. And then I shall use this list in my NLP models.<\/p>\n<p>I am using the following code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    def load_file(bucket, key, sep=','):\n        client = boto3.client('s3')\n        obj = client.get_object(Bucket=bucket, Key=key)\n        data = obj['Body'].read().decode('utf-8')\n   \n        text = open(data)\n        string_io = StringIO(data) \n        return pd.read_csv(string_io, sep=sep)\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code>    file = load_file(&quot;bucket&quot;, 'key',sep=',')\n\n<\/code><\/pre>\n<p>I am getting the following error:<\/p>\n<hr \/>\n<blockquote>\n<p>OSError: [Errno 36] File name too long:<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-03 14:17:12.93 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|text|boto3|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_date":"2021-01-03 13:43:17.11 UTC",
        "Owner_last_access_date":"2021-01-18 12:51:14.517 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-01-04 15:03:17.003 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Writing preprocessed output CSV to S3 from Scikit Learn image on Sagemaker",
        "Question_body":"<p>My problem: writing out a CSV file to S3 from inside a Sagemaker SKLearn image. I know how to write CSVs to S3 from a notebook - that is working fine. It's within the docker image that I'm unable to get it to work.<\/p>\n<p>This is a preprocessing.py script called as an entry_point parameter to the SKLearn estimator. The purpose is to pre-process the data prior to running an inference. It's the first step in my inference pipeline.<\/p>\n<p><strong>Everything is working as expected in my preprocessing script, except outputting the file at the end<\/strong>.<\/p>\n<ol>\n<li>Attempt #1 - this <strong>produces a CSV file that has strange binary-looking data at the beginning and end of the file<\/strong> (before the first cell and after the last cell of the CSV). It's almost a valid CSV but not quite. <em>See the image at the end.<\/em><\/li>\n<\/ol>\n<pre><code>def write_joblib(file, path):\n    s3_bucket, s3_key = path.split('\/')[2], path.split('\/')[3:]\n    s3_key = '\/'.join(s3_key)\n    with BytesIO() as f:\n        joblib.dump(file, f)\n        f.seek(0)\n        boto3.client(&quot;s3&quot;).upload_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n    \npredictors_csv = predictors_df.to_csv(index = False)\nwrite_joblib(predictors_csv, predictors_s3_uri)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Attempt #2 - I used StringIO rather than BytesIO. However, this produced a zero-byte file on S3.<\/li>\n<li>Attempt #3 - I tried boto3.client('s3').put_object(...) but got ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied<\/li>\n<\/ol>\n<p>I believe I am almost there with Attempt #1 above. I assume it's an encoding issue. If I can fix the structure of the CSV file to remove the non-text characters at the start it will be working. A screenshot of the CSV in a Notepad++ is below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nXHMx.png\" rel=\"nofollow noreferrer\">Notice the non-character text at the start of the CSV file below<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-26 02:09:04.647 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":207,
        "Owner_creation_date":"2020-12-26 00:56:36.687 UTC",
        "Owner_last_access_date":"2021-11-24 11:16:14.577 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Livy logs location(in S3) for an EMR cluster(debuging Neither SparkSession nor HiveContext\/SqlContext is available)",
        "Question_body":"<p>I'm using AWS SageMaker connected to an EMR cluster via Livy, with a &quot;normal&quot; session(default session config) the connection is created, and spark context works fine. but when adding<\/p>\n<pre><code>spark.pyspark.python&quot;:&quot;.\/ANACONDA\/env_name\/bin\/python3&quot;,\n&quot;spark.yarn.dist.archives&quot;:&quot;s3:\/\/&lt;path&gt;\/env_name.tar.gz#ANACONDA&quot;\n<\/code><\/pre>\n<p>The session is not created and an error is thrown:<\/p>\n<blockquote>\n<p>Neither SparkSession nor HiveContext\/SqlContext is available<\/p>\n<\/blockquote>\n<p>If I remove the spark.pyspark.python line, it takes some time(because it is distributing the .tar.gz file to executors) but it works, session and spark context arre created(but I cannot use the environment in the .tar.gz), so I guess it has something to do with <strong>spark.pyspark.python<\/strong><\/p>\n<p>Given that context: I'm trying to debug what's happening and for that, I want to check the Livy logs, but I cannot find them, I know they should be in S3 <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/spark-driver-logs-emr-cluster\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/spark-driver-logs-emr-cluster\/<\/a> but I cannot find them anywhere, can anyone guide me to the logs location? or any idea on how to debug the issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-18 07:07:52.393 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|apache-spark|amazon-emr|amazon-sagemaker|livy",
        "Question_view_count":127,
        "Owner_creation_date":"2016-06-15 18:26:00.217 UTC",
        "Owner_last_access_date":"2022-09-20 17:52:45.723 UTC",
        "Owner_location":"Guatemala City, Guatemala Department, Guatemala",
        "Owner_reputation":3287,
        "Owner_up_votes":99,
        "Owner_down_votes":24,
        "Owner_views":435,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Getting this ERROR Main: - Unable to infer schema for JSON. It must be specified manually",
        "Question_body":"<p>I'm trying to merge my ground truth values with data captured(for Model Quality Monitoring) using <em>sagemaker-model-monitor-groundtruth-merger<\/em> container image.<\/p>\n<pre><code>def run_merge_job_processor(\n        region,\n        instance_type,\n        role,\n        bucket_name,\n        groundtruth_path,\n        endpoint_input,\n        merge_path,\n        instance_count=1,\n        ):\n     \n    \n    groundtruth_input_1 = ProcessingInput(input_name=&quot;groundtruth_input_1&quot;,\n                              source=&quot;s3:\/\/{}\/{}&quot;.format(bucket_name, groundtruth_path),\n                              destination=f&quot;\/opt\/ml\/processing\/groundtruth{datetime.utcnow():%Y\/%m\/%d\/%H}&quot;,\n                              s3_data_type=&quot;S3Prefix&quot;,\n                              s3_input_mode=&quot;File&quot;)\n    \n    &quot;\/&quot;.join(endpoint_input.split(&quot;\/&quot;)[3:])\n    \n    endpoint_input_1 = ProcessingInput(\n                          input_name=&quot;endpoint_input_1&quot;,\n                          source=&quot;s3:\/\/{}\/{}&quot;.format(bucket_name, endpoint_input),\n                          destination=&quot;\/opt\/ml\/processing\/input_data\/{}&quot;.format(&quot;\/&quot;.join(endpoint_input.split(&quot;\/&quot;)[3:])),\n                          s3_data_type=&quot;S3Prefix&quot;,\n                          s3_input_mode=&quot;File&quot;)\n     \n    output = ProcessingOutput(\n                          output_name=&quot;result&quot;,\n                          source=&quot;\/opt\/ml\/processing\/output&quot;,\n                          destination=f&quot;s3:\/\/{bucket_name}\/{merge_path}&quot;)\n    \n    \n    inputs = [ groundtruth_input_1, endpoint_input_1 ]\n    outputs = [ output ]\n    \n    env = {\n        &quot;dataset_format&quot;: &quot;{\\&quot;sagemakerCaptureJson\\&quot;: {\\&quot;captureIndexNames\\&quot;: [\\&quot;endpointInput\\&quot;,\\&quot;endpointOutput\\&quot;]}}&quot;,\n        'dataset_source': '\/opt\/ml\/processing\/input_data',\n        'ground_truth_source': '\/opt\/ml\/processing\/groundtruth',\n        'output_path': '\/opt\/ml\/processing\/output'\n    }\n    \n    processor = Processor(image_uri=get_model_monitor_container_uri(region),\n                          instance_count=instance_count,\n                          instance_type=instance_type,\n                          role=role,\n                          env=env,\n                          )\n    \n    return processor.run(\n        inputs=inputs,\n        outputs=outputs,\n        wait=True\n<\/code><\/pre>\n<p>The above code that I've used for creating the processing job\nwhere my processing job is getting failed with the following error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5oxhi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5oxhi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kNcrN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kNcrN.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-19 10:35:43.043 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":26,
        "Owner_creation_date":"2022-08-10 08:15:11.5 UTC",
        "Owner_last_access_date":"2022-09-24 14:19:19.21 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 16:27:28.437 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_date":"2013-02-25 19:50:19.38 UTC",
        "Owner_last_access_date":"2022-09-23 22:29:29.277 UTC",
        "Owner_location":"Puebla",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-31 18:44:45.577 UTC",
        "Answer_last_edit_date":"2019-01-31 19:17:47.733 UTC",
        "Answer_score":2.0,
        "Question_last_edit_date":"2019-02-07 04:20:17.233 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Use AWS Lambda to run Sagemaker Batch Transform job",
        "Question_body":"<p>I would like to place a csv file in an S3 bucket and get predictions from a Sagemaker model using batch transform job automatically. I would like to do that by using s3 event notification (upon csv upload) to trigger a Lambda function which would do a batch transform job. The lambda function I have written so far is this:<\/p>\n<pre><code>\nimport boto3\nsagemaker = boto3.client('sagemaker')\n\ninput_data_path = 's3:\/\/yeex\/upload\/examples.csv'.format(default_bucket, 's3:\/\/yeex\/upload\/', 'examples.csv')\noutput_data_path = 's3:\/\/nooz\/download\/'.format(default_bucket, 's3:\/\/nooz\/download')\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = y_xgboost_21,\n    instance_count = 1,\n    instance_type = 'ml.m5.large',\n    strategy = 'SingleRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='y-test-batch',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path, \n                        content_type = 'text\/csv', \n                        split_type = 'Line')\n\n<\/code><\/pre>\n<p>The error it returns is that object sagemaker does not have module transform\nWhat is the syntax I should use in Lambda function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-27 16:49:08.033 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":777,
        "Owner_creation_date":"2021-05-09 12:10:07.177 UTC",
        "Owner_last_access_date":"2022-04-26 14:25:52.117 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-28 14:59:41.823 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can you add widgets to SageMaker Notebooks similar to Azure DataBricks?",
        "Question_body":"<p>I have used Azure DataBricks in my earlier job and it comes with extended support for Notebook widgets to execute notebook manually\/ commission a notebook job by selecting some values (Ideally your run state params or variables.)<\/p>\n<p>For information here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/notebooks\/widgets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/notebooks\/widgets<\/a><\/p>\n<p>Is there a similar service or option that I can build while working on SageMaker notebooks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-14 02:59:32.897 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|azure-databricks|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_date":"2020-02-23 07:50:57.04 UTC",
        "Owner_last_access_date":"2022-09-16 18:42:35.31 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"boto3 how to get the logstream form a sagemaker transform job?",
        "Question_body":"<p>i am able to crete the job and it fail, using boto3<\/p>\n<pre><code>import boto3\nsession = boto3.session.Session()\n\n\nclient = session.client('sagemaker')\ndescibe = client.describe_transform_job(TransformJobName=&quot;my_transform_job_name&quot;)\n<\/code><\/pre>\n<p>in the ui i can see the button to go to the logs, i can use boto3 to retrive the logs if hardcode the group name and the log-stream.<\/p>\n<p>but how can i get the Log stream from the batch transfrom job?  shouldnt be a field with logstream or something like that in the &quot;.describe_transform_job&quot;?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-09 22:02:00.01 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":167,
        "Owner_creation_date":"2012-09-27 18:26:03.723 UTC",
        "Owner_last_access_date":"2022-09-22 02:14:04.407 UTC",
        "Owner_location":"austin",
        "Owner_reputation":1584,
        "Owner_up_votes":139,
        "Owner_down_votes":1,
        "Owner_views":192,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS EventBridge triggers SageMaker pipeline successfully but says failure",
        "Question_body":"<p>I have a SageMaker pipeline set up to transform audio data and run a model training script.  We are using custom containers for both parts if that matters.  The pipeline runs end to end when I test it in SageMaker with no issues.<\/p>\n<p>I set up an EventBridge rule to trigger the pipeline overnight at regular intervals.  This triggers the pipeline successfully, but in my invocations dashboard it appears that the jobs fail.<\/p>\n<p>I'm using the default event bus.  The execution role has all Read, Write, and List on my pipeline's ARN.<\/p>\n<p>While the pipelines appear to run successfully, it would be nice if I could rely on the dashboards in the EventBridge console.  Is there anywhere else I should be looking to debug?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uP4Lt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uP4Lt.png\" alt=\"Failed Invocations\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SsKDI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SsKDI.png\" alt=\"SageMaker Pipelines are Successful\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-06 14:53:50.593 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":20,
        "Owner_creation_date":"2013-08-16 20:23:45.67 UTC",
        "Owner_last_access_date":"2022-09-23 21:13:36.407 UTC",
        "Owner_location":null,
        "Owner_reputation":1105,
        "Owner_up_votes":658,
        "Owner_down_votes":2,
        "Owner_views":222,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Monitoring the performance of ML model on EC2 Instance",
        "Question_body":"<p>If we go back and use dockerized ML models on EC2 Instances - is there any native way to check the model metrics (for classification, for example, accuracy, precision, recall and f1-score)? For sure, Cloudwatch can be used but it will just gives the overall info regarding endpoint, disk utilisation, etc., but not ML model metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-16 13:14:39 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-ec2|amazon-sagemaker|production",
        "Question_view_count":67,
        "Owner_creation_date":"2013-12-08 08:33:34.717 UTC",
        "Owner_last_access_date":"2022-09-21 18:08:28.293 UTC",
        "Owner_location":null,
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-03-16 19:07:31.6 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Possible to attach Elastic IP to sagemaker notebook instance?",
        "Question_body":"<p>I want to connect to a database running in different cloud provider and it is exposed publicly.<\/p>\n\n<p>I need to connect to that database from sagemaker notebook instance.<\/p>\n\n<p>But the public ip of the sagemaker notebook instance needs to be whitelisted on the other side.<\/p>\n\n<p>Is it possible to attach elastic ip to sagemaker notebook instance as I don't see any option to attach eip to sagemaker notebook instance?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2019-11-20 14:10:12.963 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":708,
        "Owner_creation_date":"2013-05-30 13:54:41.203 UTC",
        "Owner_last_access_date":"2022-09-23 09:43:32.973 UTC",
        "Owner_location":"Bangalore, India",
        "Owner_reputation":3577,
        "Owner_up_votes":2395,
        "Owner_down_votes":20,
        "Owner_views":626,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to automate jupyter notebook execution on aws?",
        "Question_body":"<p>I got a task to complete where I need to automate Jupyter notebook execution on AWS. I'm totally new to AWS environment so don't have any idea how to do it efficiently. Things I need to do are the following -<\/p>\n<ol>\n<li>Need REST API(s) to start and stop Jupyter notebook execution on AWS.<\/li>\n<li>Need to send parameters to the notebook while calling using API.<\/li>\n<\/ol>\n<p>What are the AWS components I need, to perform the above task?<\/p>\n<pre><code>import boto3,time\n \nemr = boto3.client(\n    'emr',\n    region_name='us-west-1'\n)\n \n \n \nstart_resp = emr.start_notebook_execution(\n    EditorId='e-40AC8ZO6EGGCPJ4DLO48KGGGI',\n    RelativePath='boto3_demo.ipynb',\n    ExecutionEngine={'Id':'j-1HYZS6JQKV11Q'},\n    ServiceRole='EMR_Notebooks_DefaultRole'\n)\n \nexecution_id = start_resp[&quot;NotebookExecutionId&quot;]\nprint(execution_id)\nprint(&quot;\\n&quot;)\n \n \ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\n \nprint(describe_response)\nprint(&quot;\\n&quot;)\n \n \n \nlist_response = emr.list_notebook_executions()\nprint(&quot;Existing notebook executions:\\n&quot;)\nfor execution in list_response['NotebookExecutions']:\n    print(execution)\n    print(&quot;\\n&quot;)\n \n \n \nprint(&quot;Sleeping for 5 sec...&quot;)\ntime.sleep(5)\n \nprint(&quot;Stop execution &quot; + execution_id)\nemr.stop_notebook_execution(NotebookExecutionId=execution_id)\ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\nprint(describe_response)\nprint(&quot;\\n&quot;)    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2021-10-30 13:09:27.78 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|jupyter-notebook|amazon-sagemaker|aws-emr-studio",
        "Question_view_count":274,
        "Owner_creation_date":"2016-01-12 12:36:46.603 UTC",
        "Owner_last_access_date":"2022-09-23 16:01:34.33 UTC",
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-30 19:50:50.2 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Call Sagemaker Tensorflow Resnet50 Endpoint from node.js",
        "Question_body":"<p><strong>The Question<\/strong><\/p>\n\n<p>I want to know the equivalent in node.js to this Python code:<\/p>\n\n<pre><code>from keras.preprocessing import image\nfrom PIL import Image\nfrom keras.applications.resnet50 import preprocess_input\n\nraw_img = image.load_img(\"some\/path\").resize((224, 224), Image.NEAREST)\nimg = preprocess_input(image.img_to_array(raw_img))\n<\/code><\/pre>\n\n<p><strong>Context<\/strong><\/p>\n\n<p>I uploaded Keras's ResNet50 model to a SageMaker endpoint. I can call it from Python using the code below:<\/p>\n\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker')\n\nfrom keras.preprocessing import image\nfrom PIL import Image\nfrom keras.applications.resnet50 import preprocess_input\n\nraw_img = image.load_img(\"some\/path\").resize((224, 224), Image.NEAREST)\nimg = preprocess_input(image.img_to_array(raw_img))\n\nresponse = client.invoke_endpoint(\n  EndpointName=\"SAGEMAKER_ENDPOINT_NAME\",\n  Body=json.dumps({ \"instances\": [ img.tolist() ] }),\n  ContentType=\"application\/json\"\n)\n<\/code><\/pre>\n\n<p>Now I need to do the same from node.js. I figured out how to hit the endpoint using the <code>aws-sdk<\/code>:<\/p>\n\n<pre><code>import * as aws from 'aws-sdk';\nconst sageMaker = new aws.SageMakerRuntime({\n  region: 'ap-northeast-1'\n});\nsageMaker.invokeEndpoint({\n  EndpointName: endpointName,\n  Body: input,\n  ContentType: \"application\/json\",\n}, (error, res) =&gt; {\n  if (error) { return reject(error); }\n  \/\/ YEAH\n})\n<\/code><\/pre>\n\n<p>But I cannot figure our how to generate the <code>input<\/code> json, that is, the equivalent of this python snippet:<\/p>\n\n<pre><code>from keras.preprocessing import image\nfrom PIL import Image\nfrom keras.applications.resnet50 import preprocess_input\n\nraw_img = image.load_img(\"some\/path\").resize((224, 224), Image.NEAREST)\nimg = preprocess_input(image.img_to_array(raw_img))\n<\/code><\/pre>\n\n<p>Is there any library to achieve the same, or do I need to reinvent the wheel?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-28 12:15:47.793 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|node.js|keras|amazon-sagemaker",
        "Question_view_count":533,
        "Owner_creation_date":"2010-09-30 04:45:01.247 UTC",
        "Owner_last_access_date":"2022-09-24 22:55:49.96 UTC",
        "Owner_location":"Japan",
        "Owner_reputation":1178,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":132,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to save models in S3 and checkpoints in Sagemaker",
        "Question_body":"<p>I am using Sagemaker with Tensorflow version 1.12.0 and a conda_tensorflor_p36 kernel. I am able to successfully train, evaluate the validation data, and predict in a deployed model. I am training with a training script.<\/p>\n<p>I want to be able to save the model in S3 and then to load it. It supposes that the model will be automatically saved but it does not happen.<\/p>\n<p>I am using the following.\nIn the script:<\/p>\n<pre><code>    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--learning-rate', type=float, default=0.001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n        \n    args, _ = parser.parse_known_args()\n\n    epochs     = args.epochs\n    lr         = args.learning_rate\n    batch_size = args.batch_size\n    gpu_count  = args.gpu_count\n    model_dir  = args.model_dir\n    training_dir   = args.training\n    validation_dir = args.validation\n...\n\n    # save Keras model for Tensorflow Serving\n    sess = K.get_session()\n    tensorflow.saved_model.simple_save(\n        sess,\n        os.path.join(model_dir, 'model\/1'),\n        inputs={'inputs': model.input},\n        outputs={t.name: t for t in model.outputs})\n<\/code><\/pre>\n<p>In the notebook<\/p>\n<pre><code>model_path = 's3:\/\/{}\/{}'.format(bucket, model_channel)\n\ntf_estimator = TensorFlow(entry_point='train_cnn.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',  #We use the local instance\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}, #One epoch just to check everything is ok\n                          model_dir=model_path\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n<p>I got as exit<\/p>\n<pre><code>Creating tmp9hs8n9sq_algo-1-xp4eu_1 ... \nAttaching to tmp9hs8n9sq_algo-1-xp4eu_12mdone\nalgo-1-xp4eu_1  | 2020-07-17 21:40:18,234 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\nalgo-1-xp4eu_1  | 2020-07-17 21:40:18,240 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\nalgo-1-xp4eu_1  | 2020-07-17 21:40:18,523 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\nalgo-1-xp4eu_1  | 2020-07-17 21:40:18,541 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\nalgo-1-xp4eu_1  | 2020-07-17 21:40:18,555 sagemaker-containers INFO     Invoking user script\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | Training Env:\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | {\nalgo-1-xp4eu_1  |     &quot;additional_framework_parameters&quot;: {},\nalgo-1-xp4eu_1  |     &quot;channel_input_dirs&quot;: {\nalgo-1-xp4eu_1  |         &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;,\nalgo-1-xp4eu_1  |         &quot;validation&quot;: &quot;\/opt\/ml\/input\/data\/validation&quot;\nalgo-1-xp4eu_1  |     },\nalgo-1-xp4eu_1  |     &quot;current_host&quot;: &quot;algo-1-xp4eu&quot;,\nalgo-1-xp4eu_1  |     &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\nalgo-1-xp4eu_1  |     &quot;hosts&quot;: [\nalgo-1-xp4eu_1  |         &quot;algo-1-xp4eu&quot;\nalgo-1-xp4eu_1  |     ],\nalgo-1-xp4eu_1  |     &quot;hyperparameters&quot;: {\nalgo-1-xp4eu_1  |         &quot;epochs&quot;: 1,\nalgo-1-xp4eu_1  |         &quot;model_dir&quot;: &quot;s3:\/\/mybucket\/myprefix\/models&quot;\nalgo-1-xp4eu_1  |     },\nalgo-1-xp4eu_1  |     &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\nalgo-1-xp4eu_1  |     &quot;input_data_config&quot;: {\nalgo-1-xp4eu_1  |         &quot;training&quot;: {\nalgo-1-xp4eu_1  |             &quot;TrainingInputMode&quot;: &quot;File&quot;\nalgo-1-xp4eu_1  |         },\nalgo-1-xp4eu_1  |         &quot;validation&quot;: {\nalgo-1-xp4eu_1  |             &quot;TrainingInputMode&quot;: &quot;File&quot;\nalgo-1-xp4eu_1  |         }\nalgo-1-xp4eu_1  |     },\nalgo-1-xp4eu_1  |     &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\nalgo-1-xp4eu_1  |     &quot;is_master&quot;: true,\nalgo-1-xp4eu_1  |     &quot;job_name&quot;: &quot;sagemaker-tensorflow-scriptmode-2020-07-17-21-40-14-146&quot;,\nalgo-1-xp4eu_1  |     &quot;log_level&quot;: 20,\nalgo-1-xp4eu_1  |     &quot;master_hostname&quot;: &quot;algo-1-xp4eu&quot;,\nalgo-1-xp4eu_1  |     &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\nalgo-1-xp4eu_1  |     &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-487764570858\/sagemaker-tensorflow-scriptmode-2020-07-17-21-40-14-146\/source\/sourcedir.tar.gz&quot;,\nalgo-1-xp4eu_1  |     &quot;module_name&quot;: &quot;train_cnn&quot;,\nalgo-1-xp4eu_1  |     &quot;network_interface_name&quot;: &quot;eth0&quot;,\nalgo-1-xp4eu_1  |     &quot;num_cpus&quot;: 4,\nalgo-1-xp4eu_1  |     &quot;num_gpus&quot;: 0,\nalgo-1-xp4eu_1  |     &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\nalgo-1-xp4eu_1  |     &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\nalgo-1-xp4eu_1  |     &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\nalgo-1-xp4eu_1  |     &quot;resource_config&quot;: {\nalgo-1-xp4eu_1  |         &quot;current_host&quot;: &quot;algo-1-xp4eu&quot;,\nalgo-1-xp4eu_1  |         &quot;hosts&quot;: [\nalgo-1-xp4eu_1  |             &quot;algo-1-xp4eu&quot;\nalgo-1-xp4eu_1  |         ]\nalgo-1-xp4eu_1  |     },\nalgo-1-xp4eu_1  |     &quot;user_entry_point&quot;: &quot;train_cnn.py&quot;\nalgo-1-xp4eu_1  | }\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | Environment variables:\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | SM_HOSTS=[&quot;algo-1-xp4eu&quot;]\nalgo-1-xp4eu_1  | SM_NETWORK_INTERFACE_NAME=eth0\nalgo-1-xp4eu_1  | SM_HPS={&quot;epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/mybucket\/myprefix\/models&quot;}\nalgo-1-xp4eu_1  | SM_USER_ENTRY_POINT=train_cnn.py\nalgo-1-xp4eu_1  | SM_FRAMEWORK_PARAMS={}\nalgo-1-xp4eu_1  | SM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1-xp4eu&quot;,&quot;hosts&quot;:[&quot;algo-1-xp4eu&quot;]}\nalgo-1-xp4eu_1  | SM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;TrainingInputMode&quot;:&quot;File&quot;},&quot;validation&quot;:{&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nalgo-1-xp4eu_1  | SM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nalgo-1-xp4eu_1  | SM_CHANNELS=[&quot;training&quot;,&quot;validation&quot;]\nalgo-1-xp4eu_1  | SM_CURRENT_HOST=algo-1-xp4eu\nalgo-1-xp4eu_1  | SM_MODULE_NAME=train_cnn\nalgo-1-xp4eu_1  | SM_LOG_LEVEL=20\nalgo-1-xp4eu_1  | SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nalgo-1-xp4eu_1  | SM_INPUT_DIR=\/opt\/ml\/input\nalgo-1-xp4eu_1  | SM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nalgo-1-xp4eu_1  | SM_OUTPUT_DIR=\/opt\/ml\/output\nalgo-1-xp4eu_1  | SM_NUM_CPUS=4\nalgo-1-xp4eu_1  | SM_NUM_GPUS=0\nalgo-1-xp4eu_1  | SM_MODEL_DIR=\/opt\/ml\/model\nalgo-1-xp4eu_1  | SM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-487764570858\/sagemaker-tensorflow-scriptmode-2020-07-17-21-40-14-146\/source\/sourcedir.tar.gz\nalgo-1-xp4eu_1  | SM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;,&quot;validation&quot;:&quot;\/opt\/ml\/input\/data\/validation&quot;},&quot;current_host&quot;:&quot;algo-1-xp4eu&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1-xp4eu&quot;],&quot;hyperparameters&quot;:{&quot;epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/mybucket\/myprefix\/models&quot;},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;TrainingInputMode&quot;:&quot;File&quot;},&quot;validation&quot;:{&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;sagemaker-tensorflow-scriptmode-2020-07-17-21-40-14-146&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1-xp4eu&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-487764570858\/sagemaker-tensorflow-scriptmode-2020-07-17-21-40-14-146\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_cnn&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1-xp4eu&quot;,&quot;hosts&quot;:[&quot;algo-1-xp4eu&quot;]},&quot;user_entry_point&quot;:&quot;train_cnn.py&quot;}\nalgo-1-xp4eu_1  | SM_USER_ARGS=[&quot;--epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/mybucket\/myprefix\/models&quot;]\nalgo-1-xp4eu_1  | SM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nalgo-1-xp4eu_1  | SM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nalgo-1-xp4eu_1  | SM_CHANNEL_VALIDATION=\/opt\/ml\/input\/data\/validation\nalgo-1-xp4eu_1  | SM_HP_EPOCHS=1\nalgo-1-xp4eu_1  | SM_HP_MODEL_DIR=s3:\/\/mybucket\/myprefix\/models\nalgo-1-xp4eu_1  | PYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | Invoking script with the following command:\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | \/usr\/bin\/python train_cnn.py --epochs 1 --model_dir s3:\/\/mybucket\/myprefix\/models\nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | \nalgo-1-xp4eu_1  | Collecting matplotlib\nalgo-1-xp4eu_1  |   Downloading https:\/\/files.pythonhosted.org\/packages\/93\/ae\/81b1c98ae97350711adb021ee12ea678b37f608ec2faa35c3a7db11795fa\/matplotlib-3.3.0-1-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.5MB 4.3MB\/s eta 0:00:01\nalgo-1-xp4eu_1  | Collecting kiwisolver&gt;=1.0.1 (from matplotlib)\nalgo-1-xp4eu_1  |   Downloading https:\/\/files.pythonhosted.org\/packages\/ae\/23\/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7\/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 29.2MB\/s ta 0:00:01\nalgo-1-xp4eu_1  | Requirement already satisfied: numpy&gt;=1.15 in \/usr\/local\/lib\/python3.6\/dist-packages (from matplotlib) (1.16.3)\nalgo-1-xp4eu_1  | Collecting cycler&gt;=0.10 (from matplotlib)\nalgo-1-xp4eu_1  |   Downloading https:\/\/files.pythonhosted.org\/packages\/f7\/d2\/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61\/cycler-0.10.0-py2.py3-none-any.whl\nalgo-1-xp4eu_1  | Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.3 (from matplotlib)\nalgo-1-xp4eu_1  |   Downloading https:\/\/files.pythonhosted.org\/packages\/8a\/bb\/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d\/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 25.2MB\/s ta 0:00:01\nalgo-1-xp4eu_1  | Requirement already satisfied: python-dateutil&gt;=2.1 in \/usr\/local\/lib\/python3.6\/dist-packages (from matplotlib) (2.8.0)\nalgo-1-xp4eu_1  | Collecting pillow&gt;=6.2.0 (from matplotlib)\nalgo-1-xp4eu_1  |   Downloading https:\/\/files.pythonhosted.org\/packages\/30\/bf\/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41\/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.2MB 15.3MB\/s ta 0:00:01\nalgo-1-xp4eu_1  | Requirement already satisfied: six in \/usr\/local\/lib\/python3.6\/dist-packages (from cycler&gt;=0.10-&gt;matplotlib) (1.12.0)\nalgo-1-xp4eu_1  | Installing collected packages: kiwisolver, cycler, pyparsing, pillow, matplotlib\nalgo-1-xp4eu_1  |   Found existing installation: Pillow 6.0.0\nalgo-1-xp4eu_1  |     Uninstalling Pillow-6.0.0:\nalgo-1-xp4eu_1  |       Successfully uninstalled Pillow-6.0.0\nalgo-1-xp4eu_1  | Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.0 pillow-7.2.0 pyparsing-2.4.7\nalgo-1-xp4eu_1  | You are using pip version 18.1, however version 20.2b1 is available.\nalgo-1-xp4eu_1  | You should consider upgrading via the 'pip install --upgrade pip' command.\nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | Layer (type)                 Output Shape              Param #   \nalgo-1-xp4eu_1  | =================================================================\nalgo-1-xp4eu_1  | conv2d (Conv2D)              (None, 64, 64, 32)        320       \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dropout (Dropout)            (None, 32, 32, 32)        0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dropout_1 (Dropout)          (None, 16, 16, 64)        0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dropout_2 (Dropout)          (None, 8, 8, 128)         0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | flatten (Flatten)            (None, 4096)              0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dense (Dense)                (None, 128)               524416    \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dropout_3 (Dropout)          (None, 128)               0         \nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | dense_1 (Dense)              (None, 1)                 129       \nalgo-1-xp4eu_1  | =================================================================\nalgo-1-xp4eu_1  | Total params: 912,385\nalgo-1-xp4eu_1  | Trainable params: 912,385\nalgo-1-xp4eu_1  | Non-trainable params: 0\nalgo-1-xp4eu_1  | _________________________________________________________________\nalgo-1-xp4eu_1  | None\nalgo-1-xp4eu_1  | Train on 26 samples, validate on 2 samples\nalgo-1-xp4eu_1  | Epoch 1\/1\nalgo-1-xp4eu_1  |  - 2s - loss: 1.1576 - acc: 0.5000 - val_loss: 2.5328 - val_acc: 0.5000\nalgo-1-xp4eu_1  | Validation loss    : 2.5328116416931152\nalgo-1-xp4eu_1  | Validation accuracy: 0.5\nalgo-1-xp4eu_1  | Keys dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\nalgo-1-xp4eu_1  | WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/saved_model\/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\nalgo-1-xp4eu_1  | Instructions for updating:\nalgo-1-xp4eu_1  | Pass your op to the equivalent parameter main_op instead.\nalgo-1-xp4eu_1  | 2020-07-17 21:40:28,943 sagemaker-containers INFO     Reporting training SUCCESS\ntmp9hs8n9sq_algo-1-xp4eu_1 exited with code 0\nAborting on container exit...\n===== Job Complete =====\n<\/code><\/pre>\n<p><strong>But the model is not saved in the bucket. Also, what could I do If I want to have checkpoints of the model?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-17 22:02:54.427 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|save|amazon-sagemaker|checkpoint",
        "Question_view_count":2904,
        "Owner_creation_date":"2011-10-03 04:45:31.843 UTC",
        "Owner_last_access_date":"2022-05-10 00:16:20.007 UTC",
        "Owner_location":"Mexico",
        "Owner_reputation":380,
        "Owner_up_votes":22,
        "Owner_down_votes":1,
        "Owner_views":128,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How can I preprocess inputs sent to a hugging face estimator?",
        "Question_body":"<p>I've been reviewing tutorials like the one found <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">here<\/a> which detail how to train a huggingface estimator (specifically, a transformer model) and then deploy it to sagemaker.<\/p>\n<p>The tutorial linked above trains an estimator via:<\/p>\n<pre><code>huggingface_estimator = HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py37',\n                            role=role,\n                            hyperparameters = {'epochs': 1,\n                                               'train_batch_size': 32,\n                                               'model_name':'distilbert-base-uncased'\n                                                })\n<\/code><\/pre>\n<p>Where <code>train.py<\/code> outlines a training script which tokenizes input data, and then fine-tunes a transformer model on the training data. <em>Note: in this example, the training data is hard coded into <code>train.py<\/code>, but that is not the source of the issue I'm encountering.<\/em><\/p>\n<p>The model is fit using<\/p>\n<pre><code>huggingface_estimator.fit()\n<\/code><\/pre>\n<p>and is then deployed using<\/p>\n<pre><code>predictor = huggingface_estimator.deploy(1,&quot;ml.g4dn.xlarge&quot;)\n<\/code><\/pre>\n<p>But then this deployed model is used to make a prediction via:<\/p>\n<pre><code>sentiment_input= {&quot;inputs&quot; : &quot;I love using the new Inference DLC.&quot;}\npredictor.predict(sentiment_input)\n<\/code><\/pre>\n<p>The problem is that no one specifed <em>anywhere<\/em> how this input is to be preprocessed: the model does not work on raw text, the text must be tokenized. Even the official <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs<\/a> for sagemaker don't seem to outline how preprocessing for a deployed model is handled.<\/p>\n<p>How can I specify a preprocessing step for my model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 15:40:14.277 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":8,
        "Owner_creation_date":"2019-06-17 15:38:47.3 UTC",
        "Owner_last_access_date":"2022-09-25 02:14:02.827 UTC",
        "Owner_location":"Ontario, Canada",
        "Owner_reputation":4613,
        "Owner_up_votes":1267,
        "Owner_down_votes":633,
        "Owner_views":401,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 16:00:15.88 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2016-03-21 08:49:39.92 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.84 UTC",
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-01-16 20:30:04.95 UTC",
        "Answer_last_edit_date":"2020-01-20 07:54:05.7 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"\"Sagemaker Notebook with Interactive Session -- Install packages",
        "Question_body":"<p>We have followed this doc to spin up notebook running with interactive sessions. We want to add a few python packages to the environment to assist with development (i.e. pyright). I have added the pip install at the bottom, stopped the instance, restart instance, run &quot;import pyright&quot;, but I get &quot;ModuleNotFoundError: No module named 'pyright'&quot;<\/p>\n<pre><code>#!\/bin\/bash\nset -ex\nsudo -u ec2-user -i &lt;&lt;'EOF'\n \nANACONDA_DIR=\/home\/ec2-user\/anaconda3\n \n# Create and Activate Conda Env\necho &quot;Creating glue_pyspark conda enviornment&quot;\nconda create --name glue_pyspark python=3.7 ipykernel jupyter nb_conda -y\n \necho &quot;Activating glue_pyspark&quot;\nsource activate glue_pyspark\n \n# Install Glue Sessions to Env\necho &quot;Installing AWS Glue Sessions with pip&quot;\npip install aws-glue-sessions\n \n# Clone glue_pyspark to glue_scala. This is required because I had to match kernel naming conventions to their environments and couldn't have two kernels in one conda env. \necho &quot;Cloning glue_pyspark to glue_scala&quot;\nconda create --name glue_scala --clone glue_pyspark\n \n# Remove python3 kernel from glue_pyspark\nrm -r ${ANACONDA_DIR}\/envs\/glue_pyspark\/share\/jupyter\/kernels\/python3\nrm -r ${ANACONDA_DIR}\/envs\/glue_scala\/share\/jupyter\/kernels\/python3\n \n# Copy kernels to Jupyter kernel env (Discoverable by conda_nb_kernel)\necho &quot;Copying Glue PySpark Kernel&quot;\ncp -r ${ANACONDA_DIR}\/envs\/glue_pyspark\/lib\/python3.7\/site-packages\/aws_glue_interactive_sessions_kernel\/glue_pyspark\/ ${ANACONDA_DIR}\/envs\/glue_pyspark\/share\/jupyter\/kernels\/glue_pyspark\/\n \necho &quot;Copying Glue Spark Kernel&quot;\nmkdir ${ANACONDA_DIR}\/envs\/glue_scala\/share\/jupyter\/kernels\ncp -r ${ANACONDA_DIR}\/envs\/glue_scala\/lib\/python3.7\/site-packages\/aws_glue_interactive_sessions_kernel\/glue_spark\/ ${ANACONDA_DIR}\/envs\/glue_scala\/share\/jupyter\/kernels\/glue_spark\/\n \necho &quot;Changing Jupyter kernel manager from EnvironmentKernelSpecManager to CondaKernelSpecManager&quot;\nJUPYTER_CONFIG=\/home\/ec2-user\/.jupyter\/jupyter_notebook_config.py\n \nsed -i '\/EnvironmentKernelSpecManager\/ s\/^\/#\/' ${JUPYTER_CONFIG}\necho &quot;c.CondaKernelSpecManager.name_format='conda_{environment}'&quot; &gt;&gt; ${JUPYTER_CONFIG}\necho &quot;c.CondaKernelSpecManager.env_filter='anaconda3$|JupyterSystemEnv$|\/R$'&quot; &gt;&gt; ${JUPYTER_CONFIG}\n\n# Install python modules to env\npip install &quot;pyright&quot;\nEOF\nsystemctl restart jupyter-server  \n<\/code><\/pre>\n<p>Am I missing something in the script? I assumed just &quot;pip install &quot;pyright&quot;&quot; would've worked.<\/p>\n<p>Update:\nI have included the following under the pip install aws-glue-sessions:<\/p>\n<blockquote>\n<p>pip install &quot;pyright&quot;\nand\npip install pyright<\/p>\n<\/blockquote>\n<p>When I check the CloudWatch logs, I see that the package is being downloaded... I would assume it means it's installed.\n[1]: <a href=\"https:\/\/i.stack.imgur.com\/JeKce.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/JeKce.png<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-20 23:21:16.617 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_date":"2015-08-17 20:28:43.25 UTC",
        "Owner_last_access_date":"2022-09-24 20:02:38.657 UTC",
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-21 03:09:59.607 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Is it a good idea to store my dataset in my notebook instance in sagemaker?",
        "Question_body":"<p>I'm new to AWS and I am considering to use amazon sagemaker to train my deep learning model because I'm having memory issues due to the large dataset and neural network that I have to train. I'm confused whether to store my data in my notebook instance or in S3? If I store it in my s3 would I be able to access it to train on my notebook instance? I'm confused on the concepts. Can anyone explain the use of S3 in machine learning in AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 01:17:18.143 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2020-11-21 06:04:32.327 UTC",
        "Owner_last_access_date":"2021-06-03 07:19:55.19 UTC",
        "Owner_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Owner_reputation":97,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":"<p>Yes you can use S3 as storage for your training datasets.<\/p>\n<p>Refer diagram in this link describing how everything works together: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html<\/a><\/p>\n<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I\/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.<\/p>\n<\/blockquote>\n<p>The blog also contains python code snippets using Pipe input mode for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 03:29:38.833 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Question_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-26 13:34:39.743 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":774,
        "Owner_creation_date":"2019-10-02 15:22:33.88 UTC",
        "Owner_last_access_date":"2021-06-07 03:53:56.07 UTC",
        "Owner_location":"Philippines",
        "Owner_reputation":98,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-28 10:54:28.823 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Cannot see the kernel Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized) in sagemaker",
        "Question_body":"<p>Amazon sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-kernels.html\" rel=\"nofollow noreferrer\">documentation<\/a>\nstates that <strong>TensorFlow 2.3 Python 3.7 GPU Optimized<\/strong> kernel should be available to use when a sagemaker notebook instance is used. But when I use a <em>ml.p2.xlarge<\/em> (us-west-2) <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">amazon sagemaker notebook instance<\/a> I cannot see the <em>TensorFlow 2.3 Python 3.7 GPU Optimized<\/em> kernel<\/p>\n<p>I can see other kernles such as<\/p>\n<ul>\n<li>Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized)<\/li>\n<li>Python 3 (MXNet 1.8 Python 3.7 GPU Optimized)<\/li>\n<\/ul>\n<p>Do I need to enable some particular setting to see <em>TensorFlow 2.3 Python 3.7 GPU Optimized<\/em> kernel<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-21 09:39:23.917 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":381,
        "Owner_creation_date":"2014-12-15 09:33:28.49 UTC",
        "Owner_last_access_date":"2022-09-22 13:59:19.453 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":661,
        "Owner_up_votes":18,
        "Owner_down_votes":3,
        "Owner_views":63,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-05 06:23:56.78 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":266,
        "Owner_creation_date":"2014-08-22 21:51:20.997 UTC",
        "Owner_last_access_date":"2022-09-25 01:52:01.297 UTC",
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2758,
        "Owner_up_votes":603,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-05 12:24:52.607 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Store input data on \/opt\/ml\/ of container using AWS Step Functions",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/step-functions-data-science-sdk\/automate_model_retraining_workflow\/automate_model_retraining_workflow.ipynb\" rel=\"nofollow noreferrer\">this<\/a> tutorial to setup a re-training of the Prophet model on a Docker image that is stored on ECR.<\/p>\n\n<p>When I get to the Training_Step, this setup is not able to put the input data to the \/opt\/ml\/input\/data folder on the docker container. I can pick it up from the S3 folder, but since the training is happening on the container and it needs the input data to be present in the \/opt\/ml... location, the AWS Step Function setup is failing at the Training_Step.<\/p>\n\n<p>Any inputs on how to push the input data collected from S3 to the docker container (\/opt\/ml\/input\/data) would be greatly appreciated. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-12 11:09:25.55 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|docker|amazon-sagemaker|aws-step-functions",
        "Question_view_count":295,
        "Owner_creation_date":"2014-07-01 02:04:57.097 UTC",
        "Owner_last_access_date":"2021-08-04 21:45:35.39 UTC",
        "Owner_location":null,
        "Owner_reputation":688,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How do we create a SageMaker pipeline using AWS CloudFormation?",
        "Question_body":"<p>I have tried using the <code>AWS::SageMaker::Pipeline<\/code> resource in CloudFormation.<\/p>\n<p>I want to give a pipeline definition in JSON format in CloudFormation. But there aren't any documentation available for that. There is documentation only for a Python SDK pipeline definition.<\/p>\n<p>How can I create an <em><a href=\"https:\/\/en.wikipedia.org\/wiki\/MLOps\" rel=\"nofollow noreferrer\">MLOps<\/a> SageMaker<\/em> pipeline using CloudFormation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-23 10:26:46.177 UTC",
        "Question_favorite_count":0.0,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mlops",
        "Question_view_count":383,
        "Owner_creation_date":"2022-02-23 10:20:13.137 UTC",
        "Owner_last_access_date":"2022-09-21 10:21:25.34 UTC",
        "Owner_location":null,
        "Owner_reputation":16,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-25 16:34:50.643 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to convert binary file to pandas dataframe",
        "Question_body":"<p>I use Amazon Sagemaker for model training and prediction. I have a problem with the returned data with predictions.  I am trying to convert prediction data to pandas dataframe format.<\/p>\n<p>After the model is deployed:<\/p>\n<pre><code>from sagemaker.serializers import CSVSerializer\n\nxgb_predictor=estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    serializer=CSVSerializer()\n)\n\n<\/code><\/pre>\n<p>I made a prediction on the test data:<\/p>\n<pre><code>predictions=xgb_predictor.predict(first_day.to_numpy())\n\n<\/code><\/pre>\n<p>The returned prediction results are in a binary file<\/p>\n<pre><code>predictions\n<\/code><\/pre>\n<pre><code>b'2.092024326324463\\n10.584211349487305\\n18.23127555847168\\n2.092024326324463\\n8.308058738708496\\n32.35516357421875\\n4.129155158996582\\n7.429899215698242\\n55.65376281738281\\n116.5504379272461\\n1.0734045505523682\\n5.29403018951416\\n1.0924320220947266\\n1.9484598636627197\\n5.29403018951416\\n2.190509080886841\\n2.085641860961914\\n2.092024326324463\\n7.674410343170166\\n2.1198673248291016\\n5.293967247009277\\n7.088096618652344\\n2.092024326324463\\n10.410735130310059\\n10.36008358001709\\n2.092024326324463\\n10.565692901611328\\n15.495997428894043\\n15.61841106414795\\n1.0533703565597534\\n6.262670993804932\\n31.02411460876465\\n10.43086051940918\\n3.116995096206665\\n3.2846100330352783\\n108.82835388183594\\n26.210166931152344\\n1.0658172369003296\\n10.55643367767334\\n6.245237350463867\\n15.951444625854492\\n10.195240020751953\\n1.0734045505523682\\n48.720497131347656\\n2.119992256164551\\n9.41071605682373\\n2.241959810256958\\n3.1907501220703125\\n10.415051460266113\\n1.2154537439346313\\n2.13691782951355\\n31.1861515045166\\n3.0827555656433105\\n6.261478424072266\\n5.279026985168457\\n15.897627830505371\\n20.483125686645508\\n20.874958038330078\\n53.2086296081543\\n10.731611251831055\\n2.115110397338867\\n13.79739761352539\\n2.1198673248291016\\n26.628803253173828\\n10.030998229980469\\n15.897627830505371\\n5.278475284576416\\n45.371158599853516\\n2.2791690826416016\\n15.58777141571045\\n15.947166442871094\\n30.88138771057129\\n10.388553619384766\\n48.22294235229492\\n10.565692901611328\\n20.808977127075195\\n10.388553619384766\\n15.910200119018555\\n8.252408981323242\\n1.109586238861084\\n15.58777141571045\\n13.718815803527832\\n3.1227424144744873\\n32.171592712402344\\n10.524396896362305\\n15.897627830505371\\n2.092024326324463\\n14.52088737487793\\n5.293967247009277\\n57.61208724975586\\n21.161712646484375\\n14.173937797546387\\n5.230247974395752\\n16.257652282714844\n\n<\/code><\/pre>\n<p>How can I convert prediction data to pandas dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-02 13:13:08.163 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|predict",
        "Question_view_count":31,
        "Owner_creation_date":"2014-10-26 22:15:02.887 UTC",
        "Owner_last_access_date":"2022-09-08 16:27:45.44 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>you mean this:<\/p>\n<pre><code>import pandas as pd\n\na = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\\n&quot;)\n\ndf = pd.DataFrame(data=a)\ndf.head()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-02 14:04:44.3 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS sagemaker training data returns UnexpectedStatusException, Reason: AlgorithmError: framework error:",
        "Question_body":"<p>I am trying to learn AWS by following tutorial on <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?nc1=h_ls\" rel=\"nofollow noreferrer\">AWS website<\/a>.<\/p>\n<p>I followed each step but step 4c where I train model with data returns error.<\/p>\n<p><code>xgb.fit({'train': s3_input_train})<\/code><\/p>\n<pre><code>UnexpectedStatusException: Error for Training job sagemaker-xgboost-2022-09-13-14-05-21-675: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_algorithm_toolkit\/hyperparameter_validation.py&quot;, line 278, in validate\n    hyperparameter_obj = self.hyperparameters[hp]\nKeyError: 'silent'\n<\/code><\/pre>\n<p>I searched for solution but couldn't find it.<\/p>\n<p>Does this have something to do with my location? Because I set my region to ap-northeast-2.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-13 14:54:31.74 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":21,
        "Owner_creation_date":"2017-01-07 05:02:49.59 UTC",
        "Owner_last_access_date":"2022-09-14 14:26:06.26 UTC",
        "Owner_location":"Korea",
        "Owner_reputation":87,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Question_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-02 00:08:19.55 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":215,
        "Owner_creation_date":"2014-02-17 03:18:20.777 UTC",
        "Owner_last_access_date":"2022-09-25 01:28:19.613 UTC",
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":133,
        "Owner_up_votes":304,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-04 21:33:00.71 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Model Monitoring error while performing latest_execution.describe()",
        "Question_body":"<p>I am trying to create a Model Monitor job for Sagemaker endpoint. The baseline constraints and statistics were computed successfully, but on scheduling a Monitoring Cron job, I am facing this error from Spark.<\/p>\n<p>Dataset: <a href=\"https:\/\/www.kaggle.com\/shivachandel\/kc-house-data\" rel=\"nofollow noreferrer\">KC House Data<\/a><\/p>\n<pre><code>monitor.create_monitoring_schedule(\nendpoint_input=endpoint_input,\nmonitor_schedule_name=monitor_schedule_name,\npost_analytics_processor_script=None,\noutput_s3_uri=monitor_report_path,\nstatistics=monitor.baseline_statistics(),\nconstraints=monitor.suggested_constraints(),\nschedule_cron_expression=CronExpressionGenerator.hourly(),\nenable_cloudwatch_metrics=True\n)\nmonitor.describe_schedule()\n<\/code><\/pre>\n<p>Output: <br>\n'MonitoringScheduleStatus': 'Scheduled'<\/p>\n<pre><code>import time\nexecutions = []\nwhile len(executions) == 0:\nprint('Checking for executions...')\nexecutions = monitor.list_executions()\ntime.sleep(100)\n\n\n\nlatest_execution = monitor.list_executions()[-1]\nlatest_execution.wait(logs=True)\nlatest_execution.describe()\n<\/code><\/pre>\n<p>Output : <br><\/p>\n<pre><code>2022-03-11 06:07:24 INFO SparkContext:54 - Successfully stopped SparkContext\n2022-03-11 06:07:24 ERROR Main:97 - Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 9, algo-1, executor 1): org.json4s.package$MappingException: Do not know how to convert JArray(List(JString(bedrooms), JString(bathrooms), JString(sqft_living), JString(sqft_above), JString(grade), JString(floors), JString(view), JString(sqft_lot), JString(floors), JString(waterfront), JString(zipcode))) into class java.lang.String\n#011at org.json4s.Extraction$.convert(Extraction.scala:608)\n#011at org.json4s.Extraction$.extract(Extraction.scala:350)\n#011at org.json4s.Extraction$$anonfun$extract$5.apply(Extraction.scala:334)\n#011at org.json4s.Extraction$$anonfun$extract$5.apply(Extraction.scala:334)\n#011at scala.collection.immutable.List.map(List.scala:273)\n#011at org.json4s.Extraction$.extract(Extraction.scala:334)\n#011at org.json4s.Extraction$.extract(Extraction.scala:42)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-11 08:26:08.8 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|java|scala|apache-spark|amazon-sagemaker",
        "Question_view_count":113,
        "Owner_creation_date":"2020-10-15 20:08:21.423 UTC",
        "Owner_last_access_date":"2022-09-23 11:50:01.703 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Automatic hyperparameter tuning in Sagemaker aws failed to run",
        "Question_body":"<p>I'm using SageMaker to train my model. And to have better results i'm trying to run the <strong>automatic hyperparameters tuning<\/strong>. <strong>The training without using this method is running just fine<\/strong> and giving the result needed, but once I try to run it using this method it gives me and error that is similar to the following error in training jobs (num_filters and learning_rate change):<\/p>\n<pre><code>algorithmerror: ExecuteUserScriptError: Command &quot;\/usr\/bin\/python3 script_unet.py --batch_size 54 --learning_rate 0.0002596573898074083\n--model_dir s3:\/\/sagemaker-us-east-2-6713267672\/tensorflow-training-2020-07-04-10-02-56-198\/model\/tensorflow-training-200704-1002-002-b7291d39\/model --num_filters 46&quot;\n<\/code><\/pre>\n<p>I have tried many other batch sizes just to be sure that it is not a memory problem and It always gives the same error, so i guess it's not.\nI need a h5 model extension to use it externally, that is why i'm using that saving lines to a bucket named models-pfe.<\/p>\n<p>The model script i'm using is the following:<\/p>\n<pre><code>#Dependencies:\nimport argparse, os\nimport numpy as np\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.optimizers import Adam\nfrom keras.utils import multi_gpu_model\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\n\nprint(&quot;All the dependencies imported&quot;)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument('--epochs', type=int, default=60)\n    parser.add_argument('--num_filters', type=int, default=32)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n    parser.add_argument('--batch_size', type=int, default=64)\n    parser.add_argument('--model_dir', type=str, default='s3:\/\/model-pfe')\n    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n    parser.add_argument('--testing', type=str, default=os.environ['SM_CHANNEL_TESTING'])\n    parser.add_argument('--access_key', type=str)\n    parser.add_argument('--secret_key', type=str)\n\n\n\n    args, _ = parser.parse_known_args()\n\n    epochs       = args.epochs\n    num_filters  = args.num_filters\n    lr           = args.learning_rate\n    batch_size   = args.batch_size\n    model_dir    = args.model_dir\n    training_dir = args.training\n    testing_dir  = args.testing\n    access_key  = args.access_key\n    secret_key  = args.secret_key\n\n\n\n    X_train = np.load(os.path.join(training_dir, 'training.npz'))['image']\n    Y_train = np.load(os.path.join(training_dir, 'training.npz'))['label']\n    X_test  = np.load(os.path.join(testing_dir, 'testing.npz'))['image']\n    Y_test  = np.load(os.path.join(testing_dir, 'testing.npz'))['label']\n\n    # input image dimensions\n    img_rows, img_cols = 512,512\n\n    # Tensorflow needs image channels last, e.g. (batch size, width, height, channels)\n    K.set_image_data_format('channels_last')\n    print(K.image_data_format())\n\n\n\n    print('X_train shape:', X_train.shape)\n    print(X_train.shape[0], 'train samples')\n    print(X_test.shape[0], 'test samples')\n\n    # Normalize pixel values\n    X_train   = X_train.astype('float32')\n    X_test    = X_test.astype('float32')\n    X_train  \/= 255\n    X_test   \/= 255\n\n\n    # U-Net model\n    inputs = Input((512, 512, 3))\n    c1 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n    c1 = Dropout(0.1) (c1)\n    c1 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n    p1 = MaxPooling2D((2, 2)) (c1)\n\n    c2 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n    c2 = Dropout(0.1) (c2)\n    c2 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n    p2 = MaxPooling2D((2, 2)) (c2)\n\n    c3 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n    c3 = Dropout(0.2) (c3)\n    c3 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n    p3 = MaxPooling2D((2, 2)) (c3)\n\n    c4 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n    c4 = Dropout(0.2) (c4)\n    c4 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\n    c5 = Conv2D(num_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n    c5 = Dropout(0.3) (c5)\n    c5 = Conv2D(num_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n\n    u6 = Conv2DTranspose(num_filters*8, (2, 2), strides=(2, 2), padding='same') (c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n    c6 = Dropout(0.2) (c6)\n    c6 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n\n    u7 = Conv2DTranspose(num_filters*4, (2, 2), strides=(2, 2), padding='same') (c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n    c7 = Dropout(0.2) (c7)\n    c7 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n\n    u8 = Conv2DTranspose(num_filters*2, (2, 2), strides=(2, 2), padding='same') (c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n    c8 = Dropout(0.1) (c8)\n    c8 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n\n    u9 = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same') (c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n    c9 = Dropout(0.1) (c9)\n    c9 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n\n    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n    model = Model(inputs=[inputs], outputs=[outputs])\n    print(model.summary())\n\n    # Use GPUs (for ml.p2.8xlarge = 8 GPUs)\n    model = multi_gpu_model(model, gpus=8)\n\n    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    #Fit model\n    results = model.fit(X_train, Y_train,\n                        validation_data=(X_test, Y_test),\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=1,\n                        shuffle=True)\n\n    \n    # Validation evaluation\n    score= model.evaluate(X_test, Y_test)\n    print('Validation loss    :', score[0])\n    print('Validation accuracy:', score[1])\n\n    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n    \n    def upload_to_aws(local_file, bucket, s3_file):\n        try:\n            s3.upload_file(local_file, bucket, s3_file)\n            print(&quot;Upload Successful&quot;)\n            return True\n        except FileNotFoundError:\n            print(&quot;The file was not found&quot;)\n            return False\n        except NoCredentialsError:\n            print(&quot;Credentials not available&quot;)\n            return False\n    model.save('model.h5')\n    upload_to_aws('model.h5','models-pfe',&quot;model.h5&quot;)\n<\/code><\/pre>\n<p>and to run this script on the automatic hyperparameters tuning i'm using the following script:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\n#My data location in s3\ntraining_input_path=&quot;s3:\/\/sagemaker-us-east-2-6713267672\/pfe-unet\/training\/training.npz&quot;\nvalidation_input_path=&quot;s3:\/\/sagemaker-us-east-2-6713267672\/pfe-unet\/validation\/testing.npz&quot;\n\n\nfrom sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='script_unet.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.8xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={\n                              'epochs': 60,\n                              'batch_size': 32, \n                              'access_key'   : '',\n                              'secret_key'   : ''}\n                         )\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n\nhyperparameter_ranges = {\n    'num_filters'  : IntegerParameter(32,64),\n    'learning_rate': ContinuousParameter(0.0001, 0.005)}\n\nobjective_metric_name = 'loss'\nobjective_type = 'Minimize'\nmetric_definitions = [{'Name': 'loss','Regex': 'loss = ([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(tf_estimator,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=6,\n                            max_parallel_jobs=1,\n                            objective_type=objective_type,\n                            early_stopping_type='Auto')\n\ntuner.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n<p>I have changed my bucket name, secret key and access key for security purposes<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2020-07-06 11:11:13.723 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-ec2|deep-learning|amazon-sagemaker",
        "Question_view_count":376,
        "Owner_creation_date":"2020-06-08 03:36:14.463 UTC",
        "Owner_last_access_date":"2020-12-26 01:39:19.783 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-07-06 16:19:26.093 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"cmd: mlflow sagemaker build-and-push-container gives FileNotFoundError:",
        "Question_body":"<p>I don't understand what type of files I missing<\/p>\n<blockquote>\n<p>and I used this code to connect AWS ECR<\/p>\n<\/blockquote>\n<pre><code>setx AWS_ACCESS_KEY_ID AKIAIOSFODNN7EXAMPLE\nsetx AWS_SECRET_ACCESS_KEY wJalrXUtnFEMI\/K7MDENG\/bPxRfiCYEXAMPLEKEY\nsetx AWS_DEFAULT_REGION us-west-2\n<\/code><\/pre>\n<blockquote>\n<p>at mlflow artifact directory shown below and here python isn't a python.exe it's just a file name<\/p>\n<\/blockquote>\n<pre><code>(deploy_ml) D:\\****\\Python\\mlruns\\1\\2877b0a860934a179723fd11ed946589\\artifacts\\random-forest-model&gt;\n\n\n(deploy_ml) D:\\***\\Python\\mlruns\\1\\2877b0a86093***723fd11ed946589\\artifacts\\random-forest-model&gt;mlflow sagemaker  build-and-push-container\n2021\/09\/14 22:22:13 INFO mlflow.models.docker_utils: Building docker image with name mlflow-pyfunc\nFIND: Parameter format not correct\nTraceback (most recent call last):\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\s***\\Anaconda3\\envs\\deploy_ml\\Scripts\\mlflow.exe\\__main__.py&quot;, line 7, in &lt;module&gt;\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1137, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1062, in main\n    rv = self.invoke(ctx)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1668, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1668, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;c:\\users\\***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;c:\\users\\***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 763, in invoke\n    return __callback(*args, **kwargs)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\mlflow\\sagemaker\\cli.py&quot;, line 280, in build_and_push_container\n    custom_setup_steps_hook=setup_container,\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\mlflow\\models\\docker_utils.py&quot;, line 114, in _build_image\n    universal_newlines=True,\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\subprocess.py&quot;, line 729, in __init__\n    restore_signals, start_new_session)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\subprocess.py&quot;, line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-14 17:16:10.42 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr|mlflow",
        "Question_view_count":173,
        "Owner_creation_date":"2021-08-03 07:46:47.183 UTC",
        "Owner_last_access_date":"2021-12-03 06:36:36.86 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker Endpoint \"Killed\" during serialization",
        "Question_body":"<p>I am trying to deploy my first ML model using sagemaker. and I have managed to get to the point of serialization. Here is my code:<\/p>\n<p>Basically what happens is, i get a list of files stored on s3 (data), each &quot;stack&quot; of pixels is one ml instance. So Pixel 0\/0 of image 1 and Pixel 0\/0 from image 2 .. is one instance. So I read the files stored in s3 and reshape from nXm image array to a m*n List.\nFrom this list i generate the JSON serialized string.<\/p>\n<pre><code>        js = {'instances': []}\n        s3 = boto3.client('s3', \n            aws_access_key_id=self.config.aws_clientID,\n            aws_secret_access_key=self.config.aws_clientSECRET,\n            region_name= self.config.aws_reagion\n            )\n\n\n        s3.download_file(self.config.aws_bucket_name, data[0][0], 'entry.tif')\n        first_file,gtags = FileHelpers.read_tiff('entry.tif')\n        datasets = np.zeros(shape=(first_file.shape[0],first_file.shape[1],2))\n        os.remove('entry.tif')  \n\n  \n\n        i=0\n        for row in data:            \n            for entry in data[row]:\n                #download the file from s3 read teh content and delete the temp file again\n                logging.info(f&quot;downloading {entry} from {self.config.aws_bucket_name}&quot; )\n                s3.download_file(self.config.aws_bucket_name, entry, 'entry.tif')\n                d,gtags =  FileHelpers.read_tiff('entry.tif')\n                datasets[:,:,i]=d[:]\n                os.remove('entry.tif')\n                i=i+1\n\n        logging.info(f&quot;datasets loaded shape: {first_file.shape}, i:{i}&quot;)\n        reshaped_dataset = datasets.reshape(first_file.shape[0]*first_file.shape[1],i)\n        logging.info(f&quot;datasets reshaped: {len(reshaped_dataset)} rows&quot;)\n        for row in reshaped_dataset:\n            js['instances'].append({'features': row.tolist()})\n\n        logging.info(&quot;instance rows created&quot;)\n\n        res = JSONSerializer.serialize(self,js)\n        logging.info(&quot;serialization finished&quot;)\n        return res\n\n<\/code><\/pre>\n<p>I manage to get to the line of code <code> logging.info(f&quot;datasets reshaped: {len(reshaped_dataset)} rows&quot;)<\/code> and then I get &quot;Killed&quot; in my console:<\/p>\n<pre><code>...\nINFO:root:datasets loaded shape: (2014, 2014), i:2\nINFO:root:datasets reshaped: 4056196 rows\nKilled\n<\/code><\/pre>\n<p>This code works fine when in local mode. I am using a ml.m5.2xlarge instance but also tried with a ml.m5.4xlarge instance.<\/p>\n<p>Any help greatly appreciated.\nThanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-23 13:08:41.523 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":15,
        "Owner_creation_date":"2015-01-28 08:48:12.287 UTC",
        "Owner_last_access_date":"2022-09-23 09:01:34.86 UTC",
        "Owner_location":"Graz, Austria",
        "Owner_reputation":916,
        "Owner_up_votes":41,
        "Owner_down_votes":9,
        "Owner_views":101,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to ensure libraries installed using sagemaker?",
        "Question_body":"<p>I have a jupyter notebook with a standard template code like so<\/p>\n\n<p>from sagemaker.tensorflow import TensorFlow<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\ntf_estimator = TensorFlow(entry_point='sagemaker_predict_2.py', role=role,\n                          training_steps=10000, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                          framework_version='1.10.0')\ntf_estimator.fit('s3:\/\/XXX-sagemaker\/XXX')\n<\/code><\/pre>\n\n<p>This kicks off fine but eventually throws an error<\/p>\n\n<pre><code>2018-11-27 06:21:12 Starting - Starting the training job...\n2018-11-27 06:21:15 Starting - Launching requested ML instances.........\n2018-11-27 06:22:44 Starting - Preparing the instances for training...\n2018-11-27 06:23:35 Downloading - Downloading input data...\n2018-11-27 06:24:03 Training - Downloading the training image......\n2018-11-27 06:25:12 Training - Training image download completed. Training in progress..\n2018-11-27 06:25:11,813 INFO - root - running container entrypoint\n2018-11-27 06:25:11,813 INFO - root - starting train task\n2018-11-27 06:25:11,833 INFO - container_support.training - Training starting\n2018-11-27 06:25:15,306 ERROR - container_support.training - uncaught exception during training: No module named keras\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\n    fw.train()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/tf_container\/train_entry_point.py\", line 143, in train\n    customer_script = env.import_user_module()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 101, in import_user_module\n    user_module = importlib.import_module(script)\n  File \"\/usr\/lib\/python2.7\/importlib\/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"\/opt\/ml\/code\/sagemaker_predict_2.py\", line 7, in &lt;module&gt;\n    import keras\nImportError: No module named keras  \n<\/code><\/pre>\n\n<p>My <code>sagemaker_predict_2.py<\/code> needs some of these libraries:<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport sys\nimport keras\nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras_contrib.layers import CRF\n<\/code><\/pre>\n\n<p>I suppose it has no problem importing <code>pandas<\/code> and <code>numpy<\/code>, but dies when importing <code>keras<\/code>. I thought <code>keras<\/code> was standard in the notebook. When I kick this script off, does it have some other uninitialized environment?<\/p>\n\n<p>Also, I believe <code>keras_contrib<\/code> is not standard, so I will need a way to install that. How do I do that?<\/p>\n\n<p>I tried <code>!pip install keras<\/code> in the cell above but it reported that <code>Requirement already satisfied<\/code>, so it seems my jupyter environment has the library. But kicking off the <code>sagemaker_predict_2.py<\/code> must be in a different environment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-27 06:30:19.93 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":3809,
        "Owner_creation_date":"2011-10-21 21:58:08.81 UTC",
        "Owner_last_access_date":"2022-09-17 00:51:12.053 UTC",
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to configure dynamically a AWS Sage Maker task with AWS Step Function",
        "Question_body":"<p>I'm trying to build an ML pipeline using AWS Step Function.<\/p>\n\n<p>I would like to configure the 'CreateHyperParameterTuningJob' dynamically depending on the input of the task.\nHere is a screenshot of the State Machine that I'm trying to build:\n<a href=\"https:\/\/i.stack.imgur.com\/06Ctf.png\" rel=\"nofollow noreferrer\">ML State Machine <\/a><\/p>\n\n<p>When I try to create this State Machine, I got the following error: <\/p>\n\n<ul>\n<li><strong>The value for the field 'MaxParallelTrainingJobs' must be an INTEGER<\/strong><\/li>\n<\/ul>\n\n<p>I'm struggling to figure out what is the issue here. \n<strong>Do you have any suggestion to make the SM configuration dynamic with Step Function?<\/strong> Is it even possible?<\/p>\n\n<p>Here is the input data passed to the <strong>'Run training job'<\/strong> task:<\/p>\n\n<pre><code>{\n  \"client_id\": \"test\",\n  \"training_job_definition\": {\n    \"AlgorithmSpecification\": {\n      \"TrainingImage\": \"433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest\",\n      \"TrainingInputMode\": \"File\"\n    },\n    \"ResourceConfig\": {\n      \"InstanceCount\": 1,\n      \"InstanceType\": \"ml.m5.large\",\n      \"VolumeSizeInGB\": 5\n    },\n    \"StaticHyperParameters\": {\n      \"num_round\": 750\n    },\n    \"StoppingCondition\": {\n      \"MaxRuntimeInSeconds\": 900\n    },\n    \"InputDataConfig\": [\n      {\n        \"ChannelName\": \"train\",\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"csv\",\n        \"DataSource\": {\n          \"S3DataSource\": {\n            \"S3DataDistributionType\": \"FullyReplicated\",\n            \"S3DataType\": \"S3Prefix\",\n            \"S3Uri\": \"...\"\n          }\n        }\n      },\n      {\n        \"ChannelName\": \"validation\",\n        \"CompressionType\": \"None\",\n        \"ContentType\": \"csv\",\n        \"DataSource\": {\n          \"S3DataSource\": {\n            \"S3DataDistributionType\": \"FullyReplicated\",\n            \"S3DataType\": \"S3Prefix\",\n            \"S3Uri\": \"...\"\n          }\n        }\n      }\n    ],\n    \"OutputDataConfig\": {\n      \"S3OutputPath\": \"...\"\n    },\n    \"RoleArn\": \"arn:aws:iam::679298748479:role\/landingzone_sagemaker_role\"\n  },\n  \"hyper_parameter_tuning_job_config\": {\n    \"HyperParameterTuningJobObjective\": {\n      \"MetricName\": \"validation:rmse\",\n      \"Type\": \"Minimize\"\n    },\n    \"Strategy\": \"Bayesian\",\n    \"ResourceLimits\": {\n      \"MaxParallelTrainingJobs\": 2,\n      \"MaxNumberOfTrainingJobs\": 10\n    },\n    \"ParameterRanges\": {\n      \"ContinuousParameterRanges\": [\n        {\n          \"Name\": \"eta\",\n          \"MinValue\": 0.01,\n          \"MaxValue\": 0.04\n        },\n        {\n          \"Name\": \"gamma\",\n          \"MinValue\": 0,\n          \"MaxValue\": 100\n        },\n        {\n          \"Name\": \"subsample\",\n          \"MinValue\": 0.6,\n          \"MaxValue\": 1\n        },\n        {\n          \"Name\": \"lambda\",\n          \"MinValue\": 0,\n          \"MaxValue\": 5\n        },\n        {\n          \"Name\": \"alpha\",\n          \"MinValue\": 0,\n          \"MaxValue\": 2\n        }\n      ],\n      \"IntegerParameterRanges\": [\n        {\n          \"Name\": \"max_depth\",\n          \"MinValue\": 5,\n          \"MaxValue\": 10\n        }\n      ]\n}\n<\/code><\/pre>\n\n<p>}\n}<\/p>\n\n<p>Here is JSON file that describes the State Machine: <\/p>\n\n<pre><code>{\n  \"StartAt\": \"Generate Training Dataset\",\n  \"States\": {\n    \"Generate Training Dataset\": {\n      \"Resource\": \"arn:aws:lambda:uswest-2:012345678912:function:StepFunctionsSample-SageMaLambdaForDataGeneration-1TF67BUE5A12U\",\n      \"Type\": \"Task\",\n      \"Next\": \"Run training job\"\n    },\n    \"Run training job\": {\n      \"Resource\": \"arn:aws:states:::sagemaker:createHyperParameterTuningJob.sync\",\n      \"Parameters\": {\n        \"HyperParameterTuningJobName.$\": \"$.execution_date\",\n        \"HyperParameterTuningJobConfig\": {\n          \"HyperParameterTuningJobObjective\": {\n            \"MetricName\": \"$.hyper_parameter_tuning_job_config.HyperParameterTuningJobObjective.MetricName\",\n            \"Type\": \"Minimize\"\n          },\n          \"Strategy\": \"$.hyper_parameter_tuning_job_config.Strategy\",\n          \"ResourceLimits\": {\n            \"MaxParallelTrainingJobs\": \"$.hyper_parameter_tuning_job_config.ResourceLimits.MaxParallelTrainingJobs\",\n            \"MaxNumberOfTrainingJobs\": \"$.hyper_parameter_tuning_job_config.ResourceLimits.MaxNumberOfTrainingJobs\"\n          },\n          \"ParameterRanges\": \"$.hyper_parameter_tuning_job_config.ParameterRanges\"\n        },\n        \"TrainingJobDefinition\": {\n          \"AlgorithmSpecification\": \"$.training_job_definition.AlgorithmSpecification\",\n          \"StoppingCondition\": \"$.training_job_definition.StoppingCondition\",\n          \"ResourceConfig\": \"$.training_job_definition.ResourceConfig\",\n          \"RoleArn\": \"$.training_job_definition.RoleArn\",\n          \"InputDataConfig\": \"$.training_job_definition.InputDataConfig\",\n          \"OutputDataConfig\": \"$.training_job_definition.OutputDataConfig\",\n          \"StaticHyperParameters\": \"$.training_job_definition.StaticHyperParameters\"\n        },\n        \"HyperParameterTuningJobConfig.ResourceLimits\": \"\"\n      },\n      \"Type\": \"Task\",\n      \"End\": true\n    }\n  }\n}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-06 02:34:24.05 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|state-machine|amazon-sagemaker|aws-step-functions",
        "Question_view_count":185,
        "Owner_creation_date":"2020-02-06 02:10:38.237 UTC",
        "Owner_last_access_date":"2021-11-11 07:07:20.72 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-02-06 02:43:18.1 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker estimator `source_dir` from S3",
        "Question_body":"<p>I would like to start a training job using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">SageMaker TensorFlow Estimator<\/a> in a script mode.<br>\nMy problem is that I don't have my training code locally or in a git repo, but only in S3 \"directory\" and <code>source_dir<\/code> parameter requires a local file or usage of git.<\/p>\n\n<p>Is the only way to copy the files locally from s3 (which is problematic with python) or can I do it in a nicer way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-28 07:16:05.907 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":1215,
        "Owner_creation_date":"2013-02-26 14:42:26.643 UTC",
        "Owner_last_access_date":"2022-03-03 08:15:15.967 UTC",
        "Owner_location":null,
        "Owner_reputation":2945,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":89,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Access Denied read open data into Sagemaker",
        "Question_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-02-10 23:28:00.153 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker|opendata",
        "Question_view_count":1192,
        "Owner_creation_date":"2011-02-23 18:00:07.147 UTC",
        "Owner_last_access_date":"2022-09-24 18:44:13.9 UTC",
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":9271,
        "Owner_up_votes":2074,
        "Owner_down_votes":44,
        "Owner_views":1819,
        "Answer_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-02-11 05:57:43.813 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to return name of AWS Sagemaker notebook instance within python script?",
        "Question_body":"<p>Bit of an edge case ask, basically I'm trying to write a general purpose script that can be set up as a cronjob on AWS Sagemaker that will notify users if an instance is still active. <strong>If possible, I'd like to be able to call name of the AWS Sagemaker instance (different from the name of the active jupyter notebook) within the cronjob.<\/strong><\/p>\n\n<p>Haven't been able to find anything in the boto3 or sagemaker documentation specific to this. I know that the instance name is contained within the URL path, but similarly haven't found a way to reference back to this. I'm expecting this isn't an intended functionality so will probably just go ahead with setting values manually in a config file, but if anyone has any creative solutions I'd appreciate the input!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-01-28 19:51:20.94 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":926,
        "Owner_creation_date":"2016-03-03 06:30:06.313 UTC",
        "Owner_last_access_date":"2022-09-23 17:07:37.883 UTC",
        "Owner_location":null,
        "Owner_reputation":757,
        "Owner_up_votes":83,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-01-29 17:57:04.603 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS secret manager access from Sagemaker ECR container",
        "Question_body":"<p>I have a secret stored in AWS Secret Manager.\nI am trying to access that secret from a container which is in ECR.<\/p>\n<p>When I execute the container, the error message I get is:<\/p>\n<pre><code>  File &quot;\/opt\/program\/train&quot;, line 65, in get_secret\n    SecretId=secret_name\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 895, in _make_api_call\n    operation_model, request_dict, request_context\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 917, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 116, in make_request\n    return self._send_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 195, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    operation_name=operation_model.name,\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>The container is using python and I am using the same function given as an example when I setup the secret manager entry for python3\nThe same function works well in my local environment where I am authenticated via CLI.<\/p>\n<p>The function is:<\/p>\n<pre><code>import boto3\nimport base64\nfrom botocore.exceptions import ClientError\n\n\ndef get_secret():\n\n    secret_name = &quot;secret1&quot;\n    region_name = &quot;us-east-1&quot;\n\n    # Create a Secrets Manager client\n    session = boto3.session.Session()\n    client = session.client(\n        service_name='secretsmanager',\n        region_name=region_name\n    )\n\n    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n    # See https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/apireference\/API_GetSecretValue.html\n    # We rethrow the exception by default.\n\n    try:\n        get_secret_value_response = client.get_secret_value(\n            SecretId=secret_name\n        )\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'DecryptionFailureException':\n            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n            # An error occurred on the server side.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidParameterException':\n            # You provided an invalid value for a parameter.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidRequestException':\n            # You provided a parameter value that is not valid for the current state of the resource.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n            # We can't find the resource that you asked for.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n    else:\n        # Decrypts secret using the associated KMS key.\n        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n        if 'SecretString' in get_secret_value_response:\n            secret = get_secret_value_response['SecretString']\n        else:\n            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n            \n    # Your code goes here. \n<\/code><\/pre>\n<p>But in the docker container I have not performed a CLI authentication. I have given the sagemaker access role the KMS permissions for my region.\nAny thoughts on how I can get a sagemaker container to access secret manager?\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-18 19:36:35.43 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|secretsmanager",
        "Question_view_count":25,
        "Owner_creation_date":"2016-03-17 06:30:24.61 UTC",
        "Owner_last_access_date":"2022-08-28 15:19:22.88 UTC",
        "Owner_location":null,
        "Owner_reputation":981,
        "Owner_up_votes":64,
        "Owner_down_votes":1,
        "Owner_views":74,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Shutdown AWS Sagemaker without using Python script",
        "Question_body":"<p>I am trying to find a solution to auto shutdown AWS Sagemaker instance after 1 hour of idleness. I found the below solution for this but at our environment the Sagemaker cant access any code from internet so wget will fail. Is there any way to achieve this result without using the script from below solution? Like defining everything in the bash shell and then run it using cron job?<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-06 19:54:49.717 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":272,
        "Owner_creation_date":"2019-11-04 11:49:07.877 UTC",
        "Owner_last_access_date":"2022-09-23 17:14:57.857 UTC",
        "Owner_location":null,
        "Owner_reputation":553,
        "Owner_up_votes":63,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-08-07 11:18:31.733 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Is it possible to modify an existing AWS implementation of a deep learning model?",
        "Question_body":"<p>I wish to modify an existing model implementation in order to add an additional upsampling layer to a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">semantic segmentation algorithm that has previously been implemented in AWS<\/a>.<\/p>\n<p>It appears that Sagemaker refers to <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/tree\/4e1d44d718ca998523527aa2039cde2184d31296\" rel=\"nofollow noreferrer\">this repo<\/a>, and I'm hoping to modify <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/blob\/4e1d44d718ca998523527aa2039cde2184d31296\/gluoncv\/model_zoo\/deeplabv3.py\" rel=\"nofollow noreferrer\">the deeplab model<\/a> to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image (i.e., statistically downscale the original imagery).<\/p>\n<p>(<a href=\"https:\/\/www.researchgate.net\/publication\/349804789_A_Deep_Learning_Approach_to_Downscale_Geostationary_Satellite_Imagery_for_Decision_Support_in_High_Impact_Wildfires\" rel=\"nofollow noreferrer\">This technique has been demonstrated with UNET architectures.<\/a>)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-29 17:28:16.723 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|deep-learning|image-segmentation|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_date":"2020-03-30 01:11:00.043 UTC",
        "Owner_last_access_date":"2022-09-14 06:02:30.527 UTC",
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Question_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-29 06:35:38.62 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":1519,
        "Owner_creation_date":"2017-07-30 08:26:08.107 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.87 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-08-29 11:06:41.19 UTC",
        "Answer_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":"2018-08-29 11:06:57.293 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"why Sagemaker Processing have several ouputs",
        "Question_body":"<p>I have been using AWS Sagemaker for years.<\/p>\n<p>I don't understand why processing jobs can have several outputs ? In what kind of scenario, can you use more than one output destination ?<\/p>\n<p>When I say several outputs I refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingOutputConfig.html\" rel=\"nofollow noreferrer\">ProcessingOutputConfig<\/a> containing an array<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-22 09:10:00.477 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":12,
        "Owner_creation_date":"2014-07-12 09:14:39.23 UTC",
        "Owner_last_access_date":"2022-09-23 13:24:43.56 UTC",
        "Owner_location":"Paris, France",
        "Owner_reputation":1075,
        "Owner_up_votes":67,
        "Owner_down_votes":7,
        "Owner_views":199,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon Sagemaker: write your own inference",
        "Question_body":"<p>I was evaluating what is needed to write your own Estimator in Sagemaker. I was following this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\/container\" rel=\"nofollow noreferrer\">here<\/a> and it's well explained and quite simple.<\/p>\n<p>My question is regarding the inference <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">here<\/a>. I see an example in which we can feed the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/e648e9a6f596263c7683635d1a55f1729b08277d\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L60\" rel=\"nofollow noreferrer\">invocations endpoint<\/a> a CSV. What if I want to just post a string or even individual parameters? What's the best practise for that? I see there is a condition like:<\/p>\n<pre><code>if flask.request.content_type == &quot;text\/csv&quot;:\n<\/code><\/pre>\n<p>Should we add more like those to support different formats or should we create a new endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-31 14:41:58.507 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":168,
        "Owner_creation_date":"2011-12-02 00:24:29.84 UTC",
        "Owner_last_access_date":"2022-07-18 13:43:46.98 UTC",
        "Owner_location":"Cork, Ireland",
        "Owner_reputation":482,
        "Owner_up_votes":25,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Answer_body":"<p>You need to add support for more content types.<\/p>\n<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application\/json&quot; MIME media type (<a href=\"https:\/\/stackoverflow.com\/questions\/477816\/what-is-the-correct-json-content-type\">What is the correct JSON content type?<\/a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.<\/p>\n<p>For example, if you have two parameters <code>age<\/code> and <code>gender<\/code> you want to pass to your model. You can put them in the following Json datastructure:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;age&quot;: ...,\n &quot;gender&quot;: ...\n}\n<\/code><\/pre>\n<p>Then add support for loading the Json and extracting the parameters in the backend as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if flask.request.content_type == &quot;application\/json&quot;:\n    data = flask.request.data.decode(&quot;utf-8&quot;)\n    data = json.loads(data)\n    parameter1 = data['age']\n    parameter2 = data['gender']\n    ...\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-08 10:14:20.113 UTC",
        "Answer_last_edit_date":"2021-09-08 10:41:56.313 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to specify max runtime using TensorFlow estimator with Python Sagemaker SDK?",
        "Question_body":"<p>Using the Python Sagemaker SDK, one can launch a training job using TensorFlow with the following code:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nsess = sagemaker.Session()\ntf_estimator = TensorFlow(...)\ntf_estimator.fit(...)\n<\/code><\/pre>\n\n<p>Is it possible to specify the maximum runtime of the training somewhere in this script?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-16 13:46:18.14 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":839,
        "Owner_creation_date":"2014-06-14 14:25:34.69 UTC",
        "Owner_last_access_date":"2022-09-25 03:04:43.037 UTC",
        "Owner_location":"Porto, Portugal",
        "Owner_reputation":5998,
        "Owner_up_votes":2638,
        "Owner_down_votes":56,
        "Owner_views":426,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker deploy custom script",
        "Question_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-04 23:36:35.073 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":386,
        "Owner_creation_date":"2016-04-12 06:57:10.317 UTC",
        "Owner_last_access_date":"2022-09-24 00:22:01.487 UTC",
        "Owner_location":null,
        "Owner_reputation":404,
        "Owner_up_votes":46,
        "Owner_down_votes":2,
        "Owner_views":127,
        "Answer_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-05 00:36:40.33 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Batch transform sparse matrix with AWS SageMaker Python SDK",
        "Question_body":"<p>I have successfully trained a Scikit-Learn LSVC model with AWS SageMaker.<br>\nI want to make batch prediction (aka. batch transform) on a relatively big dataset which is a scipy sparse matrix with shape 252772 x 185128. (The number of features is high because there is one-hot-encoding of bag-of-words and ngrams features).   <\/p>\n\n<p>I struggle because of:   <\/p>\n\n<ul>\n<li><p>the size of the data   <\/p><\/li>\n<li><p>the format of the data<\/p><\/li>\n<\/ul>\n\n<p>I did several experiments to check what was going on:    <\/p>\n\n<h3>1. predict locally on sample sparse matrix data<\/h3>\n\n<p><strong>It works<\/strong><br>\nDeserialize the model artifact locally on a SageMaker notebook and predict on a sample of the sparse matrix.<br>\nThis was just to check that the model can predict on this kind of data.<\/p>\n\n<h3>2. Batch Transform on a sample csv data<\/h3>\n\n<p><strong>It works<\/strong><br>\nLaunch a Batch Transform Job on SageMaker and request to transform a small sample in dense csv format : it works but does not scale, obviously.<br>\nThe code is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='text\/csv',\n   split_type=None)   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n\n<\/code><\/pre>\n\n<p>where:  <\/p>\n\n<ul>\n<li>'my_script.py' implements a simple <code>model_fn<\/code> to deserialize the model artifact: <\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<ul>\n<li><code>batch_data<\/code> is the s3 path for the csv file.  <\/li>\n<\/ul>\n\n<h3>3. Batch Transform of a sample dense numpy dataset.<\/h3>\n\n<p><strong>It works<\/strong><br>\nI prepared a sample of the data and saved it to s3 in Numpy <code>.npy<\/code> format. According to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#sagemaker-scikit-learn-model-server\" rel=\"noreferrer\">this documentation<\/a>, SageMaker Scikit-learn model server can deserialize NPY-formatted data (along with JSON and CSV data).<br>\nThe only difference with the previous experiment (2) is the argument <code>content_type='application\/x-npy'<\/code> in <code>transformer.transform(...)<\/code>.   <\/p>\n\n<p>This solution does not scale and we would like to pass a Scipy sparse matrix: <\/p>\n\n<h3>4. Batch Transform of a big sparse matrix.<\/h3>\n\n<p><strong>Here is the problem<\/strong><br>\nSageMaker Python SDK does not support sparse matrix format out of the box.<br>\nFollowing this:  <\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/<\/a>    <\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/55479366\/errors-running-sagemaker-batch-transformation-with-lda-model\">Errors running Sagemaker Batch Transformation with LDA model<\/a>  <\/li>\n<\/ul>\n\n<p>I used <code>write_spmatrix_to_sparse_tensor<\/code> to write the data to protobuf format on s3. The function I used is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def write_protobuf(X_sparse, bucket, prefix, obj):\n    \"\"\"Write sparse matrix to protobuf format at location bucket\/prefix\/obj.\"\"\"\n    buf = io.BytesIO()\n    write_spmatrix_to_sparse_tensor(file=buf, array=X_sparse, labels=None)\n    buf.seek(0)\n    key = '{}\/{}'.format(prefix, obj)\n    boto3.resource('s3').Bucket(bucket).Object(key).upload_fileobj(buf)\n    return 's3:\/\/{}\/{}'.format(bucket, key)\n<\/code><\/pre>\n\n<p>Then the code used for launching the batch transform job is:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='application\/x-recordio-protobuf',\n   split_type='RecordIO')   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>sagemaker_containers._errors.ClientError: Content type application\/x-recordio-protobuf is not supported by this framework.\n<\/code><\/pre>\n\n<p><strong>Questions:<\/strong><br>\n(Reference doc for Transformer: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html<\/a>) <\/p>\n\n<ul>\n<li>If <code>content_type='application\/x-recordio-protobuf'<\/code> is not allowed, what should I use?<\/li>\n<li>Is <code>split_type='RecordIO'<\/code> the proper setting in this context?   <\/li>\n<li>Should I provide an <code>input_fn<\/code> function in my script to deserialize the data?<\/li>\n<li>Is there another better approach to tackle this problem?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2019-10-16 09:56:46.713 UTC",
        "Question_favorite_count":null,
        "Question_score":5,
        "Question_tags":"python|amazon-web-services|scikit-learn|sparse-matrix|amazon-sagemaker",
        "Question_view_count":834,
        "Owner_creation_date":"2014-07-30 10:25:22.23 UTC",
        "Owner_last_access_date":"2022-09-22 19:20:13.617 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":1714,
        "Owner_up_votes":133,
        "Owner_down_votes":6,
        "Owner_views":307,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to plot a Partial dependency plot with sagemaker downloaded model, xgb booster object?",
        "Question_body":"<p>I have an XGBoost model trained in Sagemaker and downloaded on my local. When you unzip the tar.gz file you get a model object(booster) file.\nI want to plot a Partial Dependency Plots for some features of my dataset (batch transformed with predictions) using the Sagemaker model instance. However, I get the following error.<\/p>\n<p><strong>ValueError: 'estimator' must be a fitted regressor or classifier.<\/strong><\/p>\n<p>The code that I'm using to plot the graph is as below-<\/p>\n<pre><code>from sklearn.inspection import partial_dependence\npardep = partial_dependence(model, X, 'num_org')\n<\/code><\/pre>\n<p>Here model is &lt;xgboost.core.Booster at 0x7f9b7b9a2250&gt;\nX is a data frame of the batch -transform with ~100 columns and 'num_org' is what I want to plot with respect to predictions.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2020-08-03 15:48:57.927 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":107,
        "Owner_creation_date":"2013-04-13 15:21:29.147 UTC",
        "Owner_last_access_date":"2022-04-21 10:59:02.42 UTC",
        "Owner_location":null,
        "Owner_reputation":141,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-08-03 15:52:32.683 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker Studio Lab - Permission denied",
        "Question_body":"<p>I want to use AWS SageMaker Studio Lab for training some deep learning models but when I want to use my train.py SageMaker returns an error about stdnfile.<\/p>\n<p><code>OSError: sndfile library not found<\/code><\/p>\n<p>After that I found out I can fix that error with<\/p>\n<p><code>apt-get install libsndfile1 -y<\/code><\/p>\n<p>but every time when I try, I'm getting an error in the below.<\/p>\n<p><code>E: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)<\/code><\/p>\n<p>I tried to use <code>sudo<\/code> because AWS SageMaker uses a Linux-based distro but It returns<\/p>\n<p><code>bash: sudo: command not found<\/code><\/p>\n<p>I tried <code>su<\/code> either and It wants a password from me. I tried my account password but it rejects.<\/p>\n<p>Btw I saw an answer in the different question for <code>--allow-root<\/code> but It didn't work either.<\/p>\n<p>p.s.: I use the same python file and <code>apt-get install libsndfile1 -y<\/code> in AWS SageMaker Studio and It worked well. AWS SageMaker servers type were <strong>ml.m5.24xlarge<\/strong> and <strong>ml.t3.medium<\/strong> (CPU Servers). I request a GPU Server in AWS SageMaker Studio but I'm still waiting (<strong>ml.g4dn.xlarge<\/strong>). On the other hand, I'm trying to train with GPU in AWS SageMaker Studio Lab.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-25 09:02:07.697 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|linux|sudo|amazon-sagemaker",
        "Question_view_count":169,
        "Owner_creation_date":"2020-09-16 07:13:23.403 UTC",
        "Owner_last_access_date":"2022-09-20 20:32:50.01 UTC",
        "Owner_location":"Turkey",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Issue with numpy dependency when building docker image",
        "Question_body":"<p>I am trying to create a Sagemaker endpoint for model inference using the Build your own algorithm container (<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html<\/a>) but am having an issue when installing Numpy in the creation of the image.<\/p>\n<p>We've already previously have gotten it to work with our old model, but the new vowpal wabbit model requires numpy, scikit-learn, pandas and vowpal wabbit library which is causing it to fail in the docker build. I'm not sure if we should continue using this container or should migrate to a python one or sagemaker one, but would need to support nginx.<\/p>\n<p>#EDIT: Forgot to mention that when I build it locally, it is created successfully but when fails through Cloudformation.<\/p>\n<p>Dockerfile here:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        wget \\\n        python3-pip \\\n        python3.8 \\\n        python3-setuptools \\\n        nginx \\\n        ca-certificates &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Get all python packages without excess cache created by pip.\nCOPY requirements.txt .\nRUN pip3 install --upgrade pip setuptools wheel\nRUN pip3 --no-cache-dir install -r requirements.txt\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\n# create directories for storing model and vectorizer\nRUN mkdir model &amp;&amp; mkdir vectorizer\n\n# Give permissions to run scripts\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\n<\/code><\/pre>\n<p>requirements.txt here:<\/p>\n<pre><code>sagemaker==2.25.1\ntyping-extensions==3.7.4.3\nnumpy==1.20.1\nboto3==1.17.12\nawscli==1.19.12\npython-dotenv==0.15.0\nflask==1.1.2\nscikit-learn==1.0.0\npandas==1.3.5\nvowpalwabbit==8.11.0\n<\/code><\/pre>\n<p>Full traceback here:<\/p>\n<pre><code>Running setup.py install for numpy: started\n\n    Running setup.py install for numpy: finished with status 'error'\n\n    Complete output from command \/usr\/bin\/python3 -u -c &quot;import setuptools, tokenize;__file__='\/tmp\/pip-build-cd653krx\/numpy\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))&quot; install --record \/tmp\/pip-q3eo46tw-record\/install-record.txt --single-version-externally-managed --compile:\n\n    Running from numpy source directory.\n\n    Note: if you need reliable uninstall behavior, then install\n\n    with pip instead of using `setup.py install`:\n\n      - `pip install .`       (from a git repo or downloaded source\n\n                               release)\n\n      - `pip install numpy`   (last NumPy release on PyPi)\n\n    Cythonizing sources\n\n    Processing numpy\/random\/_bounded_integers.pxd.in\n\n    Processing numpy\/random\/_bounded_integers.pyx.in\n\n    Traceback (most recent call last):\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 53, in process_pyx\n\n        import Cython\n\n    ModuleNotFoundError: No module named 'Cython'\n\n    The above exception was the direct cause of the following exception:\n\n    Traceback (most recent call last):\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 234, in &lt;module&gt;\n\n        main()\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 230, in main\n\n        find_process_files(root_dir)\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 221, in find_process_files\n\n        process(root_dir, fromfile, tofile, function, hash_db)\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 187, in process\n\n        processor_function(fromfile, tofile)\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 90, in process_tempita_pyx\n\n        process_pyx(pyxfile, tofile)\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/tools\/cythonize.py&quot;, line 60, in process_pyx\n\n        raise OSError(msg) from e\n\n    OSError: Cython needs to be installed in Python as a module\n\n    Traceback (most recent call last):\n\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/setup.py&quot;, line 450, in &lt;module&gt;\n\n        setup_package()\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/setup.py&quot;, line 432, in setup_package\n\n        generate_cython()\n\n      File &quot;\/tmp\/pip-build-cd653krx\/numpy\/setup.py&quot;, line 237, in generate_cython\n\n        raise RuntimeError(&quot;Running cythonize failed!&quot;)\n\n    RuntimeError: Running cythonize failed!\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-01-05 11:27:40.977 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|docker|numpy|amazon-sagemaker|vowpalwabbit",
        "Question_view_count":383,
        "Owner_creation_date":"2018-05-14 09:05:19.837 UTC",
        "Owner_last_access_date":"2022-07-01 15:18:50.603 UTC",
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-05 12:06:30.527 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Use `sentence-transformers` inside of a Tensorflow-recommendation keras model in SageMaker",
        "Question_body":"<p>I've been going crazy for a few days over a problem that I thought trivial. My end-goal is to deploy to AWS Sagemaker a Tensorflow model that uses a simple string as input, calculates the embedding using a 'sentence-transformer' pre-trained model and eventually uses TensorFlow Recommenders to suggest the knn among a collection of embedding I already have calculated. I would like to do this entirely from the model, including the preprocessing (tokenization).<\/p>\n<p>I made the predictions works with different approaches in my notebook. I start having troubles when I try to save my model.<\/p>\n<p>The problem seems to be that HF's AutoTokenizer needs a pure <code>List of Strings<\/code> as input, and I hit a roadblock whenever I try to save my model using , and trying to go around this with tf.py_function using <a href=\"https:\/\/stackoverflow.com\/questions\/71411065\/use-sentence-transformers-inside-of-a-keras-model\">this approach<\/a> results in problems with Sagemaker.<\/p>\n<p><strong>My approaches so far:<\/strong><\/p>\n<p><strong>1. THE 'I THOUGHT IT WAS SO SIMPLE'<\/strong><\/p>\n<pre><code>   startups_ids: list, startup_vectors\n):\n   import tensorflow as tf\n   import tensorflow_recommenders as tfrs\n   import numpy as np\n   from random import randint\n    \n   exported_model = tfrs.layers.factorized_top_k.BruteForce(SentenceTransformer(&quot;all-mpnet-base-v2&quot;).encode)\n   exported_model.index(np.array(startup_vectors), np.array(startups_ids))\n   \n   # TESTS the model\n   #for some reason this seems to be needed in order to save the model :\/ \n   # https:\/\/github.com\/tensorflow\/recommenders\/issues\/131\n\n   test = exported_model(['Test Text Query'])\n\n   print(test)\n   \n   return exported_model\n\n\ntext_to_startup_model(search_db_ids, search_db_embeddings)\n#--&gt; WORKS PERFECTLY, AS I GET SOME SUGGESTIONS\n\ntf.saved_model.save(text_to_startup_model(search_db_ids, search_db_embeddings), export_dir=&quot;\/home\/nicholas\/test_model_save\/1&quot;)\n\n#TypeError                                 Traceback (most recent call last)\n# \/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/notebooks\/t2s_different_approaches.ipynb Cell 5 in &lt;cell line: 22&gt;()\n#      19 text_to_startup_model(search_db_ids, search_db_embeddings)\n#      20 #--&gt; WORKS PERFECTLY, AS I GET SOME SUGGESTIONS\n# ---&gt; 22 tf.saved_model.save(text_to_startup_model(search_db_ids, search_db_embeddings), export_dir=&quot;\/home\/nicholas\/test_model_save\/1&quot;)\n\n# File ~\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/tensorflow\/python\/saved_model\/save.py:1334, in save(obj, export_dir, signatures, options)\n#    1332 # pylint: enable=line-too-long\n#    1333 metrics.IncrementWriteApi(_SAVE_V2_LABEL)\n# -&gt; 1334 save_and_return_nodes(obj, export_dir, signatures, options)\n#    1335 metrics.IncrementWrite(write_version=&quot;2&quot;)\n# \n# .........\n# \n# \n# File ~\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/def_function.py:677, in Function._defun_with_scope.&lt;locals&gt;.wrapped_fn(*args, **kwds)\n#     673 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access\n#     674   # __wrapped__ allows AutoGraph to swap in a converted function. We give\n#     675   # the function a weak reference to itself to avoid a reference cycle.\n#     676   with OptionalXlaContext(compile_with_xla):\n# --&gt; 677     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n#     678   return out\n\n# File ~\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/func_graph.py:1147, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)\n#    1145 except Exception as e:  # pylint:disable=broad-except\n#    1146   if hasattr(e, &quot;ag_error_metadata&quot;):\n# -&gt; 1147     raise e.ag_error_metadata.to_exception(e)\n#    1148   else:\n#    1149     raise\n\n# TypeError: in user code:\n\n#     File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/keras\/saving\/saving_utils.py&quot;, line 138, in _wrapped_model  *\n#         outputs = model(*args, **kwargs)\n#     File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/keras\/utils\/traceback_utils.py&quot;, line 67, in error_handler  **\n#         raise e.with_traceback(filtered_tb) from None\n\n#     TypeError: Exception encountered when calling layer &quot;brute_force_3&quot; (type BruteForce).\n    \n#     in user code:\n    \n#         File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/tensorflow_recommenders\/layers\/factorized_top_k.py&quot;, line 567, in call  *\n#             queries = self.query_model(queries)\n#         File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/sentence_transformers\/SentenceTransformer.py&quot;, line 160, in encode  *\n#             features = self.tokenize(sentences_batch)\n#         File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/sentence_transformers\/SentenceTransformer.py&quot;, line 318, in tokenize  *\n#             return self._first_module().tokenize(texts)\n#         File &quot;\/home\/nicholas\/Documents\/Dev\/Rialto-predict-1\/venv\/lib\/python3.10\/site-packages\/sentence_transformers\/models\/Transformer.py&quot;, line 102, in tokenize  *\n#             batch1.append(text_tuple[0])\n    \n#         TypeError: 'NoneType' object is not subscriptable\n    \n# ...\n    \n#     Call arguments received:\n#       \u2022 queries=['None']\n#       \u2022 k=None\n<\/code><\/pre>\n<p><strong>2. THE tf.py_function<\/strong>\nAs from my understanding the problem with the first approach is that it has no knowledge of the input type\/value this second approach, from <a href=\"https:\/\/stackoverflow.com\/questions\/71411065\/use-sentence-transformers-inside-of-a-keras-model\">Use `sentence-transformers` inside of a keras model<\/a> was supposedly gonna work, as it uses tf.py_function to accept a List of Strings as first input, without complaining.<\/p>\n<pre><code>def approach_2(startups_ids: list, startup_vectors):\n        import tensorflow as tf\n        import tensorflow_recommenders as tfrs\n        import numpy as np\n        from transformers import MPNetTokenizer, TFMPNetModel\n\n        # Here it loads the specific pre-trained model we are using for Rialto\n        tokenizer = MPNetTokenizer.from_pretrained(\n            &quot;sentence-transformers\/all-mpnet-base-v2&quot;\n        )\n        model = TFMPNetModel.from_pretrained(\n            &quot;sentence-transformers\/all-mpnet-base-v2&quot;, from_pt=True\n        )\n\n        class SBert(tf.keras.layers.Layer):\n            def __init__(self, tokenizer, model):\n                super(SBert, self).__init__()\n\n                self.tokenizer = tokenizer\n                self.model = model\n\n            def tf_encode(self, inputs):\n                def encode(inputs):\n                    inputs = [x[0].decode(&quot;utf-8&quot;) for x in inputs.numpy()]\n                    outputs = self.tokenizer(\n                        inputs, padding=True, truncation=True, return_tensors=&quot;tf&quot;\n                    )\n                    return outputs[&quot;input_ids&quot;], outputs[&quot;attention_mask&quot;]\n\n                return tf.py_function(\n                    func=encode, inp=[inputs], Tout=[tf.int32, tf.int32]\n                )\n\n            def process(self, i, a):\n                def __call(i, a):\n                    model_output = self.model(\n                        {&quot;input_ids&quot;: i.numpy(), &quot;attention_mask&quot;: a.numpy()}\n                    )\n                    return model_output[0]\n\n                return tf.py_function(func=__call, inp=[i, a], Tout=[tf.float32])\n\n            def mean_pooling(self, model_output, attention_mask):\n\n                token_embeddings = tf.squeeze(tf.stack(model_output), axis=0)\n                input_mask_expanded = tf.cast(\n                    tf.broadcast_to(\n                        tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)\n                    ),\n                    tf.float32,\n                )\n                a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1)\n                b = tf.clip_by_value(\n                    tf.math.reduce_sum(input_mask_expanded, axis=1),\n                    1e-9,\n                    tf.float32.max,\n                )\n                embeddings = a \/ b\n                embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)\n\n                return embeddings\n\n            def call(self, inputs):\n                input_ids, attention_mask = self.tf_encode(inputs)\n                model_output = self.process(input_ids, attention_mask)\n                embeddings = self.mean_pooling(model_output, attention_mask)\n                return embeddings\n\n        #  Uses the keras-ified model in a Keras model\n        sbert = SBert(tokenizer, model)\n        inputs = tf.keras.layers.Input((1,), dtype=tf.string)\n        outputs = sbert(inputs)\n        model = tf.keras.Model(inputs, outputs)\n\n        # Implements the model we just build for top KNN retrieval, from the pool of pre-calculated startups embeddings.\n        exported_model = tfrs.layers.factorized_top_k.BruteForce(model)\n        exported_model.index(np.array(startup_vectors), np.array(startups_ids))\n\n        # TESTS the model\n        # for some reason this seems to be needed in order to save the model :\/\n        # https:\/\/github.com\/tensorflow\/recommenders\/issues\/131\n\n        print(exported_model(tf.constant([&quot;'Test Text Query'&quot;])))\n\n        return exported_model\n\n\nmodel_to_store_1 = approach_2(search_db_ids, search_db_embeddings)\n\ntf.saved_model.save(model_to_store_1, export_dir=&quot;\/home\/nicholas\/test_model_save\/2&quot;)\n\n# THIS ONE WORKS LIKE A CHARM, saving the model and everything. Deploy on sagemaker is successful.\n \n# FAILS TO WORK ON SAGEMAKER. BELOW THE LOGS WHEN THE MODEL IS CALLED\n\n# ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{\n#     &quot;error&quot;: &quot;No OpKernel was registered to support Op 'EagerPyFunc' used by {{node StatefulPartitionedCall\/brute_force\/model\/s_bert\/EagerPyFunc}} with these attrs: [is_async=false, Tin=[DT_STRING], _output_shapes=[&lt;unknown&gt;, &lt;unknown&gt;], Tout=[DT_INT32, DT_INT32], token=\\&quot;pyfunc_4\\&quot;]\\nRegistered devices: [CPU]\\nRegistered kernels:\\n  &lt;no registered kernels&gt;\\n\\n\\t [[StatefulPartitionedCall\/brute_force\/model\/s_bert\/EagerPyFunc]]\\n\\t [[StatefulPartitionedCall]]&quot;\n# }&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/rialto-t2s-model-endpoint in account 634470116418 for more information\n<\/code><\/pre>\n<p>As you can see from the log, that the problem seems to be with the Eager mode and py_functions. I tried to google and found absolutely nothing on how to address this issue.<\/p>\n<p><strong>3. THE Classes approach<\/strong><\/p>\n<p>I've tried implementing something building upon <a href=\"https:\/\/www.philschmid.de\/tensorflow-sentence-transformers\" rel=\"nofollow noreferrer\">this article<\/a>, but I am running into similar issues that with the first approach, as when I go to save the model, the expected input clashed with the requirements of tokenizer.<\/p>\n<p>EDIT 1 - here a coolab showcasing the approach: <a href=\"https:\/\/colab.research.google.com\/drive\/1gibFdEoHTs0hzD5yiXzLT_-asmilUoAQ?usp=sharing#scrollTo=TibAssWm3D5e\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1gibFdEoHTs0hzD5yiXzLT_-asmilUoAQ?usp=sharing#scrollTo=TibAssWm3D5e<\/a><\/p>\n<hr \/>\n<p>All of this journey triggered some questions:<\/p>\n<p><strong>Question 1<\/strong> Is this even a best practice? Should I serve my model the tokenized sentences as a tensor?<\/p>\n<p><strong>Question 2<\/strong> How the hell do I make it work? :)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2022-09-08 14:03:52.027 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"tensorflow|amazon-sagemaker|huggingface-transformers|sentence-transformers",
        "Question_view_count":53,
        "Owner_creation_date":"2020-07-20 13:18:09.42 UTC",
        "Owner_last_access_date":"2022-09-23 13:24:48.093 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-09-12 07:12:13.047 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Data format for calling an AWS SageMaker object detection model",
        "Question_body":"<p>I have trained an object detection model in AWS SageMaker and created an endpoint for it. The endpoint is called via a lambda function that is accessed through an api gateway. So far so good.<\/p>\n\n<p>Now I want to call the api from an angular application - upload a picture and get back the predictions. But I am having trouble figuring out the correct way to do it. The aws documentation I've seen so far doesn't go into much detail on that part.<\/p>\n\n<p>I got the image as a blob, captured from an html canvas. I tried to convert the blob to a byte array:<\/p>\n\n<pre><code>    fileReader.onload = function () {\n    arrayBuffer = this.result;\n\n    var byteArray = new Uint8Array(arrayBuffer);\n\n    that.http.post&lt;any&gt;(that.url, byteArray.toString(), {\n      headers: new HttpHeaders().set('X-Api-Key', that.apiKey).set(\"Content-Type\", \"image\/jpeg\")\n    }).toPromise().then((result) =&gt; {\n      resolve(result);\n    });\n  };\n  fileReader.readAsArrayBuffer(blob);\n<\/code><\/pre>\n\n<p>The response is:<\/p>\n\n<pre><code>{\"message\":\"Received client error (400) from model with message \\\"unable to evaluate payload provided\\\".}\n<\/code><\/pre>\n\n<p>Has anyone done this yet? What is the correct way to submit an image?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2018-09-27 09:40:42.34 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":489,
        "Owner_creation_date":"2011-09-15 09:33:33.173 UTC",
        "Owner_last_access_date":"2022-09-23 13:35:09.13 UTC",
        "Owner_location":null,
        "Owner_reputation":399,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"what is the unversioned models in sagemaker?",
        "Question_body":"<p>I read the sagemaker model registry <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model_package\" rel=\"nofollow noreferrer\">doc<\/a>. It mentioned versioned models and unversioned models. But the doc lacks a clear description of how these two behave differently for model management\/deployment.<\/p>\n<p>Here is the definition in the doc<\/p>\n<blockquote>\n<p>There are two types of model packages:<\/p>\n<p>Versioned - a model that is part of a model group in the model registry.\nUnversioned - a model package that is not part of a model group.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-18 18:27:46.377 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":99,
        "Owner_creation_date":"2017-10-10 00:57:43.513 UTC",
        "Owner_last_access_date":"2022-09-25 05:33:39.507 UTC",
        "Owner_location":null,
        "Owner_reputation":635,
        "Owner_up_votes":6,
        "Owner_down_votes":5,
        "Owner_views":43,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Amazon sagemaker Lifecycle configuration not working",
        "Question_body":"<p>I have the following Lifecycle configuration file in Amazon sagemaker<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource activate conda_pytorch_p36\n\n# Replace myPackage with the name of the package you want to install.\nconda install -c pytorch torchtext\n# You can also perform &quot;conda install&quot; here as well.\n\nsource deactivate\n\nEOF\n<\/code><\/pre>\n<p>But when I try to import torchtext on a jupyter notebook inside the pytorch_conda_p36 virtual environment I get a module not found error<\/p>\n<p>This is my code<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>from sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='sagemaker-us-east-2-964130302244'\ndata_dir = 'dataset'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_dir)\n\nimport torch\nimport torch.utils.data as tud\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import Counter, defaultdict\nimport operator\nimport os, math\nimport numpy as np\nimport random\nimport copy\nimport s3fs\nimport torchtext\nfs = s3fs.S3FileSystem()\n<\/code><\/pre>\n<p>The error I get is the following<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-2-c9a96a48b0b7&gt; in &lt;module&gt;\n     11 import copy\n     12 import s3fs\n---&gt; 13 import torchtext\n     14 fs = s3fs.S3FileSystem()\n     15 # from nltk import word_tokenize\n\nModuleNotFoundError: No module named 'torchtext'\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-06-23 12:01:42.137 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|pytorch|amazon-sagemaker|torchtext",
        "Question_view_count":376,
        "Owner_creation_date":"2020-01-24 01:21:01.013 UTC",
        "Owner_last_access_date":"2020-09-19 22:50:22.223 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-06-23 13:05:59.21 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Athena query in Sage maker notebook. How to get the location of output file for reusability?",
        "Question_body":"<p>My query worked:<\/p>\n<pre><code>from pyathena import connect\nimport pandas as pd\nconn = connect(s3_staging_dir='s3:\/\/alphabucket\/query-results\/myfolder\/',region_name='us-east-1')\n\ndf = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)\n<\/code><\/pre>\n<p>How ever.. next time when I run the notebook, I would like to avoid running the query again.<\/p>\n<p>I am looking for API that would return me the result file.<\/p>\n<p>eg:<\/p>\n<pre><code>df = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)\nfile = conn.last_query_output_location() # Hypothetical function Doesnt Work\nprint(file) # --&gt; s3:\/\/alphabucket\/query-results\/myfolder\/2021\/07\/23\/dfjj00772hh.csv\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-23 18:20:12.717 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-athena|amazon-sagemaker",
        "Question_view_count":481,
        "Owner_creation_date":"2021-01-22 15:51:57.83 UTC",
        "Owner_last_access_date":"2021-07-27 15:29:36.38 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-07-24 05:38:39.44 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can we use data directly from RDS or df as a data source for training job in Sagemaker, rather than pulling it from from s3 or EFS?",
        "Question_body":"<p>I am using Sagemaker platform for model development and deployment. Data is read from RDS tables and then spitted to train and test df.\nTo create the training job in Sagemaker, I found that it takes data source only as s3 and EFS. For that I need to keep train and test data back to s3, which is repeating the data storing process in RDS and s3.\nI would want to directly pass the df from RDS as a parameter in tarining job code. Is there any way we can pass df in fit method<\/p>\n<pre><code>    image=&quot;581132636225.dkr.ecr.ap-south-1.amazonaws.com\/sagemaker-ols-model:latest&quot;\n    model_output_folder = &quot;model-output&quot;\n    print(image)\n    tree = sagemaker.estimator.Estimator(\n        image,\n        role,\n        1,\n        &quot;ml.c4.2xlarge&quot;,\n        output_path=&quot;s3:\/\/{}\/{}&quot;.format(sess.default_bucket(), model_output_folder),\n        sagemaker_session=sess,\n    )\n\n**tree.fit({'train': &quot;s3_path_having_test_data&quot;}, wait=True)**\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-04 11:35:23.9 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|data-science|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2018-12-27 10:32:44.167 UTC",
        "Owner_last_access_date":"2022-02-02 07:38:56.8 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-04 17:22:12.063 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Streamlit Browser App does not open from Sagemaker Terminal",
        "Question_body":"<p>I am working on building a movie recommendation engine on aws sagemaker environment and i plan to show working demonstration for the same. I am trying to use the streamlit library for running the app.<\/p>\n<p>After running the command: streamlit run app.py, it provides me 2 urls i.e. Network and External urls.<\/p>\n<p>The issues is none of the url works. It results in connection timed out, the server does not respond.<\/p>\n<p>Please help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-08-15 15:48:54.87 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|user-interface|jupyter|amazon-sagemaker|streamlit",
        "Question_view_count":724,
        "Owner_creation_date":"2020-05-31 11:45:58.247 UTC",
        "Owner_last_access_date":"2021-01-25 14:00:10.543 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-08-17 05:35:20.803 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-29 05:43:39.707 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1826,
        "Owner_creation_date":"2019-05-29 03:08:01.007 UTC",
        "Owner_last_access_date":"2021-06-28 08:28:46.13 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-31 19:12:00.12 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Schedule the deployment of a sagemaker model",
        "Question_body":"<p>I'm trying out SageMaker and I've created a model using autopilot. The point is that SageMaker only allows you to deploy directly to an endpoint. But since I'll only be using the model a couple of times a day, what is the most direct way to  schedule deployments by events (for example when loading new csv's into an s3 directory or when I see a queue in sqs) or at least periodically?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-04 22:39:47.5 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_date":"2018-12-07 18:01:35.74 UTC",
        "Owner_last_access_date":"2022-05-08 18:44:56.82 UTC",
        "Owner_location":"Santiago, Chile",
        "Owner_reputation":410,
        "Owner_up_votes":20,
        "Owner_down_votes":1,
        "Owner_views":46,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-03-04 22:54:22.607 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"subprocess.run produces fatal error: SSL validation failed",
        "Question_body":"<p>I am trying to run this in my quest to start using sagemaker\/s3\/aws (in windows sorry) :<\/p>\n<pre><code>import subprocess\nimport boto3\nimport os\n\ns3_bucket_name = &quot;dududu&quot;\nmlruns_direc = &quot;C:\\bla\\sagemaker\\mlrun&quot;\n\ns3 = boto3.client('s3', verify=False)\n\n# session = boto3.Session(\n#     aws_access_key_id=os.environ.get(&quot;awsaccesskey&quot;),\n#     aws_secret_access_key=os.environ.get(&quot;awssecret&quot;)\n# )\n\noutput = subprocess.run(\n    [&quot;aws&quot;,  &quot;s3&quot;, &quot;sync&quot;, &quot;{}&quot;.format(mlruns_direc)\n    , &quot;s3:\/\/{}&quot;.format(s3_bucket_name)], stdout=subprocess.PIPE, encoding=&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>but I get:<\/p>\n<pre><code>fatal error: SSL validation failed for https:\/\/xxxx.s3.eu-west-2.amazonaws.com\/?list-type=2&amp;prefix=&amp;encoding-type=url [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1125)\n<\/code><\/pre>\n<p>Could this be related to the company's proxy\/network as some people suggested?<\/p>\n<p>I managed to upload data programmatically to an S3 bucket from my machine though. Any pointers very much welcome.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-19 12:37:53.333 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|boto|amazon-sagemaker",
        "Question_view_count":80,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Run multiple ipynb and R notebooks in Sagameker Instance",
        "Question_body":"<p>Question on running multiple ipynb and r notebooks inside sagemaker instance using life cycle config or other way.<\/p>\n<p>So I have four notebooks, 3 ipynb and 1 r. These should run in order and should wait for first one to finish. Question is how should we achieve that?<\/p>\n<p><code>Lambda can start the instance-&gt; Life cycle policy will execute the notebooks BUT how the run order can be accomplished? <\/code><\/p>\n<p>Thoughts? Not sure if step function can run these notebooks 1 by 1. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-22 05:01:30.813 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":25,
        "Owner_creation_date":"2017-11-05 18:04:36.277 UTC",
        "Owner_last_access_date":"2022-09-03 01:17:57.907 UTC",
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Training Job in Sagemaker gives error in locating file in S3 to docker image path",
        "Question_body":"<p>I am trying to use <code>scikit_bring_your_own\/container\/decision_trees\/train mode<\/code>, running in AWS CLI, I had no issues. Trying to replicate in Creating Sagemaker Training Job , facing issue in loading data from S3 to docker image path.<\/p>\n\n<p>In CLI command we used specify the <code>docker run -v $(pwd)\/test_dir:\/opt\/ml --rm ${image} train<\/code> from where the input needs to referred. <\/p>\n\n<p>In training job, mentioned the S3 bucket location and output path for model artifacts.<\/p>\n\n<pre><code>Error entered in the exception as in train - \"container\/decision_trees\/train\"\nraise ValueError(('There are no files in {}.\\n' + \n'This usually indicates that the channel ({}) was incorrectly specified,\\n'  + \n'the data specification in S3 was incorrectly specified or the role specified\\n' +\n'does not have permission to access the data.').format(training_path, channel_name))\n\nTraceback (most recent call last):\nFile \"\/opt\/program\/train\", line 55, in train\n'does not have permission to access the data.').format(training_path, channel_name)) \n<\/code><\/pre>\n\n<p>So not understanding is there any tweaking required or any access missing.<\/p>\n\n<p>kindly help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-02 04:34:29.423 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":930,
        "Owner_creation_date":"2018-03-11 19:02:28.873 UTC",
        "Owner_last_access_date":"2020-09-13 11:30:22.13 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-05-23 22:11:49.48 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to label a text with multiple paragraphs in AWS Ground Truth?",
        "Question_body":"<p>I was trying setup a single label labeling task in AWS Groundtruth through the console. My goal is to match some users in social media and for each user I have several possible candidates out of which one should be selected (label). My CSV looks like this:<\/p>\n<pre><code>firtname | lastname | candidates\n\nRomeo      Montague      x\nJuliet     Capulet       x\n<\/code><\/pre>\n<p>Instead of &quot;x&quot;, I would like to have something like this<\/p>\n<pre><code>candidate_1\ndescription_1\nlink_1\n\ncandidate_2\ndescription_2\nlink_2\n\ncandidate_3\ndescription_3\nlink_3\n<\/code><\/pre>\n<p>The human worked should then select whereas the correct label is candidate_1, candidate_2 or candidate_3 or none of the above.<\/p>\n<p>I am aware Sagemaker ground truth does not accept new lines characters and that it renders it in HTML so I tried to input the following:<\/p>\n<pre><code>candidate_1 &lt;br\/&gt; description &lt;br\/&gt; link &lt;br\/&gt;&lt;br\/&gt; candidate_2 &lt;br\/&gt; description &lt;br\/&gt; link &lt;br\/&gt;&lt;br\/&gt; candidate_3 &lt;br\/&gt; description &lt;br\/&gt; link &lt;br\/&gt;&lt;br\/&gt; \n<\/code><\/pre>\n<p>unfortunately, when I take a look at the console, the input on the left does not render correctly:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kfuiE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kfuiE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The line breaks within the div tag seem to be simply ignored by the UI.<\/p>\n<p>I found <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=302236\" rel=\"nofollow noreferrer\">this post<\/a> which contains the answer but I am struggling to adapt to my concrete use case.<\/p>\n<p>How can I change my csv so that the multiple paragraphs get rendered corrected?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2021-05-26 09:16:33.42 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|text|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":161,
        "Owner_creation_date":"2017-03-24 14:05:50 UTC",
        "Owner_last_access_date":"2022-09-24 21:27:16.943 UTC",
        "Owner_location":"Z\u00fcrich, Su\u00efssa",
        "Owner_reputation":921,
        "Owner_up_votes":75,
        "Owner_down_votes":10,
        "Owner_views":147,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-05-26 11:54:00.03 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker not outputting Tensorboard logs to S3 during training",
        "Question_body":"<p>I'm training a model with Tensorflow using Amazon Sagemaker, and I'd like to be able to monitor training progress while the job is running. During training however, no Tensorboard files are output to S3, only once the training job is completed are the files uploaded to S3. After training has completed, I can download the files and see that Tensorboard has been logging values correctly throughout training, despite only being updated in S3 once after training completes.<\/p>\n<p>I'd like to know why Sagemaker isn't uploading the Tensorboard information to S3 throughout the training process?<\/p>\n<p>Here is the code from my notebook on Sagemaker that kicks off the training job<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker.debugger import DebuggerHookConfig, CollectionConfig, TensorBoardOutputConfig\n\nimport time\n\nbucket = 'my-bucket'\noutput_prefix = 'training-jobs'\nmodel_name = 'my-model'\ndataset_name = 'my-dataset'\ndataset_path = f's3:\/\/{bucket}\/datasets\/{dataset_name}'\n\noutput_path = f's3:\/\/{bucket}\/{output_prefix}'\njob_name = f'{model_name}-{dataset_name}-training-{time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())}'\ns3_checkpoint_path = f&quot;{output_path}\/{job_name}\/checkpoints&quot; # Checkpoints are updated live as expected\ns3_tensorboard_path = f&quot;{output_path}\/{job_name}\/tensorboard&quot; # Tensorboard data isn't appearing here until the training job has completed\n\ntensorboard_output_config = TensorBoardOutputConfig(\n    s3_output_path=s3_tensorboard_path,\n    container_local_output_path= '\/opt\/ml\/output\/tensorboard' # I have confirmed this is the unaltered path being provided to tf.summary.create_file_writer()\n)\n\nrole = sagemaker.get_execution_role()\n\nestimator = TensorFlow(entry_point='main.py', source_dir='.\/', role=role, max_run=60*60*24*5,\n                           output_path=output_path,\n                           checkpoint_s3_uri=s3_checkpoint_path,\n                           tensorboard_output_config=tensorboard_output_config,\n                           instance_count=1, instance_type='ml.g4dn.xlarge',\n                           framework_version='2.3.1', py_version='py37', script_mode=True)\n\ndpe_estimator.fit({'train': dataset_path}, wait=True, job_name=job_name)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":11,
        "Question_creation_date":"2021-01-14 00:01:29.257 UTC",
        "Question_favorite_count":1.0,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|tensorflow|tensorboard|amazon-sagemaker",
        "Question_view_count":916,
        "Owner_creation_date":"2020-03-31 21:57:13.903 UTC",
        "Owner_last_access_date":"2021-04-24 23:47:26.393 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-01-14 00:13:09.01 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"how to debug invocation timeout error in sagemaker batch transform?",
        "Question_body":"<p>I am experimenting with sagemaker, using a container from list here , <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> to run my model and overwriting model_fn and predict_fn functions in inference.py file for loading model and prediction as shown in link here (<a href=\"https:\/\/github.com\/PacktPublishing\/Learn-Amazon-SageMaker-second-edition\/blob\/main\/Chapter%2007\/huggingface\/src\/torchserve-predictor.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/PacktPublishing\/Learn-Amazon-SageMaker-second-edition\/blob\/main\/Chapter%2007\/huggingface\/src\/torchserve-predictor.py<\/a>) .\nI keep getting invocations timeout error =&gt; &quot;Model server did not respond to \/invocations request within 3600 seconds&quot;. am i missing anything in my inference.py code , as to adding something to response to the ping\/healthcheck?<\/p>\n<pre><code>file : inference.py\n\nimport json\nimport torch\nfrom transformers import AutoConfig, AutoTokenizer, DistilBertForSequenceClassification\n\nJSON_CONTENT_TYPE = 'application\/json'\n\ndef model_fn(model_dir):\n    config_path = '{}\/config.json'.format(model_dir)\n    model_path =  '{}\/pytorch_model.bin'.format(model_dir)\n    config = AutoConfig.from_pretrained(config_path)\n   ...\n\ndef predict_fn(input_data, model):\n    \/\/return predictions\n...\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-12 17:57:00.013 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":184,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Identifying user from AWS Sagemaker Studio generated EFS storage",
        "Question_body":"<p>When a sagemaker studio domain is created. An EFS storage is associated with the domain. As the assigned users log into Sagemaker studio, a corresponding home directory is created.<\/p>\n<p>Using a separate EC2 instance, I mounted the EFS storage that was created to try to see whether is it possible to look at each of the individual home domains. I noticed that each of these home directories are shown in terms of numbers (e.g 200000, 200005). Is there a specific rule on how this folders are named? Is it possible to trace the folders back to a particular user or whether this is done by design?<\/p>\n<p>(currently doing exploration on my personal aws account)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-16 03:29:37.253 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-efs",
        "Question_view_count":302,
        "Owner_creation_date":"2022-02-16 03:15:56.94 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.55 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Yes, if you <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/list-user-profiles.html\" rel=\"nofollow noreferrer\">list<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-user-profile.html\" rel=\"nofollow noreferrer\">describe<\/a> the domain users, you'll get back the user's <code>HomeEfsFileSystemUid<\/code> value.<br \/>\nHere's a CLI example:<\/p>\n<pre><code>aws sagemaker describe-user-profile --domain-id d-lcn1vbt47yku --user-profile-name default-1588670743757\n{\n    ...\n    &quot;UserProfileName&quot;: &quot;default-1588670743757&quot;,\n    &quot;HomeEfsFileSystemUid&quot;: &quot;200005&quot;,\n    ...\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-17 21:39:04.567 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Passing multiple field payload in AWS API gateway",
        "Question_body":"<p>I am following the documentation at <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/<\/a> to setup an API gateway in front of sagemaker-api.<\/p>\n<p>In Section 5 <code>Step 5: Create a mapping template for request integration<\/code> there is a snippet of code that maps <code>user_id<\/code> to the sagemaker api's payload.\nThe specific snippet of code I am referring to is<\/p>\n<pre><code>{\n  &quot;instances&quot;: [\n#set( $user_id = $input.params(&quot;user_id&quot;) )\n#set( $items = $input.params(&quot;items&quot;) )\n#foreach( $item in $items.split(&quot;,&quot;) )\n    {&quot;in0&quot;: [$user_id], &quot;in1&quot;: [$item]}#if( $foreach.hasNext ),#end\n    $esc.newline\n#end\n  ]\n}\n<\/code><\/pre>\n<p>I have a multi-field JSON that I want to pass through as-is.\nMy incoming JSON is of the form:<\/p>\n<pre><code>{\n  &quot;USER&quot;: &quot;HENRY&quot;,\n  &quot;YR&quot;: &quot;2022&quot;,\n  &quot;CAR&quot;:&quot;HONDA&quot;,\n  &quot;MILES&quot;:&quot;2025&quot;\n}\n<\/code><\/pre>\n<p>I will not have multiple entries like in the example. But I need to forward this entire payload to the endpoint. How do I achieve this?\nThanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-26 00:47:54.107 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":16,
        "Owner_creation_date":"2016-03-17 06:30:24.61 UTC",
        "Owner_last_access_date":"2022-08-28 15:19:22.88 UTC",
        "Owner_location":null,
        "Owner_reputation":981,
        "Owner_up_votes":64,
        "Owner_down_votes":1,
        "Owner_views":74,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to create an aws sagemaker project using terraform?",
        "Question_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-03 13:54:49.18 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":298,
        "Owner_creation_date":"2022-05-25 20:48:45.307 UTC",
        "Owner_last_access_date":"2022-09-15 14:23:04.53 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-03 15:14:54.487 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-06-03 14:20:14.543 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker SKLearn entry point in a subdirectory?",
        "Question_body":"<p>Can I specify SageMaker estimator's entry point script to be in a subdirectory? So far, it fails for me. Here is what I want to do:<\/p>\n\n<pre><code>sklearn = SKLearn(\n    entry_point=\"RandomForest\/my_script.py\",\n    source_dir=\"..\/\",\n    hyperparameters={...\n<\/code><\/pre>\n\n<p>I want to do this so I don't have to break my directory structure. I have some modules, which I use in several sagemaker projects, and each project lives in its own directory:<\/p>\n\n<pre><code>my_git_repo\/\n\n  RandomForest\/\n    my_script.py\n    my_sagemaker_notebook.ipynb\n\n  TensorFlow\/\n    my_script.py\n    my_other_sagemaker_notebook.ipynb\n\nmodule_imported_in_both_scripts.py\n<\/code><\/pre>\n\n<p>If I try to run this, SageMaker fails because it seems to parse the name of the entry point script to make a module name out of it, and it does not do a good job:<\/p>\n\n<pre><code>\/usr\/bin\/python3 -m RandomForest\/my_script --bootstrap True --case nf_2 --max_features 0.5 --min_impurity_decrease 5.323785009485933e-06 --model_name model --n_estimators 455 --oob_score True\n\n...\n\n\/usr\/bin\/python3: No module named RandomForest\/my_script\n\n<\/code><\/pre>\n\n<p>Anyone knows a way around this other than putting <code>my_script.py<\/code> in the <code>source_dir<\/code>?<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54314876\/aws-sagemaker-sklearn-entry-point-allow-multiple-script\">Related to this question<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-02 03:39:06.7 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":843,
        "Owner_creation_date":"2013-05-08 18:53:12.833 UTC",
        "Owner_last_access_date":"2022-09-23 20:31:17.31 UTC",
        "Owner_location":null,
        "Owner_reputation":1245,
        "Owner_up_votes":1495,
        "Owner_down_votes":0,
        "Owner_views":109,
        "Answer_body":"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941<\/a> which should also solve this issue, but for now, you do need to put <code>my_script.py<\/code> in <code>source_dir<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-24 17:57:09.097 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker endpoint servving doesnt work for multiple inputs (mulit input-output LSTM)",
        "Question_body":"<p>I have a LSTM network that has 3 inputs and 3 outputs(built with <a href=\"https:\/\/keras.io\/guides\/functional_api\/\" rel=\"nofollow noreferrer\">functional api<\/a> in Tf.keras) , that I am trying to deploy as sagemaker endpoint. I have input shape of (None,10,1) for each input\/feature, which means 10 timesteps.(I later concatenate the embeddings, but its irrelevant here)<\/p>\n<p>Everything works fine on training time on sagemaker training jobs as well and training completes and artifacts are made successfully. But at time of invocation, endpoint is not working to predict <code>1 example, having 10 timesteps with 3 inputs<\/code> , I have tried multiple things but cant provide three inputs for prediction(input_1,input_2,input_1).<\/p>\n<p>As I said that each input has 10 timesteps, so have shape (10,1). Endpoint only returns the output if I format my payload as below, but by doing so, it treats each time-step as separate example\/instance and return 10 predictions for each output<\/p>\n<pre><code>{'inputs':{\n           'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\n           'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n           'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\n          } # gives len(pred['output_1&quot;])) == 10\n<\/code><\/pre>\n<p>This is expected as it consider this request as 10 examples, but in my case it is one example with 10-timesteps for each feature (1,10,1). So I tried different things from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html\" rel=\"nofollow noreferrer\">documentation<\/a>. Like using instances.<\/p>\n<pre><code>{'instances': [\n                {\n                      'input_1': [[0],[0], [0],[0],[2],[12],[11], [7], [7], [2]],\n                      'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n                      'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]\n                }\n              ]\n}\n<\/code><\/pre>\n<p>But it gives this error.<\/p>\n<blockquote>\n<p>transpose expects a vector of size 4. But input(1) is a vector of\nsize 3\\n\\t [[{{node transpose_1}}]]\\n\\t\n[[functional_1\/lstm\/PartitionedCall]]\\n\\t\n[[StatefulPartitionedCall\/StatefulPartitionedCall]]&quot;\\n}&quot;}&quot;<\/p>\n<\/blockquote>\n<p>Document also gives example and says<\/p>\n<blockquote>\n<p>for models with multiple named inputs, just include all the keys in the input dict<\/p>\n<\/blockquote>\n<p>but when I use this, I get error saying <code>Missing 'inputs' or 'instances' key\\&quot;\\n}&quot;<\/code><\/p>\n<pre><code>{'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\n 'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n 'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\n<\/code><\/pre>\n<p>My invocation code is below.<\/p>\n<pre><code>import boto3\nimport json\n\nsm = boto3.client('sagemaker-runtime')    \nendpoint_name = &quot;tensorflow--------------------4&quot;\nresponse = sm.invoke_endpoint(EndpointName=endpoint_name, \n                              Body=json.dumps(payload),\n                              ContentType='application\/json')\n<\/code><\/pre>\n<p>I am not sure how to solve this issue, looking forward for help<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-15 13:58:28.907 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|lstm|amazon-sagemaker|tensorflow-serving",
        "Question_view_count":163,
        "Owner_creation_date":"2014-05-27 17:36:45.023 UTC",
        "Owner_last_access_date":"2022-09-24 22:00:51.297 UTC",
        "Owner_location":"London, Uk",
        "Owner_reputation":19335,
        "Owner_up_votes":345,
        "Owner_down_votes":77,
        "Owner_views":1053,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-01-15 14:15:23.127 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to get bearer token AWS for Postman",
        "Question_body":"<p>I am using AWS sagemaker, and I have created an endpoint. I want to test endpoint on postman app. I give endpoint URL and JSON body to postman app. But I get this error that <code>&quot;message&quot;: &quot;Missing Authentication Token&quot;<\/code> I need to know from where I 'll get bearer token so that I can give it to postman app.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-20 09:57:55.733 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":1080,
        "Owner_creation_date":"2019-09-07 18:22:12.003 UTC",
        "Owner_last_access_date":"2022-08-28 17:07:56.487 UTC",
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Answer_body":"<p>I am answering my own question after searching and reading forums,<\/p>\n<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure<\/code> command.\nFor configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.\nAfter configuration by running this command, <code>aws ecr get-authorization-token<\/code>, we can get authorizationToken. <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/ecr\/get-authorization-token.html\" rel=\"nofollow noreferrer\">here<\/a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-20 21:09:13.59 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker - using cross validation instead of dedicated validation set?",
        "Question_body":"<p>When I train my model locally I use a 20% test set and then cross validation. Sagameker seems like it needs a dedicated valdiation set (at least in the tutorials I've followed). Currently I have 20% test, 10% validation leaving 70% to train - so I lose 10% of my training data compared to when I train locally, and there is some performance loss as a results of this. <\/p>\n\n<p>I could just take my locally trained models and overwrite the sagemaker models stored in s3, but that seems like a bit of a work around. Is there a way to use Sagemaker without having to have a dedicated validation set? <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-23 16:47:42.943 UTC",
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|cross-validation|amazon-sagemaker",
        "Question_view_count":1401,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to read bucket image from AWS S3 into Sagemaker Jupyter Instance",
        "Question_body":"<p>I am very new to AWS and the cloud environment. I am a machine learning engineer, I am planning to build a custom CNN into the AWS environment to predict a given image has an iPhone present or not.<\/p>\n<p><strong>What I have done:<\/strong><\/p>\n<p><em><strong>Step 1:<\/strong><\/em><\/p>\n<p>I have created a S3 bucket for iPhone classifier with the below folder structure :<\/p>\n<pre><code> Iphone_Classifier &gt; Train &gt; Yes_iphone_images &gt; 1000 images\n                           &gt; No_iphone_images  &gt; 1000 images\n\n                   &gt; Dev   &gt; Yes_iphone_images &gt; 100 images\n                           &gt; No_iphone_images  &gt; 100 images\n\n                   &gt; Test  &gt; 30 random images\n<\/code><\/pre>\n<p>Permission - &gt; <strong>Block all public access<\/strong><\/p>\n<p><em><strong>Step 2:<\/strong><\/em><\/p>\n<p>Then I go to Amazon Sagemaker, and create an instance:<\/p>\n<p>I select the following<\/p>\n<pre><code> Name: some-xyz,\n Type: ml.t2.medium\n IAM : created new IAM role ( root access was enabled.)\n others: All others were in default\n<\/code><\/pre>\n<p>Then the notebook instance was created and opened.<\/p>\n<p><em><strong>Step 3:<\/strong><\/em><\/p>\n<p>Once I had the instance opened,<\/p>\n<pre><code>1. I used to prefer - conda_tensorflow2_p36 as interpreter\n2. Created a new Jupyter notebook and stated.\n3. I checked image classification examples but was confused, and most others used CSV files, but I want to retrieve images from S3 buckets. \n<\/code><\/pre>\n<p><em><strong>Question:<\/strong><\/em><\/p>\n<pre><code>1. How simply can we access the S3 bucket image dataset from the Jupiter Instances of Sagemaker? \n2. I exactly need the reference code to access the S3 bucket images. \n3. Is it a good approach to copy the data to the notebook or is it better to work from the S3 bucket.\n<\/code><\/pre>\n<p><em><strong>What I have tried was:<\/strong><\/em><\/p>\n<pre><code>import boto3\nclient = boto3.client('s3')\n\n# I tried this one and failed\n#path = 's3:\/\/iphone\/Train\/Yes_iphone_images\/100.png'\n\n# I tried this one and failed\npath = 's3:\/\/iphone\/Test\/10.png'\n\n# I uploaded to the notebook instance an image file and when I try to read it works\n#path = 'thiyaga.jpg'\nprint(path)\n\nimport cv2\nfrom matplotlib import pyplot as plt\nprint(cv2.__version__)\nplt.imshow(img)\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-09 15:38:47.557 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2065,
        "Owner_creation_date":"2011-12-19 06:44:22.387 UTC",
        "Owner_last_access_date":"2022-09-23 16:20:11.07 UTC",
        "Owner_location":"Finland",
        "Owner_reputation":677,
        "Owner_up_votes":20,
        "Owner_down_votes":3,
        "Owner_views":161,
        "Answer_body":"<p>If your image is binary-encoded, you could try this:<\/p>\n<pre><code>import boto3 \nimport matplotlib.pyplot as plt \n\n# Define Bucket and Key \ns3_bucket, s3_key = 'YOUR_BUCKET', 'YOUR_IMAGE_KEY'\n\nwith BytesIO() as f:\n    boto3.client(&quot;s3&quot;).download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n    f.seek(0)\n    img = plt.imread(f, format='png')\n<\/code><\/pre>\n<p>in other case, the following code works out (based on the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/1.9.42\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">documentation<\/a>):<\/p>\n<pre><code>s3 = boto3.resource('s3')\n\nimg = s3.Bucket(s3_bucket).download_file(s3_key, 'local_image.jpg')\n<\/code><\/pre>\n<p>In both cases, you can visualize the image with <code>plt.imshow(img)<\/code>.<\/p>\n<p>In your path example <code>path = 's3:\/\/iphone\/Test\/10.png'<\/code>, the bucket and key will be <code>s3_bucket = 'iphone'<\/code> and <code>s3_key=Test\/10.png<\/code><\/p>\n<p>Additional Resources: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-11 17:07:57.847 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2020-08-09 16:14:33.19 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker Minimum Configuration",
        "Question_body":"<p>Why do I need Container for AWS SageMaker? If I want to run Scikit Learn on SageMaker's Jupyter notebook for self learning purposes, do I still need to configure Container for it?<\/p>\n\n<p>What is the minimum configuration on SageMaker I will need if I just want to learn Scikit Learn? For example, I want to run Scikit Learn's Decision Tree algorithm with a set of training data and a set of test data. What do I need to do on SageMaker to perform the tasks? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-12 04:20:34.91 UTC",
        "Question_favorite_count":2.0,
        "Question_score":9,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":849,
        "Owner_creation_date":"2016-04-20 00:33:54.223 UTC",
        "Owner_last_access_date":"2022-04-02 22:37:53.663 UTC",
        "Owner_location":"San Jose, CA, United States",
        "Owner_reputation":1075,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":181,
        "Answer_body":"<p>You don't need much. Just an AWS Account with the correlated permissions on your role.\nInside the AWS SageMaker Console you can just run an AWS Notebook Instance with one click. There is Sklearn preinstalled and you can use it out of the box. No special container needed.<\/p>\n\n<p>As minimum you just need your AWS Account with the correlated permissions to create EC2 Instances and read \/ write from your S3. Thats all, just try it. :)<\/p>\n\n<p>Use this as a starting point: <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker\/\" rel=\"noreferrer\">Amazon SageMaker \u2013 Accelerating Machine Learning<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/98gRb.png\" rel=\"noreferrer\">You can also access it via the Jupyter Terminal<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-05-14 14:52:02.537 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_last_edit_date":"2018-07-10 08:33:12.337 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Question_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-01 09:01:31.31 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|hyperparameters",
        "Question_view_count":686,
        "Owner_creation_date":"2011-12-25 10:19:41.143 UTC",
        "Owner_last_access_date":"2022-09-22 10:21:32.26 UTC",
        "Owner_location":null,
        "Owner_reputation":9050,
        "Owner_up_votes":1458,
        "Owner_down_votes":21,
        "Owner_views":1750,
        "Answer_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-22 16:57:22.863 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Removing annotators in case of bad performance in AWS ground truth",
        "Question_body":"<p>Is there a way in AWS Ground Truth to validate annotations from workers during the labeling task and <strong>remove the worker<\/strong> if the performance is really poor <strong>before<\/strong> they finish the task?<\/p>\n<p>I know that there are other services that sometimes present an instance for which the label is known to the worker. In case the workers do not label this correctly, they get kicked out. This is to assure quality of the annotations.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-24 15:12:45.487 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":17,
        "Owner_creation_date":"2011-02-15 15:41:59.7 UTC",
        "Owner_last_access_date":"2022-09-02 07:08:03.02 UTC",
        "Owner_location":"Helsinki, Finland",
        "Owner_reputation":143,
        "Owner_up_votes":1708,
        "Owner_down_votes":1,
        "Owner_views":78,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Sagemaker lifecycle config: could not find conda environment conda_python3 | works fine in terminal",
        "Question_body":"<p>I have prototyped some code using conda_python3 environment in SageMaker notebook instance. When I open terminal, activate environment and run code it works well but when I try to automate the process by using lifecycle configuration, system is unable to identify the environment. I get the following message on cloud watch<\/p>\n<pre><code>2020-07-16T11:25:37.576+05:30\nCould not find conda environment: conda_python3\n\n2020-07-16T11:25:37.576+05:30\nYou can list all discoverable environments with `conda info --envs`.\n<\/code><\/pre>\n<p>My life cycle config code:<\/p>\n<pre><code>#!bin\/bash\n\nset -e\n\nENVIRONMENT=conda_python3\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\n\npython \/home\/ec2-user\/SageMaker\/scrub_testing.py\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n<\/code><\/pre>\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-16 06:35:58.587 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|anaconda|conda|amazon-sagemaker",
        "Question_view_count":760,
        "Owner_creation_date":"2015-09-25 10:16:37.327 UTC",
        "Owner_last_access_date":"2020-11-25 09:09:00.757 UTC",
        "Owner_location":"Dallas, TX, USA",
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker: visualizing training statistics",
        "Question_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-07 01:18:04.34 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"tensorflow|data-visualization|tensorboard|amazon-sagemaker",
        "Question_view_count":415,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-23 22:07:36.183 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-22 10:50:48.223 UTC",
        "Question_favorite_count":1.0,
        "Question_score":6,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":7404,
        "Owner_creation_date":"2019-05-22 09:45:28.153 UTC",
        "Owner_last_access_date":"2022-09-02 09:43:13.34 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-23 14:25:59.713 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":13.0,
        "Question_last_edit_date":"2019-05-22 11:21:34.857 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"S3 permission error when running sagemaker python sdk sklearn in local mode",
        "Question_body":"<p>I created a training script with hard coded input. It works as expected using a training job but I couldn't make it work using local mode.\nIt brings up a container on my local docker and exits with code (1)<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>estimator = SKLearn(entry_point=\"train_model.py\",\n                    train_instance_type=\"local\")\nestimator.fit()\n<\/code><\/pre>\n\n<p>Here is the exception:<\/p>\n\n<pre><code>2020-02-22 06:21:05,470 sagemaker-containers INFO Imported framework sagemaker_sklearn_container.training\n2020-02-22 06:21:05,480 sagemaker-containers INFO No GPUs detected (normal if no gpus installed)\n2020-02-22 06:21:05,504 sagemaker_sklearn_container.training INFO Invoking user training script.\n2020-02-22 06:21:06,407 sagemaker-containers ERROR Reporting training FAILURE\n2020-02-22 06:21:06,407 sagemaker-containers ERROR framework error:\nTraceback (most recent call last):\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 81, in train\nentrypoint()\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 36, in main\ntrain(framework.training_env())\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 32, in train\ntraining_environment.to_env_vars(), training_environment.module_name)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 301, in run_module\n_files.download_and_extract(uri, _env.code_dir)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_files.py\", line 129, in download_and_extract\ns3_download(uri, dst)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_files.py\", line 164, in s3_download\ns3.Bucket(bucket).download_file(key, dst)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nextra_args=ExtraArgs, callback=Callback)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nfuture.result()\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/s3transfer\/futures.py\", line 106, in result\nreturn self._coordinator.result()\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/s3transfer\/futures.py\", line 265, in result\nraise self._exception\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/s3transfer\/tasks.py\", line 255, in _main\nself._submit(transfer_future=transfer_future, **kwargs)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/s3transfer\/download.py\", line 345, in _submit\n**transfer_future.meta.call_args.extra_args\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\nreturn self._make_api_call(operation_name, kwargs)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\nraise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\nAn error occurred (403) when calling the HeadObject operation: Forbidden\ntmpe_msr8pi_algo-1-kt1vh_1 exited with code 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-25 13:59:56.183 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":388,
        "Owner_creation_date":"2018-01-08 12:12:56.887 UTC",
        "Owner_last_access_date":"2022-09-22 05:47:28.513 UTC",
        "Owner_location":null,
        "Owner_reputation":459,
        "Owner_up_votes":22,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-01 20:28:20.607 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"javascript|promise|async-await|amazon-sagemaker|mechanicalturk",
        "Question_view_count":63,
        "Owner_creation_date":"2017-01-03 11:49:04.907 UTC",
        "Owner_last_access_date":"2022-02-04 13:53:41.673 UTC",
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-05-01 20:43:38.457 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker won't let me use an assumed role?",
        "Question_body":"<p>I am trying to create a <code>HyperparameterTuner<\/code> job in Sagemaker. We only use assumed roles, so my ARN looks like:<\/p>\n<p><code>arn:aws:sts::123456789012:assumed-role\/ROLE_NAME\/email@company.com<\/code><\/p>\n<p>However, when I try to create a job according to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/xgboost_direct_marketing\/hpo_xgboost_direct_marketing_sagemaker_python_sdk.ipynb\" rel=\"nofollow noreferrer\">AWS's tutorial<\/a>, I get the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>(not all code included)\n&gt;&gt;&gt; tuner = HyperparameterTuner(\n    xgb, objective_metric_name, hyperparameter_ranges, max_jobs=20, max_parallel_jobs=3\n)\n&gt;&gt;&gt; tuner.fit()\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob \noperation: 1 validation error detected: Value \n'arn:aws:sts::123456789012:assumed-role\/ROLE_NAME\/email@company.com' at \n'trainingJobDefinition.roleArn' failed to satisfy constraint: Member must satisfy regular expression \npattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>The error message states that the role must have <code>iam::123:role\/<\/code>. This is problematic because assumed roles look like <code>sts::123:assumed-role\/<\/code>.<\/p>\n<p>Is there some way to format my assumed role into something the <code>HyperparameterTuner<\/code> will accept, or are assumed roles simply not compatible with AWS Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-07 15:09:42.78 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":48,
        "Owner_creation_date":"2019-03-26 19:13:06.803 UTC",
        "Owner_last_access_date":"2022-09-22 20:50:27.163 UTC",
        "Owner_location":null,
        "Owner_reputation":514,
        "Owner_up_votes":47,
        "Owner_down_votes":1,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Creating Sagemaker training job with terraform?",
        "Question_body":"<p>I am new to terraform and was looking through the documentation. From what I can tell, there's nothing in terraform with regards to create a training job that has the model artifacts. Does this mean I can't use terraform to set up the full sagemaker pipeline? It seems to me you would have to first create the training job in some way, and then you can use terraform to create a model enpoint that uses what is there (but you can't do the training job itself with terraform).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-07-02 13:20:42.21 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":938,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"AWS Sagemaker Notebook with multiple users",
        "Question_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-23 15:50:35.243 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1907,
        "Owner_creation_date":"2017-12-21 19:07:16.66 UTC",
        "Owner_last_access_date":"2022-09-23 04:14:35.81 UTC",
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-24 09:11:16.743 UTC",
        "Answer_last_edit_date":"2020-03-24 19:21:35.677 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"TesseractError running with Python3",
        "Question_body":"<p>I am running tesseract to automatically recognise text on my input image. However I get the following error<\/p>\n<pre><code>TesseractError                            Traceback (most recent call last)\n&lt;ipython-input-33-644ae6e68f14&gt; in &lt;module&gt;\n     10 img = Image.open(filename)\n     11 \n---&gt; 12 text = pytesseract.image_to_string(img) # , config=custom_config\n\n~\/SageMaker\/Image_processing\/Matching_exercise\/pytesseract.py in image_to_string(image, lang, config, nice, output_type, timeout)\n    415         Output.DICT: lambda: {'text': run_and_get_output(*args)},\n    416         Output.STRING: lambda: run_and_get_output(*args),\n--&gt; 417     }[output_type]()\n    418 \n    419 \n\n~\/SageMaker\/Image_processing\/Matching_exercise\/pytesseract.py in &lt;lambda&gt;()\n    414         Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n    415         Output.DICT: lambda: {'text': run_and_get_output(*args)},\n--&gt; 416         Output.STRING: lambda: run_and_get_output(*args),\n    417     }[output_type]()\n    418 \n\n~\/SageMaker\/Image_processing\/Matching_exercise\/pytesseract.py in run_and_get_output(image, extension, lang, config, nice, timeout, return_bytes)\n    282         }\n    283 \n--&gt; 284         run_tesseract(**kwargs)\n    285         filename = kwargs['output_filename_base'] + extsep + extension\n    286         with open(filename, 'rb') as output_file:\n\n~\/SageMaker\/Image_processing\/Matching_exercise\/pytesseract.py in run_tesseract(input_filename, output_filename_base, extension, lang, config, nice, timeout)\n    258     with timeout_manager(proc, timeout) as error_string:\n    259         if proc.returncode:\n--&gt; 260             raise TesseractError(proc.returncode, get_errors(error_string))\n    261 \n    262 \n\nTesseractError: (-11, 'TessdataManager loaded -1140457472 types of tesseract data files. TessdataManager: seek to offset 127828764786688 - start of tessdatatype 0 (config))')\n<\/code><\/pre>\n<p>Can anyone help me please, I googled it and found nothing.<\/p>\n<p>Is this error caused by the environment I am using. I am using AWS Sagemaker Jupyter-Notebook, I installed tesseract-ocr, and I called it out using the following code.<\/p>\n<pre><code>img = cv2.imread('my_image.jpg')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nfilename = &quot;{}.png&quot;.format(os.getpid())\ncv2.imwrite(filename, gray)\n\npytesseract.tesseract_cmd = '\/usr\/local\/bin\/tesseract' \n# custom_config = r'--oem 3 --psm 6'\n\nimg = Image.open(filename)\n\ntext = pytesseract.image_to_string(img)  # config=custom_config\n<\/code><\/pre>\n<p>Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-29 10:30:42.11 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-ec2|text|tesseract|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_date":"2019-05-03 02:30:17.1 UTC",
        "Owner_last_access_date":"2022-04-28 07:49:17.123 UTC",
        "Owner_location":null,
        "Owner_reputation":237,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Preprocess input data before making predictions inside Amazon SageMaker",
        "Question_body":"<p>I have a Keras\/tensorflow model that we have trained by ourselves which does image related prediction. I have followed this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">trained keras model tutorial<\/a> to deploy the model in Sagemaker and can invoke the endpoint for prediction.<\/p>\n\n<p>Now on my client side code, before making the prediction by calling the Sagemaker endpoint, I need to download the image and do some preprocessing. Instead of doing this in the client side, I want to do this entire process in SageMaker. How do I do that?<\/p>\n\n<p>It seems I need to update the entry point python code <code>train.py<\/code> as mentioned here:<\/p>\n\n<pre><code>sagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n\n<p>Other articles indicates that I need to override <code>input_fn<\/code> function to capture the preprocessing. But these <a href=\"https:\/\/stackoverflow.com\/questions\/49581156\/how-can-i-preprocess-input-data-before-making-predictions-in-sagemaker\">articles<\/a> refer to steps used if using MXNet framework. But my model is based on Keras\/tensorflow framework. <\/p>\n\n<p>So I am not sure how to override the <code>input_fn<\/code> function. Can anyone please suggest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-02-26 07:19:38.66 UTC",
        "Question_favorite_count":3.0,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":1311,
        "Owner_creation_date":"2012-06-12 04:14:13.3 UTC",
        "Owner_last_access_date":"2022-08-13 03:10:50.523 UTC",
        "Owner_location":null,
        "Owner_reputation":2448,
        "Owner_up_votes":445,
        "Owner_down_votes":5,
        "Owner_views":337,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to run sagemaker processing and training job?",
        "Question_body":"<p>I have created a processing and training job in sagemaker from console that is of left side of the panel of sagemaker, that processing job has no option to run it from console. Can someone tell how I can run that training job? Do I need a sagemaker notebook to run it? Any ideas?<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/Hm0rs.png\" alt=\"\" \/><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-10-11 16:42:42.33 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":124,
        "Owner_creation_date":"2018-04-06 02:05:12.4 UTC",
        "Owner_last_access_date":"2021-10-11 16:36:09.603 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-10-11 19:08:00.03 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Create a custom AWS Sagemaker Estimator with a configurable entrypoint",
        "Question_body":"<p>I'm writing a custom <code>Estimator<\/code> in AWS Sagemaker , for a framework that isn't supported out-of-the-box. I have my own docker image for training, with the training code bundled into the image, which forces me to rebuild the image every time the code changes. <\/p>\n\n<p>What I would like to do, is create an Estimator that uses this image, and accepts a file as an entry-point, like the built-in framework estimators do (e.g Tensorflow). <\/p>\n\n<p>From reading the source code for the Sagemaker python SDK, I found the <code>sagemaker.estimator.Framework<\/code> class, which accepts the <code>entry_point<\/code> argument, and from which the built-in frameworks estimators inherit. However, the documentation doesn't really show how to inherit from that class in my own code. <\/p>\n\n<p>Is it possible to write a custom Estimator class that inherits from <code>Framework<\/code>, or is there another way to create a custom estimator that receives an <code>entry-point<\/code> argument?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-10 14:55:10.947 UTC",
        "Question_favorite_count":1.0,
        "Question_score":5,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1663,
        "Owner_creation_date":"2014-06-27 21:42:28.753 UTC",
        "Owner_last_access_date":"2022-09-15 08:10:21.47 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":496,
        "Owner_up_votes":73,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Why is the code not able to find the file specified in the AWS S3 path, when I can find it manually?",
        "Question_body":"<p>I have a bucket called <code>my_bucket<\/code> and a folder in it called <code>Images<\/code>. I am trying to read the files (images) inside the <code>Image<\/code> folder.<\/p>\n\n<pre><code>file = pd.read_csv(some_csv_file)\nX = file.values[:,0]\n\nrole = get_execution_role()\nbucket='my_bucket'\ndata_key = 'Images'\ndata_dir = 's3:\/\/{}\/{}'.format(bucket, data_key)\ns = '\/'\n\nfor img_name in X:\n    seq = (data_dir, img_name)\n    img_path = s.join(seq)\n    img = imread(img_path)\n<\/code><\/pre>\n\n<p>But it gives the following error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-20-a273242ed30e&gt; in &lt;module&gt;()\n     43     img_path = s.join(seq)\n     44     print(img_path)\n---&gt; 45     img = imread(img_path)\n     46     img = imresize(img, (32, 32))\n     47     img = img.astype('float32') # this will help us in later stage\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/numpy\/lib\/utils.py in newfunc(*args, **kwds)\n     99             \"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\n    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n--&gt; 101             return func(*args, **kwds)\n    102 \n    103         newfunc = _set_function_name(newfunc, old_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/scipy\/misc\/pilutil.py in imread(name, flatten, mode)\n    162     \"\"\"\n    163 \n--&gt; 164     im = Image.open(name)\n    165     return fromimage(im, flatten=flatten, mode=mode)\n    166 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/PIL\/Image.py in open(fp, mode)\n   2541 \n   2542     if filename:\n-&gt; 2543         fp = builtins.open(filename, \"rb\")\n   2544         exclusive_fp = True\n   2545 \n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/my_bucket\/Images\/377.jpg'\n<\/code><\/pre>\n\n<p><code>377.jpg<\/code> is the first row in <code>X<\/code>. I checked manually in the S3 storage; this file is present there. So, why am I getting this error, and how to fix it? The only reason I can think of is, maybe the process of specifying the S3 path is wrong - but in the S3 documentation, the process to specify storage is given as <code>'s3:\/\/{}\/{}'.format(bucket, data_key)<\/code>. Moreover, in the last line of the error message, the filename is <code>s3:\/\/my_bucket\/Images\/377.jpg<\/code>, which is the path I navigate manually to locate the file in the bucket.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2018-06-08 10:01:28.657 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":3376,
        "Owner_creation_date":"2015-09-06 08:28:25.783 UTC",
        "Owner_last_access_date":"2022-09-21 06:01:13.377 UTC",
        "Owner_location":null,
        "Owner_reputation":3240,
        "Owner_up_votes":982,
        "Owner_down_votes":4,
        "Owner_views":509,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker Studio Image - pip not found and no python 3 in terminal for Python 3 notebook instance",
        "Question_body":"<p>Launched Data Science Python 3 instance in the SageMaker Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xk2gZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xk2gZ.png\" alt=\"enter image description here\" \/><\/a>\nLaunched a terminal from the Notebook menu &quot;Launch terminal in current SageMaker image&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rBVMv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rBVMv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tried to run pip but it says not found. Do I need to install pip by myself?<\/p>\n<pre><code>root@datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:~\/# pip\nbash: pip: command not found\n<\/code><\/pre>\n<p>Alto there is no Python3 available apparently from within the terminal.<\/p>\n<pre><code>root@datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:~# python3 \nbash: python3: command not found\n\nroot@datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:~# python --version\nPython 2.7.16\n\nroot@datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:\/usr\/bin# ls -lrt | grep python\nlrwxrwxrwx 1 root root        29 Mar  4  2019 pyversions -&gt; ..\/share\/python\/pyversions.py\nlrwxrwxrwx 1 root root         9 Mar  4  2019 python2 -&gt; python2.7\nlrwxrwxrwx 1 root root         7 Mar  4  2019 python -&gt; python2\n-rwxr-xr-x 1 root root      1056 Mar  4  2019 dh_python2\n-rwxr-xr-x 1 root root   3689352 Oct 10  2019 python2.7\nlrwxrwxrwx 1 root root        23 Oct 10  2019 pdb2.7 -&gt; ..\/lib\/python2.7\/pdb.py\n<\/code><\/pre>\n<p>However, the notebook says it is Python3.<\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n---\n3.7.10\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Us4D6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Us4D6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And can run pip in the cell.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bLjjq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bLjjq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Appreciate any explanation what is going on.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-01 01:01:38.097 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":505,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-01-01 02:21:12.343 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Question_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2019-09-05 19:25:33.23 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"gpu|h2o|amazon-sagemaker|automl",
        "Question_view_count":462,
        "Owner_creation_date":"2017-10-26 10:07:59.113 UTC",
        "Owner_last_access_date":"2021-09-21 19:53:06.417 UTC",
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-09-05 23:49:07.09 UTC",
        "Answer_last_edit_date":"2019-09-06 05:30:43.127 UTC",
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Question_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 09:36:23.027 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_date":"2020-11-21 06:04:32.327 UTC",
        "Owner_last_access_date":"2021-06-03 07:19:55.19 UTC",
        "Owner_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Owner_reputation":97,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 09:58:14.7 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker BlazingText Importing a FastText model and continue training",
        "Question_body":"<p>I would like to start off a pretrained FastText model such as <a href=\"https:\/\/fasttext.cc\/docs\/en\/crawl-vectors.html\" rel=\"nofollow noreferrer\">these<\/a>, and continue training on a different dataset and finally export back the trained model.<\/p>\n\n<p>Is it possible ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-10 13:18:57.3 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|fasttext",
        "Question_view_count":457,
        "Owner_creation_date":"2011-09-13 14:34:36.963 UTC",
        "Owner_last_access_date":"2022-09-04 15:49:39.16 UTC",
        "Owner_location":"Lyon, France",
        "Owner_reputation":4510,
        "Owner_up_votes":131,
        "Owner_down_votes":15,
        "Owner_views":285,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Open .Parquet Files from AWS S3 in AWS SageMaker(jupyter notebook)",
        "Question_body":"<p>When I try to open .parquet files that I have in my AWS S3 using Jupyter Notebook, it says that Jupyter cant open it and its giving me an error. I'm wondering if it is incompatible or there is a workaround to it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-22 14:57:21.553 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|jupyter-notebook|parquet|amazon-sagemaker",
        "Question_view_count":2911,
        "Owner_creation_date":"2017-07-18 02:02:41.2 UTC",
        "Owner_last_access_date":"2022-07-30 14:44:42.427 UTC",
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to defined Serializer\/Deserializer while sagemaker create_endpoint using boto3 sdk",
        "Question_body":"<p>How can I defined serializer\/deserializer while using boto3 sdk client to create an sagemaker endpoint the same as passing them while using sagemaker sdk deploy() function like following?<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\n\npredictor = estimator.deploy(\n    instance_type='ml.m4.xlarge',\n    initial_instance_count=1,\n    serializer=JSONSerializer(),\n    deserializer=JSONDeserializer(),\n)\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>    ......\n    endpoint_configs = sm_client.list_endpoint_configs(NameContains=endpoint_config_name)\n    if len(endpoint_configs['EndpointConfigs']) &gt; 0:\n        sm_client.delete_endpoint_config(EndpointConfigName = endpoint_config_name)\n    \n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n           {\n               &quot;InstanceType&quot;: &quot;ml.m4.xlarge&quot;,\n               &quot;InitialVariantWeight&quot;: 1,\n               &quot;InitialInstanceCount&quot;: 1,\n               &quot;ModelName&quot;: model_name,\n               &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n           }\n        ]\n   )\n\n   endpoints = sm_client.list_endpoints(NameContains=endpoint_name)\n\n   if len(endpoints['Endpoints']) &gt; 0:\n       create_endpoint_response = sm_client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n    else:\n        create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-12 07:44:17 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_date":"2012-11-14 02:09:44.69 UTC",
        "Owner_last_access_date":"2022-09-06 07:51:31.27 UTC",
        "Owner_location":null,
        "Owner_reputation":213,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"NVMe SSD disk on AWS Sagemaker Notebook Instances",
        "Question_body":"<p>Sagemake Studio added support for instances with attached high-performance SSDs. Among others, the M5d instance features an attached SSD. (See, for instance here <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/08\/amazon-sagemaker-supports-m5d-r5-p3dn-g4dn-instances-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">AWS Blog: Amazon SageMaker now supports M5d, R5, P3dn, and G4dn instances for SageMaker Notebook Instances<\/a>)<\/p>\n<p>My question is, how do I make use of the SSD?<\/p>\n<p>The block device is not mounted when I start a Notebook with a Kernel on an M5d instance.\nBelow is a listing of the mount command form inside the notebook. I can see it when executing <code>lsblk<\/code>. (output is also below). However, there is no way I can format or mount the disk as the environment is restricted by Linux capabilities.<\/p>\n<pre><code>!mount\n\noverlay on \/ type overlay (rw,relatime,lowerdir=\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/JGWGMF4ZZ6PHEWK6JM4VICT7E4:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/66JWDG6GVXACNLSFJJBEOVZUO4:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/3U4YS24OOOKWXXZ7GZ3N4YOJZU:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/XOM2F2QYQYJMEUDYRFWJF637G2:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/4SSLKXEKJRW2M3MDUY6GRWQZOR:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/MDSB4ZKIAUXBZ3U5S4YM4DDOLQ:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/NE42TLPPST4P26YJZXL7KQYD5V:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/ZQA5ST7XAT673MNTWDL4YWBHLL:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/UZYHAWYHUINESI25LGXT5V2RHY:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/QECAGRX3FEW5FAJXK5DACNEFKA:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/IBZBCWE3M7DNAUXEOAHQUCM5HG:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/VQMIS7KW6ZB5VOQNKQE3J2RF4J:\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/l\/UOWDHH53G2E442U6NNUMBITSRJ,upperdir=\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd\/diff,workdir=\/var\/lib\/dataroot\/data-root\/200010.1001\/overlay2\/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd\/work)\nproc on \/proc type proc (rw,nosuid,nodev,noexec,relatime)\ntmpfs on \/dev type tmpfs (rw,nosuid,size=65536k,mode=755,uid=200010,gid=1001)\ndevpts on \/dev\/pts type devpts (rw,nosuid,noexec,relatime,gid=10000004,mode=620,ptmxmode=666)\nsysfs on \/sys type sysfs (ro,nosuid,nodev,noexec,relatime)\ntmpfs on \/sys\/fs\/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755,uid=200010,gid=1001)\ncgroup on \/sys\/fs\/cgroup\/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=\/usr\/lib\/systemd\/systemd-cgroups-agent,name=systemd)\ncgroup on \/sys\/fs\/cgroup\/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on \/sys\/fs\/cgroup\/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\ncgroup on \/sys\/fs\/cgroup\/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on \/sys\/fs\/cgroup\/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\ncgroup on \/sys\/fs\/cgroup\/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on \/sys\/fs\/cgroup\/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\ncgroup on \/sys\/fs\/cgroup\/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on \/sys\/fs\/cgroup\/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on \/sys\/fs\/cgroup\/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)\ncgroup on \/sys\/fs\/cgroup\/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\nmqueue on \/dev\/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)\nshm on \/dev\/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k,uid=200010,gid=1001)\n127.0.0.1:\/200010 on \/root type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,noresvport,proto=tcp,port=20486,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)\n\/dev\/nvme0n1p1 on \/opt\/.sagemakerinternal type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/etc\/resolv.conf type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/etc\/hostname type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/etc\/hosts type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/var\/log\/studio type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/var\/log\/apps type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n\/dev\/nvme0n1p1 on \/opt\/ml\/metadata\/resource-metadata.json type xfs (ro,noatime,attr2,inode64,usrquota,prjquota)\ndevtmpfs on \/dev\/null type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/dev\/random type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/dev\/full type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/dev\/tty type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/dev\/zero type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/dev\/urandom type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\nproc on \/proc\/bus type proc (ro,nosuid,nodev,noexec,relatime)\nproc on \/proc\/fs type proc (ro,nosuid,nodev,noexec,relatime)\nproc on \/proc\/irq type proc (ro,nosuid,nodev,noexec,relatime)\nproc on \/proc\/sys type proc (ro,nosuid,nodev,noexec,relatime)\nproc on \/proc\/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)\ntmpfs on \/proc\/acpi type tmpfs (ro,relatime,uid=200010,gid=1001)\ndevtmpfs on \/proc\/kcore type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/proc\/keys type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/proc\/latency_stats type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/proc\/timer_list type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on \/proc\/sched_debug type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ntmpfs on \/sys\/firmware type tmpfs (ro,relatime,uid=200010,gid=1001)\n<\/code><\/pre>\n<pre><code>!lsblk\n\nNAME          MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nnvme1n1       259:0    0 279.4G  0 disk \nnvme0n1       259:1    0   124G  0 disk \n\u251c\u2500nvme0n1p1   259:2    0   124G  0 part \/opt\/ml\/metadata\/resource-metadata.json\n\u2514\u2500nvme0n1p128 259:3    0     1M  0 part \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-27 21:28:17.233 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_date":"2016-11-18 12:14:15.223 UTC",
        "Owner_last_access_date":"2022-09-16 21:21:44.413 UTC",
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-08-27 22:56:19.97 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"How to integrate CEPH with Amazon-S3?",
        "Question_body":"<p>I'm trying to adapt the open-source project <a href=\"https:\/\/github.com\/open-mmlab\/mmfashion\" rel=\"nofollow noreferrer\">mmfashion<\/a> on Amazon SageMaker that requires the <a href=\"https:\/\/ceph.io\/\" rel=\"nofollow noreferrer\">CEPH<\/a> module for backend. Unfortunately <code>pip install ceph<\/code> doesn't work. The only work-around was to build the <a href=\"https:\/\/github.com\/ceph\/ceph\" rel=\"nofollow noreferrer\">ceph source-code<\/a> manually by running in my container:<\/p>\n<pre><code>!git clone git:\/\/github.com\/ceph\/ceph \n!git submodule update --init --recursive\n<\/code><\/pre>\n<p>This does allow me to import <code>ceph<\/code> successfully. But it throws the following error when it comes to fecthing data from Amazon S3:<\/p>\n<pre><code>AttributeError: module 'ceph' has no attribute 'S3Client'\n<\/code><\/pre>\n<p>Has someone integrated <a href=\"https:\/\/ceph.io\/\" rel=\"nofollow noreferrer\">CEPH<\/a> with Amazon S3 Bucket or has suggestions in the same line on how to tackle this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-14 21:45:37.467 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker|ceph|cephfs|cephadm",
        "Question_view_count":197,
        "Owner_creation_date":"2013-06-26 10:56:55.017 UTC",
        "Owner_last_access_date":"2021-11-08 07:29:55.553 UTC",
        "Owner_location":"Kolkata, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Problems with Image Label Adjustment Job in Amazon Sagemaker Ground Truth",
        "Question_body":"<p>I'm trying to create a Image Label Adjustment Job in Ground Truth and I'm having some trouble. The thing is that I have a dataset of images, in which there are pre-made bounding boxes. I have an external python script that creates the &quot;dataset.manifest&quot; file with the json's of each image. Here are the first four lines of that manifest file:<\/p>\n<pre><code>{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126254-camera_2_0022.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 747, &quot;left&quot;: 840}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126259-camera_2_0028.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 1359, &quot;left&quot;: 527}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126256-camera_3_0006.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 322, &quot;left&quot;: 1154}, {&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 633, &quot;left&quot;: 968}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;3&quot;: &quot;FF&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126253-camera_2_0019.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 2, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 428, &quot;left&quot;: 1058}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;2&quot;: &quot;DD&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n<\/code><\/pre>\n<p>Now the problem is that I'm creating private jobs in Amazon Sagemaker to try it out. I have the manifest file and the images in a S3 bucket, and it actually kinda works. So I select the input manifest, activate the &quot;Existing-labels display options&quot;. The existing labels for the bounding boxes do not appear automatically, so I have to enter them manually (don't know why), but if I do that and try the preview before creating the adjustment job, the bounding boxes appear perfectly and I can adjust them. The thing is that, me being the only worker invited for the job, the job never apears to start working on it, and it just auto-completes. I can see later that the images are there with my pre-made bounding boxes, but the job never appears to adjust those boxes. I don't have the &quot;Automated data labeling&quot; option activated. Is there something missing in my manifest file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-16 16:00:22.603 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|bounding-box",
        "Question_view_count":183,
        "Owner_creation_date":"2022-01-24 17:36:34.9 UTC",
        "Owner_last_access_date":"2022-09-24 04:13:29.37 UTC",
        "Owner_location":"Chile",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-16 20:43:33.643 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-13 00:23:26.6 UTC",
        "Question_favorite_count":3.0,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-03-20 21:40:34.27 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":14.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_title":"SageMaker exception",
        "Question_body":"<p>I am trying to create a model on AWS Sagemaker by triggering it from Java.<\/p>\n<p>The steps I am following are the following,<\/p>\n<ul>\n<li>Upload the csv of the training data with the target field as the first one - WORKED<\/li>\n<li>Create a Training Job using SageMaker by using createTrainingJob API from the AWS SDK - Issue here<\/li>\n<\/ul>\n<p>I am getting the following exception while I try to create the image<\/p>\n<p><code>Cannot find the requested image: 777445444252.dkr.ecr.eu-west-1.amazonaws.com\/sagemakerimage with tag: latest and digest: null. Please check if your ECR image exists and role arn:aws:iam::777445444252:role\/sagemaker has proper pull permissions for SageMaker: ecr:BatchCheckLayerAvailability, ecr:BatchGetImage, ecr:GetDownloadUrlForLayer<\/code><\/p>\n<p>I have created role <code>sagemaker<\/code> with permissions as,\n<a href=\"https:\/\/i.stack.imgur.com\/f5T6W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f5T6W.png\" alt=\"enter image description here\" \/><\/a>\nso that it can access all repositories in ECR with full permissions.<\/p>\n<p>I have also made sure I have the repository <code>sagemakerimage<\/code> created in ECR\nBut I still get this error. I am not sure what is wrong in what I am trying to do.<\/p>\n<p>Also, I thought the image was supposed to be created?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-05-05 15:27:33.83 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"java|amazon-web-services|amazon-sagemaker",
        "Question_view_count":147,
        "Owner_creation_date":"2016-09-09 06:10:10.977 UTC",
        "Owner_last_access_date":"2022-02-28 12:37:18.717 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-05-05 16:17:33.733 UTC",
        "Question_exclusive_tag":"Amazon SageMaker"
    }
]