[
    {
        "Question_title":"How to change Sklearn flavors version in mlflow on azure machine learning?",
        "Question_body":"<p>I need to change the flavors &quot;sklearn_version&quot; in mlflow from &quot;0.22.1&quot; to &quot;1.0.0&quot; on azure machine learning when I log my trained model, since this model will be incompatible with the sklearn version that I am using for deployment during inference. I could change the version of sklearn in conda.yml file by setting &quot;conda_env&quot; in<\/p>\n<p><code>mlflow.sklearn.log_model(conda_env= 'my_env')<\/code><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/URygm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/URygm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here is the screen shot of requirements.txt\n<a href=\"https:\/\/i.stack.imgur.com\/8us2o.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8us2o.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>however, sklearn version under flavors in MLmodel file remains unchanged and that is the file that causes problem:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XfWvJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XfWvJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and here is script that I use to create this mlflow experiment in azure machine learning notebooks.<\/p>\n<pre><code>import mlflow\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.mlflow import register_model\n\n\ndef run_model(ws, experiment_name, run_name, x_train, y_train):\n    \n    # set up MLflow to track the metrics\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n    mlflow.set_experiment(experiment_name)  \n    \n    with mlflow.start_run(run_name=run_name) as run:\n        \n        # fit model\n        regression_model = DecisionTreeRegressor()\n        regression_model.fit(x_train, y_train)\n    \n        # log training score \n        training_score = regression_model.score(x_train, y_train)\n        mlflow.log_metric(&quot;Training score&quot;, training_score)\n\n        my_conda_env = {\n                    &quot;name&quot;: &quot;mlflow-env&quot;,\n                    &quot;channels&quot;: [&quot;conda-forge&quot;],\n                    &quot;dependencies&quot;: [\n                        &quot;python=3.8.5&quot;,\n                        {\n                            &quot;pip&quot;: [\n                                &quot;pip&quot;,\n                                &quot;scikit-learn~=1.0.0&quot;,\n                                &quot;uuid==1.30&quot;,\n                                &quot;lz4==4.0.0&quot;,\n                                &quot;psutil==5.9.0&quot;,\n                                &quot;cloudpickle==1.6.0&quot;,\n                                &quot;mlflow&quot;,\n                            ],\n                        },\n                    ],\n                }\n\n        \n        # register the model\n        mlflow.sklearn.log_model(regression_model, &quot;model&quot;, conda_env=my_conda_env)\n\n    model_uri = f&quot;runs:\/{run.info.run_id}\/model&quot;\n    model = mlflow.register_model(model_uri, &quot;sklearn_regression_model&quot;)\n\nif __name__ == '__main__':\n\n    # connect to your workspace\n    ws = Workspace.from_config()\n\n    # create experiment and start logging to a new run in the experiment\n    experiment_name = &quot;exp_name&quot;\n\n    # mlflow run name\n    run_name= '1234'\n\n  \n    # get train data\n    x_train, y_train  = get_train_data()\n    \n    run_model(ws, experiment_name, run_name, x_train, y_train)\n<\/code><\/pre>\n<p>Any idea how can change the flavor sklearn version in MLmodel file from <strong>&quot;0.22.1&quot;<\/strong> to <strong>&quot;1.0.0&quot;<\/strong> in my script?<\/p>\n<p>With many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-20 08:38:43.913 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|azure|scikit-learn|azure-machine-learning-service|mlflow",
        "Question_view_count":102,
        "Owner_creation_date":"2017-01-16 23:04:42.44 UTC",
        "Owner_last_access_date":"2022-09-23 06:48:12.703 UTC",
        "Owner_location":null,
        "Owner_reputation":83,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-06-20 13:14:39.31 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Tracking SageMaker Estimator with MLFlow",
        "Question_body":"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.<\/p>\n\n<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a \"log_model\" funcion in it's \"mlflow.sagemaker\" module, I tried to use the \"mlflow.pyfunc\" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?<\/p>\n\n<p>My code for now:<\/p>\n\n<p><code>mlflow.pyfunc.log_model(model)<\/code><\/p>\n\n<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is<\/p>\n\n<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel<\/code><\/p>\n\n<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-23 01:06:27.047 UTC",
        "Question_favorite_count":1.0,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":437,
        "Owner_creation_date":"2019-01-20 22:33:06.667 UTC",
        "Owner_last_access_date":"2022-09-18 13:59:17.783 UTC",
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":"<p>You cannot use pyfunc to store Any type object.<\/p>\n\n<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here \n <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#example-saving-an-xgboost-model-in-mlflow-format<\/a><\/p>\n\n<p>example with loader:<\/p>\n\n<pre><code>    model_uri = 'model.pkl'\n\n    with open(model_uri, 'wb') as f:\n        pickle.dump(model, f)\n\n    mlflow.log_artifact(model_uri, 'model')\n\n    mlflow.pyfunc.log_model(\n        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'\n    )\n<\/code><\/pre>\n\n<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.<\/p>\n\n<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.<\/p>\n\n<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=\"https:\/\/github.com\/odahu\/sagemaker-mlflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/odahu\/sagemaker-mlflow-container<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-24 09:24:49.803 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2020-04-23 01:30:39.877 UTC",
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Using pytorch_lightning.loggers.MLFlowLogger with azure machine learning studio raises exception mlflow.exceptions.RestException: BAD_REQUEST",
        "Question_body":"<p>I'm trying to locally train pytorch_lightning model and log metrics using  pytorch_lightning.loggers.MLFlowLogger.<\/p>\n<p>It was working fine until last weekend. Now training crashes with error:<\/p>\n<pre><code>mlflow.exceptions.RestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Metric once published using sync API should always use sync API to publish following metrics', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '', 'request': ''}, 'Environment': 'northeurope', 'Location': 'northeurope', 'Time': '2021-07-27T14:06:23.7035319+00:00', 'ComponentName': 'run-history', 'error_code': 'BAD_REQUEST'}\n<\/code><\/pre>\n<p>How to fix this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-27 14:52:11.183 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|mlflow|pytorch-lightning",
        "Question_view_count":176,
        "Owner_creation_date":"2021-07-06 14:42:05.933 UTC",
        "Owner_last_access_date":"2022-07-01 12:54:17.217 UTC",
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"How to run the mlflow web-based user interface from Amazon SageMaker?",
        "Question_body":"<p>I want to use the mlflow web-based user interface from a notebook on Amazon SageMaker. But the given address <a href=\"http:\/\/127.0.0.1:5000\" rel=\"noreferrer\">http:\/\/127.0.0.1:5000<\/a> doesn't seeem to work.<\/p>\n\n<p>I have installed mlflow on a SageMaker notebook.<\/p>\n\n<p>This code runs nicely:<\/p>\n\n<pre><code>import mlflow\nmlflow.start_run()\nmlflow.log_param(\"my\", \"param\")\nmlflow.log_metric(\"score\", 100)\nmlflow.end_run()\n<\/code><\/pre>\n\n<p>And then if I run <\/p>\n\n<pre><code>! mlflow ui\n<\/code><\/pre>\n\n<p>I get the expected result: <\/p>\n\n<pre><code>[2019-04-09 11:15:52 +0000] [17980] [INFO] Starting gunicorn 19.9.0\n[2019-04-09 11:15:52 +0000] [17980] [INFO] Listening at: http:\/\/127.0.0.1:5000 (17980)\n[2019-04-09 11:15:52 +0000] [17980] [INFO] Using worker: sync\n[2019-04-09 11:15:52 +0000] [17983] [INFO] Booting worker with pid: 17983\n<\/code><\/pre>\n\n<p>However, after that when going to <code>http:\/\/127.0.0.1:5000<\/code> in my browser, nothing loads. <\/p>\n\n<p>My guess it that <code>127.0.0.1<\/code> is not the right address, but how can I know which address to use instead?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-04-09 12:40:21.92 UTC",
        "Question_favorite_count":1.0,
        "Question_score":8,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":1359,
        "Owner_creation_date":"2019-04-09 11:30:39.983 UTC",
        "Owner_last_access_date":"2021-01-26 11:37:19.97 UTC",
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Run experiments on Azure ML with Kedro and Mlflow",
        "Question_body":"<p>I'm trying to run the whole Kedro pipeline as an Azure ML experiment. I had two options here. The first one was to use the built-in logging feature of Azure ML and the second one was to use the azumeml-mlflow package that integrates Azure ML with Mlflow.<\/p>\n<p>I only tried the second approach as I did not know how to implement the Run() method of Azure ML inside the Kedro hooks.<\/p>\n<p>So, for the second approach, I presumed everything should be the same as when using Mlflow only. However, I couldn't get it to work even though it worked well outside of the Kedro structure ==&gt; I could launch experiments from other scripts.<\/p>\n<p>What I get with Kedro is that the pipeline runs well but nothing happens on Azure ML.<\/p>\n<p>Here's the code (hooks are inside a ModelTrackingHooks class):<\/p>\n<pre><code>@hook_impl\ndef before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to start an MLflow run\n    with the same run_id as the Kedro pipeline run.\n    &quot;&quot;&quot;\n\n\n    # Get Azure workspace\n    ws = Workspace.get(name=&quot;...&quot;,\n                       subscription_id=&quot;...&quot;,\n                       resource_group=&quot;...&quot;)\n    # Set tracking uri\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n\n    # Create an Azure ML experiment in the workspace\n    experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')\n    mlflow.set_experiment(experiment.name)\n\n    #Start logging\n    mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n    mlflow.log_params(run_params)        \n\n@hook_impl\ndef after_node_run(\n    self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.\n    In this example, we will:\n    * Log the parameters after the data splitting node runs.\n    * Log the model after the model training node runs.\n    * Log the model's metrics after the model evaluating node runs.\n    &quot;&quot;&quot;\n    \n    if node._func_name == &quot;cross_val&quot;:\n        mlflow.log_params(\n            {&quot;best_estimator&quot;: outputs[&quot;best_estimator&quot;],\n             &quot;best_params&quot;: outputs[&quot;best_params&quot;]}\n        )\n        model = outputs[&quot;validated_model&quot;]\n        mlflow.sklearn.log_model(model, &quot;model&quot;)\n\n    elif node._func_name == &quot;fit_and_save_transformer&quot;:\n        transformer = outputs[&quot;custom_transformer&quot;]\n        mlflow.sklearn.log_model(transformer, &quot;customer_transformer&quot;)\n\n    elif node._func_name == &quot;classification_reporting&quot;:\n        mlflow.log_metrics(outputs[&quot;metrics&quot;])\n    \n\n@hook_impl\ndef after_pipeline_run(self) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to end the MLflow run\n    after the Kedro pipeline finishes.\n    &quot;&quot;&quot;\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>Am I doing it the wrong way ?<\/p>\n<p>Do you have any idea or examples on how to use Kedro and Azure ML by leveraging only the built-in capabilities of Azure ML (i.e. without going through Mlflow) ?<\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-17 19:26:27.91 UTC",
        "Question_favorite_count":3.0,
        "Question_score":0,
        "Question_tags":"python|mlflow|azure-machine-learning-service|kedro|mlops",
        "Question_view_count":266,
        "Owner_creation_date":"2020-04-10 11:23:52.39 UTC",
        "Owner_last_access_date":"2022-08-12 18:19:53.33 UTC",
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-11-17 21:20:25.437 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Register SageMaker model in MLflow",
        "Question_body":"<p>MLflow can be used to track (hyper)parameters and metrics when training machine learning models. It stores the trained model as an artifact for every experiment. These models then can be directly deployed as <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#deploy-a-python-function-model-on-amazon-sagemaker\" rel=\"nofollow noreferrer\">SageMaker endpoints<\/a>.<\/p>\n<p>Is it possible to do it the other way around, too, i.e. to register models trained in SageMaker into MLflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-23 13:34:36.023 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":133,
        "Owner_creation_date":"2013-10-28 11:07:46.783 UTC",
        "Owner_last_access_date":"2022-09-21 20:35:26.05 UTC",
        "Owner_location":"Vienna, Austria",
        "Owner_reputation":690,
        "Owner_up_votes":707,
        "Owner_down_votes":1,
        "Owner_views":174,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Running mlflow ui in AWS Sagemaker",
        "Question_body":"<p>I want to run mlflow UI in sagemaker but it simply does not work, When it outputs the http address going to it results in a &quot;this site cannot be reached&quot;<\/p>\n<p>Here is the code:<\/p>\n<pre><code>def mlflow_test(server_uri, experiment_name):\n    mlflow.set_tracking_uri(server_uri)\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run():\n        params = {\n            &quot;n-estimators&quot;: 100,\n            &quot;min-samples-leaf&quot;: 10,\n            &quot;features&quot;: 'feature_test'\n        }\n        mlflow.log_params(params)\n        mlflow.log_metric('foo', 5)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>running that code will return:<\/p>\n<pre><code>[2022-05-24 15:48:44 +0000] [27820] [INFO] Starting gunicorn 20.1.0\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Listening at: http:\/\/127.0.0.1:5000 (27820)\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Using worker: sync\n[2022-05-24 15:48:44 +0000] [27823] [INFO] Booting worker with pid: 27823\n<\/code><\/pre>\n<p>Going to the <a href=\"http:\/\/127.0.0.1:5000\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000<\/a> link won't work. Anyone know how to get mlflow ui running in sagemaker? There's not much info on this that's at an easy to understand level. I just want to log my metrics and params in sagemaker and view them using the mlflow ui<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-24 15:54:09.377 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":93,
        "Owner_creation_date":"2021-03-17 15:21:32.347 UTC",
        "Owner_last_access_date":"2022-07-14 10:53:41.117 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Does MLflow allow to log artifacts from remote locations like S3?",
        "Question_body":"<h2>My setting<\/h2>\n<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>\/opt\/ml\/model<\/code> directory, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">which is reserved by SageMaker to pack models<\/a> as a <code>.tar.gz<\/code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">Scenario 4<\/a>). Note that this is a different S3 bucket than SageMaker's.<\/p>\n<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.<\/p>\n<p>A limitation is that, as I understand it, the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">MLflow API for logging artifacts<\/a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.<\/p>\n<h2>My questions<\/h2>\n<ul>\n<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?<\/li>\n<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-12 10:49:12.913 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow|mlops",
        "Question_view_count":533,
        "Owner_creation_date":"2016-01-08 20:55:48.08 UTC",
        "Owner_last_access_date":"2022-09-23 10:18:38.03 UTC",
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":118,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"how to save mlflow metrics and paramters to an s3 bucket without a server?",
        "Question_body":"<p>I want to save the parameters and metrics gotten from mlflow into an s3 bucket. Usually I get these from setting the <code>tracking_uri<\/code> in mlflow and that saves it on a server but I can't have a server in this case(was told no) and just want to store my parameters and metrics on the s3 bucket in the same manner as it would using the <code>tracking_uri<\/code>.<\/p>\n<p>I can store the artifacts on the s3 bucket without issue but not the params\/metrics.<\/p>\n<p>Here is some code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def mlflow_testing():\n    \n    tracking_uri =  &quot;s3:\/\/bucket_name\/mlflow\/&quot;,\n    experiment_name = &quot;test&quot;,\n    artifact_uri= &quot;s3:\/\/bucket_name\/mlflow\/&quot;\n    \n    mlflow.set_tracking_uri(tracking_uri)\n    mlflow.create_experiment(experiment_name, artifact_uri)\n    mlflow.set_experiment(experiment_name)\n    \n    with mlflow.start_run() as run:\n        mlflow.log_param(&quot;test1&quot;, 0)\n        mlflow.log_metric(&quot;test2&quot;, 1)\n    \n        with open(&quot;test.txt&quot;, &quot;w&quot;) as f:\n            f.write(&quot;this is an artifact&quot;)\n    \n        mlflow.log_artifact(&quot;test.txt&quot;)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>This is capable of storing the artifact text file on the s3 bucket(so long as I make the uri a local path like <code>local_data\/mlflow<\/code> instead of the s3 bucket).<\/p>\n<p>Setting the s3 bucket for the <code>tracking_uri<\/code> results in this error:<\/p>\n<pre><code>mlflow.tracking.registry.UnsupportedModelRegistryStoreURIException:\nModel registry functionality is unavailable; got unsupported URI\n's3:\/\/bucket_location\/mlflow\/' for model registry data storage.\nSupported URI schemes are: ['', 'file', 'databricks', 'http', 'https',\n'postgresql', 'mysql', 'sqlite', 'mssql']. See\nhttps:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to\nrun an MLflow server against one of the supported backend storage\nlocations.\n<\/code><\/pre>\n<p>Does anyone have advice on getting around this without setting up a server? I just want those metrics and params.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-14 08:41:29.357 UTC",
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow",
        "Question_view_count":818,
        "Owner_creation_date":"2021-03-17 15:21:32.347 UTC",
        "Owner_last_access_date":"2022-07-14 10:53:41.117 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-14 14:14:43.397 UTC",
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Sagemaker Train Job can't connect to ec2 instance",
        "Question_body":"<p>I have MLFlow server running on ec2 instance, port 5000.<\/p>\n<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.<\/p>\n<p>ec2 instance inbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/VXwid.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VXwid.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>SageMaker outbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>These 2 security groups are in the same VPC<\/p>\n<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=\"https:\/\/stackoverflow.com\/questions\/45416882\/aws-security-group-include-another-security-group\">here<\/a>), but connection fails<\/p>\n<p>SageMaker job init:<\/p>\n<pre><code>   role = &quot;ml_sagemaker&quot;\n   security_group_ids = ['sg-04868acca16e81183']\n   bucket = sagemaker_session.default_bucket()  \n   out_path = f&quot;s3:\/\/{bucket}\/{project_name}&quot;\n\n   estimator = PyTorch(entry_point='run_train.py',\n                       source_dir='.',\n                       sagemaker_session=sagemaker_session,\n                       instance_type=instance_type,\n                       instance_count=1,\n                       framework_version='1.5.0',\n                       py_version='py3',\n                       role=role,\n                       security_group_ids=security_group_ids,\n                       hyperparameters={},\n                       )\n   ....\n\n<\/code><\/pre>\n<p>Inside <code>run_train.py<\/code>:<\/p>\n<pre><code>import mlflow\ntracking_uri = &quot;http:\/\/172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP\nmlflow.set_tracking_uri(tracking_uri)\nmlflow.log_param(&quot;test_param&quot;, 3)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n<\/code><\/pre>\n<p><strong>However<\/strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here is SageMaker Notebook configurations:<\/p>\n<img src=\"https:\/\/i.stack.imgur.com\/bslu8.png\" width=\"300\" \/>\n<p>How can I connect to ec2 instance from SageMaker Training Job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-19 17:18:49.957 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow",
        "Question_view_count":608,
        "Owner_creation_date":"2015-07-28 15:46:05.74 UTC",
        "Owner_last_access_date":"2022-09-22 13:24:29.297 UTC",
        "Owner_location":null,
        "Owner_reputation":653,
        "Owner_up_votes":216,
        "Owner_down_votes":1,
        "Owner_views":76,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker",
        "Question_body":"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()<\/code>.<\/p>\n<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))<\/code> to infer input and output signatures.<\/p>\n<p>Model is deployed successfully. When trying to query the model I get <code>ModelError<\/code> (full traceback below).<\/p>\n<p>To query the model, I am using precisely the same <code>test_example<\/code> that I used for <code>infer_signature()<\/code>:<\/p>\n<p><code>test_example = [['This is the subject', 'This is the body']]<\/code><\/p>\n<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()<\/code> as that is not <code>json<\/code>-serializeable.<\/p>\n<p>To query the model I tried two different approaches:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nSAGEMAKER_REGION = 'us-west-2'\nMODEL_NAME = '...'\n\nclient = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)\n\n# Approach 1\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=json.dumps(test_example),\n                ContentType=&quot;application\/json&quot;,\n            )\n\n# Approach 2\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),\n                ContentType=&quot;application\/json; format=pandas-split&quot;,\n            )\n<\/code><\/pre>\n<p>but they result in the same error.<\/p>\n<p>Will be grateful for your suggestions.<\/p>\n<p>Thank you!<\/p>\n<p>Note: I am using Python 3 and all <strong>strings are unicode<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;\n      2                 EndpointName=MODEL_NAME,\n      3                 Body=test_example,\n----&gt; 4                 ContentType=&quot;application\/json; format=pandas-split&quot;,\n      5             )\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    394             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 395             return self._make_api_call(operation_name, kwargs)\n    396 \n    397         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    724             error_class = self.exceptions.from_code(error_code)\n--&gt; 725             raise error_class(parsed_response, operation_name)\n    726         else:\n    727             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\n<\/code><\/pre>\n<p>Environment info:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],\n 'dependencies': ['python=3.6.10',\n  'pip==21.3.1',\n  'pytorch=1.10.2',\n  'cudatoolkit=10.2',\n  {'pip': ['mlflow==1.22.0',\n    'transformers==4.17.0',\n    'datasets==1.18.4',\n    'cloudpickle==1.3.0']}],\n 'name': 'bert_bec_test_env'}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-09 11:56:29.87 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|nlp|amazon-sagemaker|mlflow",
        "Question_view_count":61,
        "Owner_creation_date":"2017-03-23 13:26:01.927 UTC",
        "Owner_last_access_date":"2022-09-22 20:27:37.72 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":83,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-03-09 14:44:47.963 UTC",
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"MlflowException: API request to ...URL... failed to return code 200 after 3 tries",
        "Question_body":"<p>I am currently trying to track my machine learning model metrics using the MLFlow API in Azure Databricks.<\/p>\n\n<p>I registered the experiment under my team's machine learning workspace and had tried a few metric log commands that worked but were simply used as a test.<\/p>\n\n<p>My notebook ran a for loop logging metrics per calculation within the loop.\nIt took a while (3-5 seconds) before sending out the error.<\/p>\n\n<p>I tried to look at the experiment metrics and it seems to have logged a bit of the for loop's metrics before crashing.<\/p>\n\n<p>Not sure as to why it does it and now it throws the exception to my earlier test calls to log metrics.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-06 20:25:38.017 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"azure-databricks|mlflow|azure-machine-learning-service",
        "Question_view_count":696,
        "Owner_creation_date":"2019-10-19 18:12:14.813 UTC",
        "Owner_last_access_date":"2020-08-17 16:11:58.507 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 08:38:22.897 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-07-28 15:35:17.217 UTC",
        "Owner_last_access_date":"2022-03-28 15:52:24.487 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-14 22:27:30.467 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"How to have my customized score file deployed on Azure with azure.mlflow sdk?",
        "Question_body":"<p>I have a customized score.py file which was generated within databricks but I didn't find a way to deploy it on a container.<\/p>\n\n<p>I am using the mlflow.azureml, on my image creation I couldn't find how to specify the score.py in particular.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.azureml\n\nmodel_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \n                                                      workspace=workspace,\n                                                      model_name=\"my_model\",\n                                                      image_name=\"image_name\",\n                                                      description=\"Predicts\",\n                                                      synchronous=False)\n<\/code><\/pre>\n\n<p>Is there a way to specify the score.py using the lib?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-22 15:10:44.15 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|mlflow",
        "Question_view_count":226,
        "Owner_creation_date":"2014-04-03 02:59:11.19 UTC",
        "Owner_last_access_date":"2021-03-13 21:24:17.03 UTC",
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"sagemaker endpoint invocation randomly throws error",
        "Question_body":"<p>Env:<\/p>\n\n<ul>\n<li>XGBoost model trained on static data (excel).<\/li>\n<li>Model Saved and deployed to Sagemaker with MLFlow.<\/li>\n<li>Sagemaker Model and Sagemaker Endpoint are running.<\/li>\n<\/ul>\n\n<p>Invocation:\n - I invoke the model with new data via REST Request (POSTMAN) and via boto3.sagemaker<\/p>\n\n<pre><code>client.invoke_endpoint(\n    EndpointName=\"xxxxxxx\",\n    Body=data,\n    ContentType=\"application\/json\",\n    Accept=\"string\",\n)\n<\/code><\/pre>\n\n<p>Problem:\nIt seems that Sagemaker randomly (~50% of the time) fails with following Exception:<\/p>\n\n<pre><code>{\n\"ErrorCode\": \"CLIENT_ERROR_FROM_MODEL\",\n\"LogStreamArn\": \"arn:aws:logs:eu-central-1:xxxxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/xxxxxxxx\",\n\"Message\": \"Received client error (400) from mfs-xxxxxxxxx-model-dztwabjotscyoc-zx7lzyfg with message \\\"{\\\"error_code\\\": \\\"BAD_REQUEST\\\", \\\"message\\\": \\\"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\\\", \\\"stack_trace\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\\\\\\\", line 196, in transformation\\\\n    raw_predictions = model.predict(data)\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/mlflow\/xgboost.py\\\\\\\", line 198, in predict\\\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/xgboost\/core.py\\\\\\\", line 1443, in predict\\\\n    self._validate_features(data)\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/xgboost\/core.py\\\\\\\", line 1862, in _validate_features\\\\n    data.feature_names))\\\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\\\ntraining data did not have the following fields: 83, 85, 84, 82\\\\n\\\"}\\\". See https:\/\/eu-central-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-central-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/xxxxxxxxx in account xxxxxxxfor more information.\",\n\"OriginalMessage\": \"{\\\"error_code\\\": \\\"BAD_REQUEST\\\", \\\"message\\\": \\\"Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\\\", \\\"stack_trace\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\\\\\\\", line 196, in transformation\\\\n    raw_predictions = model.predict(data)\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/mlflow\/xgboost.py\\\\\\\", line 198, in predict\\\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/xgboost\/core.py\\\\\\\", line 1443, in predict\\\\n    self._validate_features(data)\\\\n  File \\\\\\\"\/miniconda\/envs\/custom_env\/lib\/python3.6\/site-packages\/xgboost\/core.py\\\\\\\", line 1862, in _validate_features\\\\n    data.feature_names))\\\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\\\ntraining data did not have the following fields: 83, 85, 84, 82\\\\n\\\"}\",\n\"OriginalStatusCode\": 400\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>After this Exception, i hit again \"Send Request\" in Postman (with the exact same Body Data) and get a successful response:<\/p>\n\n<pre><code>[\n0.9989840388298035\n]\n<\/code><\/pre>\n\n<p>The problem is exactly the same (randomly) when using boto3.sagemaker either in AWS Lambda or locally when testing.<\/p>\n\n<p><strong>UPDATE 1:<\/strong>\nWhen i load the exported function with mlflow.xgboost.load_model() and run the prediction, it works everytime. Here is the code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    import mlflow\n    from mlflow import xgboost\n    from xgboost import DMatrix\n    import numpy as np\n    import pandas as pd\n    base_path = \"src\/models\/\"\n\n\n    model_path_name_a = f\"{base_path}\/model_a\"\n    model_path_name_b = f\"{base_path}\/model_b\"\n    # xgboost.log_model(xgb_model_a, artifact_path='https:\/\/mlflow-server-tracking.s3.eu-central-1.amazonaws.com')\n    xgb_model_a = xgboost.load_model(model_path_name_a)\n    xgb_model_b = xgboost.load_model(model_path_name_b)\n\n    X_test = DMatrix(\n        np.array(\n            [\n                [7.60000e+01, 2.57000e+02, 9.25200e+03, 2.00400e+03, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]\n            ]\n        )\n    )\n\n\n    ############# Prediction Model A ################\n    y_predA = xgb_model_a.predict(X_test)\n    print(f\"y_predA: {y_predA}\")\n\n    ############# Prediction Model B ################\n    y_predB = xgb_model_b.predict(X_test)\n    print(f\"y_predA: {y_predB}\")\n<\/code><\/pre>\n\n<p>Does that mean that the problem is with Sagemaker Inference\/Endpoint Service?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_date":"2020-04-15 09:13:53.927 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker|mlflow",
        "Question_view_count":558,
        "Owner_creation_date":"2017-02-19 13:12:04.9 UTC",
        "Owner_last_access_date":"2022-08-26 11:25:49.307 UTC",
        "Owner_location":"Nuremberg, Germany",
        "Owner_reputation":177,
        "Owner_up_votes":227,
        "Owner_down_votes":5,
        "Owner_views":82,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-04-15 11:20:45.103 UTC",
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Why I got an invalid bucket name error using dvc mlflow on macos",
        "Question_body":"<p>Could anyone tell what's the reason for error:<\/p>\n<p>botocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]<\/em>:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9-]{1,63}$&quot;<\/p>\n<p>I try to use mlflow with docker.\n.env file contains:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/127.0.0.1:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>Also tried to use:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/localhost:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>docker-compose contains:<\/p>\n<pre><code>... \n   mlflow:\n        restart: always\n        image: mlflow_server\n        container_name: mlflow_server\n        ports:\n          - &quot;5000:5000&quot;\n        networks:\n          - postgres\n          - s3\n        environment:\n          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n          - MLFLOW_S3_ENDPOINT_URL=http:\/\/nginx:9000\n        command: mlflow server --backend-store-uri postgresql:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@db\/${POSTGRES_DB} --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/ --host 0.0.0.0\n...\n<\/code><\/pre>\n<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2022-05-21 21:10:53.863 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|mlflow|mlops|dvc",
        "Question_view_count":117,
        "Owner_creation_date":"2019-12-29 15:05:47.33 UTC",
        "Owner_last_access_date":"2022-09-20 12:21:31.067 UTC",
        "Owner_location":"Saint Petersburg",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-05-24 06:16:27.987 UTC",
        "Question_valid_tags":[
            "DVC",
            "MLFlow"
        ]
    },
    {
        "Question_title":"RuntimeError: Java gateway process exited before sending its port number when Deploying Pyspark model to Azure Container Instance",
        "Question_body":"<p>I am trying to deploy a PySpark model trained in Azure Databricks with MLflow to an ACI in Azure Machine Learning.<\/p>\n<p>I am following the steps in this link:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks<\/a><\/p>\n<p>but I get this error:<\/p>\n<pre><code>SPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2022-02-21 09:29:30,269 | root | INFO | Starting up app insights client\nlogging socket was found. logging is available.\nlogging socket was found. logging is available.\n2022-02-21 09:29:30,270 | root | INFO | Starting up request id generator\n2022-02-21 09:29:30,270 | root | INFO | Starting up app insight hooks\n2022-02-21 09:29:30,270 | root | INFO | Invoking user's init function\nJAVA_HOME is not set\n2022-02-21 09:29:31,267 | root | ERROR | User's init function failed\n2022-02-21 09:29:31,268 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 191, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/execution_script.py&quot;, line 15, in init\n    model = load_model(model_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/spark.py&quot;, line 703, in _load_pyfunc\n    pyspark.sql.SparkSession.builder.config(&quot;spark.python.worker.reuse&quot;, True)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py&quot;, line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 144, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 339, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py&quot;, line 108, in launch_gateway\n    raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>from mlflow.deployments import get_deploy_client\n\n# set the tracking uri as the deployment client\nclient = get_deploy_client(mlflow.get_tracking_uri())\n\n# set the model path \nmodel_path = &quot;k_means_model&quot;\n\n    # define the model path and the name is the service name\n    # the model gets registered automatically and a name is autogenerated using the &quot;name&quot; parameter below \n    client.create_deployment(model_uri='runs:\/{}\/{}'.format(run_id, model_path), name = 'k-means-model-ml-flow')\n<\/code><\/pre>\n<p>While my model settings are:<\/p>\n<pre><code>artifact_path: k_means_model\ndatabricks_runtime: 10.3.x-cpu-ml-scala2.12\nflavors:\n  python_function:\n    data: sparkml\n    env: conda.yaml\n    loader_module: mlflow.spark\n    python_version: 3.8.10\n  spark:\n    model_data: sparkml\n    pyspark_version: 3.2.1\nmodel_uuid: 76ba9dfb01e1428ab8145a161ec3cf32\nrun_id: c0090fa9-b382-45b8-be08-d05e16f3cd62\nutc_time_created: '2022-02-21 08:47:34.967167'\n<\/code><\/pre>\n<p>Can someone help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-02-21 09:34:04.4 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"pyspark|azure-databricks|azure-machine-learning-service|mlflow",
        "Question_view_count":289,
        "Owner_creation_date":"2021-11-02 14:59:46.583 UTC",
        "Owner_last_access_date":"2022-07-12 04:13:09.513 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2022-02-21 10:39:12.347 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Issues with deploying spark and mlflow to sagemaker",
        "Question_body":"<p>My goal is to deploy a spark\/mlflow to sagemaker with the following command:<\/p>\n<pre><code>    mlflow sagemaker deploy .. \n<\/code><\/pre>\n<p>I've successfully pushed a image to EC2 with<\/p>\n<pre><code>mlflow sagemaker build-and-push-container\n<\/code><\/pre>\n<p>I encounter errors when attempting to run mlflow sagemaker deploy:<\/p>\n<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/127.0.0.1:8000\/ping&quot;, host: &quot;model.aws.local:8080&quot;\njava.io.IOException: Failed to connect to model.aws.local\/172.17.0.2:34473\n<\/code><\/pre>\n<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \nspark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          \n<\/code><\/pre>\n<p>However this outputted the following error:<\/p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org\/apache\/commons\/configuration\/Configuration\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)\n    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)\n    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)\n    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)\n    at scala.Option.getOrElse(Option.scala:189)\n    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)\n    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)\n    at scala.Option.map(Option.scala:230)\n    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)\n    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n    ... 20 more\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nInput In [7], in &lt;cell line: 3&gt;()\n      1 # Create Spark Session\n      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \n----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py:228, in SparkSession.Builder.getOrCreate(self)\n    226         sparkConf.set(key, value)\n    227     # This SparkContext may be an existing one.\n--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)\n    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230 # by all sessions.\n    231 session = SparkSession(sc)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:384, in SparkContext.getOrCreate(cls, conf)\n    382 with SparkContext._lock:\n    383     if SparkContext._active_spark_context is None:\n--&gt; 384         SparkContext(conf=conf or SparkConf())\n    385     return SparkContext._active_spark_context\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    140     raise ValueError(\n    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;\n    142         &quot; is not allowed as it is a security risk.&quot;)\n--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145 try:\n    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n    147                   conf, jsc, profiler_cls)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    329 with SparkContext._lock:\n    330     if not SparkContext._gateway:\n--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)\n    332         SparkContext._jvm = SparkContext._gateway.jvm\n    334     if instance:\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)\n    105     time.sleep(0.1)\n    107 if not os.path.isfile(conn_info_file):\n--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    110 with open(conn_info_file, &quot;rb&quot;) as info:\n    111     gateway_port = read_int(info)\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-17 22:00:01.223 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|apache-spark|deployment|amazon-sagemaker|mlflow",
        "Question_view_count":59,
        "Owner_creation_date":"2022-01-13 10:12:59.037 UTC",
        "Owner_last_access_date":"2022-09-23 16:16:27.333 UTC",
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-16 12:59:50.477 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|powerbi|mlflow|azure-machine-learning-service",
        "Question_view_count":405,
        "Owner_creation_date":"2020-09-16 12:42:46.047 UTC",
        "Owner_last_access_date":"2021-03-12 15:06:56.23 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-20 12:28:40.243 UTC",
        "Answer_last_edit_date":"2020-09-23 10:12:37.377 UTC",
        "Answer_score":0.0,
        "Question_last_edit_date":"2020-09-23 10:11:20.503 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"cmd: mlflow sagemaker build-and-push-container gives FileNotFoundError:",
        "Question_body":"<p>I don't understand what type of files I missing<\/p>\n<blockquote>\n<p>and I used this code to connect AWS ECR<\/p>\n<\/blockquote>\n<pre><code>setx AWS_ACCESS_KEY_ID AKIAIOSFODNN7EXAMPLE\nsetx AWS_SECRET_ACCESS_KEY wJalrXUtnFEMI\/K7MDENG\/bPxRfiCYEXAMPLEKEY\nsetx AWS_DEFAULT_REGION us-west-2\n<\/code><\/pre>\n<blockquote>\n<p>at mlflow artifact directory shown below and here python isn't a python.exe it's just a file name<\/p>\n<\/blockquote>\n<pre><code>(deploy_ml) D:\\****\\Python\\mlruns\\1\\2877b0a860934a179723fd11ed946589\\artifacts\\random-forest-model&gt;\n\n\n(deploy_ml) D:\\***\\Python\\mlruns\\1\\2877b0a86093***723fd11ed946589\\artifacts\\random-forest-model&gt;mlflow sagemaker  build-and-push-container\n2021\/09\/14 22:22:13 INFO mlflow.models.docker_utils: Building docker image with name mlflow-pyfunc\nFIND: Parameter format not correct\nTraceback (most recent call last):\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\s***\\Anaconda3\\envs\\deploy_ml\\Scripts\\mlflow.exe\\__main__.py&quot;, line 7, in &lt;module&gt;\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1137, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1062, in main\n    rv = self.invoke(ctx)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1668, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1668, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;c:\\users\\***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;c:\\users\\***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\click\\core.py&quot;, line 763, in invoke\n    return __callback(*args, **kwargs)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\mlflow\\sagemaker\\cli.py&quot;, line 280, in build_and_push_container\n    custom_setup_steps_hook=setup_container,\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\site-packages\\mlflow\\models\\docker_utils.py&quot;, line 114, in _build_image\n    universal_newlines=True,\n  File &quot;c:\\users\\s***\\anaconda3\\envs\\deploy_ml\\lib\\subprocess.py&quot;, line 729, in __init__\n    restore_signals, start_new_session)\n  File &quot;c:\\users\\s****\\anaconda3\\envs\\deploy_ml\\lib\\subprocess.py&quot;, line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-14 17:16:10.42 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr|mlflow",
        "Question_view_count":173,
        "Owner_creation_date":"2021-08-03 07:46:47.183 UTC",
        "Owner_last_access_date":"2021-12-03 06:36:36.86 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data",
        "Question_body":"<p>I want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in <a href=\"https:\/\/towardsdatascience.com\/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198\" rel=\"nofollow noreferrer\">this blog post<\/a>.<\/p>\n<pre><code>import mlflow.sagemaker as mfs\n\nrun_id = run_id # the model you want to deploy - this run_id was saved when we trained our model\nregion = &quot;us-east-1&quot; # region of your account\naws_id = &quot;XXXXXXXXXXX&quot; # from the aws-cli output\narn = &quot;arn:aws:iam::XXXXXXXXXXX:role\/your-role&quot;\napp_name = &quot;iris-rf-1&quot;\nmodel_uri = &quot;mlruns\/%s\/%s\/artifacts\/random-forest-model&quot; % (experiment_id,run_id) # edit this path based on your working directory\nimage_url = aws_id + &quot;.dkr.ecr.&quot; + region + &quot;.amazonaws.com\/mlflow-pyfunc:1.2.0&quot; # change to your mlflow version\n\nmfs.deploy(app_name=app_name, \n           model_uri=model_uri, \n           region_name=region, \n           mode=&quot;create&quot;,\n           execution_role_arn=arn,\n           image_url=image_url)\n<\/code><\/pre>\n<p>But I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.<\/p>\n<p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at <a href=\"https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz\" rel=\"nofollow noreferrer\">https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz<\/a>. Please ensure that the role &quot;arn:aws:iam::xxx:role\/mlflow-sagemaker-dev&quot; exists and that its trust relationship policy allows the action &quot;sts:AssumeRole&quot; for the service principal &quot;sagemaker.amazonaws.com&quot;. Also ensure that the role has &quot;s3:GetObject&quot; permissions and that the object is located in us-east-1.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-28 15:47:29.71 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":1789,
        "Owner_creation_date":"2016-12-22 14:48:27.07 UTC",
        "Owner_last_access_date":"2021-05-11 00:09:27.51 UTC",
        "Owner_location":"Raleigh, NC, United States",
        "Owner_reputation":69,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"What is the difference between deploying models in MLflow and Sagemaker?",
        "Question_body":"<p>I could do\n<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda<\/code><\/p>\n<p>and<\/p>\n<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234<\/code><\/p>\n<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?<\/p>\n<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-22 00:30:20.887 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"rest|deployment|amazon-sagemaker|endpoint|mlflow",
        "Question_view_count":26,
        "Owner_creation_date":"2017-02-23 06:31:44.143 UTC",
        "Owner_last_access_date":"2022-09-24 07:48:55.5 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-26 12:33:35.98 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_date":"2020-02-19 08:37:57.803 UTC",
        "Owner_last_access_date":"2022-09-23 17:24:33.503 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-27 12:38:02.123 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":"2022-08-27 18:36:19.893 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Use an Azure ML compute cluster to run Kedro + Mlflow pipeline",
        "Question_body":"<p>I want to use an Azure Machine Learning compute cluster as a compute target to run a Kedro pipeline integrated with Mlflow.<\/p>\n<p>Here's the code snippet (hooks.py) that integrates experiment tracking using Mlflow and Azure ML as backend\/artifact stores.<\/p>\n<pre><code>&quot;&quot;&quot;Project hooks.&quot;&quot;&quot;\nfrom typing import Any, Dict, Iterable, Optional\nimport git\nimport os\nimport mlflow\nimport mlflow.sklearn\nfrom kedro.config import ConfigLoader\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\nfrom kedro.pipeline.node import Node\nfrom kedro.versioning import Journal\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\n\nclass ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self,\n        conf_paths: Iterable[str],\n        env: str,\n        extra_params: Dict[str, Any],\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n\n\nclass ModelTrackingHooks:\n    &quot;&quot;&quot;Namespace for grouping all model-tracking hooks with MLflow together.&quot;&quot;&quot;\n\n    @hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n\n        # Get Azure workspace\n        ws = Workspace.get(name=workspace_name,\n                           subscription_id=subscription_id,\n                           resource_group=resource_group)\n\n        # Set tracking uri\n        mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n\n        # Create an Azure ML experiment in the workspace\n        experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')\n        mlflow.set_experiment(experiment.name)\n\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n    ) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.\n        In this example, we will:\n        * Log the parameters after the data splitting node runs.\n        * Log the model after the model training node runs.\n        * Log the model's metrics after the model evaluating node runs.\n        &quot;&quot;&quot;\n        if node._func_name == &quot;function_name&quot;:\n            mlflow.log_metrics(...)\n\n    @hook_impl\n    def after_pipeline_run(self) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to end the MLflow run\n        after the Kedro pipeline finishes.\n        &quot;&quot;&quot;\n        mlflow.end_run()\n<\/code><\/pre>\n<p>This works well on a <strong>compute instance<\/strong> that I created in my Azure ML workspace, simply by doing the following :<\/p>\n<ol>\n<li><code>git clone<\/code> the source code into the Azure ML compute instance<\/li>\n<li>Do a <code>kedro run<\/code> in the compute instance Terminal<\/li>\n<\/ol>\n<p>That's ok but what I really want is to use <strong>compute clusters<\/strong> to deal with hyperparameter tuning and other heavy workloads... I Just want to mention here that I still want to git clone to the compute instance and submit the run to the compute cluster from within the compute instance (but if anyone has a better approach, please feel free to share).<\/p>\n<p>I know of two ways (listed below) to specify a compute cluster as a compute target in Azure ML but both require to pass a <code>script<\/code> parameter.<\/p>\n<ol>\n<li>Pure Azure ML <code>ScriptRunConfig()<\/code> method to submit experiments by specifying <code>script<\/code> and <code>compute_target<\/code> parameters. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Submit remote run with Azure Ml<\/a><\/li>\n<li>Mlflow integration with Azure ML : that requires to add an MLproject file to the project folder. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-mlflow-projects\" rel=\"nofollow noreferrer\">Submit an mlflow project run<\/a>.<\/li>\n<\/ol>\n<p>I tried for quite some time now to figure out how to do that within the Kedro structure but without success. So my question here, what's the best way to push experiment runs in a Kedro Pipeline to Azure ML compute clusters?<\/p>\n<p>Thank you in advance for your help !<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2021-12-13 17:56:35.97 UTC",
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|azure|mlflow|azure-machine-learning-service|kedro",
        "Question_view_count":271,
        "Owner_creation_date":"2020-04-10 11:23:52.39 UTC",
        "Owner_last_access_date":"2022-08-12 18:19:53.33 UTC",
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2021-12-14 08:40:44.52 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Using MLflow and Sagemaker with preprocessing steps",
        "Question_body":"<p>I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html\" rel=\"nofollow noreferrer\">mlflow_pyfunc<\/a> is closer to what I want, but I'm not sure if it is compatible with Sagemaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-20 11:52:22.35 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker|mlflow",
        "Question_view_count":34,
        "Owner_creation_date":"2016-10-17 11:39:35.777 UTC",
        "Owner_last_access_date":"2022-07-28 17:26:21.417 UTC",
        "Owner_location":null,
        "Owner_reputation":109,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"How to log metrics to Azure ML Metrics Tab",
        "Question_body":"<p>I have the following train.py file<\/p>\n<pre><code>import argparse\nimport os\nimport numpy as np\nimport glob\n# import joblib\nimport mlflow\nimport logging\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.dataset import Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\n\n# let user feed in 2 parameters, the dataset to mount or download,\n# and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;\n)\nargs = parser.parse_args()\n\ntablename = args.tablename\n\n\nsubscription_id = ''\nresource_group = 'mlplayground'\nworkspace_name = 'mlplayground'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name=tablename)\ndata = dataset.to_pandas_dataframe()\n\n# use mlflow autologging\nmlflow.autolog()\n\ndata.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)\none_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) \none_hot_city = pd.get_dummies(data.city_name, prefix='city')\n\n#removing categorical features \ndata.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  \n\n#Merging one hot encoded features with our dataset 'data' \ndata=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) \n\ndata['pricepersqm'] = data.price \/ data.house_area\n\nx=data.drop('price',axis=1) \ny=data.price \n\nX_df = DataFrame(x, columns= data.columns)\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)\n\n#Converting the data into proper LGB Dataset Format\nd_train=lgb.Dataset(X_train, label=y_train)\n\n\n#Declaring the parameters\nparams = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 10,\n    'learning_rate': 0.01,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}\n\nprint(&quot;Train a LightGBM Regression model&quot;)\nclf=lgb.train(params,d_train,1000)\n\n#model prediction on X_test\nprint(&quot;Predict the test set&quot;)\ny_pred=clf.predict(X_test)\n\n#using RMSE error metric\nmse =mean_squared_error(y_pred,y_test)\nprint(&quot;RMSE: &quot;, mse**0.5)\nmlflow.log_metric(&quot;RMSE&quot;, mse**0.5)\n<\/code><\/pre>\n<p>And then from a notebook file I use the following:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core import Experiment\n\n# connect to your workspace\nws = Workspace.from_config()\n\nexperiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;\nexp = Experiment(workspace=ws, name=experiment_name)\n\n\n\nfrom azureml.core.environment import Environment\n\n# use a curated environment that has already been built for you\n\nenv = Environment.get(workspace=ws, \n                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, \n                      version=1)\n\nfrom azureml.core import ScriptRunConfig\n\nargs = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;&quot;,\n    script=&quot;train.py&quot;,\n    arguments=args,\n    compute_target=&quot;local&quot;,\n    environment=env,\n)\n\nrun = exp.submit(config=src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.<\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-17 09:44:50.553 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|mlflow",
        "Question_view_count":40,
        "Owner_creation_date":"2011-04-05 19:05:03.093 UTC",
        "Owner_last_access_date":"2022-09-16 12:42:27.473 UTC",
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"MLFlow creates a new experiment run when logging manually along with autolog",
        "Question_body":"<p>I am using MLFlow to log metrics and artefacts in the AzureML workspace. With <code>autolog<\/code>, tensorflow training metrics are available in the experiment run in the AzureML workspace. Along with auto-logging of metrics - I want to log extra metrics and plots in the same experiment run. Doing it with MLFlow - it is creating a new experiment run.<\/p>\n<p><strong>Auto logging:<\/strong><\/p>\n<p><code>mlflow.autolog()<\/code><\/p>\n<p><strong>Manual logging:<\/strong><\/p>\n<p><code>mlflow.log_metric(f&quot;label-A&quot;, random.randint(80, 90))<\/code><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DC8S4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DC8S4.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Expected:<\/strong>\nManually logged metrics are available in the same experiment run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-09 12:19:22.673 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"metrics|mlflow|azure-machine-learning-service",
        "Question_view_count":316,
        "Owner_creation_date":"2015-02-16 15:05:31.103 UTC",
        "Owner_last_access_date":"2022-09-11 17:21:50.487 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":3982,
        "Owner_up_votes":171,
        "Owner_down_votes":17,
        "Owner_views":577,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-06 13:22:27.343 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"azure|mlflow|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":73,
        "Owner_creation_date":"2019-04-05 20:51:24.963 UTC",
        "Owner_last_access_date":"2022-09-24 07:41:48.093 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":438,
        "Owner_up_votes":15,
        "Owner_down_votes":2,
        "Owner_views":120,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 05:29:14.31 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    },
    {
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-07-04 06:12:59.043 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_date":"2019-07-04 06:06:43.123 UTC",
        "Owner_last_access_date":"2019-07-18 00:21:59.09 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "DVC",
            "Pachyderm"
        ]
    },
    {
        "Question_title":"Serving models from mlflow registry to sagemaker",
        "Question_body":"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local<\/code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:<\/p>\n<pre><code>export MODEL_PATH=models:\/churn-lgb-test\/2\nexport LOCAL_PORT=8000\nmlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine\/mlflow-pyfunc:1.6.0\n<\/code><\/pre>\n<p>it starts the container and I immediately get this error:<\/p>\n<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)\n[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)\n[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)\n[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 209, in run\n    self.sleep()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 357, in sleep\n    ready = select.select([self.PIPE[0]], [], [], 1.0)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n    sys.exit(run())\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n    super().run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n    Arbiter(self).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n    self.stop()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n    time.sleep(0.1)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\ncreating and activating custom environment\nGot sigterm signal, exiting.\n[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)\n[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)\n[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-27 13:26:19.06 UTC",
        "Question_favorite_count":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mlflow",
        "Question_view_count":429,
        "Owner_creation_date":"2020-06-05 19:27:43.857 UTC",
        "Owner_last_access_date":"2020-11-19 23:45:52.31 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":null,
        "Question_valid_tags":[
            "Amazon SageMaker",
            "MLFlow"
        ]
    },
    {
        "Question_title":"Registering models from Databricks to Azure ML and save Azure ML image into provided ACR(Non Default ACR of AML Workspace)",
        "Question_body":"<p>I'm trying to register a data bricks model tp <code>Azure ML<\/code> workspace with <code>mlflow.azure.base_image<\/code> model. But with this method, we can save the <code>Azure ML<\/code> image to default <code>ACR<\/code> connected to the <code>Azure ML<\/code> workspace.<\/p>\n<p>But I want to save the <code>Azure ML<\/code> image to another existing <code>ACR<\/code>. Need help in figuring out the design.<\/p>\n<p>The method I'm using is as follows<\/p>\n<pre><code>    workspace = Workspace.create(name = workspace_name,\n                                 location = workspace_location,\n                                 resource_group = resource_group,\n                                 subscription_id = subscription_id,\n                                 auth=svc_pr,\n                                 exist_ok=True)\n\n    import mlflow.azureml\n\n    model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \n                                                          workspace=workspace,\n                                                          model_name=&quot;winequality&quot;,\n                                                          image_name=&quot;winequality&quot;,\n                                                          description=&quot;Sklearn ElasticNet image for predicting wine quality&quot;,\n                                                          synchronous=True)\n\n    #model_image.wait_for_creation(show_output=True)\n    print(&quot;Access the following URI for build logs: {}&quot;.format(model_image.image_build_log_uri))                                    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-22 11:02:29.39 UTC",
        "Question_favorite_count":null,
        "Question_score":0,
        "Question_tags":"docker|azure-databricks|mlflow|azure-machine-learning-service",
        "Question_view_count":158,
        "Owner_creation_date":"2018-10-30 10:19:53.043 UTC",
        "Owner_last_access_date":"2022-09-22 15:34:05.27 UTC",
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_last_edit_date":null,
        "Answer_score":null,
        "Question_last_edit_date":"2020-10-22 11:10:53.553 UTC",
        "Question_valid_tags":[
            "Azure Machine Learning",
            "MLFlow"
        ]
    }
]